<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="eng">
<DOC id="L0C049DSG" lang="eng" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="1986" raw_text_md5="1c9e9781bed6be75f4032b15d68080df">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="82">
<ORIGINAL_TEXT>Giuliani says Dr. Fauci funded Wuhan virology lab in spite of official prohibition</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="8">Giuliani</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="10" end_char="13">says</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="15" end_char="16">Dr</TOKEN>
<TOKEN id="token-0-3" pos="punct" morph="none" start_char="17" end_char="17">.</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="19" end_char="23">Fauci</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="25" end_char="30">funded</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="32" end_char="36">Wuhan</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="38" end_char="45">virology</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="47" end_char="49">lab</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="51" end_char="52">in</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="54" end_char="58">spite</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="60" end_char="61">of</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="63" end_char="70">official</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="72" end_char="82">prohibition</TOKEN>
</SEG>
<SEG id="segment-1" start_char="86" end_char="192">
<ORIGINAL_TEXT>Donald Trump’s personal lawyer, Rudy Giuliani, revealed that he believes White House coronavirus expert Dr.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="86" end_char="91">Donald</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="93" end_char="99">Trump’s</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="101" end_char="108">personal</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="110" end_char="115">lawyer</TOKEN>
<TOKEN id="token-1-4" pos="punct" morph="none" start_char="116" end_char="116">,</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="118" end_char="121">Rudy</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="123" end_char="130">Giuliani</TOKEN>
<TOKEN id="token-1-7" pos="punct" morph="none" start_char="131" end_char="131">,</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="133" end_char="140">revealed</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="142" end_char="145">that</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="147" end_char="148">he</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="150" end_char="157">believes</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="159" end_char="163">White</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="165" end_char="169">House</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="171" end_char="181">coronavirus</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="183" end_char="188">expert</TOKEN>
<TOKEN id="token-1-16" pos="word" morph="none" start_char="190" end_char="191">Dr</TOKEN>
<TOKEN id="token-1-17" pos="punct" morph="none" start_char="192" end_char="192">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="194" end_char="263">
<ORIGINAL_TEXT>Anthony Fauci knew more about the Wuhan virology lab he is letting on.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="194" end_char="200">Anthony</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="202" end_char="206">Fauci</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="208" end_char="211">knew</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="213" end_char="216">more</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="218" end_char="222">about</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="224" end_char="226">the</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="228" end_char="232">Wuhan</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="234" end_char="241">virology</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="243" end_char="245">lab</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="247" end_char="248">he</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="250" end_char="251">is</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="253" end_char="259">letting</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="261" end_char="262">on</TOKEN>
<TOKEN id="token-2-13" pos="punct" morph="none" start_char="263" end_char="263">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="266" end_char="314">
<ORIGINAL_TEXT>Giuliani explained during a Sunday interview with</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="266" end_char="273">Giuliani</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="275" end_char="283">explained</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="285" end_char="290">during</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="292" end_char="292">a</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="294" end_char="299">Sunday</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="301" end_char="309">interview</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="311" end_char="314">with</TOKEN>
</SEG>
<SEG id="segment-4" start_char="317" end_char="335">
<ORIGINAL_TEXT>The Cats Roundtable</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="317" end_char="319">The</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="321" end_char="324">Cats</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="326" end_char="335">Roundtable</TOKEN>
</SEG>
<SEG id="segment-5" start_char="338" end_char="462">
<ORIGINAL_TEXT>that in 2014, the Obama administration had "prohibited" the allocation of federal funds to all virology labs, even in the US.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="338" end_char="341">that</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="343" end_char="344">in</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="346" end_char="349">2014</TOKEN>
<TOKEN id="token-5-3" pos="punct" morph="none" start_char="350" end_char="350">,</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="352" end_char="354">the</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="356" end_char="360">Obama</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="362" end_char="375">administration</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="377" end_char="379">had</TOKEN>
<TOKEN id="token-5-8" pos="punct" morph="none" start_char="381" end_char="381">"</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="382" end_char="391">prohibited</TOKEN>
<TOKEN id="token-5-10" pos="punct" morph="none" start_char="392" end_char="392">"</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="394" end_char="396">the</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="398" end_char="407">allocation</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="409" end_char="410">of</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="412" end_char="418">federal</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="420" end_char="424">funds</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="426" end_char="427">to</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="429" end_char="431">all</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="433" end_char="440">virology</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="442" end_char="445">labs</TOKEN>
<TOKEN id="token-5-20" pos="punct" morph="none" start_char="446" end_char="446">,</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="448" end_char="451">even</TOKEN>
<TOKEN id="token-5-22" pos="word" morph="none" start_char="453" end_char="454">in</TOKEN>
<TOKEN id="token-5-23" pos="word" morph="none" start_char="456" end_char="458">the</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="460" end_char="461">US</TOKEN>
<TOKEN id="token-5-25" pos="punct" morph="none" start_char="462" end_char="462">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="465" end_char="550">
<ORIGINAL_TEXT>"Despite that, Dr. Fauci gave $3.7 million to the Wuhan laboratory," Giuliani claimed.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="punct" morph="none" start_char="465" end_char="465">"</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="466" end_char="472">Despite</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="474" end_char="477">that</TOKEN>
<TOKEN id="token-6-3" pos="punct" morph="none" start_char="478" end_char="478">,</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="480" end_char="481">Dr</TOKEN>
<TOKEN id="token-6-5" pos="punct" morph="none" start_char="482" end_char="482">.</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="484" end_char="488">Fauci</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="490" end_char="493">gave</TOKEN>
<TOKEN id="token-6-8" pos="unknown" morph="none" start_char="495" end_char="498">$3.7</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="500" end_char="506">million</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="508" end_char="509">to</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="511" end_char="513">the</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="515" end_char="519">Wuhan</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="521" end_char="530">laboratory</TOKEN>
<TOKEN id="token-6-14" pos="punct" morph="none" start_char="531" end_char="532">,"</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="534" end_char="541">Giuliani</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="543" end_char="549">claimed</TOKEN>
<TOKEN id="token-6-17" pos="punct" morph="none" start_char="550" end_char="550">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="552" end_char="780">
<ORIGINAL_TEXT>"And then even after the State Department issued reports about how unsafe that laboratory was, and how suspicious they were in the way they were developing a virus that could be transmitted to humans, we never pulled that money."</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="punct" morph="none" start_char="552" end_char="552">"</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="553" end_char="555">And</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="557" end_char="560">then</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="562" end_char="565">even</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="567" end_char="571">after</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="573" end_char="575">the</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="577" end_char="581">State</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="583" end_char="592">Department</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="594" end_char="599">issued</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="601" end_char="607">reports</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="609" end_char="613">about</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="615" end_char="617">how</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="619" end_char="624">unsafe</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="626" end_char="629">that</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="631" end_char="640">laboratory</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="642" end_char="644">was</TOKEN>
<TOKEN id="token-7-16" pos="punct" morph="none" start_char="645" end_char="645">,</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="647" end_char="649">and</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="651" end_char="653">how</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="655" end_char="664">suspicious</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="666" end_char="669">they</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="671" end_char="674">were</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="676" end_char="677">in</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="679" end_char="681">the</TOKEN>
<TOKEN id="token-7-24" pos="word" morph="none" start_char="683" end_char="685">way</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="687" end_char="690">they</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="692" end_char="695">were</TOKEN>
<TOKEN id="token-7-27" pos="word" morph="none" start_char="697" end_char="706">developing</TOKEN>
<TOKEN id="token-7-28" pos="word" morph="none" start_char="708" end_char="708">a</TOKEN>
<TOKEN id="token-7-29" pos="word" morph="none" start_char="710" end_char="714">virus</TOKEN>
<TOKEN id="token-7-30" pos="word" morph="none" start_char="716" end_char="719">that</TOKEN>
<TOKEN id="token-7-31" pos="word" morph="none" start_char="721" end_char="725">could</TOKEN>
<TOKEN id="token-7-32" pos="word" morph="none" start_char="727" end_char="728">be</TOKEN>
<TOKEN id="token-7-33" pos="word" morph="none" start_char="730" end_char="740">transmitted</TOKEN>
<TOKEN id="token-7-34" pos="word" morph="none" start_char="742" end_char="743">to</TOKEN>
<TOKEN id="token-7-35" pos="word" morph="none" start_char="745" end_char="750">humans</TOKEN>
<TOKEN id="token-7-36" pos="punct" morph="none" start_char="751" end_char="751">,</TOKEN>
<TOKEN id="token-7-37" pos="word" morph="none" start_char="753" end_char="754">we</TOKEN>
<TOKEN id="token-7-38" pos="word" morph="none" start_char="756" end_char="760">never</TOKEN>
<TOKEN id="token-7-39" pos="word" morph="none" start_char="762" end_char="767">pulled</TOKEN>
<TOKEN id="token-7-40" pos="word" morph="none" start_char="769" end_char="772">that</TOKEN>
<TOKEN id="token-7-41" pos="word" morph="none" start_char="774" end_char="778">money</TOKEN>
<TOKEN id="token-7-42" pos="punct" morph="none" start_char="779" end_char="780">."</TOKEN>
</SEG>
<SEG id="segment-8" start_char="784" end_char="935">
<ORIGINAL_TEXT>The Daily Mail reported that the National Institutes for Health, which is headed up by Dr. Fauci, awarded a $3.7 million grant to the Wuhan lab in 2015.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="784" end_char="786">The</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="788" end_char="792">Daily</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="794" end_char="797">Mail</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="799" end_char="806">reported</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="808" end_char="811">that</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="813" end_char="815">the</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="817" end_char="824">National</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="826" end_char="835">Institutes</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="837" end_char="839">for</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="841" end_char="846">Health</TOKEN>
<TOKEN id="token-8-10" pos="punct" morph="none" start_char="847" end_char="847">,</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="849" end_char="853">which</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="855" end_char="856">is</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="858" end_char="863">headed</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="865" end_char="866">up</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="868" end_char="869">by</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="871" end_char="872">Dr</TOKEN>
<TOKEN id="token-8-17" pos="punct" morph="none" start_char="873" end_char="873">.</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="875" end_char="879">Fauci</TOKEN>
<TOKEN id="token-8-19" pos="punct" morph="none" start_char="880" end_char="880">,</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="882" end_char="888">awarded</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="890" end_char="890">a</TOKEN>
<TOKEN id="token-8-22" pos="unknown" morph="none" start_char="892" end_char="895">$3.7</TOKEN>
<TOKEN id="token-8-23" pos="word" morph="none" start_char="897" end_char="903">million</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="905" end_char="909">grant</TOKEN>
<TOKEN id="token-8-25" pos="word" morph="none" start_char="911" end_char="912">to</TOKEN>
<TOKEN id="token-8-26" pos="word" morph="none" start_char="914" end_char="916">the</TOKEN>
<TOKEN id="token-8-27" pos="word" morph="none" start_char="918" end_char="922">Wuhan</TOKEN>
<TOKEN id="token-8-28" pos="word" morph="none" start_char="924" end_char="926">lab</TOKEN>
<TOKEN id="token-8-29" pos="word" morph="none" start_char="928" end_char="929">in</TOKEN>
<TOKEN id="token-8-30" pos="word" morph="none" start_char="931" end_char="934">2015</TOKEN>
<TOKEN id="token-8-31" pos="punct" morph="none" start_char="935" end_char="935">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="938" end_char="975">
<ORIGINAL_TEXT>"So, something here is going on, John.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="punct" morph="none" start_char="938" end_char="938">"</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="939" end_char="940">So</TOKEN>
<TOKEN id="token-9-2" pos="punct" morph="none" start_char="941" end_char="941">,</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="943" end_char="951">something</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="953" end_char="956">here</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="958" end_char="959">is</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="961" end_char="965">going</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="967" end_char="968">on</TOKEN>
<TOKEN id="token-9-8" pos="punct" morph="none" start_char="969" end_char="969">,</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="971" end_char="974">John</TOKEN>
<TOKEN id="token-9-10" pos="punct" morph="none" start_char="975" end_char="975">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="977" end_char="1042">
<ORIGINAL_TEXT>I don’t want to make any accusations," the former NYC mayor added.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="977" end_char="977">I</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="979" end_char="983">don’t</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="985" end_char="988">want</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="990" end_char="991">to</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="993" end_char="996">make</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="998" end_char="1000">any</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1002" end_char="1012">accusations</TOKEN>
<TOKEN id="token-10-7" pos="punct" morph="none" start_char="1013" end_char="1014">,"</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1016" end_char="1018">the</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1020" end_char="1025">former</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1027" end_char="1029">NYC</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1031" end_char="1035">mayor</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1037" end_char="1041">added</TOKEN>
<TOKEN id="token-10-13" pos="punct" morph="none" start_char="1042" end_char="1042">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1046" end_char="1186">
<ORIGINAL_TEXT>"But there was more knowledge about what was going on in China with our scientific people than they disclosed to us when this first came out.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="punct" morph="none" start_char="1046" end_char="1046">"</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1047" end_char="1049">But</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1051" end_char="1055">there</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1057" end_char="1059">was</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1061" end_char="1064">more</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1066" end_char="1074">knowledge</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1076" end_char="1080">about</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1082" end_char="1085">what</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1087" end_char="1089">was</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1091" end_char="1095">going</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1097" end_char="1098">on</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1100" end_char="1101">in</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1103" end_char="1107">China</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1109" end_char="1112">with</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1114" end_char="1116">our</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1118" end_char="1127">scientific</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1129" end_char="1134">people</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1136" end_char="1139">than</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1141" end_char="1144">they</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1146" end_char="1154">disclosed</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1156" end_char="1157">to</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1159" end_char="1160">us</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1162" end_char="1165">when</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1167" end_char="1170">this</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1172" end_char="1176">first</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1178" end_char="1181">came</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="1183" end_char="1185">out</TOKEN>
<TOKEN id="token-11-27" pos="punct" morph="none" start_char="1186" end_char="1186">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1188" end_char="1292">
<ORIGINAL_TEXT>Just think of it: If this laboratory turns out to be the place where the virus came from, we paid for it.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1188" end_char="1191">Just</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1193" end_char="1197">think</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1199" end_char="1200">of</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1202" end_char="1203">it</TOKEN>
<TOKEN id="token-12-4" pos="punct" morph="none" start_char="1204" end_char="1204">:</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1206" end_char="1207">If</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1209" end_char="1212">this</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1214" end_char="1223">laboratory</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1225" end_char="1229">turns</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1231" end_char="1233">out</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1235" end_char="1236">to</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1238" end_char="1239">be</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1241" end_char="1243">the</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1245" end_char="1249">place</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1251" end_char="1255">where</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1257" end_char="1259">the</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1261" end_char="1265">virus</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1267" end_char="1270">came</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="1272" end_char="1275">from</TOKEN>
<TOKEN id="token-12-19" pos="punct" morph="none" start_char="1276" end_char="1276">,</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1278" end_char="1279">we</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1281" end_char="1284">paid</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1286" end_char="1288">for</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1290" end_char="1291">it</TOKEN>
<TOKEN id="token-12-24" pos="punct" morph="none" start_char="1292" end_char="1292">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1294" end_char="1339">
<ORIGINAL_TEXT>We paid for the damn virus that’s killing us."</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1294" end_char="1295">We</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1297" end_char="1300">paid</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1302" end_char="1304">for</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1306" end_char="1308">the</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1310" end_char="1313">damn</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1315" end_char="1319">virus</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1321" end_char="1326">that’s</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1328" end_char="1334">killing</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1336" end_char="1337">us</TOKEN>
<TOKEN id="token-13-9" pos="punct" morph="none" start_char="1338" end_char="1339">."</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1342" end_char="1520">
<ORIGINAL_TEXT>Reports are mounting that the virus was leaked from a virology lab located just miles from the controversial wet market in Wuhan that was originally blamed for spawning the virus.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1342" end_char="1348">Reports</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1350" end_char="1352">are</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1354" end_char="1361">mounting</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1363" end_char="1366">that</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1368" end_char="1370">the</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1372" end_char="1376">virus</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1378" end_char="1380">was</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1382" end_char="1387">leaked</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1389" end_char="1392">from</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1394" end_char="1394">a</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1396" end_char="1403">virology</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1405" end_char="1407">lab</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="1409" end_char="1415">located</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="1417" end_char="1420">just</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="1422" end_char="1426">miles</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="1428" end_char="1431">from</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="1433" end_char="1435">the</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="1437" end_char="1449">controversial</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="1451" end_char="1453">wet</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="1455" end_char="1460">market</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="1462" end_char="1463">in</TOKEN>
<TOKEN id="token-14-21" pos="word" morph="none" start_char="1465" end_char="1469">Wuhan</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="1471" end_char="1474">that</TOKEN>
<TOKEN id="token-14-23" pos="word" morph="none" start_char="1476" end_char="1478">was</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="1480" end_char="1489">originally</TOKEN>
<TOKEN id="token-14-25" pos="word" morph="none" start_char="1491" end_char="1496">blamed</TOKEN>
<TOKEN id="token-14-26" pos="word" morph="none" start_char="1498" end_char="1500">for</TOKEN>
<TOKEN id="token-14-27" pos="word" morph="none" start_char="1502" end_char="1509">spawning</TOKEN>
<TOKEN id="token-14-28" pos="word" morph="none" start_char="1511" end_char="1513">the</TOKEN>
<TOKEN id="token-14-29" pos="word" morph="none" start_char="1515" end_char="1519">virus</TOKEN>
<TOKEN id="token-14-30" pos="punct" morph="none" start_char="1520" end_char="1520">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1522" end_char="1561">
<ORIGINAL_TEXT>The matter is still under investigation.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1522" end_char="1524">The</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1526" end_char="1531">matter</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1533" end_char="1534">is</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="1536" end_char="1540">still</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1542" end_char="1546">under</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1548" end_char="1560">investigation</TOKEN>
<TOKEN id="token-15-6" pos="punct" morph="none" start_char="1561" end_char="1561">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1564" end_char="1717">
<ORIGINAL_TEXT>Giuliani pressed for a more official probe, however, remarking that "today, if I were U.S. attorney, I’d open an investigation into the Wuhan laboratory."</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1564" end_char="1571">Giuliani</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1573" end_char="1579">pressed</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1581" end_char="1583">for</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1585" end_char="1585">a</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1587" end_char="1590">more</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1592" end_char="1599">official</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="1601" end_char="1605">probe</TOKEN>
<TOKEN id="token-16-7" pos="punct" morph="none" start_char="1606" end_char="1606">,</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="1608" end_char="1614">however</TOKEN>
<TOKEN id="token-16-9" pos="punct" morph="none" start_char="1615" end_char="1615">,</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="1617" end_char="1625">remarking</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="1627" end_char="1630">that</TOKEN>
<TOKEN id="token-16-12" pos="punct" morph="none" start_char="1632" end_char="1632">"</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="1633" end_char="1637">today</TOKEN>
<TOKEN id="token-16-14" pos="punct" morph="none" start_char="1638" end_char="1638">,</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="1640" end_char="1641">if</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="1643" end_char="1643">I</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="1645" end_char="1648">were</TOKEN>
<TOKEN id="token-16-18" pos="unknown" morph="none" start_char="1650" end_char="1652">U.S</TOKEN>
<TOKEN id="token-16-19" pos="punct" morph="none" start_char="1653" end_char="1653">.</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="1655" end_char="1662">attorney</TOKEN>
<TOKEN id="token-16-21" pos="punct" morph="none" start_char="1663" end_char="1663">,</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="1665" end_char="1667">I’d</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="1669" end_char="1672">open</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="1674" end_char="1675">an</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="1677" end_char="1689">investigation</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="1691" end_char="1694">into</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="1696" end_char="1698">the</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="1700" end_char="1704">Wuhan</TOKEN>
<TOKEN id="token-16-29" pos="word" morph="none" start_char="1706" end_char="1715">laboratory</TOKEN>
<TOKEN id="token-16-30" pos="punct" morph="none" start_char="1716" end_char="1717">."</TOKEN>
</SEG>
<SEG id="segment-17" start_char="1720" end_char="1759">
<ORIGINAL_TEXT>"And I’d want to know, what did we know?</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="punct" morph="none" start_char="1720" end_char="1720">"</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="1721" end_char="1723">And</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="1725" end_char="1727">I’d</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="1729" end_char="1732">want</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="1734" end_char="1735">to</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="1737" end_char="1740">know</TOKEN>
<TOKEN id="token-17-6" pos="punct" morph="none" start_char="1741" end_char="1741">,</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="1743" end_char="1746">what</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="1748" end_char="1750">did</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="1752" end_char="1753">we</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="1755" end_char="1758">know</TOKEN>
<TOKEN id="token-17-11" pos="punct" morph="none" start_char="1759" end_char="1759">?</TOKEN>
</SEG>
<SEG id="segment-18" start_char="1761" end_char="1820">
<ORIGINAL_TEXT>How much did we know about how bad the practices were there?</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="1761" end_char="1763">How</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="1765" end_char="1768">much</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="1770" end_char="1772">did</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="1774" end_char="1775">we</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="1777" end_char="1780">know</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="1782" end_char="1786">about</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="1788" end_char="1790">how</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="1792" end_char="1794">bad</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="1796" end_char="1798">the</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="1800" end_char="1808">practices</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="1810" end_char="1813">were</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="1815" end_char="1819">there</TOKEN>
<TOKEN id="token-18-12" pos="punct" morph="none" start_char="1820" end_char="1820">?</TOKEN>
</SEG>
<SEG id="segment-19" start_char="1822" end_char="1839">
<ORIGINAL_TEXT>Who knew about it?</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="1822" end_char="1824">Who</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="1826" end_char="1829">knew</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="1831" end_char="1835">about</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="1837" end_char="1838">it</TOKEN>
<TOKEN id="token-19-4" pos="punct" morph="none" start_char="1839" end_char="1839">?</TOKEN>
</SEG>
<SEG id="segment-20" start_char="1841" end_char="1871">
<ORIGINAL_TEXT>And who sent them money anyway?</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="1841" end_char="1843">And</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="1845" end_char="1847">who</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="1849" end_char="1852">sent</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="1854" end_char="1857">them</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="1859" end_char="1863">money</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="1865" end_char="1870">anyway</TOKEN>
<TOKEN id="token-20-6" pos="punct" morph="none" start_char="1871" end_char="1871">?</TOKEN>
</SEG>
<SEG id="segment-21" start_char="1873" end_char="1982">
<ORIGINAL_TEXT>And that person would sure as heck be in front of a grand jury trying to explain to me — what are you asleep?"</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="1873" end_char="1875">And</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="1877" end_char="1880">that</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="1882" end_char="1887">person</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="1889" end_char="1893">would</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="1895" end_char="1898">sure</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="1900" end_char="1901">as</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="1903" end_char="1906">heck</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="1908" end_char="1909">be</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="1911" end_char="1912">in</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="1914" end_char="1918">front</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="1920" end_char="1921">of</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="1923" end_char="1923">a</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="1925" end_char="1929">grand</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="1931" end_char="1934">jury</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="1936" end_char="1941">trying</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="1943" end_char="1944">to</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="1946" end_char="1952">explain</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="1954" end_char="1955">to</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="1957" end_char="1958">me</TOKEN>
<TOKEN id="token-21-19" pos="punct" morph="none" start_char="1960" end_char="1960">—</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="1962" end_char="1965">what</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="1967" end_char="1969">are</TOKEN>
<TOKEN id="token-21-22" pos="word" morph="none" start_char="1971" end_char="1973">you</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="1975" end_char="1980">asleep</TOKEN>
<TOKEN id="token-21-24" pos="punct" morph="none" start_char="1981" end_char="1982">?"</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
