<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CABX" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="3197" raw_text_md5="c62144077531cf29335673626d8e2c73">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="21">
<ORIGINAL_TEXT>Coronavirus cover-up?</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="11">Coronavirus</TOKEN>
<TOKEN id="token-0-1" pos="unknown" morph="none" start_char="13" end_char="20">cover-up</TOKEN>
<TOKEN id="token-0-2" pos="punct" morph="none" start_char="21" end_char="21">?</TOKEN>
</SEG>
<SEG id="segment-1" start_char="23" end_char="89">
<ORIGINAL_TEXT>First case confirmed on Nov 17 NOT end of December, China data says</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="23" end_char="27">First</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="29" end_char="32">case</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="34" end_char="42">confirmed</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="44" end_char="45">on</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="47" end_char="49">Nov</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="51" end_char="52">17</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="54" end_char="56">NOT</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="58" end_char="60">end</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="62" end_char="63">of</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="65" end_char="72">December</TOKEN>
<TOKEN id="token-1-10" pos="punct" morph="none" start_char="73" end_char="73">,</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="75" end_char="79">China</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="81" end_char="84">data</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="86" end_char="89">says</TOKEN>
</SEG>
<SEG id="segment-2" start_char="93" end_char="378">
<ORIGINAL_TEXT>Data compiled by the authorities suggesting the outbreak got underway significantly will doubtless prove concerning to the international medical community - and Chinese health officials now scrambling to trace undocumented cases in a bid to better understand how the illness has spread.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="93" end_char="96">Data</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="98" end_char="105">compiled</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="107" end_char="108">by</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="110" end_char="112">the</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="114" end_char="124">authorities</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="126" end_char="135">suggesting</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="137" end_char="139">the</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="141" end_char="148">outbreak</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="150" end_char="152">got</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="154" end_char="161">underway</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="163" end_char="175">significantly</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="177" end_char="180">will</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="182" end_char="190">doubtless</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="192" end_char="196">prove</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="198" end_char="207">concerning</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="209" end_char="210">to</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="212" end_char="214">the</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="216" end_char="228">international</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="230" end_char="236">medical</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="238" end_char="246">community</TOKEN>
<TOKEN id="token-2-20" pos="punct" morph="none" start_char="248" end_char="248">-</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="250" end_char="252">and</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="254" end_char="260">Chinese</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="262" end_char="267">health</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="269" end_char="277">officials</TOKEN>
<TOKEN id="token-2-25" pos="word" morph="none" start_char="279" end_char="281">now</TOKEN>
<TOKEN id="token-2-26" pos="word" morph="none" start_char="283" end_char="292">scrambling</TOKEN>
<TOKEN id="token-2-27" pos="word" morph="none" start_char="294" end_char="295">to</TOKEN>
<TOKEN id="token-2-28" pos="word" morph="none" start_char="297" end_char="301">trace</TOKEN>
<TOKEN id="token-2-29" pos="word" morph="none" start_char="303" end_char="314">undocumented</TOKEN>
<TOKEN id="token-2-30" pos="word" morph="none" start_char="316" end_char="320">cases</TOKEN>
<TOKEN id="token-2-31" pos="word" morph="none" start_char="322" end_char="323">in</TOKEN>
<TOKEN id="token-2-32" pos="word" morph="none" start_char="325" end_char="325">a</TOKEN>
<TOKEN id="token-2-33" pos="word" morph="none" start_char="327" end_char="329">bid</TOKEN>
<TOKEN id="token-2-34" pos="word" morph="none" start_char="331" end_char="332">to</TOKEN>
<TOKEN id="token-2-35" pos="word" morph="none" start_char="334" end_char="339">better</TOKEN>
<TOKEN id="token-2-36" pos="word" morph="none" start_char="341" end_char="350">understand</TOKEN>
<TOKEN id="token-2-37" pos="word" morph="none" start_char="352" end_char="354">how</TOKEN>
<TOKEN id="token-2-38" pos="word" morph="none" start_char="356" end_char="358">the</TOKEN>
<TOKEN id="token-2-39" pos="word" morph="none" start_char="360" end_char="366">illness</TOKEN>
<TOKEN id="token-2-40" pos="word" morph="none" start_char="368" end_char="370">has</TOKEN>
<TOKEN id="token-2-41" pos="word" morph="none" start_char="372" end_char="377">spread</TOKEN>
<TOKEN id="token-2-42" pos="punct" morph="none" start_char="378" end_char="378">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="380" end_char="538">
<ORIGINAL_TEXT>Up to now, it was widely believed that the disease emerged at the end of November after cases associated with a seafood market in Wuhan began to be identified.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="380" end_char="381">Up</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="383" end_char="384">to</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="386" end_char="388">now</TOKEN>
<TOKEN id="token-3-3" pos="punct" morph="none" start_char="389" end_char="389">,</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="391" end_char="392">it</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="394" end_char="396">was</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="398" end_char="403">widely</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="405" end_char="412">believed</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="414" end_char="417">that</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="419" end_char="421">the</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="423" end_char="429">disease</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="431" end_char="437">emerged</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="439" end_char="440">at</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="442" end_char="444">the</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="446" end_char="448">end</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="450" end_char="451">of</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="453" end_char="460">November</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="462" end_char="466">after</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="468" end_char="472">cases</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="474" end_char="483">associated</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="485" end_char="488">with</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="490" end_char="490">a</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="492" end_char="498">seafood</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="500" end_char="505">market</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="507" end_char="508">in</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="510" end_char="514">Wuhan</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="516" end_char="520">began</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="522" end_char="523">to</TOKEN>
<TOKEN id="token-3-28" pos="word" morph="none" start_char="525" end_char="526">be</TOKEN>
<TOKEN id="token-3-29" pos="word" morph="none" start_char="528" end_char="537">identified</TOKEN>
<TOKEN id="token-3-30" pos="punct" morph="none" start_char="538" end_char="538">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="541" end_char="760">
<ORIGINAL_TEXT>The World Health Organization issued a press release on January 5, warning of a "mystery lung disease" which had first been reported on New Year's Eve in Wuhan, and which had infected 44 people, leaving 11 seriously ill.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="541" end_char="543">The</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="545" end_char="549">World</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="551" end_char="556">Health</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="558" end_char="569">Organization</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="571" end_char="576">issued</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="578" end_char="578">a</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="580" end_char="584">press</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="586" end_char="592">release</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="594" end_char="595">on</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="597" end_char="603">January</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="605" end_char="605">5</TOKEN>
<TOKEN id="token-4-11" pos="punct" morph="none" start_char="606" end_char="606">,</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="608" end_char="614">warning</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="616" end_char="617">of</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="619" end_char="619">a</TOKEN>
<TOKEN id="token-4-15" pos="punct" morph="none" start_char="621" end_char="621">"</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="622" end_char="628">mystery</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="630" end_char="633">lung</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="635" end_char="641">disease</TOKEN>
<TOKEN id="token-4-19" pos="punct" morph="none" start_char="642" end_char="642">"</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="644" end_char="648">which</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="650" end_char="652">had</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="654" end_char="658">first</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="660" end_char="663">been</TOKEN>
<TOKEN id="token-4-24" pos="word" morph="none" start_char="665" end_char="672">reported</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="674" end_char="675">on</TOKEN>
<TOKEN id="token-4-26" pos="word" morph="none" start_char="677" end_char="679">New</TOKEN>
<TOKEN id="token-4-27" pos="word" morph="none" start_char="681" end_char="686">Year's</TOKEN>
<TOKEN id="token-4-28" pos="word" morph="none" start_char="688" end_char="690">Eve</TOKEN>
<TOKEN id="token-4-29" pos="word" morph="none" start_char="692" end_char="693">in</TOKEN>
<TOKEN id="token-4-30" pos="word" morph="none" start_char="695" end_char="699">Wuhan</TOKEN>
<TOKEN id="token-4-31" pos="punct" morph="none" start_char="700" end_char="700">,</TOKEN>
<TOKEN id="token-4-32" pos="word" morph="none" start_char="702" end_char="704">and</TOKEN>
<TOKEN id="token-4-33" pos="word" morph="none" start_char="706" end_char="710">which</TOKEN>
<TOKEN id="token-4-34" pos="word" morph="none" start_char="712" end_char="714">had</TOKEN>
<TOKEN id="token-4-35" pos="word" morph="none" start_char="716" end_char="723">infected</TOKEN>
<TOKEN id="token-4-36" pos="word" morph="none" start_char="725" end_char="726">44</TOKEN>
<TOKEN id="token-4-37" pos="word" morph="none" start_char="728" end_char="733">people</TOKEN>
<TOKEN id="token-4-38" pos="punct" morph="none" start_char="734" end_char="734">,</TOKEN>
<TOKEN id="token-4-39" pos="word" morph="none" start_char="736" end_char="742">leaving</TOKEN>
<TOKEN id="token-4-40" pos="word" morph="none" start_char="744" end_char="745">11</TOKEN>
<TOKEN id="token-4-41" pos="word" morph="none" start_char="747" end_char="755">seriously</TOKEN>
<TOKEN id="token-4-42" pos="word" morph="none" start_char="757" end_char="759">ill</TOKEN>
<TOKEN id="token-4-43" pos="punct" morph="none" start_char="760" end_char="760">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="763" end_char="830">
<ORIGINAL_TEXT>However, it now appears the disease originated considerably earlier.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="763" end_char="769">However</TOKEN>
<TOKEN id="token-5-1" pos="punct" morph="none" start_char="770" end_char="770">,</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="772" end_char="773">it</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="775" end_char="777">now</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="779" end_char="785">appears</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="787" end_char="789">the</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="791" end_char="797">disease</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="799" end_char="808">originated</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="810" end_char="821">considerably</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="823" end_char="829">earlier</TOKEN>
<TOKEN id="token-5-10" pos="punct" morph="none" start_char="830" end_char="830">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="833" end_char="996">
<ORIGINAL_TEXT>Data gathered by the Chinese government indicates at least 266 people who were infected last year, all of whom were placed under medical surveillance at some stage.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="833" end_char="836">Data</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="838" end_char="845">gathered</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="847" end_char="848">by</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="850" end_char="852">the</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="854" end_char="860">Chinese</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="862" end_char="871">government</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="873" end_char="881">indicates</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="883" end_char="884">at</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="886" end_char="890">least</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="892" end_char="894">266</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="896" end_char="901">people</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="903" end_char="905">who</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="907" end_char="910">were</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="912" end_char="919">infected</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="921" end_char="924">last</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="926" end_char="929">year</TOKEN>
<TOKEN id="token-6-16" pos="punct" morph="none" start_char="930" end_char="930">,</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="932" end_char="934">all</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="936" end_char="937">of</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="939" end_char="942">whom</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="944" end_char="947">were</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="949" end_char="954">placed</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="956" end_char="960">under</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="962" end_char="968">medical</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="970" end_char="981">surveillance</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="983" end_char="984">at</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="986" end_char="989">some</TOKEN>
<TOKEN id="token-6-27" pos="word" morph="none" start_char="991" end_char="995">stage</TOKEN>
<TOKEN id="token-6-28" pos="punct" morph="none" start_char="996" end_char="996">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="999" end_char="1135">
<ORIGINAL_TEXT>A proportion of the figure has probably been backdated as a result of health authorities testing specimens taken from suspected patients.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="999" end_char="999">A</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="1001" end_char="1010">proportion</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="1012" end_char="1013">of</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="1015" end_char="1017">the</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="1019" end_char="1024">figure</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="1026" end_char="1028">has</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="1030" end_char="1037">probably</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="1039" end_char="1042">been</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="1044" end_char="1052">backdated</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="1054" end_char="1055">as</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="1057" end_char="1057">a</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="1059" end_char="1064">result</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="1066" end_char="1067">of</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="1069" end_char="1074">health</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="1076" end_char="1086">authorities</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="1088" end_char="1094">testing</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="1096" end_char="1104">specimens</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="1106" end_char="1110">taken</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="1112" end_char="1115">from</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="1117" end_char="1125">suspected</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="1127" end_char="1134">patients</TOKEN>
<TOKEN id="token-7-21" pos="punct" morph="none" start_char="1135" end_char="1135">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1138" end_char="1244">
<ORIGINAL_TEXT>The data suggests the first person to have contracted Covid-19 may have been a 55-year-old, on November 17.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="1138" end_char="1140">The</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="1142" end_char="1145">data</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="1147" end_char="1154">suggests</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1156" end_char="1158">the</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1160" end_char="1164">first</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1166" end_char="1171">person</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="1173" end_char="1174">to</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="1176" end_char="1179">have</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1181" end_char="1190">contracted</TOKEN>
<TOKEN id="token-8-9" pos="unknown" morph="none" start_char="1192" end_char="1199">Covid-19</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="1201" end_char="1203">may</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="1205" end_char="1208">have</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="1210" end_char="1213">been</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="1215" end_char="1215">a</TOKEN>
<TOKEN id="token-8-14" pos="unknown" morph="none" start_char="1217" end_char="1227">55-year-old</TOKEN>
<TOKEN id="token-8-15" pos="punct" morph="none" start_char="1228" end_char="1228">,</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="1230" end_char="1231">on</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="1233" end_char="1240">November</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="1242" end_char="1243">17</TOKEN>
<TOKEN id="token-8-19" pos="punct" morph="none" start_char="1244" end_char="1244">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1247" end_char="1519">
<ORIGINAL_TEXT>Afterwards, between one and five new cases a day were reported, with the number of cases rising to 27 by December 15, and to 60 by December 20 - 11 days before the date mentioned in the WHO's press release, although the WHO's website puts the first infection to December 8.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1247" end_char="1256">Afterwards</TOKEN>
<TOKEN id="token-9-1" pos="punct" morph="none" start_char="1257" end_char="1257">,</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="1259" end_char="1265">between</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1267" end_char="1269">one</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1271" end_char="1273">and</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1275" end_char="1278">five</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1280" end_char="1282">new</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1284" end_char="1288">cases</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1290" end_char="1290">a</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="1292" end_char="1294">day</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1296" end_char="1299">were</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1301" end_char="1308">reported</TOKEN>
<TOKEN id="token-9-12" pos="punct" morph="none" start_char="1309" end_char="1309">,</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1311" end_char="1314">with</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1316" end_char="1318">the</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1320" end_char="1325">number</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="1327" end_char="1328">of</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1330" end_char="1334">cases</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1336" end_char="1341">rising</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1343" end_char="1344">to</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1346" end_char="1347">27</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1349" end_char="1350">by</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1352" end_char="1359">December</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="1361" end_char="1362">15</TOKEN>
<TOKEN id="token-9-24" pos="punct" morph="none" start_char="1363" end_char="1363">,</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="1365" end_char="1367">and</TOKEN>
<TOKEN id="token-9-26" pos="word" morph="none" start_char="1369" end_char="1370">to</TOKEN>
<TOKEN id="token-9-27" pos="word" morph="none" start_char="1372" end_char="1373">60</TOKEN>
<TOKEN id="token-9-28" pos="word" morph="none" start_char="1375" end_char="1376">by</TOKEN>
<TOKEN id="token-9-29" pos="word" morph="none" start_char="1378" end_char="1385">December</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1387" end_char="1388">20</TOKEN>
<TOKEN id="token-9-31" pos="punct" morph="none" start_char="1390" end_char="1390">-</TOKEN>
<TOKEN id="token-9-32" pos="word" morph="none" start_char="1392" end_char="1393">11</TOKEN>
<TOKEN id="token-9-33" pos="word" morph="none" start_char="1395" end_char="1398">days</TOKEN>
<TOKEN id="token-9-34" pos="word" morph="none" start_char="1400" end_char="1405">before</TOKEN>
<TOKEN id="token-9-35" pos="word" morph="none" start_char="1407" end_char="1409">the</TOKEN>
<TOKEN id="token-9-36" pos="word" morph="none" start_char="1411" end_char="1414">date</TOKEN>
<TOKEN id="token-9-37" pos="word" morph="none" start_char="1416" end_char="1424">mentioned</TOKEN>
<TOKEN id="token-9-38" pos="word" morph="none" start_char="1426" end_char="1427">in</TOKEN>
<TOKEN id="token-9-39" pos="word" morph="none" start_char="1429" end_char="1431">the</TOKEN>
<TOKEN id="token-9-40" pos="word" morph="none" start_char="1433" end_char="1437">WHO's</TOKEN>
<TOKEN id="token-9-41" pos="word" morph="none" start_char="1439" end_char="1443">press</TOKEN>
<TOKEN id="token-9-42" pos="word" morph="none" start_char="1445" end_char="1451">release</TOKEN>
<TOKEN id="token-9-43" pos="punct" morph="none" start_char="1452" end_char="1452">,</TOKEN>
<TOKEN id="token-9-44" pos="word" morph="none" start_char="1454" end_char="1461">although</TOKEN>
<TOKEN id="token-9-45" pos="word" morph="none" start_char="1463" end_char="1465">the</TOKEN>
<TOKEN id="token-9-46" pos="word" morph="none" start_char="1467" end_char="1471">WHO's</TOKEN>
<TOKEN id="token-9-47" pos="word" morph="none" start_char="1473" end_char="1479">website</TOKEN>
<TOKEN id="token-9-48" pos="word" morph="none" start_char="1481" end_char="1484">puts</TOKEN>
<TOKEN id="token-9-49" pos="word" morph="none" start_char="1486" end_char="1488">the</TOKEN>
<TOKEN id="token-9-50" pos="word" morph="none" start_char="1490" end_char="1494">first</TOKEN>
<TOKEN id="token-9-51" pos="word" morph="none" start_char="1496" end_char="1504">infection</TOKEN>
<TOKEN id="token-9-52" pos="word" morph="none" start_char="1506" end_char="1507">to</TOKEN>
<TOKEN id="token-9-53" pos="word" morph="none" start_char="1509" end_char="1516">December</TOKEN>
<TOKEN id="token-9-54" pos="word" morph="none" start_char="1518" end_char="1518">8</TOKEN>
<TOKEN id="token-9-55" pos="punct" morph="none" start_char="1519" end_char="1519">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1522" end_char="1660">
<ORIGINAL_TEXT>On the first day of 2020, the figure stood at 381 - several days before the first reports of the illness began to surface in Western media.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1522" end_char="1523">On</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1525" end_char="1527">the</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1529" end_char="1533">first</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1535" end_char="1537">day</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1539" end_char="1540">of</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1542" end_char="1545">2020</TOKEN>
<TOKEN id="token-10-6" pos="punct" morph="none" start_char="1546" end_char="1546">,</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1548" end_char="1550">the</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1552" end_char="1557">figure</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1559" end_char="1563">stood</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1565" end_char="1566">at</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1568" end_char="1570">381</TOKEN>
<TOKEN id="token-10-12" pos="punct" morph="none" start_char="1572" end_char="1572">-</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1574" end_char="1580">several</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1582" end_char="1585">days</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1587" end_char="1592">before</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1594" end_char="1596">the</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1598" end_char="1602">first</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1604" end_char="1610">reports</TOKEN>
<TOKEN id="token-10-19" pos="word" morph="none" start_char="1612" end_char="1613">of</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="1615" end_char="1617">the</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="1619" end_char="1625">illness</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="1627" end_char="1631">began</TOKEN>
<TOKEN id="token-10-23" pos="word" morph="none" start_char="1633" end_char="1634">to</TOKEN>
<TOKEN id="token-10-24" pos="word" morph="none" start_char="1636" end_char="1642">surface</TOKEN>
<TOKEN id="token-10-25" pos="word" morph="none" start_char="1644" end_char="1645">in</TOKEN>
<TOKEN id="token-10-26" pos="word" morph="none" start_char="1647" end_char="1653">Western</TOKEN>
<TOKEN id="token-10-27" pos="word" morph="none" start_char="1655" end_char="1659">media</TOKEN>
<TOKEN id="token-10-28" pos="punct" morph="none" start_char="1660" end_char="1660">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1663" end_char="1863">
<ORIGINAL_TEXT>Scientists are now urgently trying to identify patient zero, which would enable them to trace the source of the disease, which epidemiologists think jumped to humans from a wild animal, possibly a bat.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1663" end_char="1672">Scientists</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1674" end_char="1676">are</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1678" end_char="1680">now</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1682" end_char="1689">urgently</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1691" end_char="1696">trying</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1698" end_char="1699">to</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1701" end_char="1708">identify</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1710" end_char="1716">patient</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1718" end_char="1721">zero</TOKEN>
<TOKEN id="token-11-9" pos="punct" morph="none" start_char="1722" end_char="1722">,</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1724" end_char="1728">which</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1730" end_char="1734">would</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1736" end_char="1741">enable</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1743" end_char="1746">them</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1748" end_char="1749">to</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1751" end_char="1755">trace</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1757" end_char="1759">the</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1761" end_char="1766">source</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1768" end_char="1769">of</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1771" end_char="1773">the</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1775" end_char="1781">disease</TOKEN>
<TOKEN id="token-11-21" pos="punct" morph="none" start_char="1782" end_char="1782">,</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1784" end_char="1788">which</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1790" end_char="1804">epidemiologists</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1806" end_char="1810">think</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1812" end_char="1817">jumped</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="1819" end_char="1820">to</TOKEN>
<TOKEN id="token-11-27" pos="word" morph="none" start_char="1822" end_char="1827">humans</TOKEN>
<TOKEN id="token-11-28" pos="word" morph="none" start_char="1829" end_char="1832">from</TOKEN>
<TOKEN id="token-11-29" pos="word" morph="none" start_char="1834" end_char="1834">a</TOKEN>
<TOKEN id="token-11-30" pos="word" morph="none" start_char="1836" end_char="1839">wild</TOKEN>
<TOKEN id="token-11-31" pos="word" morph="none" start_char="1841" end_char="1846">animal</TOKEN>
<TOKEN id="token-11-32" pos="punct" morph="none" start_char="1847" end_char="1847">,</TOKEN>
<TOKEN id="token-11-33" pos="word" morph="none" start_char="1849" end_char="1856">possibly</TOKEN>
<TOKEN id="token-11-34" pos="word" morph="none" start_char="1858" end_char="1858">a</TOKEN>
<TOKEN id="token-11-35" pos="word" morph="none" start_char="1860" end_char="1862">bat</TOKEN>
<TOKEN id="token-11-36" pos="punct" morph="none" start_char="1863" end_char="1863">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1866" end_char="2011">
<ORIGINAL_TEXT>None of the nine cases reported in November - four men and five women, aged between 39 and 39 - has so far been confirmed as being such a patient.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1866" end_char="1869">None</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1871" end_char="1872">of</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1874" end_char="1876">the</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1878" end_char="1881">nine</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1883" end_char="1887">cases</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1889" end_char="1896">reported</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1898" end_char="1899">in</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1901" end_char="1908">November</TOKEN>
<TOKEN id="token-12-8" pos="punct" morph="none" start_char="1910" end_char="1910">-</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1912" end_char="1915">four</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1917" end_char="1919">men</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1921" end_char="1923">and</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1925" end_char="1928">five</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1930" end_char="1934">women</TOKEN>
<TOKEN id="token-12-14" pos="punct" morph="none" start_char="1935" end_char="1935">,</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1937" end_char="1940">aged</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1942" end_char="1948">between</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1950" end_char="1951">39</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="1953" end_char="1955">and</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="1957" end_char="1958">39</TOKEN>
<TOKEN id="token-12-20" pos="punct" morph="none" start_char="1960" end_char="1960">-</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1962" end_char="1964">has</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1966" end_char="1967">so</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1969" end_char="1971">far</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1973" end_char="1976">been</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1978" end_char="1986">confirmed</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="1988" end_char="1989">as</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="1991" end_char="1995">being</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="1997" end_char="2000">such</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="2002" end_char="2002">a</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="2004" end_char="2010">patient</TOKEN>
<TOKEN id="token-12-31" pos="punct" morph="none" start_char="2011" end_char="2011">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="2014" end_char="2023">
<ORIGINAL_TEXT>DON'T MISS</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="2014" end_char="2018">DON'T</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="2020" end_char="2023">MISS</TOKEN>
</SEG>
<SEG id="segment-14" start_char="2028" end_char="2076">
<ORIGINAL_TEXT>Dr Li Wenliang has been hailed as a hero in China</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="2028" end_char="2029">Dr</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="2031" end_char="2032">Li</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="2034" end_char="2041">Wenliang</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="2043" end_char="2045">has</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="2047" end_char="2050">been</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="2052" end_char="2057">hailed</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="2059" end_char="2060">as</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="2062" end_char="2062">a</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="2064" end_char="2067">hero</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="2069" end_char="2070">in</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="2072" end_char="2076">China</TOKEN>
</SEG>
<SEG id="segment-15" start_char="2080" end_char="2232">
<ORIGINAL_TEXT>A report published in medical journal The Lancet by doctors from Wuhan's Jinyintan Hospital previously put dated the first known infection to December 1.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="2080" end_char="2080">A</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="2082" end_char="2087">report</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="2089" end_char="2097">published</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="2099" end_char="2100">in</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="2102" end_char="2108">medical</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="2110" end_char="2116">journal</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="2118" end_char="2120">The</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="2122" end_char="2127">Lancet</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="2129" end_char="2130">by</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="2132" end_char="2138">doctors</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="2140" end_char="2143">from</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="2145" end_char="2151">Wuhan's</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="2153" end_char="2161">Jinyintan</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="2163" end_char="2170">Hospital</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="2172" end_char="2181">previously</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="2183" end_char="2185">put</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="2187" end_char="2191">dated</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="2193" end_char="2195">the</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="2197" end_char="2201">first</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="2203" end_char="2207">known</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="2209" end_char="2217">infection</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="2219" end_char="2220">to</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="2222" end_char="2229">December</TOKEN>
<TOKEN id="token-15-23" pos="word" morph="none" start_char="2231" end_char="2231">1</TOKEN>
<TOKEN id="token-15-24" pos="punct" morph="none" start_char="2232" end_char="2232">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="2235" end_char="2454">
<ORIGINAL_TEXT>Speaking to Chinese magazine People in an interview which was later censored, Dr Ai Fen said she was reprimanded after telling superiors about about a "Sars-like" virus-related illness affecting patients in mid-December.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="2235" end_char="2242">Speaking</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="2244" end_char="2245">to</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="2247" end_char="2253">Chinese</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="2255" end_char="2262">magazine</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="2264" end_char="2269">People</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="2271" end_char="2272">in</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="2274" end_char="2275">an</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="2277" end_char="2285">interview</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="2287" end_char="2291">which</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="2293" end_char="2295">was</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="2297" end_char="2301">later</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="2303" end_char="2310">censored</TOKEN>
<TOKEN id="token-16-12" pos="punct" morph="none" start_char="2311" end_char="2311">,</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2313" end_char="2314">Dr</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="2316" end_char="2317">Ai</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="2319" end_char="2321">Fen</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="2323" end_char="2326">said</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="2328" end_char="2330">she</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="2332" end_char="2334">was</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="2336" end_char="2346">reprimanded</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="2348" end_char="2352">after</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="2354" end_char="2360">telling</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="2362" end_char="2370">superiors</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="2372" end_char="2376">about</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="2378" end_char="2382">about</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="2384" end_char="2384">a</TOKEN>
<TOKEN id="token-16-26" pos="punct" morph="none" start_char="2386" end_char="2386">"</TOKEN>
<TOKEN id="token-16-27" pos="unknown" morph="none" start_char="2387" end_char="2395">Sars-like</TOKEN>
<TOKEN id="token-16-28" pos="punct" morph="none" start_char="2396" end_char="2396">"</TOKEN>
<TOKEN id="token-16-29" pos="unknown" morph="none" start_char="2398" end_char="2410">virus-related</TOKEN>
<TOKEN id="token-16-30" pos="word" morph="none" start_char="2412" end_char="2418">illness</TOKEN>
<TOKEN id="token-16-31" pos="word" morph="none" start_char="2420" end_char="2428">affecting</TOKEN>
<TOKEN id="token-16-32" pos="word" morph="none" start_char="2430" end_char="2437">patients</TOKEN>
<TOKEN id="token-16-33" pos="word" morph="none" start_char="2439" end_char="2440">in</TOKEN>
<TOKEN id="token-16-34" pos="unknown" morph="none" start_char="2442" end_char="2453">mid-December</TOKEN>
<TOKEN id="token-16-35" pos="punct" morph="none" start_char="2454" end_char="2454">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2457" end_char="2545">
<ORIGINAL_TEXT>She said: "If I had known what was to happen, I would not have cared about the reprimand.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2457" end_char="2459">She</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2461" end_char="2464">said</TOKEN>
<TOKEN id="token-17-2" pos="punct" morph="none" start_char="2465" end_char="2465">:</TOKEN>
<TOKEN id="token-17-3" pos="punct" morph="none" start_char="2467" end_char="2467">"</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2468" end_char="2469">If</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2471" end_char="2471">I</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2473" end_char="2475">had</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="2477" end_char="2481">known</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="2483" end_char="2486">what</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="2488" end_char="2490">was</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="2492" end_char="2493">to</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="2495" end_char="2500">happen</TOKEN>
<TOKEN id="token-17-12" pos="punct" morph="none" start_char="2501" end_char="2501">,</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="2503" end_char="2503">I</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="2505" end_char="2509">would</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="2511" end_char="2513">not</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="2515" end_char="2518">have</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="2520" end_char="2524">cared</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="2526" end_char="2530">about</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="2532" end_char="2534">the</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="2536" end_char="2544">reprimand</TOKEN>
<TOKEN id="token-17-21" pos="punct" morph="none" start_char="2545" end_char="2545">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2548" end_char="2616">
<ORIGINAL_TEXT>"I would have f****** talked about it to whoever, whereever I could."</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="punct" morph="none" start_char="2548" end_char="2548">"</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2549" end_char="2549">I</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="2551" end_char="2555">would</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="2557" end_char="2560">have</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2562" end_char="2562">f</TOKEN>
<TOKEN id="token-18-5" pos="punct" morph="none" start_char="2563" end_char="2568">******</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2570" end_char="2575">talked</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2577" end_char="2581">about</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2583" end_char="2584">it</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2586" end_char="2587">to</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2589" end_char="2595">whoever</TOKEN>
<TOKEN id="token-18-11" pos="punct" morph="none" start_char="2596" end_char="2596">,</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="2598" end_char="2606">whereever</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="2608" end_char="2608">I</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="2610" end_char="2614">could</TOKEN>
<TOKEN id="token-18-15" pos="punct" morph="none" start_char="2615" end_char="2616">."</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2619" end_char="2785">
<ORIGINAL_TEXT>Dr Li Wenliang, who died of the illness on February 6, was detained by police for "spreading false rumours" after taking to Twitter to warn about the mysterious virus.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2619" end_char="2620">Dr</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2622" end_char="2623">Li</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2625" end_char="2632">Wenliang</TOKEN>
<TOKEN id="token-19-3" pos="punct" morph="none" start_char="2633" end_char="2633">,</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2635" end_char="2637">who</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="2639" end_char="2642">died</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2644" end_char="2645">of</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="2647" end_char="2649">the</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="2651" end_char="2657">illness</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="2659" end_char="2660">on</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="2662" end_char="2669">February</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="2671" end_char="2671">6</TOKEN>
<TOKEN id="token-19-12" pos="punct" morph="none" start_char="2672" end_char="2672">,</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="2674" end_char="2676">was</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="2678" end_char="2685">detained</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="2687" end_char="2688">by</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="2690" end_char="2695">police</TOKEN>
<TOKEN id="token-19-17" pos="word" morph="none" start_char="2697" end_char="2699">for</TOKEN>
<TOKEN id="token-19-18" pos="punct" morph="none" start_char="2701" end_char="2701">"</TOKEN>
<TOKEN id="token-19-19" pos="word" morph="none" start_char="2702" end_char="2710">spreading</TOKEN>
<TOKEN id="token-19-20" pos="word" morph="none" start_char="2712" end_char="2716">false</TOKEN>
<TOKEN id="token-19-21" pos="word" morph="none" start_char="2718" end_char="2724">rumours</TOKEN>
<TOKEN id="token-19-22" pos="punct" morph="none" start_char="2725" end_char="2725">"</TOKEN>
<TOKEN id="token-19-23" pos="word" morph="none" start_char="2727" end_char="2731">after</TOKEN>
<TOKEN id="token-19-24" pos="word" morph="none" start_char="2733" end_char="2738">taking</TOKEN>
<TOKEN id="token-19-25" pos="word" morph="none" start_char="2740" end_char="2741">to</TOKEN>
<TOKEN id="token-19-26" pos="word" morph="none" start_char="2743" end_char="2749">Twitter</TOKEN>
<TOKEN id="token-19-27" pos="word" morph="none" start_char="2751" end_char="2752">to</TOKEN>
<TOKEN id="token-19-28" pos="word" morph="none" start_char="2754" end_char="2757">warn</TOKEN>
<TOKEN id="token-19-29" pos="word" morph="none" start_char="2759" end_char="2763">about</TOKEN>
<TOKEN id="token-19-30" pos="word" morph="none" start_char="2765" end_char="2767">the</TOKEN>
<TOKEN id="token-19-31" pos="word" morph="none" start_char="2769" end_char="2778">mysterious</TOKEN>
<TOKEN id="token-19-32" pos="word" morph="none" start_char="2780" end_char="2784">virus</TOKEN>
<TOKEN id="token-19-33" pos="punct" morph="none" start_char="2785" end_char="2785">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2788" end_char="2830">
<ORIGINAL_TEXT>The case has prompted outrage across China.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2788" end_char="2790">The</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="2792" end_char="2795">case</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="2797" end_char="2799">has</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="2801" end_char="2808">prompted</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2810" end_char="2816">outrage</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2818" end_char="2823">across</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2825" end_char="2829">China</TOKEN>
<TOKEN id="token-20-7" pos="punct" morph="none" start_char="2830" end_char="2830">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="2833" end_char="3026">
<ORIGINAL_TEXT>Meanwhile the South China Morning Post claimed although doctors in Wuhan collected samples from suspected cases in late December, red tape prevented them from confirming their findings for days.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="2833" end_char="2841">Meanwhile</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="2843" end_char="2845">the</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="2847" end_char="2851">South</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="2853" end_char="2857">China</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="2859" end_char="2865">Morning</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="2867" end_char="2870">Post</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="2872" end_char="2878">claimed</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="2880" end_char="2887">although</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="2889" end_char="2895">doctors</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="2897" end_char="2898">in</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="2900" end_char="2904">Wuhan</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="2906" end_char="2914">collected</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="2916" end_char="2922">samples</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="2924" end_char="2927">from</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="2929" end_char="2937">suspected</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="2939" end_char="2943">cases</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="2945" end_char="2946">in</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="2948" end_char="2951">late</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="2953" end_char="2960">December</TOKEN>
<TOKEN id="token-21-19" pos="punct" morph="none" start_char="2961" end_char="2961">,</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="2963" end_char="2965">red</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="2967" end_char="2970">tape</TOKEN>
<TOKEN id="token-21-22" pos="word" morph="none" start_char="2972" end_char="2980">prevented</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="2982" end_char="2985">them</TOKEN>
<TOKEN id="token-21-24" pos="word" morph="none" start_char="2987" end_char="2990">from</TOKEN>
<TOKEN id="token-21-25" pos="word" morph="none" start_char="2992" end_char="3001">confirming</TOKEN>
<TOKEN id="token-21-26" pos="word" morph="none" start_char="3003" end_char="3007">their</TOKEN>
<TOKEN id="token-21-27" pos="word" morph="none" start_char="3009" end_char="3016">findings</TOKEN>
<TOKEN id="token-21-28" pos="word" morph="none" start_char="3018" end_char="3020">for</TOKEN>
<TOKEN id="token-21-29" pos="word" morph="none" start_char="3022" end_char="3025">days</TOKEN>
<TOKEN id="token-21-30" pos="punct" morph="none" start_char="3026" end_char="3026">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="3029" end_char="3090">
<ORIGINAL_TEXT>They were also told not to disclose any details to the public.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="3029" end_char="3032">They</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="3034" end_char="3037">were</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="3039" end_char="3042">also</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="3044" end_char="3047">told</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="3049" end_char="3051">not</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="3053" end_char="3054">to</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="3056" end_char="3063">disclose</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="3065" end_char="3067">any</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="3069" end_char="3075">details</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="3077" end_char="3078">to</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="3080" end_char="3082">the</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="3084" end_char="3089">public</TOKEN>
<TOKEN id="token-22-12" pos="punct" morph="none" start_char="3090" end_char="3090">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="3093" end_char="3193">
<ORIGINAL_TEXT>Even on January 11, Wuhans health authorities continued to claim there were only 41 confirmed cases.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="3093" end_char="3096">Even</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="3098" end_char="3099">on</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="3101" end_char="3107">January</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="3109" end_char="3110">11</TOKEN>
<TOKEN id="token-23-4" pos="punct" morph="none" start_char="3111" end_char="3111">,</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="3113" end_char="3119">Wuhans</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="3121" end_char="3126">health</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="3128" end_char="3138">authorities</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="3140" end_char="3148">continued</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="3150" end_char="3151">to</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="3153" end_char="3157">claim</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="3159" end_char="3163">there</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="3165" end_char="3168">were</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="3170" end_char="3173">only</TOKEN>
<TOKEN id="token-23-14" pos="word" morph="none" start_char="3175" end_char="3176">41</TOKEN>
<TOKEN id="token-23-15" pos="word" morph="none" start_char="3178" end_char="3186">confirmed</TOKEN>
<TOKEN id="token-23-16" pos="word" morph="none" start_char="3188" end_char="3192">cases</TOKEN>
<TOKEN id="token-23-17" pos="punct" morph="none" start_char="3193" end_char="3193">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
