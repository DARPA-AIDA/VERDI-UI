<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CA6T" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="300" raw_text_md5="7972ba4fbf64fa7de3678c75d06db18b">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="79">
<ORIGINAL_TEXT>WHO experts believe badgers and rabbits could have spread coronavirus to humans</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="3">WHO</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="5" end_char="11">experts</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="13" end_char="19">believe</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="21" end_char="27">badgers</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="29" end_char="31">and</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="33" end_char="39">rabbits</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="41" end_char="45">could</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="47" end_char="50">have</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="52" end_char="57">spread</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="59" end_char="69">coronavirus</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="71" end_char="72">to</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="74" end_char="79">humans</TOKEN>
</SEG>
<SEG id="segment-1" start_char="85" end_char="152">
<ORIGINAL_TEXT>The WHO expert team finished their four-week trip to China last week</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="85" end_char="87">The</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="89" end_char="91">WHO</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="93" end_char="98">expert</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="100" end_char="103">team</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="105" end_char="112">finished</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="114" end_char="118">their</TOKEN>
<TOKEN id="token-1-6" pos="unknown" morph="none" start_char="120" end_char="128">four-week</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="130" end_char="133">trip</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="135" end_char="136">to</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="138" end_char="142">China</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="144" end_char="147">last</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="149" end_char="152">week</TOKEN>
</SEG>
<SEG id="segment-2" start_char="156" end_char="294">
<ORIGINAL_TEXT>At a press conference, they announced that any leak of the virus from a lab was unlikely, while the Wuhan wet market's role remains unclear</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="156" end_char="157">At</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="159" end_char="159">a</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="161" end_char="165">press</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="167" end_char="176">conference</TOKEN>
<TOKEN id="token-2-4" pos="punct" morph="none" start_char="177" end_char="177">,</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="179" end_char="182">they</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="184" end_char="192">announced</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="194" end_char="197">that</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="199" end_char="201">any</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="203" end_char="206">leak</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="208" end_char="209">of</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="211" end_char="213">the</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="215" end_char="219">virus</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="221" end_char="224">from</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="226" end_char="226">a</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="228" end_char="230">lab</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="232" end_char="234">was</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="236" end_char="243">unlikely</TOKEN>
<TOKEN id="token-2-18" pos="punct" morph="none" start_char="244" end_char="244">,</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="246" end_char="250">while</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="252" end_char="254">the</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="256" end_char="260">Wuhan</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="262" end_char="264">wet</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="266" end_char="273">market's</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="275" end_char="278">role</TOKEN>
<TOKEN id="token-2-25" pos="word" morph="none" start_char="280" end_char="286">remains</TOKEN>
<TOKEN id="token-2-26" pos="word" morph="none" start_char="288" end_char="294">unclear</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
