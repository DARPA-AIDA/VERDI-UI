<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CABA" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="5753" raw_text_md5="4a5a9470930c0ab65f88519a25c475f5">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="120">
<ORIGINAL_TEXT>COVID-19 May Have Been Circulating From August, 2019 – Study of Wuhan Hospital Traffic &amp;amp;amp; Online Keyword Searches</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="unknown" morph="none" start_char="1" end_char="8">COVID-19</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="10" end_char="12">May</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="14" end_char="17">Have</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="19" end_char="22">Been</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="24" end_char="34">Circulating</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="36" end_char="39">From</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="41" end_char="46">August</TOKEN>
<TOKEN id="token-0-7" pos="punct" morph="none" start_char="47" end_char="47">,</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="49" end_char="52">2019</TOKEN>
<TOKEN id="token-0-9" pos="punct" morph="none" start_char="54" end_char="54">–</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="56" end_char="60">Study</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="62" end_char="63">of</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="65" end_char="69">Wuhan</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="71" end_char="78">Hospital</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="80" end_char="86">Traffic</TOKEN>
<TOKEN id="token-0-15" pos="punct" morph="none" start_char="88" end_char="88">&amp;</TOKEN>
<TOKEN id="token-0-16" pos="unknown" morph="none" start_char="89" end_char="95">amp;amp</TOKEN>
<TOKEN id="token-0-17" pos="punct" morph="none" start_char="96" end_char="96">;</TOKEN>
<TOKEN id="token-0-18" pos="word" morph="none" start_char="98" end_char="103">Online</TOKEN>
<TOKEN id="token-0-19" pos="word" morph="none" start_char="105" end_char="111">Keyword</TOKEN>
<TOKEN id="token-0-20" pos="word" morph="none" start_char="113" end_char="120">Searches</TOKEN>
</SEG>
<SEG id="segment-1" start_char="124" end_char="171">
<ORIGINAL_TEXT>Shoppers in Wuhan, China, post-COVID-19 lockdown</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="124" end_char="131">Shoppers</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="133" end_char="134">in</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="136" end_char="140">Wuhan</TOKEN>
<TOKEN id="token-1-3" pos="punct" morph="none" start_char="141" end_char="141">,</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="143" end_char="147">China</TOKEN>
<TOKEN id="token-1-5" pos="punct" morph="none" start_char="148" end_char="148">,</TOKEN>
<TOKEN id="token-1-6" pos="unknown" morph="none" start_char="150" end_char="162">post-COVID-19</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="164" end_char="171">lockdown</TOKEN>
</SEG>
<SEG id="segment-2" start_char="175" end_char="478">
<ORIGINAL_TEXT>A novel study published in preprint form by Harvard University suggests that cases of the novel SARS-COV-2 virus that has caused a global pandemic may have begun appearing in Wuhan hospitals as early as August 2019 – several months before China officially admitted that a new coronavirus was circulating.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="175" end_char="175">A</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="177" end_char="181">novel</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="183" end_char="187">study</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="189" end_char="197">published</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="199" end_char="200">in</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="202" end_char="209">preprint</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="211" end_char="214">form</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="216" end_char="217">by</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="219" end_char="225">Harvard</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="227" end_char="236">University</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="238" end_char="245">suggests</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="247" end_char="250">that</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="252" end_char="256">cases</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="258" end_char="259">of</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="261" end_char="263">the</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="265" end_char="269">novel</TOKEN>
<TOKEN id="token-2-16" pos="unknown" morph="none" start_char="271" end_char="280">SARS-COV-2</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="282" end_char="286">virus</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="288" end_char="291">that</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="293" end_char="295">has</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="297" end_char="302">caused</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="304" end_char="304">a</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="306" end_char="311">global</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="313" end_char="320">pandemic</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="322" end_char="324">may</TOKEN>
<TOKEN id="token-2-25" pos="word" morph="none" start_char="326" end_char="329">have</TOKEN>
<TOKEN id="token-2-26" pos="word" morph="none" start_char="331" end_char="335">begun</TOKEN>
<TOKEN id="token-2-27" pos="word" morph="none" start_char="337" end_char="345">appearing</TOKEN>
<TOKEN id="token-2-28" pos="word" morph="none" start_char="347" end_char="348">in</TOKEN>
<TOKEN id="token-2-29" pos="word" morph="none" start_char="350" end_char="354">Wuhan</TOKEN>
<TOKEN id="token-2-30" pos="word" morph="none" start_char="356" end_char="364">hospitals</TOKEN>
<TOKEN id="token-2-31" pos="word" morph="none" start_char="366" end_char="367">as</TOKEN>
<TOKEN id="token-2-32" pos="word" morph="none" start_char="369" end_char="373">early</TOKEN>
<TOKEN id="token-2-33" pos="word" morph="none" start_char="375" end_char="376">as</TOKEN>
<TOKEN id="token-2-34" pos="word" morph="none" start_char="378" end_char="383">August</TOKEN>
<TOKEN id="token-2-35" pos="word" morph="none" start_char="385" end_char="388">2019</TOKEN>
<TOKEN id="token-2-36" pos="punct" morph="none" start_char="390" end_char="390">–</TOKEN>
<TOKEN id="token-2-37" pos="word" morph="none" start_char="392" end_char="398">several</TOKEN>
<TOKEN id="token-2-38" pos="word" morph="none" start_char="400" end_char="405">months</TOKEN>
<TOKEN id="token-2-39" pos="word" morph="none" start_char="407" end_char="412">before</TOKEN>
<TOKEN id="token-2-40" pos="word" morph="none" start_char="414" end_char="418">China</TOKEN>
<TOKEN id="token-2-41" pos="word" morph="none" start_char="420" end_char="429">officially</TOKEN>
<TOKEN id="token-2-42" pos="word" morph="none" start_char="431" end_char="438">admitted</TOKEN>
<TOKEN id="token-2-43" pos="word" morph="none" start_char="440" end_char="443">that</TOKEN>
<TOKEN id="token-2-44" pos="word" morph="none" start_char="445" end_char="445">a</TOKEN>
<TOKEN id="token-2-45" pos="word" morph="none" start_char="447" end_char="449">new</TOKEN>
<TOKEN id="token-2-46" pos="word" morph="none" start_char="451" end_char="461">coronavirus</TOKEN>
<TOKEN id="token-2-47" pos="word" morph="none" start_char="463" end_char="465">was</TOKEN>
<TOKEN id="token-2-48" pos="word" morph="none" start_char="467" end_char="477">circulating</TOKEN>
<TOKEN id="token-2-49" pos="punct" morph="none" start_char="478" end_char="478">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="481" end_char="816">
<ORIGINAL_TEXT>The study Tuesday found an unusual uptick in traffic at several Wuhan hospitals in the late summer and early fall 2019, as compared to the year before, as well as a significant increase in online searches in China for diseases related to "cough" as well as "diarrhoea" – the latter symptom a distinctive feature of early COVID-19 onset.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="481" end_char="483">The</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="485" end_char="489">study</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="491" end_char="497">Tuesday</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="499" end_char="503">found</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="505" end_char="506">an</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="508" end_char="514">unusual</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="516" end_char="521">uptick</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="523" end_char="524">in</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="526" end_char="532">traffic</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="534" end_char="535">at</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="537" end_char="543">several</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="545" end_char="549">Wuhan</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="551" end_char="559">hospitals</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="561" end_char="562">in</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="564" end_char="566">the</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="568" end_char="571">late</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="573" end_char="578">summer</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="580" end_char="582">and</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="584" end_char="588">early</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="590" end_char="593">fall</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="595" end_char="598">2019</TOKEN>
<TOKEN id="token-3-21" pos="punct" morph="none" start_char="599" end_char="599">,</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="601" end_char="602">as</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="604" end_char="611">compared</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="613" end_char="614">to</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="616" end_char="618">the</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="620" end_char="623">year</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="625" end_char="630">before</TOKEN>
<TOKEN id="token-3-28" pos="punct" morph="none" start_char="631" end_char="631">,</TOKEN>
<TOKEN id="token-3-29" pos="word" morph="none" start_char="633" end_char="634">as</TOKEN>
<TOKEN id="token-3-30" pos="word" morph="none" start_char="636" end_char="639">well</TOKEN>
<TOKEN id="token-3-31" pos="word" morph="none" start_char="641" end_char="642">as</TOKEN>
<TOKEN id="token-3-32" pos="word" morph="none" start_char="644" end_char="644">a</TOKEN>
<TOKEN id="token-3-33" pos="word" morph="none" start_char="646" end_char="656">significant</TOKEN>
<TOKEN id="token-3-34" pos="word" morph="none" start_char="658" end_char="665">increase</TOKEN>
<TOKEN id="token-3-35" pos="word" morph="none" start_char="667" end_char="668">in</TOKEN>
<TOKEN id="token-3-36" pos="word" morph="none" start_char="670" end_char="675">online</TOKEN>
<TOKEN id="token-3-37" pos="word" morph="none" start_char="677" end_char="684">searches</TOKEN>
<TOKEN id="token-3-38" pos="word" morph="none" start_char="686" end_char="687">in</TOKEN>
<TOKEN id="token-3-39" pos="word" morph="none" start_char="689" end_char="693">China</TOKEN>
<TOKEN id="token-3-40" pos="word" morph="none" start_char="695" end_char="697">for</TOKEN>
<TOKEN id="token-3-41" pos="word" morph="none" start_char="699" end_char="706">diseases</TOKEN>
<TOKEN id="token-3-42" pos="word" morph="none" start_char="708" end_char="714">related</TOKEN>
<TOKEN id="token-3-43" pos="word" morph="none" start_char="716" end_char="717">to</TOKEN>
<TOKEN id="token-3-44" pos="punct" morph="none" start_char="719" end_char="719">"</TOKEN>
<TOKEN id="token-3-45" pos="word" morph="none" start_char="720" end_char="724">cough</TOKEN>
<TOKEN id="token-3-46" pos="punct" morph="none" start_char="725" end_char="725">"</TOKEN>
<TOKEN id="token-3-47" pos="word" morph="none" start_char="727" end_char="728">as</TOKEN>
<TOKEN id="token-3-48" pos="word" morph="none" start_char="730" end_char="733">well</TOKEN>
<TOKEN id="token-3-49" pos="word" morph="none" start_char="735" end_char="736">as</TOKEN>
<TOKEN id="token-3-50" pos="punct" morph="none" start_char="738" end_char="738">"</TOKEN>
<TOKEN id="token-3-51" pos="word" morph="none" start_char="739" end_char="747">diarrhoea</TOKEN>
<TOKEN id="token-3-52" pos="punct" morph="none" start_char="748" end_char="748">"</TOKEN>
<TOKEN id="token-3-53" pos="punct" morph="none" start_char="750" end_char="750">–</TOKEN>
<TOKEN id="token-3-54" pos="word" morph="none" start_char="752" end_char="754">the</TOKEN>
<TOKEN id="token-3-55" pos="word" morph="none" start_char="756" end_char="761">latter</TOKEN>
<TOKEN id="token-3-56" pos="word" morph="none" start_char="763" end_char="769">symptom</TOKEN>
<TOKEN id="token-3-57" pos="word" morph="none" start_char="771" end_char="771">a</TOKEN>
<TOKEN id="token-3-58" pos="word" morph="none" start_char="773" end_char="783">distinctive</TOKEN>
<TOKEN id="token-3-59" pos="word" morph="none" start_char="785" end_char="791">feature</TOKEN>
<TOKEN id="token-3-60" pos="word" morph="none" start_char="793" end_char="794">of</TOKEN>
<TOKEN id="token-3-61" pos="word" morph="none" start_char="796" end_char="800">early</TOKEN>
<TOKEN id="token-3-62" pos="unknown" morph="none" start_char="802" end_char="809">COVID-19</TOKEN>
<TOKEN id="token-3-63" pos="word" morph="none" start_char="811" end_char="815">onset</TOKEN>
<TOKEN id="token-3-64" pos="punct" morph="none" start_char="816" end_char="816">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="819" end_char="948">
<ORIGINAL_TEXT>Analysis of hospital traffic in Wuhan China indicates shows high number of visitors, compared to the same time, in the year before</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="819" end_char="826">Analysis</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="828" end_char="829">of</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="831" end_char="838">hospital</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="840" end_char="846">traffic</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="848" end_char="849">in</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="851" end_char="855">Wuhan</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="857" end_char="861">China</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="863" end_char="871">indicates</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="873" end_char="877">shows</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="879" end_char="882">high</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="884" end_char="889">number</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="891" end_char="892">of</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="894" end_char="901">visitors</TOKEN>
<TOKEN id="token-4-13" pos="punct" morph="none" start_char="902" end_char="902">,</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="904" end_char="911">compared</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="913" end_char="914">to</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="916" end_char="918">the</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="920" end_char="923">same</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="925" end_char="928">time</TOKEN>
<TOKEN id="token-4-19" pos="punct" morph="none" start_char="929" end_char="929">,</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="931" end_char="932">in</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="934" end_char="936">the</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="938" end_char="941">year</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="943" end_char="948">before</TOKEN>
</SEG>
<SEG id="segment-5" start_char="952" end_char="1062">
<ORIGINAL_TEXT>"We observe an upward trend in hospital traffic and search volume beginning in late Summer and early Fall 2019.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="punct" morph="none" start_char="952" end_char="952">"</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="953" end_char="954">We</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="956" end_char="962">observe</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="964" end_char="965">an</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="967" end_char="972">upward</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="974" end_char="978">trend</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="980" end_char="981">in</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="983" end_char="990">hospital</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="992" end_char="998">traffic</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="1000" end_char="1002">and</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="1004" end_char="1009">search</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="1011" end_char="1016">volume</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="1018" end_char="1026">beginning</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="1028" end_char="1029">in</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="1031" end_char="1034">late</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="1036" end_char="1041">Summer</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="1043" end_char="1045">and</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="1047" end_char="1051">early</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="1053" end_char="1056">Fall</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="1058" end_char="1061">2019</TOKEN>
<TOKEN id="token-5-20" pos="punct" morph="none" start_char="1062" end_char="1062">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="1064" end_char="1284">
<ORIGINAL_TEXT>While queries of the respiratory symptom "cough" show seasonal fluctuations coinciding with yearly influenza seasons, "diarrhea" is a more COVID-19 specific symptom and only shows an association with the current epidemic.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="1064" end_char="1068">While</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="1070" end_char="1076">queries</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="1078" end_char="1079">of</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="1081" end_char="1083">the</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="1085" end_char="1095">respiratory</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="1097" end_char="1103">symptom</TOKEN>
<TOKEN id="token-6-6" pos="punct" morph="none" start_char="1105" end_char="1105">"</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="1106" end_char="1110">cough</TOKEN>
<TOKEN id="token-6-8" pos="punct" morph="none" start_char="1111" end_char="1111">"</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="1113" end_char="1116">show</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="1118" end_char="1125">seasonal</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="1127" end_char="1138">fluctuations</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="1140" end_char="1149">coinciding</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="1151" end_char="1154">with</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="1156" end_char="1161">yearly</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="1163" end_char="1171">influenza</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="1173" end_char="1179">seasons</TOKEN>
<TOKEN id="token-6-17" pos="punct" morph="none" start_char="1180" end_char="1180">,</TOKEN>
<TOKEN id="token-6-18" pos="punct" morph="none" start_char="1182" end_char="1182">"</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="1183" end_char="1190">diarrhea</TOKEN>
<TOKEN id="token-6-20" pos="punct" morph="none" start_char="1191" end_char="1191">"</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="1193" end_char="1194">is</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="1196" end_char="1196">a</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="1198" end_char="1201">more</TOKEN>
<TOKEN id="token-6-24" pos="unknown" morph="none" start_char="1203" end_char="1210">COVID-19</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="1212" end_char="1219">specific</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="1221" end_char="1227">symptom</TOKEN>
<TOKEN id="token-6-27" pos="word" morph="none" start_char="1229" end_char="1231">and</TOKEN>
<TOKEN id="token-6-28" pos="word" morph="none" start_char="1233" end_char="1236">only</TOKEN>
<TOKEN id="token-6-29" pos="word" morph="none" start_char="1238" end_char="1242">shows</TOKEN>
<TOKEN id="token-6-30" pos="word" morph="none" start_char="1244" end_char="1245">an</TOKEN>
<TOKEN id="token-6-31" pos="word" morph="none" start_char="1247" end_char="1257">association</TOKEN>
<TOKEN id="token-6-32" pos="word" morph="none" start_char="1259" end_char="1262">with</TOKEN>
<TOKEN id="token-6-33" pos="word" morph="none" start_char="1264" end_char="1266">the</TOKEN>
<TOKEN id="token-6-34" pos="word" morph="none" start_char="1268" end_char="1274">current</TOKEN>
<TOKEN id="token-6-35" pos="word" morph="none" start_char="1276" end_char="1283">epidemic</TOKEN>
<TOKEN id="token-6-36" pos="punct" morph="none" start_char="1284" end_char="1284">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="1287" end_char="1722">
<ORIGINAL_TEXT>"The increase of both signals precede the documented start of the COVID-19 pandemic in December, highlighting the value of novel digital sources for surveillance of emerging pathogen," says the preprint study, published on Harvard’s DASH repository, and authored by four experts in computational epidemiology and biomedical informatics at Harvard Medical School, Boston University School of Public Health and Boston Children’s Hospital.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="punct" morph="none" start_char="1287" end_char="1287">"</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="1288" end_char="1290">The</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="1292" end_char="1299">increase</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="1301" end_char="1302">of</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="1304" end_char="1307">both</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="1309" end_char="1315">signals</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="1317" end_char="1323">precede</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="1325" end_char="1327">the</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="1329" end_char="1338">documented</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="1340" end_char="1344">start</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="1346" end_char="1347">of</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="1349" end_char="1351">the</TOKEN>
<TOKEN id="token-7-12" pos="unknown" morph="none" start_char="1353" end_char="1360">COVID-19</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="1362" end_char="1369">pandemic</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="1371" end_char="1372">in</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="1374" end_char="1381">December</TOKEN>
<TOKEN id="token-7-16" pos="punct" morph="none" start_char="1382" end_char="1382">,</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="1384" end_char="1395">highlighting</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="1397" end_char="1399">the</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="1401" end_char="1405">value</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="1407" end_char="1408">of</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="1410" end_char="1414">novel</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="1416" end_char="1422">digital</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="1424" end_char="1430">sources</TOKEN>
<TOKEN id="token-7-24" pos="word" morph="none" start_char="1432" end_char="1434">for</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="1436" end_char="1447">surveillance</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="1449" end_char="1450">of</TOKEN>
<TOKEN id="token-7-27" pos="word" morph="none" start_char="1452" end_char="1459">emerging</TOKEN>
<TOKEN id="token-7-28" pos="word" morph="none" start_char="1461" end_char="1468">pathogen</TOKEN>
<TOKEN id="token-7-29" pos="punct" morph="none" start_char="1469" end_char="1470">,"</TOKEN>
<TOKEN id="token-7-30" pos="word" morph="none" start_char="1472" end_char="1475">says</TOKEN>
<TOKEN id="token-7-31" pos="word" morph="none" start_char="1477" end_char="1479">the</TOKEN>
<TOKEN id="token-7-32" pos="word" morph="none" start_char="1481" end_char="1488">preprint</TOKEN>
<TOKEN id="token-7-33" pos="word" morph="none" start_char="1490" end_char="1494">study</TOKEN>
<TOKEN id="token-7-34" pos="punct" morph="none" start_char="1495" end_char="1495">,</TOKEN>
<TOKEN id="token-7-35" pos="word" morph="none" start_char="1497" end_char="1505">published</TOKEN>
<TOKEN id="token-7-36" pos="word" morph="none" start_char="1507" end_char="1508">on</TOKEN>
<TOKEN id="token-7-37" pos="word" morph="none" start_char="1510" end_char="1518">Harvard’s</TOKEN>
<TOKEN id="token-7-38" pos="word" morph="none" start_char="1520" end_char="1523">DASH</TOKEN>
<TOKEN id="token-7-39" pos="word" morph="none" start_char="1525" end_char="1534">repository</TOKEN>
<TOKEN id="token-7-40" pos="punct" morph="none" start_char="1535" end_char="1535">,</TOKEN>
<TOKEN id="token-7-41" pos="word" morph="none" start_char="1537" end_char="1539">and</TOKEN>
<TOKEN id="token-7-42" pos="word" morph="none" start_char="1541" end_char="1548">authored</TOKEN>
<TOKEN id="token-7-43" pos="word" morph="none" start_char="1550" end_char="1551">by</TOKEN>
<TOKEN id="token-7-44" pos="word" morph="none" start_char="1553" end_char="1556">four</TOKEN>
<TOKEN id="token-7-45" pos="word" morph="none" start_char="1558" end_char="1564">experts</TOKEN>
<TOKEN id="token-7-46" pos="word" morph="none" start_char="1566" end_char="1567">in</TOKEN>
<TOKEN id="token-7-47" pos="word" morph="none" start_char="1569" end_char="1581">computational</TOKEN>
<TOKEN id="token-7-48" pos="word" morph="none" start_char="1583" end_char="1594">epidemiology</TOKEN>
<TOKEN id="token-7-49" pos="word" morph="none" start_char="1596" end_char="1598">and</TOKEN>
<TOKEN id="token-7-50" pos="word" morph="none" start_char="1600" end_char="1609">biomedical</TOKEN>
<TOKEN id="token-7-51" pos="word" morph="none" start_char="1611" end_char="1621">informatics</TOKEN>
<TOKEN id="token-7-52" pos="word" morph="none" start_char="1623" end_char="1624">at</TOKEN>
<TOKEN id="token-7-53" pos="word" morph="none" start_char="1626" end_char="1632">Harvard</TOKEN>
<TOKEN id="token-7-54" pos="word" morph="none" start_char="1634" end_char="1640">Medical</TOKEN>
<TOKEN id="token-7-55" pos="word" morph="none" start_char="1642" end_char="1647">School</TOKEN>
<TOKEN id="token-7-56" pos="punct" morph="none" start_char="1648" end_char="1648">,</TOKEN>
<TOKEN id="token-7-57" pos="word" morph="none" start_char="1650" end_char="1655">Boston</TOKEN>
<TOKEN id="token-7-58" pos="word" morph="none" start_char="1657" end_char="1666">University</TOKEN>
<TOKEN id="token-7-59" pos="word" morph="none" start_char="1668" end_char="1673">School</TOKEN>
<TOKEN id="token-7-60" pos="word" morph="none" start_char="1675" end_char="1676">of</TOKEN>
<TOKEN id="token-7-61" pos="word" morph="none" start_char="1678" end_char="1683">Public</TOKEN>
<TOKEN id="token-7-62" pos="word" morph="none" start_char="1685" end_char="1690">Health</TOKEN>
<TOKEN id="token-7-63" pos="word" morph="none" start_char="1692" end_char="1694">and</TOKEN>
<TOKEN id="token-7-64" pos="word" morph="none" start_char="1696" end_char="1701">Boston</TOKEN>
<TOKEN id="token-7-65" pos="word" morph="none" start_char="1703" end_char="1712">Children’s</TOKEN>
<TOKEN id="token-7-66" pos="word" morph="none" start_char="1714" end_char="1721">Hospital</TOKEN>
<TOKEN id="token-7-67" pos="punct" morph="none" start_char="1722" end_char="1722">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1725" end_char="1808">
<ORIGINAL_TEXT>Study Strengthens Theory That Virus Emerged From Source Outside Wuhan Seafood Market</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="1725" end_char="1729">Study</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="1731" end_char="1741">Strengthens</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="1743" end_char="1748">Theory</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1750" end_char="1753">That</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1755" end_char="1759">Virus</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1761" end_char="1767">Emerged</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="1769" end_char="1772">From</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="1774" end_char="1779">Source</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1781" end_char="1787">Outside</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="1789" end_char="1793">Wuhan</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="1795" end_char="1801">Seafood</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="1803" end_char="1808">Market</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1812" end_char="2077">
<ORIGINAL_TEXT>The authors said that their findings strengthen the theory that the novel coronavirus may have already been circulating in Wuhan, a city of 10 million people, prior to the identification of a large cluster of infected people associated with the Hunan Seafood Market.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1812" end_char="1814">The</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1816" end_char="1822">authors</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="1824" end_char="1827">said</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1829" end_char="1832">that</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1834" end_char="1838">their</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1840" end_char="1847">findings</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1849" end_char="1858">strengthen</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1860" end_char="1862">the</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1864" end_char="1869">theory</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="1871" end_char="1874">that</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1876" end_char="1878">the</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1880" end_char="1884">novel</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="1886" end_char="1896">coronavirus</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1898" end_char="1900">may</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1902" end_char="1905">have</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1907" end_char="1913">already</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="1915" end_char="1918">been</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1920" end_char="1930">circulating</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1932" end_char="1933">in</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1935" end_char="1939">Wuhan</TOKEN>
<TOKEN id="token-9-20" pos="punct" morph="none" start_char="1940" end_char="1940">,</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1942" end_char="1942">a</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1944" end_char="1947">city</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="1949" end_char="1950">of</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1952" end_char="1953">10</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="1955" end_char="1961">million</TOKEN>
<TOKEN id="token-9-26" pos="word" morph="none" start_char="1963" end_char="1968">people</TOKEN>
<TOKEN id="token-9-27" pos="punct" morph="none" start_char="1969" end_char="1969">,</TOKEN>
<TOKEN id="token-9-28" pos="word" morph="none" start_char="1971" end_char="1975">prior</TOKEN>
<TOKEN id="token-9-29" pos="word" morph="none" start_char="1977" end_char="1978">to</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1980" end_char="1982">the</TOKEN>
<TOKEN id="token-9-31" pos="word" morph="none" start_char="1984" end_char="1997">identification</TOKEN>
<TOKEN id="token-9-32" pos="word" morph="none" start_char="1999" end_char="2000">of</TOKEN>
<TOKEN id="token-9-33" pos="word" morph="none" start_char="2002" end_char="2002">a</TOKEN>
<TOKEN id="token-9-34" pos="word" morph="none" start_char="2004" end_char="2008">large</TOKEN>
<TOKEN id="token-9-35" pos="word" morph="none" start_char="2010" end_char="2016">cluster</TOKEN>
<TOKEN id="token-9-36" pos="word" morph="none" start_char="2018" end_char="2019">of</TOKEN>
<TOKEN id="token-9-37" pos="word" morph="none" start_char="2021" end_char="2028">infected</TOKEN>
<TOKEN id="token-9-38" pos="word" morph="none" start_char="2030" end_char="2035">people</TOKEN>
<TOKEN id="token-9-39" pos="word" morph="none" start_char="2037" end_char="2046">associated</TOKEN>
<TOKEN id="token-9-40" pos="word" morph="none" start_char="2048" end_char="2051">with</TOKEN>
<TOKEN id="token-9-41" pos="word" morph="none" start_char="2053" end_char="2055">the</TOKEN>
<TOKEN id="token-9-42" pos="word" morph="none" start_char="2057" end_char="2061">Hunan</TOKEN>
<TOKEN id="token-9-43" pos="word" morph="none" start_char="2063" end_char="2069">Seafood</TOKEN>
<TOKEN id="token-9-44" pos="word" morph="none" start_char="2071" end_char="2076">Market</TOKEN>
<TOKEN id="token-9-45" pos="punct" morph="none" start_char="2077" end_char="2077">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="2080" end_char="2424">
<ORIGINAL_TEXT>While it has been suggested that the original source of the virus was a wild animal in the market, where mammals, reptiles and other animals were kept in crowded, contained spaces in close proximity to market workers and shoppers, no direct connection to the market has been found for the first 14 individuals who became infected with the virus.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="2080" end_char="2084">While</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="2086" end_char="2087">it</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="2089" end_char="2091">has</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="2093" end_char="2096">been</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="2098" end_char="2106">suggested</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="2108" end_char="2111">that</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="2113" end_char="2115">the</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="2117" end_char="2124">original</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="2126" end_char="2131">source</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="2133" end_char="2134">of</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="2136" end_char="2138">the</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="2140" end_char="2144">virus</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="2146" end_char="2148">was</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="2150" end_char="2150">a</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="2152" end_char="2155">wild</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="2157" end_char="2162">animal</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="2164" end_char="2165">in</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="2167" end_char="2169">the</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="2171" end_char="2176">market</TOKEN>
<TOKEN id="token-10-19" pos="punct" morph="none" start_char="2177" end_char="2177">,</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="2179" end_char="2183">where</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="2185" end_char="2191">mammals</TOKEN>
<TOKEN id="token-10-22" pos="punct" morph="none" start_char="2192" end_char="2192">,</TOKEN>
<TOKEN id="token-10-23" pos="word" morph="none" start_char="2194" end_char="2201">reptiles</TOKEN>
<TOKEN id="token-10-24" pos="word" morph="none" start_char="2203" end_char="2205">and</TOKEN>
<TOKEN id="token-10-25" pos="word" morph="none" start_char="2207" end_char="2211">other</TOKEN>
<TOKEN id="token-10-26" pos="word" morph="none" start_char="2213" end_char="2219">animals</TOKEN>
<TOKEN id="token-10-27" pos="word" morph="none" start_char="2221" end_char="2224">were</TOKEN>
<TOKEN id="token-10-28" pos="word" morph="none" start_char="2226" end_char="2229">kept</TOKEN>
<TOKEN id="token-10-29" pos="word" morph="none" start_char="2231" end_char="2232">in</TOKEN>
<TOKEN id="token-10-30" pos="word" morph="none" start_char="2234" end_char="2240">crowded</TOKEN>
<TOKEN id="token-10-31" pos="punct" morph="none" start_char="2241" end_char="2241">,</TOKEN>
<TOKEN id="token-10-32" pos="word" morph="none" start_char="2243" end_char="2251">contained</TOKEN>
<TOKEN id="token-10-33" pos="word" morph="none" start_char="2253" end_char="2258">spaces</TOKEN>
<TOKEN id="token-10-34" pos="word" morph="none" start_char="2260" end_char="2261">in</TOKEN>
<TOKEN id="token-10-35" pos="word" morph="none" start_char="2263" end_char="2267">close</TOKEN>
<TOKEN id="token-10-36" pos="word" morph="none" start_char="2269" end_char="2277">proximity</TOKEN>
<TOKEN id="token-10-37" pos="word" morph="none" start_char="2279" end_char="2280">to</TOKEN>
<TOKEN id="token-10-38" pos="word" morph="none" start_char="2282" end_char="2287">market</TOKEN>
<TOKEN id="token-10-39" pos="word" morph="none" start_char="2289" end_char="2295">workers</TOKEN>
<TOKEN id="token-10-40" pos="word" morph="none" start_char="2297" end_char="2299">and</TOKEN>
<TOKEN id="token-10-41" pos="word" morph="none" start_char="2301" end_char="2308">shoppers</TOKEN>
<TOKEN id="token-10-42" pos="punct" morph="none" start_char="2309" end_char="2309">,</TOKEN>
<TOKEN id="token-10-43" pos="word" morph="none" start_char="2311" end_char="2312">no</TOKEN>
<TOKEN id="token-10-44" pos="word" morph="none" start_char="2314" end_char="2319">direct</TOKEN>
<TOKEN id="token-10-45" pos="word" morph="none" start_char="2321" end_char="2330">connection</TOKEN>
<TOKEN id="token-10-46" pos="word" morph="none" start_char="2332" end_char="2333">to</TOKEN>
<TOKEN id="token-10-47" pos="word" morph="none" start_char="2335" end_char="2337">the</TOKEN>
<TOKEN id="token-10-48" pos="word" morph="none" start_char="2339" end_char="2344">market</TOKEN>
<TOKEN id="token-10-49" pos="word" morph="none" start_char="2346" end_char="2348">has</TOKEN>
<TOKEN id="token-10-50" pos="word" morph="none" start_char="2350" end_char="2353">been</TOKEN>
<TOKEN id="token-10-51" pos="word" morph="none" start_char="2355" end_char="2359">found</TOKEN>
<TOKEN id="token-10-52" pos="word" morph="none" start_char="2361" end_char="2363">for</TOKEN>
<TOKEN id="token-10-53" pos="word" morph="none" start_char="2365" end_char="2367">the</TOKEN>
<TOKEN id="token-10-54" pos="word" morph="none" start_char="2369" end_char="2373">first</TOKEN>
<TOKEN id="token-10-55" pos="word" morph="none" start_char="2375" end_char="2376">14</TOKEN>
<TOKEN id="token-10-56" pos="word" morph="none" start_char="2378" end_char="2388">individuals</TOKEN>
<TOKEN id="token-10-57" pos="word" morph="none" start_char="2390" end_char="2392">who</TOKEN>
<TOKEN id="token-10-58" pos="word" morph="none" start_char="2394" end_char="2399">became</TOKEN>
<TOKEN id="token-10-59" pos="word" morph="none" start_char="2401" end_char="2408">infected</TOKEN>
<TOKEN id="token-10-60" pos="word" morph="none" start_char="2410" end_char="2413">with</TOKEN>
<TOKEN id="token-10-61" pos="word" morph="none" start_char="2415" end_char="2417">the</TOKEN>
<TOKEN id="token-10-62" pos="word" morph="none" start_char="2419" end_char="2423">virus</TOKEN>
<TOKEN id="token-10-63" pos="punct" morph="none" start_char="2424" end_char="2424">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="2426" end_char="2603">
<ORIGINAL_TEXT>Nor have virology samples taken from wildlife at the market been linked to SARS-COV2, the study notes, "leaving open the possibility of alternate points of origin and infection."</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="2426" end_char="2428">Nor</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="2430" end_char="2433">have</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="2435" end_char="2442">virology</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="2444" end_char="2450">samples</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="2452" end_char="2456">taken</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="2458" end_char="2461">from</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="2463" end_char="2470">wildlife</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="2472" end_char="2473">at</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="2475" end_char="2477">the</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="2479" end_char="2484">market</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="2486" end_char="2489">been</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="2491" end_char="2496">linked</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="2498" end_char="2499">to</TOKEN>
<TOKEN id="token-11-13" pos="unknown" morph="none" start_char="2501" end_char="2509">SARS-COV2</TOKEN>
<TOKEN id="token-11-14" pos="punct" morph="none" start_char="2510" end_char="2510">,</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="2512" end_char="2514">the</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="2516" end_char="2520">study</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="2522" end_char="2526">notes</TOKEN>
<TOKEN id="token-11-18" pos="punct" morph="none" start_char="2527" end_char="2527">,</TOKEN>
<TOKEN id="token-11-19" pos="punct" morph="none" start_char="2529" end_char="2529">"</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="2530" end_char="2536">leaving</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="2538" end_char="2541">open</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="2543" end_char="2545">the</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="2547" end_char="2557">possibility</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="2559" end_char="2560">of</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="2562" end_char="2570">alternate</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="2572" end_char="2577">points</TOKEN>
<TOKEN id="token-11-27" pos="word" morph="none" start_char="2579" end_char="2580">of</TOKEN>
<TOKEN id="token-11-28" pos="word" morph="none" start_char="2582" end_char="2587">origin</TOKEN>
<TOKEN id="token-11-29" pos="word" morph="none" start_char="2589" end_char="2591">and</TOKEN>
<TOKEN id="token-11-30" pos="word" morph="none" start_char="2593" end_char="2601">infection</TOKEN>
<TOKEN id="token-11-31" pos="punct" morph="none" start_char="2602" end_char="2603">."</TOKEN>
</SEG>
<SEG id="segment-12" start_char="2606" end_char="2747">
<ORIGINAL_TEXT>"Here we consider that SARS-CoV-2 may have already been circulating in the community prior to the identification of the Huanan Market cluster.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="punct" morph="none" start_char="2606" end_char="2606">"</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="2607" end_char="2610">Here</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="2612" end_char="2613">we</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="2615" end_char="2622">consider</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="2624" end_char="2627">that</TOKEN>
<TOKEN id="token-12-5" pos="unknown" morph="none" start_char="2629" end_char="2638">SARS-CoV-2</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="2640" end_char="2642">may</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="2644" end_char="2647">have</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="2649" end_char="2655">already</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="2657" end_char="2660">been</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="2662" end_char="2672">circulating</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="2674" end_char="2675">in</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="2677" end_char="2679">the</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="2681" end_char="2689">community</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="2691" end_char="2695">prior</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="2697" end_char="2698">to</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="2700" end_char="2702">the</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="2704" end_char="2717">identification</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="2719" end_char="2720">of</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="2722" end_char="2724">the</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="2726" end_char="2731">Huanan</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="2733" end_char="2738">Market</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="2740" end_char="2746">cluster</TOKEN>
<TOKEN id="token-12-23" pos="punct" morph="none" start_char="2747" end_char="2747">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="2749" end_char="3018">
<ORIGINAL_TEXT>This hypothesis is supported by emerging epidemiologic and phylogenetic evidence indicating that the virus emerged in southern China, and may have already spread internationally, and adapted for efficient human transmission by the time it was detected in late December."</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="2749" end_char="2752">This</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="2754" end_char="2763">hypothesis</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="2765" end_char="2766">is</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="2768" end_char="2776">supported</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="2778" end_char="2779">by</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="2781" end_char="2788">emerging</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="2790" end_char="2802">epidemiologic</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="2804" end_char="2806">and</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="2808" end_char="2819">phylogenetic</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="2821" end_char="2828">evidence</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="2830" end_char="2839">indicating</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="2841" end_char="2844">that</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="2846" end_char="2848">the</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="2850" end_char="2854">virus</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="2856" end_char="2862">emerged</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="2864" end_char="2865">in</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="2867" end_char="2874">southern</TOKEN>
<TOKEN id="token-13-17" pos="word" morph="none" start_char="2876" end_char="2880">China</TOKEN>
<TOKEN id="token-13-18" pos="punct" morph="none" start_char="2881" end_char="2881">,</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="2883" end_char="2885">and</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="2887" end_char="2889">may</TOKEN>
<TOKEN id="token-13-21" pos="word" morph="none" start_char="2891" end_char="2894">have</TOKEN>
<TOKEN id="token-13-22" pos="word" morph="none" start_char="2896" end_char="2902">already</TOKEN>
<TOKEN id="token-13-23" pos="word" morph="none" start_char="2904" end_char="2909">spread</TOKEN>
<TOKEN id="token-13-24" pos="word" morph="none" start_char="2911" end_char="2925">internationally</TOKEN>
<TOKEN id="token-13-25" pos="punct" morph="none" start_char="2926" end_char="2926">,</TOKEN>
<TOKEN id="token-13-26" pos="word" morph="none" start_char="2928" end_char="2930">and</TOKEN>
<TOKEN id="token-13-27" pos="word" morph="none" start_char="2932" end_char="2938">adapted</TOKEN>
<TOKEN id="token-13-28" pos="word" morph="none" start_char="2940" end_char="2942">for</TOKEN>
<TOKEN id="token-13-29" pos="word" morph="none" start_char="2944" end_char="2952">efficient</TOKEN>
<TOKEN id="token-13-30" pos="word" morph="none" start_char="2954" end_char="2958">human</TOKEN>
<TOKEN id="token-13-31" pos="word" morph="none" start_char="2960" end_char="2971">transmission</TOKEN>
<TOKEN id="token-13-32" pos="word" morph="none" start_char="2973" end_char="2974">by</TOKEN>
<TOKEN id="token-13-33" pos="word" morph="none" start_char="2976" end_char="2978">the</TOKEN>
<TOKEN id="token-13-34" pos="word" morph="none" start_char="2980" end_char="2983">time</TOKEN>
<TOKEN id="token-13-35" pos="word" morph="none" start_char="2985" end_char="2986">it</TOKEN>
<TOKEN id="token-13-36" pos="word" morph="none" start_char="2988" end_char="2990">was</TOKEN>
<TOKEN id="token-13-37" pos="word" morph="none" start_char="2992" end_char="2999">detected</TOKEN>
<TOKEN id="token-13-38" pos="word" morph="none" start_char="3001" end_char="3002">in</TOKEN>
<TOKEN id="token-13-39" pos="word" morph="none" start_char="3004" end_char="3007">late</TOKEN>
<TOKEN id="token-13-40" pos="word" morph="none" start_char="3009" end_char="3016">December</TOKEN>
<TOKEN id="token-13-41" pos="punct" morph="none" start_char="3017" end_char="3018">."</TOKEN>
</SEG>
<SEG id="segment-14" start_char="3021" end_char="3107">
<ORIGINAL_TEXT>Hospital traffic and Covid-19 symptoms search queries both rise sharply in autumn, 2019</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="3021" end_char="3028">Hospital</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="3030" end_char="3036">traffic</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="3038" end_char="3040">and</TOKEN>
<TOKEN id="token-14-3" pos="unknown" morph="none" start_char="3042" end_char="3049">Covid-19</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="3051" end_char="3058">symptoms</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="3060" end_char="3065">search</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="3067" end_char="3073">queries</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="3075" end_char="3078">both</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="3080" end_char="3083">rise</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="3085" end_char="3091">sharply</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="3093" end_char="3094">in</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="3096" end_char="3101">autumn</TOKEN>
<TOKEN id="token-14-12" pos="punct" morph="none" start_char="3102" end_char="3102">,</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="3104" end_char="3107">2019</TOKEN>
</SEG>
<SEG id="segment-15" start_char="3111" end_char="3253">
<ORIGINAL_TEXT>The study examined satellite images of traffic patterns at six Wuhan hospitals as well as several other control sites, to draw its conclusions.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="3111" end_char="3113">The</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="3115" end_char="3119">study</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="3121" end_char="3128">examined</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="3130" end_char="3138">satellite</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="3140" end_char="3145">images</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="3147" end_char="3148">of</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="3150" end_char="3156">traffic</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="3158" end_char="3165">patterns</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="3167" end_char="3168">at</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="3170" end_char="3172">six</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="3174" end_char="3178">Wuhan</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="3180" end_char="3188">hospitals</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="3190" end_char="3191">as</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="3193" end_char="3196">well</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="3198" end_char="3199">as</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="3201" end_char="3207">several</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="3209" end_char="3213">other</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="3215" end_char="3221">control</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="3223" end_char="3227">sites</TOKEN>
<TOKEN id="token-15-19" pos="punct" morph="none" start_char="3228" end_char="3228">,</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="3230" end_char="3231">to</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="3233" end_char="3236">draw</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="3238" end_char="3240">its</TOKEN>
<TOKEN id="token-15-23" pos="word" morph="none" start_char="3242" end_char="3252">conclusions</TOKEN>
<TOKEN id="token-15-24" pos="punct" morph="none" start_char="3253" end_char="3253">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="3255" end_char="3406">
<ORIGINAL_TEXT>Along with that it searched terms in the Chinese "Baidu" search engine, noting that the same method has been used to estimate influenza trends in China.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="3255" end_char="3259">Along</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="3261" end_char="3264">with</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="3266" end_char="3269">that</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="3271" end_char="3272">it</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="3274" end_char="3281">searched</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="3283" end_char="3287">terms</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="3289" end_char="3290">in</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="3292" end_char="3294">the</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="3296" end_char="3302">Chinese</TOKEN>
<TOKEN id="token-16-9" pos="punct" morph="none" start_char="3304" end_char="3304">"</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="3305" end_char="3309">Baidu</TOKEN>
<TOKEN id="token-16-11" pos="punct" morph="none" start_char="3310" end_char="3310">"</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="3312" end_char="3317">search</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="3319" end_char="3324">engine</TOKEN>
<TOKEN id="token-16-14" pos="punct" morph="none" start_char="3325" end_char="3325">,</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="3327" end_char="3332">noting</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="3334" end_char="3337">that</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="3339" end_char="3341">the</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="3343" end_char="3346">same</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="3348" end_char="3353">method</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="3355" end_char="3357">has</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="3359" end_char="3362">been</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="3364" end_char="3367">used</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="3369" end_char="3370">to</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="3372" end_char="3379">estimate</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="3381" end_char="3389">influenza</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="3391" end_char="3396">trends</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="3398" end_char="3399">in</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="3401" end_char="3405">China</TOKEN>
<TOKEN id="token-16-29" pos="punct" morph="none" start_char="3406" end_char="3406">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="3409" end_char="3585">
<ORIGINAL_TEXT>Between September and October 2019, 5 of the 6 hospitals studied show their highest relative daily volume of traffic in the series of images that were analysed, the study found.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="3409" end_char="3415">Between</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="3417" end_char="3425">September</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="3427" end_char="3429">and</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="3431" end_char="3437">October</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="3439" end_char="3442">2019</TOKEN>
<TOKEN id="token-17-5" pos="punct" morph="none" start_char="3443" end_char="3443">,</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="3445" end_char="3445">5</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="3447" end_char="3448">of</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="3450" end_char="3452">the</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="3454" end_char="3454">6</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="3456" end_char="3464">hospitals</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="3466" end_char="3472">studied</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="3474" end_char="3477">show</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="3479" end_char="3483">their</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="3485" end_char="3491">highest</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="3493" end_char="3500">relative</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="3502" end_char="3506">daily</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="3508" end_char="3513">volume</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="3515" end_char="3516">of</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="3518" end_char="3524">traffic</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="3526" end_char="3527">in</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="3529" end_char="3531">the</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="3533" end_char="3538">series</TOKEN>
<TOKEN id="token-17-23" pos="word" morph="none" start_char="3540" end_char="3541">of</TOKEN>
<TOKEN id="token-17-24" pos="word" morph="none" start_char="3543" end_char="3548">images</TOKEN>
<TOKEN id="token-17-25" pos="word" morph="none" start_char="3550" end_char="3553">that</TOKEN>
<TOKEN id="token-17-26" pos="word" morph="none" start_char="3555" end_char="3558">were</TOKEN>
<TOKEN id="token-17-27" pos="word" morph="none" start_char="3560" end_char="3567">analysed</TOKEN>
<TOKEN id="token-17-28" pos="punct" morph="none" start_char="3568" end_char="3568">,</TOKEN>
<TOKEN id="token-17-29" pos="word" morph="none" start_char="3570" end_char="3572">the</TOKEN>
<TOKEN id="token-17-30" pos="word" morph="none" start_char="3574" end_char="3578">study</TOKEN>
<TOKEN id="token-17-31" pos="word" morph="none" start_char="3580" end_char="3584">found</TOKEN>
<TOKEN id="token-17-32" pos="punct" morph="none" start_char="3585" end_char="3585">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="3587" end_char="3678">
<ORIGINAL_TEXT>"coinciding with elevated levels of Baidu search queries for the terms "diarrhea and cough".</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="punct" morph="none" start_char="3587" end_char="3587">"</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="3588" end_char="3597">coinciding</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="3599" end_char="3602">with</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="3604" end_char="3611">elevated</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="3613" end_char="3618">levels</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="3620" end_char="3621">of</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="3623" end_char="3627">Baidu</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="3629" end_char="3634">search</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="3636" end_char="3642">queries</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="3644" end_char="3646">for</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="3648" end_char="3650">the</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="3652" end_char="3656">terms</TOKEN>
<TOKEN id="token-18-12" pos="punct" morph="none" start_char="3658" end_char="3658">"</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="3659" end_char="3666">diarrhea</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="3668" end_char="3670">and</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="3672" end_char="3676">cough</TOKEN>
<TOKEN id="token-18-16" pos="punct" morph="none" start_char="3677" end_char="3678">".</TOKEN>
</SEG>
<SEG id="segment-19" start_char="3680" end_char="3828">
<ORIGINAL_TEXT>While searches for cough alone are typical of the influenza season, diarrhea has been one of the more distinctive symptoms marking onset of Covid-19.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="3680" end_char="3684">While</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="3686" end_char="3693">searches</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="3695" end_char="3697">for</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="3699" end_char="3703">cough</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="3705" end_char="3709">alone</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="3711" end_char="3713">are</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="3715" end_char="3721">typical</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="3723" end_char="3724">of</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="3726" end_char="3728">the</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="3730" end_char="3738">influenza</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="3740" end_char="3745">season</TOKEN>
<TOKEN id="token-19-11" pos="punct" morph="none" start_char="3746" end_char="3746">,</TOKEN>
<TOKEN id="token-19-12" pos="word" morph="none" start_char="3748" end_char="3755">diarrhea</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="3757" end_char="3759">has</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="3761" end_char="3764">been</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="3766" end_char="3768">one</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="3770" end_char="3771">of</TOKEN>
<TOKEN id="token-19-17" pos="word" morph="none" start_char="3773" end_char="3775">the</TOKEN>
<TOKEN id="token-19-18" pos="word" morph="none" start_char="3777" end_char="3780">more</TOKEN>
<TOKEN id="token-19-19" pos="word" morph="none" start_char="3782" end_char="3792">distinctive</TOKEN>
<TOKEN id="token-19-20" pos="word" morph="none" start_char="3794" end_char="3801">symptoms</TOKEN>
<TOKEN id="token-19-21" pos="word" morph="none" start_char="3803" end_char="3809">marking</TOKEN>
<TOKEN id="token-19-22" pos="word" morph="none" start_char="3811" end_char="3815">onset</TOKEN>
<TOKEN id="token-19-23" pos="word" morph="none" start_char="3817" end_char="3818">of</TOKEN>
<TOKEN id="token-19-24" pos="unknown" morph="none" start_char="3820" end_char="3827">Covid-19</TOKEN>
<TOKEN id="token-19-25" pos="punct" morph="none" start_char="3828" end_char="3828">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="3831" end_char="3892">
<ORIGINAL_TEXT>Chinese Officials Reject Findings of Study – Still in Preprint</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="3831" end_char="3837">Chinese</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="3839" end_char="3847">Officials</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="3849" end_char="3854">Reject</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="3856" end_char="3863">Findings</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="3865" end_char="3866">of</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="3868" end_char="3872">Study</TOKEN>
<TOKEN id="token-20-6" pos="punct" morph="none" start_char="3874" end_char="3874">–</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="3876" end_char="3880">Still</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="3882" end_char="3883">in</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="3885" end_char="3892">Preprint</TOKEN>
</SEG>
<SEG id="segment-21" start_char="3896" end_char="3943">
<ORIGINAL_TEXT>Hua Chunying, China Foreign Ministry Spokeswoman</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="3896" end_char="3898">Hua</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="3900" end_char="3907">Chunying</TOKEN>
<TOKEN id="token-21-2" pos="punct" morph="none" start_char="3908" end_char="3908">,</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="3910" end_char="3914">China</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="3916" end_char="3922">Foreign</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="3924" end_char="3931">Ministry</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="3933" end_char="3943">Spokeswoman</TOKEN>
</SEG>
<SEG id="segment-22" start_char="3947" end_char="4216">
<ORIGINAL_TEXT>Speaking at a press briefing on Tuesday, China’s foreign ministry spokeswoman Hua Chunying rejected the study’s findings, saying, "I think it is absurd, actually extremely absurd, to draw this kind of conclusion based on superficial observations such as traffic volume."</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="3947" end_char="3954">Speaking</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="3956" end_char="3957">at</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="3959" end_char="3959">a</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="3961" end_char="3965">press</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="3967" end_char="3974">briefing</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="3976" end_char="3977">on</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="3979" end_char="3985">Tuesday</TOKEN>
<TOKEN id="token-22-7" pos="punct" morph="none" start_char="3986" end_char="3986">,</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="3988" end_char="3994">China’s</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="3996" end_char="4002">foreign</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="4004" end_char="4011">ministry</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="4013" end_char="4023">spokeswoman</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="4025" end_char="4027">Hua</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="4029" end_char="4036">Chunying</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="4038" end_char="4045">rejected</TOKEN>
<TOKEN id="token-22-15" pos="word" morph="none" start_char="4047" end_char="4049">the</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="4051" end_char="4057">study’s</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="4059" end_char="4066">findings</TOKEN>
<TOKEN id="token-22-18" pos="punct" morph="none" start_char="4067" end_char="4067">,</TOKEN>
<TOKEN id="token-22-19" pos="word" morph="none" start_char="4069" end_char="4074">saying</TOKEN>
<TOKEN id="token-22-20" pos="punct" morph="none" start_char="4075" end_char="4075">,</TOKEN>
<TOKEN id="token-22-21" pos="punct" morph="none" start_char="4077" end_char="4077">"</TOKEN>
<TOKEN id="token-22-22" pos="word" morph="none" start_char="4078" end_char="4078">I</TOKEN>
<TOKEN id="token-22-23" pos="word" morph="none" start_char="4080" end_char="4084">think</TOKEN>
<TOKEN id="token-22-24" pos="word" morph="none" start_char="4086" end_char="4087">it</TOKEN>
<TOKEN id="token-22-25" pos="word" morph="none" start_char="4089" end_char="4090">is</TOKEN>
<TOKEN id="token-22-26" pos="word" morph="none" start_char="4092" end_char="4097">absurd</TOKEN>
<TOKEN id="token-22-27" pos="punct" morph="none" start_char="4098" end_char="4098">,</TOKEN>
<TOKEN id="token-22-28" pos="word" morph="none" start_char="4100" end_char="4107">actually</TOKEN>
<TOKEN id="token-22-29" pos="word" morph="none" start_char="4109" end_char="4117">extremely</TOKEN>
<TOKEN id="token-22-30" pos="word" morph="none" start_char="4119" end_char="4124">absurd</TOKEN>
<TOKEN id="token-22-31" pos="punct" morph="none" start_char="4125" end_char="4125">,</TOKEN>
<TOKEN id="token-22-32" pos="word" morph="none" start_char="4127" end_char="4128">to</TOKEN>
<TOKEN id="token-22-33" pos="word" morph="none" start_char="4130" end_char="4133">draw</TOKEN>
<TOKEN id="token-22-34" pos="word" morph="none" start_char="4135" end_char="4138">this</TOKEN>
<TOKEN id="token-22-35" pos="word" morph="none" start_char="4140" end_char="4143">kind</TOKEN>
<TOKEN id="token-22-36" pos="word" morph="none" start_char="4145" end_char="4146">of</TOKEN>
<TOKEN id="token-22-37" pos="word" morph="none" start_char="4148" end_char="4157">conclusion</TOKEN>
<TOKEN id="token-22-38" pos="word" morph="none" start_char="4159" end_char="4163">based</TOKEN>
<TOKEN id="token-22-39" pos="word" morph="none" start_char="4165" end_char="4166">on</TOKEN>
<TOKEN id="token-22-40" pos="word" morph="none" start_char="4168" end_char="4178">superficial</TOKEN>
<TOKEN id="token-22-41" pos="word" morph="none" start_char="4180" end_char="4191">observations</TOKEN>
<TOKEN id="token-22-42" pos="word" morph="none" start_char="4193" end_char="4196">such</TOKEN>
<TOKEN id="token-22-43" pos="word" morph="none" start_char="4198" end_char="4199">as</TOKEN>
<TOKEN id="token-22-44" pos="word" morph="none" start_char="4201" end_char="4207">traffic</TOKEN>
<TOKEN id="token-22-45" pos="word" morph="none" start_char="4209" end_char="4214">volume</TOKEN>
<TOKEN id="token-22-46" pos="punct" morph="none" start_char="4215" end_char="4216">."</TOKEN>
</SEG>
<SEG id="segment-23" start_char="4219" end_char="4492">
<ORIGINAL_TEXT>Other experts also urged caution in interpreting the study’s results: "It’s important to remember that the data are only correlative and (as the authors admit) cannot identify the cause of the uptick," said Paul Digard, a virologist professor at the University of Edinburgh.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="4219" end_char="4223">Other</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="4225" end_char="4231">experts</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="4233" end_char="4236">also</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="4238" end_char="4242">urged</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="4244" end_char="4250">caution</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="4252" end_char="4253">in</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="4255" end_char="4266">interpreting</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="4268" end_char="4270">the</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="4272" end_char="4278">study’s</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="4280" end_char="4286">results</TOKEN>
<TOKEN id="token-23-10" pos="punct" morph="none" start_char="4287" end_char="4287">:</TOKEN>
<TOKEN id="token-23-11" pos="punct" morph="none" start_char="4289" end_char="4289">"</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="4290" end_char="4293">It’s</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="4295" end_char="4303">important</TOKEN>
<TOKEN id="token-23-14" pos="word" morph="none" start_char="4305" end_char="4306">to</TOKEN>
<TOKEN id="token-23-15" pos="word" morph="none" start_char="4308" end_char="4315">remember</TOKEN>
<TOKEN id="token-23-16" pos="word" morph="none" start_char="4317" end_char="4320">that</TOKEN>
<TOKEN id="token-23-17" pos="word" morph="none" start_char="4322" end_char="4324">the</TOKEN>
<TOKEN id="token-23-18" pos="word" morph="none" start_char="4326" end_char="4329">data</TOKEN>
<TOKEN id="token-23-19" pos="word" morph="none" start_char="4331" end_char="4333">are</TOKEN>
<TOKEN id="token-23-20" pos="word" morph="none" start_char="4335" end_char="4338">only</TOKEN>
<TOKEN id="token-23-21" pos="word" morph="none" start_char="4340" end_char="4350">correlative</TOKEN>
<TOKEN id="token-23-22" pos="word" morph="none" start_char="4352" end_char="4354">and</TOKEN>
<TOKEN id="token-23-23" pos="punct" morph="none" start_char="4356" end_char="4356">(</TOKEN>
<TOKEN id="token-23-24" pos="word" morph="none" start_char="4357" end_char="4358">as</TOKEN>
<TOKEN id="token-23-25" pos="word" morph="none" start_char="4360" end_char="4362">the</TOKEN>
<TOKEN id="token-23-26" pos="word" morph="none" start_char="4364" end_char="4370">authors</TOKEN>
<TOKEN id="token-23-27" pos="word" morph="none" start_char="4372" end_char="4376">admit</TOKEN>
<TOKEN id="token-23-28" pos="punct" morph="none" start_char="4377" end_char="4377">)</TOKEN>
<TOKEN id="token-23-29" pos="word" morph="none" start_char="4379" end_char="4384">cannot</TOKEN>
<TOKEN id="token-23-30" pos="word" morph="none" start_char="4386" end_char="4393">identify</TOKEN>
<TOKEN id="token-23-31" pos="word" morph="none" start_char="4395" end_char="4397">the</TOKEN>
<TOKEN id="token-23-32" pos="word" morph="none" start_char="4399" end_char="4403">cause</TOKEN>
<TOKEN id="token-23-33" pos="word" morph="none" start_char="4405" end_char="4406">of</TOKEN>
<TOKEN id="token-23-34" pos="word" morph="none" start_char="4408" end_char="4410">the</TOKEN>
<TOKEN id="token-23-35" pos="word" morph="none" start_char="4412" end_char="4417">uptick</TOKEN>
<TOKEN id="token-23-36" pos="punct" morph="none" start_char="4418" end_char="4419">,"</TOKEN>
<TOKEN id="token-23-37" pos="word" morph="none" start_char="4421" end_char="4424">said</TOKEN>
<TOKEN id="token-23-38" pos="word" morph="none" start_char="4426" end_char="4429">Paul</TOKEN>
<TOKEN id="token-23-39" pos="word" morph="none" start_char="4431" end_char="4436">Digard</TOKEN>
<TOKEN id="token-23-40" pos="punct" morph="none" start_char="4437" end_char="4437">,</TOKEN>
<TOKEN id="token-23-41" pos="word" morph="none" start_char="4439" end_char="4439">a</TOKEN>
<TOKEN id="token-23-42" pos="word" morph="none" start_char="4441" end_char="4450">virologist</TOKEN>
<TOKEN id="token-23-43" pos="word" morph="none" start_char="4452" end_char="4460">professor</TOKEN>
<TOKEN id="token-23-44" pos="word" morph="none" start_char="4462" end_char="4463">at</TOKEN>
<TOKEN id="token-23-45" pos="word" morph="none" start_char="4465" end_char="4467">the</TOKEN>
<TOKEN id="token-23-46" pos="word" morph="none" start_char="4469" end_char="4478">University</TOKEN>
<TOKEN id="token-23-47" pos="word" morph="none" start_char="4480" end_char="4481">of</TOKEN>
<TOKEN id="token-23-48" pos="word" morph="none" start_char="4483" end_char="4491">Edinburgh</TOKEN>
<TOKEN id="token-23-49" pos="punct" morph="none" start_char="4492" end_char="4492">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="4494" end_char="4606">
<ORIGINAL_TEXT>"By focusing on hospitals in Wuhan, the acknowledged epicentre of the outbreak, the study forces the correlation.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="punct" morph="none" start_char="4494" end_char="4494">"</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="4495" end_char="4496">By</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="4498" end_char="4505">focusing</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="4507" end_char="4508">on</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="4510" end_char="4518">hospitals</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="4520" end_char="4521">in</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="4523" end_char="4527">Wuhan</TOKEN>
<TOKEN id="token-24-7" pos="punct" morph="none" start_char="4528" end_char="4528">,</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="4530" end_char="4532">the</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="4534" end_char="4545">acknowledged</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="4547" end_char="4555">epicentre</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="4557" end_char="4558">of</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="4560" end_char="4562">the</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="4564" end_char="4571">outbreak</TOKEN>
<TOKEN id="token-24-14" pos="punct" morph="none" start_char="4572" end_char="4572">,</TOKEN>
<TOKEN id="token-24-15" pos="word" morph="none" start_char="4574" end_char="4576">the</TOKEN>
<TOKEN id="token-24-16" pos="word" morph="none" start_char="4578" end_char="4582">study</TOKEN>
<TOKEN id="token-24-17" pos="word" morph="none" start_char="4584" end_char="4589">forces</TOKEN>
<TOKEN id="token-24-18" pos="word" morph="none" start_char="4591" end_char="4593">the</TOKEN>
<TOKEN id="token-24-19" pos="word" morph="none" start_char="4595" end_char="4605">correlation</TOKEN>
<TOKEN id="token-24-20" pos="punct" morph="none" start_char="4606" end_char="4606">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="4608" end_char="4766">
<ORIGINAL_TEXT>It would have been interesting (and possibly much more convincing) to have seen control analyses of other Chinese cities outside of the Hubei region," he said.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="4608" end_char="4609">It</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="4611" end_char="4615">would</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="4617" end_char="4620">have</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="4622" end_char="4625">been</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="4627" end_char="4637">interesting</TOKEN>
<TOKEN id="token-25-5" pos="punct" morph="none" start_char="4639" end_char="4639">(</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="4640" end_char="4642">and</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="4644" end_char="4651">possibly</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="4653" end_char="4656">much</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="4658" end_char="4661">more</TOKEN>
<TOKEN id="token-25-10" pos="word" morph="none" start_char="4663" end_char="4672">convincing</TOKEN>
<TOKEN id="token-25-11" pos="punct" morph="none" start_char="4673" end_char="4673">)</TOKEN>
<TOKEN id="token-25-12" pos="word" morph="none" start_char="4675" end_char="4676">to</TOKEN>
<TOKEN id="token-25-13" pos="word" morph="none" start_char="4678" end_char="4681">have</TOKEN>
<TOKEN id="token-25-14" pos="word" morph="none" start_char="4683" end_char="4686">seen</TOKEN>
<TOKEN id="token-25-15" pos="word" morph="none" start_char="4688" end_char="4694">control</TOKEN>
<TOKEN id="token-25-16" pos="word" morph="none" start_char="4696" end_char="4703">analyses</TOKEN>
<TOKEN id="token-25-17" pos="word" morph="none" start_char="4705" end_char="4706">of</TOKEN>
<TOKEN id="token-25-18" pos="word" morph="none" start_char="4708" end_char="4712">other</TOKEN>
<TOKEN id="token-25-19" pos="word" morph="none" start_char="4714" end_char="4720">Chinese</TOKEN>
<TOKEN id="token-25-20" pos="word" morph="none" start_char="4722" end_char="4727">cities</TOKEN>
<TOKEN id="token-25-21" pos="word" morph="none" start_char="4729" end_char="4735">outside</TOKEN>
<TOKEN id="token-25-22" pos="word" morph="none" start_char="4737" end_char="4738">of</TOKEN>
<TOKEN id="token-25-23" pos="word" morph="none" start_char="4740" end_char="4742">the</TOKEN>
<TOKEN id="token-25-24" pos="word" morph="none" start_char="4744" end_char="4748">Hubei</TOKEN>
<TOKEN id="token-25-25" pos="word" morph="none" start_char="4750" end_char="4755">region</TOKEN>
<TOKEN id="token-25-26" pos="punct" morph="none" start_char="4756" end_char="4757">,"</TOKEN>
<TOKEN id="token-25-27" pos="word" morph="none" start_char="4759" end_char="4760">he</TOKEN>
<TOKEN id="token-25-28" pos="word" morph="none" start_char="4762" end_char="4765">said</TOKEN>
<TOKEN id="token-25-29" pos="punct" morph="none" start_char="4766" end_char="4766">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="4769" end_char="4860">
<ORIGINAL_TEXT>The study is still in its preprint form, and had not undergone peer review, they also noted.</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="4769" end_char="4771">The</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="4773" end_char="4777">study</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="4779" end_char="4780">is</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="4782" end_char="4786">still</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="4788" end_char="4789">in</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="4791" end_char="4793">its</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="4795" end_char="4802">preprint</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="4804" end_char="4807">form</TOKEN>
<TOKEN id="token-26-8" pos="punct" morph="none" start_char="4808" end_char="4808">,</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="4810" end_char="4812">and</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="4814" end_char="4816">had</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="4818" end_char="4820">not</TOKEN>
<TOKEN id="token-26-12" pos="word" morph="none" start_char="4822" end_char="4830">undergone</TOKEN>
<TOKEN id="token-26-13" pos="word" morph="none" start_char="4832" end_char="4835">peer</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="4837" end_char="4842">review</TOKEN>
<TOKEN id="token-26-15" pos="punct" morph="none" start_char="4843" end_char="4843">,</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="4845" end_char="4848">they</TOKEN>
<TOKEN id="token-26-17" pos="word" morph="none" start_char="4850" end_char="4853">also</TOKEN>
<TOKEN id="token-26-18" pos="word" morph="none" start_char="4855" end_char="4859">noted</TOKEN>
<TOKEN id="token-26-19" pos="punct" morph="none" start_char="4860" end_char="4860">.</TOKEN>
</SEG>
<SEG id="segment-27" start_char="4862" end_char="5151">
<ORIGINAL_TEXT>This seemed apparent from one obvious error in the preprint PDF, which refers to a: "large decrease in hospital [traffic] voume and search query data, following the public health lockdown of Wuhan on January 23, 2019" – an apparently erroneous reference to the lockdown of January 23, 2020.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="4862" end_char="4865">This</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="4867" end_char="4872">seemed</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="4874" end_char="4881">apparent</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="4883" end_char="4886">from</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="4888" end_char="4890">one</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="4892" end_char="4898">obvious</TOKEN>
<TOKEN id="token-27-6" pos="word" morph="none" start_char="4900" end_char="4904">error</TOKEN>
<TOKEN id="token-27-7" pos="word" morph="none" start_char="4906" end_char="4907">in</TOKEN>
<TOKEN id="token-27-8" pos="word" morph="none" start_char="4909" end_char="4911">the</TOKEN>
<TOKEN id="token-27-9" pos="word" morph="none" start_char="4913" end_char="4920">preprint</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="4922" end_char="4924">PDF</TOKEN>
<TOKEN id="token-27-11" pos="punct" morph="none" start_char="4925" end_char="4925">,</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="4927" end_char="4931">which</TOKEN>
<TOKEN id="token-27-13" pos="word" morph="none" start_char="4933" end_char="4938">refers</TOKEN>
<TOKEN id="token-27-14" pos="word" morph="none" start_char="4940" end_char="4941">to</TOKEN>
<TOKEN id="token-27-15" pos="word" morph="none" start_char="4943" end_char="4943">a</TOKEN>
<TOKEN id="token-27-16" pos="punct" morph="none" start_char="4944" end_char="4944">:</TOKEN>
<TOKEN id="token-27-17" pos="punct" morph="none" start_char="4946" end_char="4946">"</TOKEN>
<TOKEN id="token-27-18" pos="word" morph="none" start_char="4947" end_char="4951">large</TOKEN>
<TOKEN id="token-27-19" pos="word" morph="none" start_char="4953" end_char="4960">decrease</TOKEN>
<TOKEN id="token-27-20" pos="word" morph="none" start_char="4962" end_char="4963">in</TOKEN>
<TOKEN id="token-27-21" pos="word" morph="none" start_char="4965" end_char="4972">hospital</TOKEN>
<TOKEN id="token-27-22" pos="punct" morph="none" start_char="4974" end_char="4974">[</TOKEN>
<TOKEN id="token-27-23" pos="word" morph="none" start_char="4975" end_char="4981">traffic</TOKEN>
<TOKEN id="token-27-24" pos="punct" morph="none" start_char="4982" end_char="4982">]</TOKEN>
<TOKEN id="token-27-25" pos="word" morph="none" start_char="4984" end_char="4988">voume</TOKEN>
<TOKEN id="token-27-26" pos="word" morph="none" start_char="4990" end_char="4992">and</TOKEN>
<TOKEN id="token-27-27" pos="word" morph="none" start_char="4994" end_char="4999">search</TOKEN>
<TOKEN id="token-27-28" pos="word" morph="none" start_char="5001" end_char="5005">query</TOKEN>
<TOKEN id="token-27-29" pos="word" morph="none" start_char="5007" end_char="5010">data</TOKEN>
<TOKEN id="token-27-30" pos="punct" morph="none" start_char="5011" end_char="5011">,</TOKEN>
<TOKEN id="token-27-31" pos="word" morph="none" start_char="5013" end_char="5021">following</TOKEN>
<TOKEN id="token-27-32" pos="word" morph="none" start_char="5023" end_char="5025">the</TOKEN>
<TOKEN id="token-27-33" pos="word" morph="none" start_char="5027" end_char="5032">public</TOKEN>
<TOKEN id="token-27-34" pos="word" morph="none" start_char="5034" end_char="5039">health</TOKEN>
<TOKEN id="token-27-35" pos="word" morph="none" start_char="5041" end_char="5048">lockdown</TOKEN>
<TOKEN id="token-27-36" pos="word" morph="none" start_char="5050" end_char="5051">of</TOKEN>
<TOKEN id="token-27-37" pos="word" morph="none" start_char="5053" end_char="5057">Wuhan</TOKEN>
<TOKEN id="token-27-38" pos="word" morph="none" start_char="5059" end_char="5060">on</TOKEN>
<TOKEN id="token-27-39" pos="word" morph="none" start_char="5062" end_char="5068">January</TOKEN>
<TOKEN id="token-27-40" pos="word" morph="none" start_char="5070" end_char="5071">23</TOKEN>
<TOKEN id="token-27-41" pos="punct" morph="none" start_char="5072" end_char="5072">,</TOKEN>
<TOKEN id="token-27-42" pos="word" morph="none" start_char="5074" end_char="5077">2019</TOKEN>
<TOKEN id="token-27-43" pos="punct" morph="none" start_char="5078" end_char="5078">"</TOKEN>
<TOKEN id="token-27-44" pos="punct" morph="none" start_char="5080" end_char="5080">–</TOKEN>
<TOKEN id="token-27-45" pos="word" morph="none" start_char="5082" end_char="5083">an</TOKEN>
<TOKEN id="token-27-46" pos="word" morph="none" start_char="5085" end_char="5094">apparently</TOKEN>
<TOKEN id="token-27-47" pos="word" morph="none" start_char="5096" end_char="5104">erroneous</TOKEN>
<TOKEN id="token-27-48" pos="word" morph="none" start_char="5106" end_char="5114">reference</TOKEN>
<TOKEN id="token-27-49" pos="word" morph="none" start_char="5116" end_char="5117">to</TOKEN>
<TOKEN id="token-27-50" pos="word" morph="none" start_char="5119" end_char="5121">the</TOKEN>
<TOKEN id="token-27-51" pos="word" morph="none" start_char="5123" end_char="5130">lockdown</TOKEN>
<TOKEN id="token-27-52" pos="word" morph="none" start_char="5132" end_char="5133">of</TOKEN>
<TOKEN id="token-27-53" pos="word" morph="none" start_char="5135" end_char="5141">January</TOKEN>
<TOKEN id="token-27-54" pos="word" morph="none" start_char="5143" end_char="5144">23</TOKEN>
<TOKEN id="token-27-55" pos="punct" morph="none" start_char="5145" end_char="5145">,</TOKEN>
<TOKEN id="token-27-56" pos="word" morph="none" start_char="5147" end_char="5150">2020</TOKEN>
<TOKEN id="token-27-57" pos="punct" morph="none" start_char="5151" end_char="5151">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="5154" end_char="5214">
<ORIGINAL_TEXT>Image Credits: José Mauquer , Nsoesie, Elaine Okanyene et al.</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="5154" end_char="5158">Image</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="5160" end_char="5166">Credits</TOKEN>
<TOKEN id="token-28-2" pos="punct" morph="none" start_char="5167" end_char="5167">:</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="5169" end_char="5172">José</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="5174" end_char="5180">Mauquer</TOKEN>
<TOKEN id="token-28-5" pos="punct" morph="none" start_char="5182" end_char="5182">,</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="5184" end_char="5190">Nsoesie</TOKEN>
<TOKEN id="token-28-7" pos="punct" morph="none" start_char="5191" end_char="5191">,</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="5193" end_char="5198">Elaine</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="5200" end_char="5207">Okanyene</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="5209" end_char="5210">et</TOKEN>
<TOKEN id="token-28-11" pos="word" morph="none" start_char="5212" end_char="5213">al</TOKEN>
<TOKEN id="token-28-12" pos="punct" morph="none" start_char="5214" end_char="5214">.</TOKEN>
</SEG>
<SEG id="segment-29" start_char="5216" end_char="5378">
<ORIGINAL_TEXT>Harvard University Pre-print Repository, , Analysis of hospital traffic and search engine data in Wuhan China indicates early disease activity in the Fall of 2019.</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="5216" end_char="5222">Harvard</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="5224" end_char="5233">University</TOKEN>
<TOKEN id="token-29-2" pos="unknown" morph="none" start_char="5235" end_char="5243">Pre-print</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="5245" end_char="5254">Repository</TOKEN>
<TOKEN id="token-29-4" pos="punct" morph="none" start_char="5255" end_char="5255">,</TOKEN>
<TOKEN id="token-29-5" pos="punct" morph="none" start_char="5257" end_char="5257">,</TOKEN>
<TOKEN id="token-29-6" pos="word" morph="none" start_char="5259" end_char="5266">Analysis</TOKEN>
<TOKEN id="token-29-7" pos="word" morph="none" start_char="5268" end_char="5269">of</TOKEN>
<TOKEN id="token-29-8" pos="word" morph="none" start_char="5271" end_char="5278">hospital</TOKEN>
<TOKEN id="token-29-9" pos="word" morph="none" start_char="5280" end_char="5286">traffic</TOKEN>
<TOKEN id="token-29-10" pos="word" morph="none" start_char="5288" end_char="5290">and</TOKEN>
<TOKEN id="token-29-11" pos="word" morph="none" start_char="5292" end_char="5297">search</TOKEN>
<TOKEN id="token-29-12" pos="word" morph="none" start_char="5299" end_char="5304">engine</TOKEN>
<TOKEN id="token-29-13" pos="word" morph="none" start_char="5306" end_char="5309">data</TOKEN>
<TOKEN id="token-29-14" pos="word" morph="none" start_char="5311" end_char="5312">in</TOKEN>
<TOKEN id="token-29-15" pos="word" morph="none" start_char="5314" end_char="5318">Wuhan</TOKEN>
<TOKEN id="token-29-16" pos="word" morph="none" start_char="5320" end_char="5324">China</TOKEN>
<TOKEN id="token-29-17" pos="word" morph="none" start_char="5326" end_char="5334">indicates</TOKEN>
<TOKEN id="token-29-18" pos="word" morph="none" start_char="5336" end_char="5340">early</TOKEN>
<TOKEN id="token-29-19" pos="word" morph="none" start_char="5342" end_char="5348">disease</TOKEN>
<TOKEN id="token-29-20" pos="word" morph="none" start_char="5350" end_char="5357">activity</TOKEN>
<TOKEN id="token-29-21" pos="word" morph="none" start_char="5359" end_char="5360">in</TOKEN>
<TOKEN id="token-29-22" pos="word" morph="none" start_char="5362" end_char="5364">the</TOKEN>
<TOKEN id="token-29-23" pos="word" morph="none" start_char="5366" end_char="5369">Fall</TOKEN>
<TOKEN id="token-29-24" pos="word" morph="none" start_char="5371" end_char="5372">of</TOKEN>
<TOKEN id="token-29-25" pos="word" morph="none" start_char="5374" end_char="5377">2019</TOKEN>
<TOKEN id="token-29-26" pos="punct" morph="none" start_char="5378" end_char="5378">.</TOKEN>
</SEG>
<SEG id="segment-30" start_char="5381" end_char="5481">
<ORIGINAL_TEXT>Combat the infodemic in health information and support health policy reporting from the global South.</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="5381" end_char="5386">Combat</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="5388" end_char="5390">the</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="5392" end_char="5400">infodemic</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="5402" end_char="5403">in</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="5405" end_char="5410">health</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="5412" end_char="5422">information</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="5424" end_char="5426">and</TOKEN>
<TOKEN id="token-30-7" pos="word" morph="none" start_char="5428" end_char="5434">support</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="5436" end_char="5441">health</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="5443" end_char="5448">policy</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="5450" end_char="5458">reporting</TOKEN>
<TOKEN id="token-30-11" pos="word" morph="none" start_char="5460" end_char="5463">from</TOKEN>
<TOKEN id="token-30-12" pos="word" morph="none" start_char="5465" end_char="5467">the</TOKEN>
<TOKEN id="token-30-13" pos="word" morph="none" start_char="5469" end_char="5474">global</TOKEN>
<TOKEN id="token-30-14" pos="word" morph="none" start_char="5476" end_char="5480">South</TOKEN>
<TOKEN id="token-30-15" pos="punct" morph="none" start_char="5481" end_char="5481">.</TOKEN>
</SEG>
<SEG id="segment-31" start_char="5483" end_char="5677">
<ORIGINAL_TEXT>Our growing network of journalists in Africa, Asia, Geneva and New York connect the dots between regional realities and the big global debates, with evidence-based, open access news and analysis.</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="word" morph="none" start_char="5483" end_char="5485">Our</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="5487" end_char="5493">growing</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="5495" end_char="5501">network</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="5503" end_char="5504">of</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="5506" end_char="5516">journalists</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="5518" end_char="5519">in</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="5521" end_char="5526">Africa</TOKEN>
<TOKEN id="token-31-7" pos="punct" morph="none" start_char="5527" end_char="5527">,</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="5529" end_char="5532">Asia</TOKEN>
<TOKEN id="token-31-9" pos="punct" morph="none" start_char="5533" end_char="5533">,</TOKEN>
<TOKEN id="token-31-10" pos="word" morph="none" start_char="5535" end_char="5540">Geneva</TOKEN>
<TOKEN id="token-31-11" pos="word" morph="none" start_char="5542" end_char="5544">and</TOKEN>
<TOKEN id="token-31-12" pos="word" morph="none" start_char="5546" end_char="5548">New</TOKEN>
<TOKEN id="token-31-13" pos="word" morph="none" start_char="5550" end_char="5553">York</TOKEN>
<TOKEN id="token-31-14" pos="word" morph="none" start_char="5555" end_char="5561">connect</TOKEN>
<TOKEN id="token-31-15" pos="word" morph="none" start_char="5563" end_char="5565">the</TOKEN>
<TOKEN id="token-31-16" pos="word" morph="none" start_char="5567" end_char="5570">dots</TOKEN>
<TOKEN id="token-31-17" pos="word" morph="none" start_char="5572" end_char="5578">between</TOKEN>
<TOKEN id="token-31-18" pos="word" morph="none" start_char="5580" end_char="5587">regional</TOKEN>
<TOKEN id="token-31-19" pos="word" morph="none" start_char="5589" end_char="5597">realities</TOKEN>
<TOKEN id="token-31-20" pos="word" morph="none" start_char="5599" end_char="5601">and</TOKEN>
<TOKEN id="token-31-21" pos="word" morph="none" start_char="5603" end_char="5605">the</TOKEN>
<TOKEN id="token-31-22" pos="word" morph="none" start_char="5607" end_char="5609">big</TOKEN>
<TOKEN id="token-31-23" pos="word" morph="none" start_char="5611" end_char="5616">global</TOKEN>
<TOKEN id="token-31-24" pos="word" morph="none" start_char="5618" end_char="5624">debates</TOKEN>
<TOKEN id="token-31-25" pos="punct" morph="none" start_char="5625" end_char="5625">,</TOKEN>
<TOKEN id="token-31-26" pos="word" morph="none" start_char="5627" end_char="5630">with</TOKEN>
<TOKEN id="token-31-27" pos="unknown" morph="none" start_char="5632" end_char="5645">evidence-based</TOKEN>
<TOKEN id="token-31-28" pos="punct" morph="none" start_char="5646" end_char="5646">,</TOKEN>
<TOKEN id="token-31-29" pos="word" morph="none" start_char="5648" end_char="5651">open</TOKEN>
<TOKEN id="token-31-30" pos="word" morph="none" start_char="5653" end_char="5658">access</TOKEN>
<TOKEN id="token-31-31" pos="word" morph="none" start_char="5660" end_char="5663">news</TOKEN>
<TOKEN id="token-31-32" pos="word" morph="none" start_char="5665" end_char="5667">and</TOKEN>
<TOKEN id="token-31-33" pos="word" morph="none" start_char="5669" end_char="5676">analysis</TOKEN>
<TOKEN id="token-31-34" pos="punct" morph="none" start_char="5677" end_char="5677">.</TOKEN>
</SEG>
<SEG id="segment-32" start_char="5679" end_char="5749">
<ORIGINAL_TEXT>To make a personal or organisational contribution click here on PayPal.</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="5679" end_char="5680">To</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="5682" end_char="5685">make</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="5687" end_char="5687">a</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="5689" end_char="5696">personal</TOKEN>
<TOKEN id="token-32-4" pos="word" morph="none" start_char="5698" end_char="5699">or</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="5701" end_char="5714">organisational</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="5716" end_char="5727">contribution</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="5729" end_char="5733">click</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="5735" end_char="5738">here</TOKEN>
<TOKEN id="token-32-9" pos="word" morph="none" start_char="5740" end_char="5741">on</TOKEN>
<TOKEN id="token-32-10" pos="word" morph="none" start_char="5743" end_char="5748">PayPal</TOKEN>
<TOKEN id="token-32-11" pos="punct" morph="none" start_char="5749" end_char="5749">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
