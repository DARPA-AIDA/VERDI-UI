<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C0495BI" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="3550" raw_text_md5="2dcb8f9db1247800bf0e6a2954602cae">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="117">
<ORIGINAL_TEXT>El documento que prueba, de acuerdo a cinco servicios de inteligencia, que China creó el Covid-19 en sus laboratorios</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="2">El</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="4" end_char="12">documento</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="14" end_char="16">que</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="18" end_char="23">prueba</TOKEN>
<TOKEN id="token-0-4" pos="punct" morph="none" start_char="24" end_char="24">,</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="26" end_char="27">de</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="29" end_char="35">acuerdo</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="37" end_char="37">a</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="39" end_char="43">cinco</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="45" end_char="53">servicios</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="55" end_char="56">de</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="58" end_char="69">inteligencia</TOKEN>
<TOKEN id="token-0-12" pos="punct" morph="none" start_char="70" end_char="70">,</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="72" end_char="74">que</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="76" end_char="80">China</TOKEN>
<TOKEN id="token-0-15" pos="word" morph="none" start_char="82" end_char="85">creó</TOKEN>
<TOKEN id="token-0-16" pos="word" morph="none" start_char="87" end_char="88">el</TOKEN>
<TOKEN id="token-0-17" pos="unknown" morph="none" start_char="90" end_char="97">Covid-19</TOKEN>
<TOKEN id="token-0-18" pos="word" morph="none" start_char="99" end_char="100">en</TOKEN>
<TOKEN id="token-0-19" pos="word" morph="none" start_char="102" end_char="104">sus</TOKEN>
<TOKEN id="token-0-20" pos="word" morph="none" start_char="106" end_char="117">laboratorios</TOKEN>
</SEG>
<SEG id="segment-1" start_char="121" end_char="144">
<ORIGINAL_TEXT>Escándalo internacional.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="121" end_char="129">Escándalo</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="131" end_char="143">internacional</TOKEN>
<TOKEN id="token-1-2" pos="punct" morph="none" start_char="144" end_char="144">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="146" end_char="411">
<ORIGINAL_TEXT>Mike Pompeo, secretario de Estado de Donald Trump, hizo público en las últimas horas una alarmante informe realizado por el servicio Five Eyes (cinco ojos), una alianza de inteligencia integrada por Australia, Canadá, Nueva Zelanda, el Reino Unidos y Estados Unidos.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="146" end_char="149">Mike</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="151" end_char="156">Pompeo</TOKEN>
<TOKEN id="token-2-2" pos="punct" morph="none" start_char="157" end_char="157">,</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="159" end_char="168">secretario</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="170" end_char="171">de</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="173" end_char="178">Estado</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="180" end_char="181">de</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="183" end_char="188">Donald</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="190" end_char="194">Trump</TOKEN>
<TOKEN id="token-2-9" pos="punct" morph="none" start_char="195" end_char="195">,</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="197" end_char="200">hizo</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="202" end_char="208">público</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="210" end_char="211">en</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="213" end_char="215">las</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="217" end_char="223">últimas</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="225" end_char="229">horas</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="231" end_char="233">una</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="235" end_char="243">alarmante</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="245" end_char="251">informe</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="253" end_char="261">realizado</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="263" end_char="265">por</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="267" end_char="268">el</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="270" end_char="277">servicio</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="279" end_char="282">Five</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="284" end_char="287">Eyes</TOKEN>
<TOKEN id="token-2-25" pos="punct" morph="none" start_char="289" end_char="289">(</TOKEN>
<TOKEN id="token-2-26" pos="word" morph="none" start_char="290" end_char="294">cinco</TOKEN>
<TOKEN id="token-2-27" pos="word" morph="none" start_char="296" end_char="299">ojos</TOKEN>
<TOKEN id="token-2-28" pos="punct" morph="none" start_char="300" end_char="301">),</TOKEN>
<TOKEN id="token-2-29" pos="word" morph="none" start_char="303" end_char="305">una</TOKEN>
<TOKEN id="token-2-30" pos="word" morph="none" start_char="307" end_char="313">alianza</TOKEN>
<TOKEN id="token-2-31" pos="word" morph="none" start_char="315" end_char="316">de</TOKEN>
<TOKEN id="token-2-32" pos="word" morph="none" start_char="318" end_char="329">inteligencia</TOKEN>
<TOKEN id="token-2-33" pos="word" morph="none" start_char="331" end_char="339">integrada</TOKEN>
<TOKEN id="token-2-34" pos="word" morph="none" start_char="341" end_char="343">por</TOKEN>
<TOKEN id="token-2-35" pos="word" morph="none" start_char="345" end_char="353">Australia</TOKEN>
<TOKEN id="token-2-36" pos="punct" morph="none" start_char="354" end_char="354">,</TOKEN>
<TOKEN id="token-2-37" pos="word" morph="none" start_char="356" end_char="361">Canadá</TOKEN>
<TOKEN id="token-2-38" pos="punct" morph="none" start_char="362" end_char="362">,</TOKEN>
<TOKEN id="token-2-39" pos="word" morph="none" start_char="364" end_char="368">Nueva</TOKEN>
<TOKEN id="token-2-40" pos="word" morph="none" start_char="370" end_char="376">Zelanda</TOKEN>
<TOKEN id="token-2-41" pos="punct" morph="none" start_char="377" end_char="377">,</TOKEN>
<TOKEN id="token-2-42" pos="word" morph="none" start_char="379" end_char="380">el</TOKEN>
<TOKEN id="token-2-43" pos="word" morph="none" start_char="382" end_char="386">Reino</TOKEN>
<TOKEN id="token-2-44" pos="word" morph="none" start_char="388" end_char="393">Unidos</TOKEN>
<TOKEN id="token-2-45" pos="word" morph="none" start_char="395" end_char="395">y</TOKEN>
<TOKEN id="token-2-46" pos="word" morph="none" start_char="397" end_char="403">Estados</TOKEN>
<TOKEN id="token-2-47" pos="word" morph="none" start_char="405" end_char="410">Unidos</TOKEN>
<TOKEN id="token-2-48" pos="punct" morph="none" start_char="411" end_char="411">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="413" end_char="646">
<ORIGINAL_TEXT>De acuerdo a la investigación oficial, la entidad asegura que el Covid-19 fue creado por científicos chinos en el laboratorio de Wuhan y denunció que el Gobierno de Xi Jinping ocultó desde diciembre la información del brote del virus.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="413" end_char="414">De</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="416" end_char="422">acuerdo</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="424" end_char="424">a</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="426" end_char="427">la</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="429" end_char="441">investigación</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="443" end_char="449">oficial</TOKEN>
<TOKEN id="token-3-6" pos="punct" morph="none" start_char="450" end_char="450">,</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="452" end_char="453">la</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="455" end_char="461">entidad</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="463" end_char="469">asegura</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="471" end_char="473">que</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="475" end_char="476">el</TOKEN>
<TOKEN id="token-3-12" pos="unknown" morph="none" start_char="478" end_char="485">Covid-19</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="487" end_char="489">fue</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="491" end_char="496">creado</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="498" end_char="500">por</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="502" end_char="512">científicos</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="514" end_char="519">chinos</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="521" end_char="522">en</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="524" end_char="525">el</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="527" end_char="537">laboratorio</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="539" end_char="540">de</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="542" end_char="546">Wuhan</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="548" end_char="548">y</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="550" end_char="557">denunció</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="559" end_char="561">que</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="563" end_char="564">el</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="566" end_char="573">Gobierno</TOKEN>
<TOKEN id="token-3-28" pos="word" morph="none" start_char="575" end_char="576">de</TOKEN>
<TOKEN id="token-3-29" pos="word" morph="none" start_char="578" end_char="579">Xi</TOKEN>
<TOKEN id="token-3-30" pos="word" morph="none" start_char="581" end_char="587">Jinping</TOKEN>
<TOKEN id="token-3-31" pos="word" morph="none" start_char="589" end_char="594">ocultó</TOKEN>
<TOKEN id="token-3-32" pos="word" morph="none" start_char="596" end_char="600">desde</TOKEN>
<TOKEN id="token-3-33" pos="word" morph="none" start_char="602" end_char="610">diciembre</TOKEN>
<TOKEN id="token-3-34" pos="word" morph="none" start_char="612" end_char="613">la</TOKEN>
<TOKEN id="token-3-35" pos="word" morph="none" start_char="615" end_char="625">información</TOKEN>
<TOKEN id="token-3-36" pos="word" morph="none" start_char="627" end_char="629">del</TOKEN>
<TOKEN id="token-3-37" pos="word" morph="none" start_char="631" end_char="635">brote</TOKEN>
<TOKEN id="token-3-38" pos="word" morph="none" start_char="637" end_char="639">del</TOKEN>
<TOKEN id="token-3-39" pos="word" morph="none" start_char="641" end_char="645">virus</TOKEN>
<TOKEN id="token-3-40" pos="punct" morph="none" start_char="646" end_char="646">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="649" end_char="790">
<ORIGINAL_TEXT>"Hay una enorme cantidad de pruebas que sostienen que fue allí donde comenzó", sostuvo Pompeo, en alusión al cuestionado laboratorio de Wuhan.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="punct" morph="none" start_char="649" end_char="649">"</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="650" end_char="652">Hay</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="654" end_char="656">una</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="658" end_char="663">enorme</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="665" end_char="672">cantidad</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="674" end_char="675">de</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="677" end_char="683">pruebas</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="685" end_char="687">que</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="689" end_char="697">sostienen</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="699" end_char="701">que</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="703" end_char="705">fue</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="707" end_char="710">allí</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="712" end_char="716">donde</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="718" end_char="724">comenzó</TOKEN>
<TOKEN id="token-4-14" pos="punct" morph="none" start_char="725" end_char="726">",</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="728" end_char="734">sostuvo</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="736" end_char="741">Pompeo</TOKEN>
<TOKEN id="token-4-17" pos="punct" morph="none" start_char="742" end_char="742">,</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="744" end_char="745">en</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="747" end_char="753">alusión</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="755" end_char="756">al</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="758" end_char="768">cuestionado</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="770" end_char="780">laboratorio</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="782" end_char="783">de</TOKEN>
<TOKEN id="token-4-24" pos="word" morph="none" start_char="785" end_char="789">Wuhan</TOKEN>
<TOKEN id="token-4-25" pos="punct" morph="none" start_char="790" end_char="790">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="792" end_char="827">
<ORIGINAL_TEXT>En diálogo con la cadena de noticias</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="792" end_char="793">En</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="795" end_char="801">diálogo</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="803" end_char="805">con</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="807" end_char="808">la</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="810" end_char="815">cadena</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="817" end_char="818">de</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="820" end_char="827">noticias</TOKEN>
</SEG>
<SEG id="segment-6" start_char="830" end_char="832">
<ORIGINAL_TEXT>ABC</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="830" end_char="832">ABC</TOKEN>
</SEG>
<SEG id="segment-7" start_char="835" end_char="1075">
<ORIGINAL_TEXT>, el secretario de Estado de Trump aseguró que China "hizo todo lo posible para asegurarse de que el mundo entero no se enterara a tiempo" del brote que, de acuerdo a los servicios de inteligencia, habría comenzado a principios de diciembre.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="punct" morph="none" start_char="835" end_char="835">,</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="837" end_char="838">el</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="840" end_char="849">secretario</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="851" end_char="852">de</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="854" end_char="859">Estado</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="861" end_char="862">de</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="864" end_char="868">Trump</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="870" end_char="876">aseguró</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="878" end_char="880">que</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="882" end_char="886">China</TOKEN>
<TOKEN id="token-7-10" pos="punct" morph="none" start_char="888" end_char="888">"</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="889" end_char="892">hizo</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="894" end_char="897">todo</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="899" end_char="900">lo</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="902" end_char="908">posible</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="910" end_char="913">para</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="915" end_char="924">asegurarse</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="926" end_char="927">de</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="929" end_char="931">que</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="933" end_char="934">el</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="936" end_char="940">mundo</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="942" end_char="947">entero</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="949" end_char="950">no</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="952" end_char="953">se</TOKEN>
<TOKEN id="token-7-24" pos="word" morph="none" start_char="955" end_char="962">enterara</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="964" end_char="964">a</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="966" end_char="971">tiempo</TOKEN>
<TOKEN id="token-7-27" pos="punct" morph="none" start_char="972" end_char="972">"</TOKEN>
<TOKEN id="token-7-28" pos="word" morph="none" start_char="974" end_char="976">del</TOKEN>
<TOKEN id="token-7-29" pos="word" morph="none" start_char="978" end_char="982">brote</TOKEN>
<TOKEN id="token-7-30" pos="word" morph="none" start_char="984" end_char="986">que</TOKEN>
<TOKEN id="token-7-31" pos="punct" morph="none" start_char="987" end_char="987">,</TOKEN>
<TOKEN id="token-7-32" pos="word" morph="none" start_char="989" end_char="990">de</TOKEN>
<TOKEN id="token-7-33" pos="word" morph="none" start_char="992" end_char="998">acuerdo</TOKEN>
<TOKEN id="token-7-34" pos="word" morph="none" start_char="1000" end_char="1000">a</TOKEN>
<TOKEN id="token-7-35" pos="word" morph="none" start_char="1002" end_char="1004">los</TOKEN>
<TOKEN id="token-7-36" pos="word" morph="none" start_char="1006" end_char="1014">servicios</TOKEN>
<TOKEN id="token-7-37" pos="word" morph="none" start_char="1016" end_char="1017">de</TOKEN>
<TOKEN id="token-7-38" pos="word" morph="none" start_char="1019" end_char="1030">inteligencia</TOKEN>
<TOKEN id="token-7-39" pos="punct" morph="none" start_char="1031" end_char="1031">,</TOKEN>
<TOKEN id="token-7-40" pos="word" morph="none" start_char="1033" end_char="1038">habría</TOKEN>
<TOKEN id="token-7-41" pos="word" morph="none" start_char="1040" end_char="1048">comenzado</TOKEN>
<TOKEN id="token-7-42" pos="word" morph="none" start_char="1050" end_char="1050">a</TOKEN>
<TOKEN id="token-7-43" pos="word" morph="none" start_char="1052" end_char="1061">principios</TOKEN>
<TOKEN id="token-7-44" pos="word" morph="none" start_char="1063" end_char="1064">de</TOKEN>
<TOKEN id="token-7-45" pos="word" morph="none" start_char="1066" end_char="1074">diciembre</TOKEN>
<TOKEN id="token-7-46" pos="punct" morph="none" start_char="1075" end_char="1075">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1077" end_char="1130">
<ORIGINAL_TEXT>"Fue un clásico esfuerzo de desinformación comunista".</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="punct" morph="none" start_char="1077" end_char="1077">"</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="1078" end_char="1080">Fue</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="1082" end_char="1083">un</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1085" end_char="1091">clásico</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1093" end_char="1100">esfuerzo</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1102" end_char="1103">de</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="1105" end_char="1118">desinformación</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="1120" end_char="1128">comunista</TOKEN>
<TOKEN id="token-8-8" pos="punct" morph="none" start_char="1129" end_char="1130">".</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1134" end_char="1355">
<ORIGINAL_TEXT>Antes de que el funcionario estadounidense se refierara a la investigación paralela, las conclusiones del reporte de 15 páginas fueron publicadas por el diario australiano The Saturday Telegraph y el inglés The Daily Mail.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1134" end_char="1138">Antes</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1140" end_char="1141">de</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="1143" end_char="1145">que</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1147" end_char="1148">el</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1150" end_char="1160">funcionario</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1162" end_char="1175">estadounidense</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1177" end_char="1178">se</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1180" end_char="1188">refierara</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1190" end_char="1190">a</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="1192" end_char="1193">la</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1195" end_char="1207">investigación</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1209" end_char="1216">paralela</TOKEN>
<TOKEN id="token-9-12" pos="punct" morph="none" start_char="1217" end_char="1217">,</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1219" end_char="1221">las</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1223" end_char="1234">conclusiones</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1236" end_char="1238">del</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="1240" end_char="1246">reporte</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1248" end_char="1249">de</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1251" end_char="1252">15</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1254" end_char="1260">páginas</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1262" end_char="1267">fueron</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1269" end_char="1278">publicadas</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1280" end_char="1282">por</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="1284" end_char="1285">el</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1287" end_char="1292">diario</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="1294" end_char="1304">australiano</TOKEN>
<TOKEN id="token-9-26" pos="word" morph="none" start_char="1306" end_char="1308">The</TOKEN>
<TOKEN id="token-9-27" pos="word" morph="none" start_char="1310" end_char="1317">Saturday</TOKEN>
<TOKEN id="token-9-28" pos="word" morph="none" start_char="1319" end_char="1327">Telegraph</TOKEN>
<TOKEN id="token-9-29" pos="word" morph="none" start_char="1329" end_char="1329">y</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1331" end_char="1332">el</TOKEN>
<TOKEN id="token-9-31" pos="word" morph="none" start_char="1334" end_char="1339">inglés</TOKEN>
<TOKEN id="token-9-32" pos="word" morph="none" start_char="1341" end_char="1343">The</TOKEN>
<TOKEN id="token-9-33" pos="word" morph="none" start_char="1345" end_char="1349">Daily</TOKEN>
<TOKEN id="token-9-34" pos="word" morph="none" start_char="1351" end_char="1354">Mail</TOKEN>
<TOKEN id="token-9-35" pos="punct" morph="none" start_char="1355" end_char="1355">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1357" end_char="1425">
<ORIGINAL_TEXT>A continuación, las pruebas que presentaron contra el gobierno chino:</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1357" end_char="1357">A</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1359" end_char="1370">continuación</TOKEN>
<TOKEN id="token-10-2" pos="punct" morph="none" start_char="1371" end_char="1371">,</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1373" end_char="1375">las</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1377" end_char="1383">pruebas</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1385" end_char="1387">que</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1389" end_char="1399">presentaron</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1401" end_char="1406">contra</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1408" end_char="1409">el</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1411" end_char="1418">gobierno</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1420" end_char="1424">chino</TOKEN>
<TOKEN id="token-10-11" pos="punct" morph="none" start_char="1425" end_char="1425">:</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1428" end_char="1535">
<ORIGINAL_TEXT>"La negación mortal de la transmisión entre seres humanos, cuando en diciembre ya se habían detectado casos"</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="punct" morph="none" start_char="1428" end_char="1428">"</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1429" end_char="1430">La</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1432" end_char="1439">negación</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1441" end_char="1446">mortal</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1448" end_char="1449">de</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1451" end_char="1452">la</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1454" end_char="1464">transmisión</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1466" end_char="1470">entre</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1472" end_char="1476">seres</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1478" end_char="1484">humanos</TOKEN>
<TOKEN id="token-11-10" pos="punct" morph="none" start_char="1485" end_char="1485">,</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1487" end_char="1492">cuando</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1494" end_char="1495">en</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1497" end_char="1505">diciembre</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1507" end_char="1508">ya</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1510" end_char="1511">se</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1513" end_char="1518">habían</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1520" end_char="1528">detectado</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1530" end_char="1534">casos</TOKEN>
<TOKEN id="token-11-19" pos="punct" morph="none" start_char="1535" end_char="1535">"</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1538" end_char="1644">
<ORIGINAL_TEXT>"El silenciamiento o la desaparición de los médicos y científicos que advirtieron lo que estaba sucediendo"</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="punct" morph="none" start_char="1538" end_char="1538">"</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1539" end_char="1540">El</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1542" end_char="1555">silenciamiento</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1557" end_char="1557">o</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1559" end_char="1560">la</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1562" end_char="1573">desaparición</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1575" end_char="1576">de</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1578" end_char="1580">los</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1582" end_char="1588">médicos</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1590" end_char="1590">y</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1592" end_char="1602">científicos</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1604" end_char="1606">que</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1608" end_char="1618">advirtieron</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1620" end_char="1621">lo</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1623" end_char="1625">que</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1627" end_char="1632">estaba</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1634" end_char="1643">sucediendo</TOKEN>
<TOKEN id="token-12-17" pos="punct" morph="none" start_char="1644" end_char="1644">"</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1647" end_char="1740">
<ORIGINAL_TEXT>"La destrucción de pruebas vitales del virus en los laboratorios de estudios genómicos chinos"</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="punct" morph="none" start_char="1647" end_char="1647">"</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1648" end_char="1649">La</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1651" end_char="1661">destrucción</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1663" end_char="1664">de</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1666" end_char="1672">pruebas</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1674" end_char="1680">vitales</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1682" end_char="1684">del</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1686" end_char="1690">virus</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1692" end_char="1693">en</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1695" end_char="1697">los</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1699" end_char="1710">laboratorios</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1712" end_char="1713">de</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1715" end_char="1722">estudios</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="1724" end_char="1732">genómicos</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="1734" end_char="1739">chinos</TOKEN>
<TOKEN id="token-13-15" pos="punct" morph="none" start_char="1740" end_char="1740">"</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1743" end_char="1804">
<ORIGINAL_TEXT>"El blanqueo de los puestos del emrcado de la fauna silvestre"</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="punct" morph="none" start_char="1743" end_char="1743">"</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1744" end_char="1745">El</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1747" end_char="1754">blanqueo</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1756" end_char="1757">de</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1759" end_char="1761">los</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1763" end_char="1769">puestos</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1771" end_char="1773">del</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1775" end_char="1781">emrcado</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1783" end_char="1784">de</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1786" end_char="1787">la</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1789" end_char="1793">fauna</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1795" end_char="1803">silvestre</TOKEN>
<TOKEN id="token-14-12" pos="punct" morph="none" start_char="1804" end_char="1804">"</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1807" end_char="1927">
<ORIGINAL_TEXT>"La negativa a proporcionar muestras de virus a científicos internacionales que trabajan en la elaboración de una vacuna"</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="punct" morph="none" start_char="1807" end_char="1807">"</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1808" end_char="1809">La</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1811" end_char="1818">negativa</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="1820" end_char="1820">a</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1822" end_char="1833">proporcionar</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1835" end_char="1842">muestras</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="1844" end_char="1845">de</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1847" end_char="1851">virus</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1853" end_char="1853">a</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="1855" end_char="1865">científicos</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1867" end_char="1881">internacionales</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="1883" end_char="1885">que</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="1887" end_char="1894">trabajan</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="1896" end_char="1897">en</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="1899" end_char="1900">la</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="1902" end_char="1912">elaboración</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="1914" end_char="1915">de</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="1917" end_char="1919">una</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="1921" end_char="1926">vacuna</TOKEN>
<TOKEN id="token-15-19" pos="punct" morph="none" start_char="1927" end_char="1927">"</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1931" end_char="2087">
<ORIGINAL_TEXT>El reporte también cuestionó a la Organización Mundial de la Salud, que no investigó las denuncias que realizaron Taiwán y Hong Kong a mediados de diciembre.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1931" end_char="1932">El</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1934" end_char="1940">reporte</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1942" end_char="1948">también</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1950" end_char="1958">cuestionó</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1960" end_char="1960">a</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1962" end_char="1963">la</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="1965" end_char="1976">Organización</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="1978" end_char="1984">Mundial</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="1986" end_char="1987">de</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="1989" end_char="1990">la</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="1992" end_char="1996">Salud</TOKEN>
<TOKEN id="token-16-11" pos="punct" morph="none" start_char="1997" end_char="1997">,</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="1999" end_char="2001">que</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2003" end_char="2004">no</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="2006" end_char="2014">investigó</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="2016" end_char="2018">las</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="2020" end_char="2028">denuncias</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="2030" end_char="2032">que</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="2034" end_char="2043">realizaron</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="2045" end_char="2050">Taiwán</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="2052" end_char="2052">y</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="2054" end_char="2057">Hong</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="2059" end_char="2062">Kong</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="2064" end_char="2064">a</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="2066" end_char="2073">mediados</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="2075" end_char="2076">de</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="2078" end_char="2086">diciembre</TOKEN>
<TOKEN id="token-16-27" pos="punct" morph="none" start_char="2087" end_char="2087">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2089" end_char="2208">
<ORIGINAL_TEXT>"Existían pruebas de transmisión entre humanos desde principios de diciembre, pero fueron negadas hasta el 20 de enero".</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="punct" morph="none" start_char="2089" end_char="2089">"</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2090" end_char="2097">Existían</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2099" end_char="2105">pruebas</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2107" end_char="2108">de</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2110" end_char="2120">transmisión</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2122" end_char="2126">entre</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2128" end_char="2134">humanos</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="2136" end_char="2140">desde</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="2142" end_char="2151">principios</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="2153" end_char="2154">de</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="2156" end_char="2164">diciembre</TOKEN>
<TOKEN id="token-17-11" pos="punct" morph="none" start_char="2165" end_char="2165">,</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="2167" end_char="2170">pero</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="2172" end_char="2177">fueron</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="2179" end_char="2185">negadas</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="2187" end_char="2191">hasta</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="2193" end_char="2194">el</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="2196" end_char="2197">20</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="2199" end_char="2200">de</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="2202" end_char="2206">enero</TOKEN>
<TOKEN id="token-17-20" pos="punct" morph="none" start_char="2207" end_char="2208">".</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2210" end_char="2402">
<ORIGINAL_TEXT>En simultáneo, el reporte advirtió que la Comisión Nacional de Salud China ordenó el tres de enero que se destruyeran las mueras de virus y emitiera una "orden de no publicación" sobre el tema.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2210" end_char="2211">En</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2213" end_char="2222">simultáneo</TOKEN>
<TOKEN id="token-18-2" pos="punct" morph="none" start_char="2223" end_char="2223">,</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="2225" end_char="2226">el</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2228" end_char="2234">reporte</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="2236" end_char="2243">advirtió</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2245" end_char="2247">que</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2249" end_char="2250">la</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2252" end_char="2259">Comisión</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2261" end_char="2268">Nacional</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2270" end_char="2271">de</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="2273" end_char="2277">Salud</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="2279" end_char="2283">China</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="2285" end_char="2290">ordenó</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="2292" end_char="2293">el</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="2295" end_char="2298">tres</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="2300" end_char="2301">de</TOKEN>
<TOKEN id="token-18-17" pos="word" morph="none" start_char="2303" end_char="2307">enero</TOKEN>
<TOKEN id="token-18-18" pos="word" morph="none" start_char="2309" end_char="2311">que</TOKEN>
<TOKEN id="token-18-19" pos="word" morph="none" start_char="2313" end_char="2314">se</TOKEN>
<TOKEN id="token-18-20" pos="word" morph="none" start_char="2316" end_char="2326">destruyeran</TOKEN>
<TOKEN id="token-18-21" pos="word" morph="none" start_char="2328" end_char="2330">las</TOKEN>
<TOKEN id="token-18-22" pos="word" morph="none" start_char="2332" end_char="2337">mueras</TOKEN>
<TOKEN id="token-18-23" pos="word" morph="none" start_char="2339" end_char="2340">de</TOKEN>
<TOKEN id="token-18-24" pos="word" morph="none" start_char="2342" end_char="2346">virus</TOKEN>
<TOKEN id="token-18-25" pos="word" morph="none" start_char="2348" end_char="2348">y</TOKEN>
<TOKEN id="token-18-26" pos="word" morph="none" start_char="2350" end_char="2357">emitiera</TOKEN>
<TOKEN id="token-18-27" pos="word" morph="none" start_char="2359" end_char="2361">una</TOKEN>
<TOKEN id="token-18-28" pos="punct" morph="none" start_char="2363" end_char="2363">"</TOKEN>
<TOKEN id="token-18-29" pos="word" morph="none" start_char="2364" end_char="2368">orden</TOKEN>
<TOKEN id="token-18-30" pos="word" morph="none" start_char="2370" end_char="2371">de</TOKEN>
<TOKEN id="token-18-31" pos="word" morph="none" start_char="2373" end_char="2374">no</TOKEN>
<TOKEN id="token-18-32" pos="word" morph="none" start_char="2376" end_char="2386">publicación</TOKEN>
<TOKEN id="token-18-33" pos="punct" morph="none" start_char="2387" end_char="2387">"</TOKEN>
<TOKEN id="token-18-34" pos="word" morph="none" start_char="2389" end_char="2393">sobre</TOKEN>
<TOKEN id="token-18-35" pos="word" morph="none" start_char="2395" end_char="2396">el</TOKEN>
<TOKEN id="token-18-36" pos="word" morph="none" start_char="2398" end_char="2401">tema</TOKEN>
<TOKEN id="token-18-37" pos="punct" morph="none" start_char="2402" end_char="2402">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2405" end_char="2417">
<ORIGINAL_TEXT>Leé también |</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2405" end_char="2407">Leé</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2409" end_char="2415">también</TOKEN>
<TOKEN id="token-19-2" pos="unknown" morph="none" start_char="2417" end_char="2417">|</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2420" end_char="2779">
<ORIGINAL_TEXT>La investigación también sostiene que el Instituto de Virología de Wuhan analizó en el año 2013 los virus vinculados a los murciélagos, cuya coincidencia genética con el Covid-19 es del 96 por ciento; al tiempo que dos años más tarde, de acuerdo al reporte, los científicos chinos descubrieron que la enfermedad era transmisible de murciélagos a seres humanos.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2420" end_char="2421">La</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="2423" end_char="2435">investigación</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="2437" end_char="2443">también</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="2445" end_char="2452">sostiene</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2454" end_char="2456">que</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2458" end_char="2459">el</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2461" end_char="2469">Instituto</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="2471" end_char="2472">de</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="2474" end_char="2482">Virología</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="2484" end_char="2485">de</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="2487" end_char="2491">Wuhan</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="2493" end_char="2499">analizó</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="2501" end_char="2502">en</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="2504" end_char="2505">el</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="2507" end_char="2509">año</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="2511" end_char="2514">2013</TOKEN>
<TOKEN id="token-20-16" pos="word" morph="none" start_char="2516" end_char="2518">los</TOKEN>
<TOKEN id="token-20-17" pos="word" morph="none" start_char="2520" end_char="2524">virus</TOKEN>
<TOKEN id="token-20-18" pos="word" morph="none" start_char="2526" end_char="2535">vinculados</TOKEN>
<TOKEN id="token-20-19" pos="word" morph="none" start_char="2537" end_char="2537">a</TOKEN>
<TOKEN id="token-20-20" pos="word" morph="none" start_char="2539" end_char="2541">los</TOKEN>
<TOKEN id="token-20-21" pos="word" morph="none" start_char="2543" end_char="2553">murciélagos</TOKEN>
<TOKEN id="token-20-22" pos="punct" morph="none" start_char="2554" end_char="2554">,</TOKEN>
<TOKEN id="token-20-23" pos="word" morph="none" start_char="2556" end_char="2559">cuya</TOKEN>
<TOKEN id="token-20-24" pos="word" morph="none" start_char="2561" end_char="2572">coincidencia</TOKEN>
<TOKEN id="token-20-25" pos="word" morph="none" start_char="2574" end_char="2581">genética</TOKEN>
<TOKEN id="token-20-26" pos="word" morph="none" start_char="2583" end_char="2585">con</TOKEN>
<TOKEN id="token-20-27" pos="word" morph="none" start_char="2587" end_char="2588">el</TOKEN>
<TOKEN id="token-20-28" pos="unknown" morph="none" start_char="2590" end_char="2597">Covid-19</TOKEN>
<TOKEN id="token-20-29" pos="word" morph="none" start_char="2599" end_char="2600">es</TOKEN>
<TOKEN id="token-20-30" pos="word" morph="none" start_char="2602" end_char="2604">del</TOKEN>
<TOKEN id="token-20-31" pos="word" morph="none" start_char="2606" end_char="2607">96</TOKEN>
<TOKEN id="token-20-32" pos="word" morph="none" start_char="2609" end_char="2611">por</TOKEN>
<TOKEN id="token-20-33" pos="word" morph="none" start_char="2613" end_char="2618">ciento</TOKEN>
<TOKEN id="token-20-34" pos="punct" morph="none" start_char="2619" end_char="2619">;</TOKEN>
<TOKEN id="token-20-35" pos="word" morph="none" start_char="2621" end_char="2622">al</TOKEN>
<TOKEN id="token-20-36" pos="word" morph="none" start_char="2624" end_char="2629">tiempo</TOKEN>
<TOKEN id="token-20-37" pos="word" morph="none" start_char="2631" end_char="2633">que</TOKEN>
<TOKEN id="token-20-38" pos="word" morph="none" start_char="2635" end_char="2637">dos</TOKEN>
<TOKEN id="token-20-39" pos="word" morph="none" start_char="2639" end_char="2642">años</TOKEN>
<TOKEN id="token-20-40" pos="word" morph="none" start_char="2644" end_char="2646">más</TOKEN>
<TOKEN id="token-20-41" pos="word" morph="none" start_char="2648" end_char="2652">tarde</TOKEN>
<TOKEN id="token-20-42" pos="punct" morph="none" start_char="2653" end_char="2653">,</TOKEN>
<TOKEN id="token-20-43" pos="word" morph="none" start_char="2655" end_char="2656">de</TOKEN>
<TOKEN id="token-20-44" pos="word" morph="none" start_char="2658" end_char="2664">acuerdo</TOKEN>
<TOKEN id="token-20-45" pos="word" morph="none" start_char="2666" end_char="2667">al</TOKEN>
<TOKEN id="token-20-46" pos="word" morph="none" start_char="2669" end_char="2675">reporte</TOKEN>
<TOKEN id="token-20-47" pos="punct" morph="none" start_char="2676" end_char="2676">,</TOKEN>
<TOKEN id="token-20-48" pos="word" morph="none" start_char="2678" end_char="2680">los</TOKEN>
<TOKEN id="token-20-49" pos="word" morph="none" start_char="2682" end_char="2692">científicos</TOKEN>
<TOKEN id="token-20-50" pos="word" morph="none" start_char="2694" end_char="2699">chinos</TOKEN>
<TOKEN id="token-20-51" pos="word" morph="none" start_char="2701" end_char="2712">descubrieron</TOKEN>
<TOKEN id="token-20-52" pos="word" morph="none" start_char="2714" end_char="2716">que</TOKEN>
<TOKEN id="token-20-53" pos="word" morph="none" start_char="2718" end_char="2719">la</TOKEN>
<TOKEN id="token-20-54" pos="word" morph="none" start_char="2721" end_char="2730">enfermedad</TOKEN>
<TOKEN id="token-20-55" pos="word" morph="none" start_char="2732" end_char="2734">era</TOKEN>
<TOKEN id="token-20-56" pos="word" morph="none" start_char="2736" end_char="2747">transmisible</TOKEN>
<TOKEN id="token-20-57" pos="word" morph="none" start_char="2749" end_char="2750">de</TOKEN>
<TOKEN id="token-20-58" pos="word" morph="none" start_char="2752" end_char="2762">murciélagos</TOKEN>
<TOKEN id="token-20-59" pos="word" morph="none" start_char="2764" end_char="2764">a</TOKEN>
<TOKEN id="token-20-60" pos="word" morph="none" start_char="2766" end_char="2770">seres</TOKEN>
<TOKEN id="token-20-61" pos="word" morph="none" start_char="2772" end_char="2778">humanos</TOKEN>
<TOKEN id="token-20-62" pos="punct" morph="none" start_char="2779" end_char="2779">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="2782" end_char="2806">
<ORIGINAL_TEXT>Desapariciones y censuras</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="2782" end_char="2795">Desapariciones</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="2797" end_char="2797">y</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="2799" end_char="2806">censuras</TOKEN>
</SEG>
<SEG id="segment-22" start_char="2809" end_char="2997">
<ORIGINAL_TEXT>Las agencias internacionales hicieron especial énfasis en la desaparición de la investigadora Huang Yan Ling, quien trabajó en el laboratorio de Wuhan y fue sindicada como la paciente cero.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="2809" end_char="2811">Las</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="2813" end_char="2820">agencias</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="2822" end_char="2836">internacionales</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="2838" end_char="2845">hicieron</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="2847" end_char="2854">especial</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="2856" end_char="2862">énfasis</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="2864" end_char="2865">en</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="2867" end_char="2868">la</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="2870" end_char="2881">desaparición</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="2883" end_char="2884">de</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="2886" end_char="2887">la</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="2889" end_char="2901">investigadora</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="2903" end_char="2907">Huang</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="2909" end_char="2911">Yan</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="2913" end_char="2916">Ling</TOKEN>
<TOKEN id="token-22-15" pos="punct" morph="none" start_char="2917" end_char="2917">,</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="2919" end_char="2923">quien</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="2925" end_char="2931">trabajó</TOKEN>
<TOKEN id="token-22-18" pos="word" morph="none" start_char="2933" end_char="2934">en</TOKEN>
<TOKEN id="token-22-19" pos="word" morph="none" start_char="2936" end_char="2937">el</TOKEN>
<TOKEN id="token-22-20" pos="word" morph="none" start_char="2939" end_char="2949">laboratorio</TOKEN>
<TOKEN id="token-22-21" pos="word" morph="none" start_char="2951" end_char="2952">de</TOKEN>
<TOKEN id="token-22-22" pos="word" morph="none" start_char="2954" end_char="2958">Wuhan</TOKEN>
<TOKEN id="token-22-23" pos="word" morph="none" start_char="2960" end_char="2960">y</TOKEN>
<TOKEN id="token-22-24" pos="word" morph="none" start_char="2962" end_char="2964">fue</TOKEN>
<TOKEN id="token-22-25" pos="word" morph="none" start_char="2966" end_char="2974">sindicada</TOKEN>
<TOKEN id="token-22-26" pos="word" morph="none" start_char="2976" end_char="2979">como</TOKEN>
<TOKEN id="token-22-27" pos="word" morph="none" start_char="2981" end_char="2982">la</TOKEN>
<TOKEN id="token-22-28" pos="word" morph="none" start_char="2984" end_char="2991">paciente</TOKEN>
<TOKEN id="token-22-29" pos="word" morph="none" start_char="2993" end_char="2996">cero</TOKEN>
<TOKEN id="token-22-30" pos="punct" morph="none" start_char="2997" end_char="2997">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="2999" end_char="3100">
<ORIGINAL_TEXT>Lo cierto es que nada se sabe de ella y su biografía fue borrada de la página oficial del laboratorio.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="2999" end_char="3000">Lo</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="3002" end_char="3007">cierto</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="3009" end_char="3010">es</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="3012" end_char="3014">que</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="3016" end_char="3019">nada</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="3021" end_char="3022">se</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="3024" end_char="3027">sabe</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="3029" end_char="3030">de</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="3032" end_char="3035">ella</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="3037" end_char="3037">y</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="3039" end_char="3040">su</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="3042" end_char="3050">biografía</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="3052" end_char="3054">fue</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="3056" end_char="3062">borrada</TOKEN>
<TOKEN id="token-23-14" pos="word" morph="none" start_char="3064" end_char="3065">de</TOKEN>
<TOKEN id="token-23-15" pos="word" morph="none" start_char="3067" end_char="3068">la</TOKEN>
<TOKEN id="token-23-16" pos="word" morph="none" start_char="3070" end_char="3075">página</TOKEN>
<TOKEN id="token-23-17" pos="word" morph="none" start_char="3077" end_char="3083">oficial</TOKEN>
<TOKEN id="token-23-18" pos="word" morph="none" start_char="3085" end_char="3087">del</TOKEN>
<TOKEN id="token-23-19" pos="word" morph="none" start_char="3089" end_char="3099">laboratorio</TOKEN>
<TOKEN id="token-23-20" pos="punct" morph="none" start_char="3100" end_char="3100">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="3103" end_char="3115">
<ORIGINAL_TEXT>Leé también |</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="3103" end_char="3105">Leé</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="3107" end_char="3113">también</TOKEN>
<TOKEN id="token-24-2" pos="unknown" morph="none" start_char="3115" end_char="3115">|</TOKEN>
</SEG>
<SEG id="segment-25" start_char="3118" end_char="3280">
<ORIGINAL_TEXT>También se incluyeron las detenciones en centros extrajudiciales del empresario Fang Bin, el abogado Chen Qiushi y el periodista de la televisión estatal Li Zehua.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="3118" end_char="3124">También</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="3126" end_char="3127">se</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="3129" end_char="3138">incluyeron</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="3140" end_char="3142">las</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="3144" end_char="3154">detenciones</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="3156" end_char="3157">en</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="3159" end_char="3165">centros</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="3167" end_char="3181">extrajudiciales</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="3183" end_char="3185">del</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="3187" end_char="3196">empresario</TOKEN>
<TOKEN id="token-25-10" pos="word" morph="none" start_char="3198" end_char="3201">Fang</TOKEN>
<TOKEN id="token-25-11" pos="word" morph="none" start_char="3203" end_char="3205">Bin</TOKEN>
<TOKEN id="token-25-12" pos="punct" morph="none" start_char="3206" end_char="3206">,</TOKEN>
<TOKEN id="token-25-13" pos="word" morph="none" start_char="3208" end_char="3209">el</TOKEN>
<TOKEN id="token-25-14" pos="word" morph="none" start_char="3211" end_char="3217">abogado</TOKEN>
<TOKEN id="token-25-15" pos="word" morph="none" start_char="3219" end_char="3222">Chen</TOKEN>
<TOKEN id="token-25-16" pos="word" morph="none" start_char="3224" end_char="3229">Qiushi</TOKEN>
<TOKEN id="token-25-17" pos="word" morph="none" start_char="3231" end_char="3231">y</TOKEN>
<TOKEN id="token-25-18" pos="word" morph="none" start_char="3233" end_char="3234">el</TOKEN>
<TOKEN id="token-25-19" pos="word" morph="none" start_char="3236" end_char="3245">periodista</TOKEN>
<TOKEN id="token-25-20" pos="word" morph="none" start_char="3247" end_char="3248">de</TOKEN>
<TOKEN id="token-25-21" pos="word" morph="none" start_char="3250" end_char="3251">la</TOKEN>
<TOKEN id="token-25-22" pos="word" morph="none" start_char="3253" end_char="3262">televisión</TOKEN>
<TOKEN id="token-25-23" pos="word" morph="none" start_char="3264" end_char="3270">estatal</TOKEN>
<TOKEN id="token-25-24" pos="word" morph="none" start_char="3272" end_char="3273">Li</TOKEN>
<TOKEN id="token-25-25" pos="word" morph="none" start_char="3275" end_char="3279">Zehua</TOKEN>
<TOKEN id="token-25-26" pos="punct" morph="none" start_char="3280" end_char="3280">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="3282" end_char="3534">
<ORIGINAL_TEXT>De acuerdo al reporte, el plan para silenciar el brote se activó el 31 de diciembre, cuando los términos "variación del SARS", "mercado de mariscos de Wuhan" y "neumonía desconocida en Wuhan" fueron eliminados de todos los motores de búsquedas en China.</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="3282" end_char="3283">De</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="3285" end_char="3291">acuerdo</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="3293" end_char="3294">al</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="3296" end_char="3302">reporte</TOKEN>
<TOKEN id="token-26-4" pos="punct" morph="none" start_char="3303" end_char="3303">,</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="3305" end_char="3306">el</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="3308" end_char="3311">plan</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="3313" end_char="3316">para</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="3318" end_char="3326">silenciar</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="3328" end_char="3329">el</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="3331" end_char="3335">brote</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="3337" end_char="3338">se</TOKEN>
<TOKEN id="token-26-12" pos="word" morph="none" start_char="3340" end_char="3345">activó</TOKEN>
<TOKEN id="token-26-13" pos="word" morph="none" start_char="3347" end_char="3348">el</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="3350" end_char="3351">31</TOKEN>
<TOKEN id="token-26-15" pos="word" morph="none" start_char="3353" end_char="3354">de</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="3356" end_char="3364">diciembre</TOKEN>
<TOKEN id="token-26-17" pos="punct" morph="none" start_char="3365" end_char="3365">,</TOKEN>
<TOKEN id="token-26-18" pos="word" morph="none" start_char="3367" end_char="3372">cuando</TOKEN>
<TOKEN id="token-26-19" pos="word" morph="none" start_char="3374" end_char="3376">los</TOKEN>
<TOKEN id="token-26-20" pos="word" morph="none" start_char="3378" end_char="3385">términos</TOKEN>
<TOKEN id="token-26-21" pos="punct" morph="none" start_char="3387" end_char="3387">"</TOKEN>
<TOKEN id="token-26-22" pos="word" morph="none" start_char="3388" end_char="3396">variación</TOKEN>
<TOKEN id="token-26-23" pos="word" morph="none" start_char="3398" end_char="3400">del</TOKEN>
<TOKEN id="token-26-24" pos="word" morph="none" start_char="3402" end_char="3405">SARS</TOKEN>
<TOKEN id="token-26-25" pos="punct" morph="none" start_char="3406" end_char="3407">",</TOKEN>
<TOKEN id="token-26-26" pos="punct" morph="none" start_char="3409" end_char="3409">"</TOKEN>
<TOKEN id="token-26-27" pos="word" morph="none" start_char="3410" end_char="3416">mercado</TOKEN>
<TOKEN id="token-26-28" pos="word" morph="none" start_char="3418" end_char="3419">de</TOKEN>
<TOKEN id="token-26-29" pos="word" morph="none" start_char="3421" end_char="3428">mariscos</TOKEN>
<TOKEN id="token-26-30" pos="word" morph="none" start_char="3430" end_char="3431">de</TOKEN>
<TOKEN id="token-26-31" pos="word" morph="none" start_char="3433" end_char="3437">Wuhan</TOKEN>
<TOKEN id="token-26-32" pos="punct" morph="none" start_char="3438" end_char="3438">"</TOKEN>
<TOKEN id="token-26-33" pos="word" morph="none" start_char="3440" end_char="3440">y</TOKEN>
<TOKEN id="token-26-34" pos="punct" morph="none" start_char="3442" end_char="3442">"</TOKEN>
<TOKEN id="token-26-35" pos="word" morph="none" start_char="3443" end_char="3450">neumonía</TOKEN>
<TOKEN id="token-26-36" pos="word" morph="none" start_char="3452" end_char="3462">desconocida</TOKEN>
<TOKEN id="token-26-37" pos="word" morph="none" start_char="3464" end_char="3465">en</TOKEN>
<TOKEN id="token-26-38" pos="word" morph="none" start_char="3467" end_char="3471">Wuhan</TOKEN>
<TOKEN id="token-26-39" pos="punct" morph="none" start_char="3472" end_char="3472">"</TOKEN>
<TOKEN id="token-26-40" pos="word" morph="none" start_char="3474" end_char="3479">fueron</TOKEN>
<TOKEN id="token-26-41" pos="word" morph="none" start_char="3481" end_char="3490">eliminados</TOKEN>
<TOKEN id="token-26-42" pos="word" morph="none" start_char="3492" end_char="3493">de</TOKEN>
<TOKEN id="token-26-43" pos="word" morph="none" start_char="3495" end_char="3499">todos</TOKEN>
<TOKEN id="token-26-44" pos="word" morph="none" start_char="3501" end_char="3503">los</TOKEN>
<TOKEN id="token-26-45" pos="word" morph="none" start_char="3505" end_char="3511">motores</TOKEN>
<TOKEN id="token-26-46" pos="word" morph="none" start_char="3513" end_char="3514">de</TOKEN>
<TOKEN id="token-26-47" pos="word" morph="none" start_char="3516" end_char="3524">búsquedas</TOKEN>
<TOKEN id="token-26-48" pos="word" morph="none" start_char="3526" end_char="3527">en</TOKEN>
<TOKEN id="token-26-49" pos="word" morph="none" start_char="3529" end_char="3533">China</TOKEN>
<TOKEN id="token-26-50" pos="punct" morph="none" start_char="3534" end_char="3534">.</TOKEN>
</SEG>
<SEG id="segment-27" start_char="3537" end_char="3546">
<ORIGINAL_TEXT>NEWSLETTER</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="3537" end_char="3546">NEWSLETTER</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
