<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATC7" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="3294" raw_text_md5="337933a80c9c075c9a4379fcae9c28f3">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="124">
<ORIGINAL_TEXT>Expertos cuestionan nueva hipótesis sobre el origen y la transmisión inicial del coronavirus que vincula a perros callejeros</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="8">Expertos</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="10" end_char="19">cuestionan</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="21" end_char="25">nueva</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="27" end_char="35">hipótesis</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="37" end_char="41">sobre</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="43" end_char="44">el</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="46" end_char="51">origen</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="53" end_char="53">y</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="55" end_char="56">la</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="58" end_char="68">transmisión</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="70" end_char="76">inicial</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="78" end_char="80">del</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="82" end_char="92">coronavirus</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="94" end_char="96">que</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="98" end_char="104">vincula</TOKEN>
<TOKEN id="token-0-15" pos="word" morph="none" start_char="106" end_char="106">a</TOKEN>
<TOKEN id="token-0-16" pos="word" morph="none" start_char="108" end_char="113">perros</TOKEN>
<TOKEN id="token-0-17" pos="word" morph="none" start_char="115" end_char="124">callejeros</TOKEN>
</SEG>
<SEG id="segment-1" start_char="128" end_char="394">
<ORIGINAL_TEXT>Un reciente estudio sugiere que los perros callejeros, en especial el tejido de sus intestinos, podrían haber contribuido a la evolución de un progenitor del SARS-CoV-2, que apunta la necesidad de incluir a los canes salvajes en los programas de vigilancias de virus.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="128" end_char="129">Un</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="131" end_char="138">reciente</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="140" end_char="146">estudio</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="148" end_char="154">sugiere</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="156" end_char="158">que</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="160" end_char="162">los</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="164" end_char="169">perros</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="171" end_char="180">callejeros</TOKEN>
<TOKEN id="token-1-8" pos="punct" morph="none" start_char="181" end_char="181">,</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="183" end_char="184">en</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="186" end_char="193">especial</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="195" end_char="196">el</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="198" end_char="203">tejido</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="205" end_char="206">de</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="208" end_char="210">sus</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="212" end_char="221">intestinos</TOKEN>
<TOKEN id="token-1-16" pos="punct" morph="none" start_char="222" end_char="222">,</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="224" end_char="230">podrían</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="232" end_char="236">haber</TOKEN>
<TOKEN id="token-1-19" pos="word" morph="none" start_char="238" end_char="248">contribuido</TOKEN>
<TOKEN id="token-1-20" pos="word" morph="none" start_char="250" end_char="250">a</TOKEN>
<TOKEN id="token-1-21" pos="word" morph="none" start_char="252" end_char="253">la</TOKEN>
<TOKEN id="token-1-22" pos="word" morph="none" start_char="255" end_char="263">evolución</TOKEN>
<TOKEN id="token-1-23" pos="word" morph="none" start_char="265" end_char="266">de</TOKEN>
<TOKEN id="token-1-24" pos="word" morph="none" start_char="268" end_char="269">un</TOKEN>
<TOKEN id="token-1-25" pos="word" morph="none" start_char="271" end_char="280">progenitor</TOKEN>
<TOKEN id="token-1-26" pos="word" morph="none" start_char="282" end_char="284">del</TOKEN>
<TOKEN id="token-1-27" pos="unknown" morph="none" start_char="286" end_char="295">SARS-CoV-2</TOKEN>
<TOKEN id="token-1-28" pos="punct" morph="none" start_char="296" end_char="296">,</TOKEN>
<TOKEN id="token-1-29" pos="word" morph="none" start_char="298" end_char="300">que</TOKEN>
<TOKEN id="token-1-30" pos="word" morph="none" start_char="302" end_char="307">apunta</TOKEN>
<TOKEN id="token-1-31" pos="word" morph="none" start_char="309" end_char="310">la</TOKEN>
<TOKEN id="token-1-32" pos="word" morph="none" start_char="312" end_char="320">necesidad</TOKEN>
<TOKEN id="token-1-33" pos="word" morph="none" start_char="322" end_char="323">de</TOKEN>
<TOKEN id="token-1-34" pos="word" morph="none" start_char="325" end_char="331">incluir</TOKEN>
<TOKEN id="token-1-35" pos="word" morph="none" start_char="333" end_char="333">a</TOKEN>
<TOKEN id="token-1-36" pos="word" morph="none" start_char="335" end_char="337">los</TOKEN>
<TOKEN id="token-1-37" pos="word" morph="none" start_char="339" end_char="343">canes</TOKEN>
<TOKEN id="token-1-38" pos="word" morph="none" start_char="345" end_char="352">salvajes</TOKEN>
<TOKEN id="token-1-39" pos="word" morph="none" start_char="354" end_char="355">en</TOKEN>
<TOKEN id="token-1-40" pos="word" morph="none" start_char="357" end_char="359">los</TOKEN>
<TOKEN id="token-1-41" pos="word" morph="none" start_char="361" end_char="369">programas</TOKEN>
<TOKEN id="token-1-42" pos="word" morph="none" start_char="371" end_char="372">de</TOKEN>
<TOKEN id="token-1-43" pos="word" morph="none" start_char="374" end_char="384">vigilancias</TOKEN>
<TOKEN id="token-1-44" pos="word" morph="none" start_char="386" end_char="387">de</TOKEN>
<TOKEN id="token-1-45" pos="word" morph="none" start_char="389" end_char="393">virus</TOKEN>
<TOKEN id="token-1-46" pos="punct" morph="none" start_char="394" end_char="394">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="397" end_char="460">
<ORIGINAL_TEXT>Este estudio, de la Universidad de Ottawa (Canadá) y que publica</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="397" end_char="400">Este</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="402" end_char="408">estudio</TOKEN>
<TOKEN id="token-2-2" pos="punct" morph="none" start_char="409" end_char="409">,</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="411" end_char="412">de</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="414" end_char="415">la</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="417" end_char="427">Universidad</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="429" end_char="430">de</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="432" end_char="437">Ottawa</TOKEN>
<TOKEN id="token-2-8" pos="punct" morph="none" start_char="439" end_char="439">(</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="440" end_char="445">Canadá</TOKEN>
<TOKEN id="token-2-10" pos="punct" morph="none" start_char="446" end_char="446">)</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="448" end_char="448">y</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="450" end_char="452">que</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="454" end_char="460">publica</TOKEN>
</SEG>
<SEG id="segment-3" start_char="463" end_char="493">
<ORIGINAL_TEXT>Molecular Biology and Evolution</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="463" end_char="471">Molecular</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="473" end_char="479">Biology</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="481" end_char="483">and</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="485" end_char="493">Evolution</TOKEN>
</SEG>
<SEG id="segment-4" start_char="496" end_char="591">
<ORIGINAL_TEXT>, proponer así una nueva hipótesis sobre el origen y transmisión inicial del actual coronavirus.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="punct" morph="none" start_char="496" end_char="496">,</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="498" end_char="505">proponer</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="507" end_char="509">así</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="511" end_char="513">una</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="515" end_char="519">nueva</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="521" end_char="529">hipótesis</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="531" end_char="535">sobre</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="537" end_char="538">el</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="540" end_char="545">origen</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="547" end_char="547">y</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="549" end_char="559">transmisión</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="561" end_char="567">inicial</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="569" end_char="571">del</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="573" end_char="578">actual</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="580" end_char="590">coronavirus</TOKEN>
<TOKEN id="token-4-15" pos="punct" morph="none" start_char="591" end_char="591">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="593" end_char="749">
<ORIGINAL_TEXT>En otras investigaciones se ha indicado que los pangolines habían sido los intermediarios del SARS-CoV-2 para llegar desde los murciélagos hasta los humanos.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="593" end_char="594">En</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="596" end_char="600">otras</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="602" end_char="616">investigaciones</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="618" end_char="619">se</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="621" end_char="622">ha</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="624" end_char="631">indicado</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="633" end_char="635">que</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="637" end_char="639">los</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="641" end_char="650">pangolines</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="652" end_char="657">habían</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="659" end_char="662">sido</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="664" end_char="666">los</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="668" end_char="681">intermediarios</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="683" end_char="685">del</TOKEN>
<TOKEN id="token-5-14" pos="unknown" morph="none" start_char="687" end_char="696">SARS-CoV-2</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="698" end_char="701">para</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="703" end_char="708">llegar</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="710" end_char="714">desde</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="716" end_char="718">los</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="720" end_char="730">murciélagos</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="732" end_char="736">hasta</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="738" end_char="740">los</TOKEN>
<TOKEN id="token-5-22" pos="word" morph="none" start_char="742" end_char="748">humanos</TOKEN>
<TOKEN id="token-5-23" pos="punct" morph="none" start_char="749" end_char="749">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="752" end_char="1151">
<ORIGINAL_TEXT>El ancestro del SARS-Cov-2 y de su pariente más cercano, un coronavirus de murciélago, "infectó el intestino de cánidos, lo que muy probablemente dio lugar a una rápida evolución del virus en los cánidos y su salto a los humanos", considera el autor del estudio Xuhua Xia, quien considera importante vigilar los coronavirus similares al SARS en los perros salvajes para la lucha contra el SARS-CoV-2.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="752" end_char="753">El</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="755" end_char="762">ancestro</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="764" end_char="766">del</TOKEN>
<TOKEN id="token-6-3" pos="unknown" morph="none" start_char="768" end_char="777">SARS-Cov-2</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="779" end_char="779">y</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="781" end_char="782">de</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="784" end_char="785">su</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="787" end_char="794">pariente</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="796" end_char="798">más</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="800" end_char="806">cercano</TOKEN>
<TOKEN id="token-6-10" pos="punct" morph="none" start_char="807" end_char="807">,</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="809" end_char="810">un</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="812" end_char="822">coronavirus</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="824" end_char="825">de</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="827" end_char="836">murciélago</TOKEN>
<TOKEN id="token-6-15" pos="punct" morph="none" start_char="837" end_char="837">,</TOKEN>
<TOKEN id="token-6-16" pos="punct" morph="none" start_char="839" end_char="839">"</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="840" end_char="846">infectó</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="848" end_char="849">el</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="851" end_char="859">intestino</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="861" end_char="862">de</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="864" end_char="870">cánidos</TOKEN>
<TOKEN id="token-6-22" pos="punct" morph="none" start_char="871" end_char="871">,</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="873" end_char="874">lo</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="876" end_char="878">que</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="880" end_char="882">muy</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="884" end_char="896">probablemente</TOKEN>
<TOKEN id="token-6-27" pos="word" morph="none" start_char="898" end_char="900">dio</TOKEN>
<TOKEN id="token-6-28" pos="word" morph="none" start_char="902" end_char="906">lugar</TOKEN>
<TOKEN id="token-6-29" pos="word" morph="none" start_char="908" end_char="908">a</TOKEN>
<TOKEN id="token-6-30" pos="word" morph="none" start_char="910" end_char="912">una</TOKEN>
<TOKEN id="token-6-31" pos="word" morph="none" start_char="914" end_char="919">rápida</TOKEN>
<TOKEN id="token-6-32" pos="word" morph="none" start_char="921" end_char="929">evolución</TOKEN>
<TOKEN id="token-6-33" pos="word" morph="none" start_char="931" end_char="933">del</TOKEN>
<TOKEN id="token-6-34" pos="word" morph="none" start_char="935" end_char="939">virus</TOKEN>
<TOKEN id="token-6-35" pos="word" morph="none" start_char="941" end_char="942">en</TOKEN>
<TOKEN id="token-6-36" pos="word" morph="none" start_char="944" end_char="946">los</TOKEN>
<TOKEN id="token-6-37" pos="word" morph="none" start_char="948" end_char="954">cánidos</TOKEN>
<TOKEN id="token-6-38" pos="word" morph="none" start_char="956" end_char="956">y</TOKEN>
<TOKEN id="token-6-39" pos="word" morph="none" start_char="958" end_char="959">su</TOKEN>
<TOKEN id="token-6-40" pos="word" morph="none" start_char="961" end_char="965">salto</TOKEN>
<TOKEN id="token-6-41" pos="word" morph="none" start_char="967" end_char="967">a</TOKEN>
<TOKEN id="token-6-42" pos="word" morph="none" start_char="969" end_char="971">los</TOKEN>
<TOKEN id="token-6-43" pos="word" morph="none" start_char="973" end_char="979">humanos</TOKEN>
<TOKEN id="token-6-44" pos="punct" morph="none" start_char="980" end_char="981">",</TOKEN>
<TOKEN id="token-6-45" pos="word" morph="none" start_char="983" end_char="991">considera</TOKEN>
<TOKEN id="token-6-46" pos="word" morph="none" start_char="993" end_char="994">el</TOKEN>
<TOKEN id="token-6-47" pos="word" morph="none" start_char="996" end_char="1000">autor</TOKEN>
<TOKEN id="token-6-48" pos="word" morph="none" start_char="1002" end_char="1004">del</TOKEN>
<TOKEN id="token-6-49" pos="word" morph="none" start_char="1006" end_char="1012">estudio</TOKEN>
<TOKEN id="token-6-50" pos="word" morph="none" start_char="1014" end_char="1018">Xuhua</TOKEN>
<TOKEN id="token-6-51" pos="word" morph="none" start_char="1020" end_char="1022">Xia</TOKEN>
<TOKEN id="token-6-52" pos="punct" morph="none" start_char="1023" end_char="1023">,</TOKEN>
<TOKEN id="token-6-53" pos="word" morph="none" start_char="1025" end_char="1029">quien</TOKEN>
<TOKEN id="token-6-54" pos="word" morph="none" start_char="1031" end_char="1039">considera</TOKEN>
<TOKEN id="token-6-55" pos="word" morph="none" start_char="1041" end_char="1050">importante</TOKEN>
<TOKEN id="token-6-56" pos="word" morph="none" start_char="1052" end_char="1058">vigilar</TOKEN>
<TOKEN id="token-6-57" pos="word" morph="none" start_char="1060" end_char="1062">los</TOKEN>
<TOKEN id="token-6-58" pos="word" morph="none" start_char="1064" end_char="1074">coronavirus</TOKEN>
<TOKEN id="token-6-59" pos="word" morph="none" start_char="1076" end_char="1084">similares</TOKEN>
<TOKEN id="token-6-60" pos="word" morph="none" start_char="1086" end_char="1087">al</TOKEN>
<TOKEN id="token-6-61" pos="word" morph="none" start_char="1089" end_char="1092">SARS</TOKEN>
<TOKEN id="token-6-62" pos="word" morph="none" start_char="1094" end_char="1095">en</TOKEN>
<TOKEN id="token-6-63" pos="word" morph="none" start_char="1097" end_char="1099">los</TOKEN>
<TOKEN id="token-6-64" pos="word" morph="none" start_char="1101" end_char="1106">perros</TOKEN>
<TOKEN id="token-6-65" pos="word" morph="none" start_char="1108" end_char="1115">salvajes</TOKEN>
<TOKEN id="token-6-66" pos="word" morph="none" start_char="1117" end_char="1120">para</TOKEN>
<TOKEN id="token-6-67" pos="word" morph="none" start_char="1122" end_char="1123">la</TOKEN>
<TOKEN id="token-6-68" pos="word" morph="none" start_char="1125" end_char="1129">lucha</TOKEN>
<TOKEN id="token-6-69" pos="word" morph="none" start_char="1131" end_char="1136">contra</TOKEN>
<TOKEN id="token-6-70" pos="word" morph="none" start_char="1138" end_char="1139">el</TOKEN>
<TOKEN id="token-6-71" pos="unknown" morph="none" start_char="1141" end_char="1150">SARS-CoV-2</TOKEN>
<TOKEN id="token-6-72" pos="punct" morph="none" start_char="1151" end_char="1151">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="1154" end_char="1458">
<ORIGINAL_TEXT>Xia lleva tiempo estudiado las firmas moleculares de los virus, pues cuando estos invaden un huésped, sus genomas suelen llevar "las cicatrices de la batalla para luchar y evadir el sistema inmunológico" del infectado a través de cambios y adaptaciones que se encuentran en sus genomas, detalla la agencia</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="1154" end_char="1156">Xia</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="1158" end_char="1162">lleva</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="1164" end_char="1169">tiempo</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="1171" end_char="1179">estudiado</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="1181" end_char="1183">las</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="1185" end_char="1190">firmas</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="1192" end_char="1202">moleculares</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="1204" end_char="1205">de</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="1207" end_char="1209">los</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="1211" end_char="1215">virus</TOKEN>
<TOKEN id="token-7-10" pos="punct" morph="none" start_char="1216" end_char="1216">,</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="1218" end_char="1221">pues</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="1223" end_char="1228">cuando</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="1230" end_char="1234">estos</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="1236" end_char="1242">invaden</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="1244" end_char="1245">un</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="1247" end_char="1253">huésped</TOKEN>
<TOKEN id="token-7-17" pos="punct" morph="none" start_char="1254" end_char="1254">,</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="1256" end_char="1258">sus</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="1260" end_char="1266">genomas</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="1268" end_char="1273">suelen</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="1275" end_char="1280">llevar</TOKEN>
<TOKEN id="token-7-22" pos="punct" morph="none" start_char="1282" end_char="1282">"</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="1283" end_char="1285">las</TOKEN>
<TOKEN id="token-7-24" pos="word" morph="none" start_char="1287" end_char="1296">cicatrices</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="1298" end_char="1299">de</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="1301" end_char="1302">la</TOKEN>
<TOKEN id="token-7-27" pos="word" morph="none" start_char="1304" end_char="1310">batalla</TOKEN>
<TOKEN id="token-7-28" pos="word" morph="none" start_char="1312" end_char="1315">para</TOKEN>
<TOKEN id="token-7-29" pos="word" morph="none" start_char="1317" end_char="1322">luchar</TOKEN>
<TOKEN id="token-7-30" pos="word" morph="none" start_char="1324" end_char="1324">y</TOKEN>
<TOKEN id="token-7-31" pos="word" morph="none" start_char="1326" end_char="1331">evadir</TOKEN>
<TOKEN id="token-7-32" pos="word" morph="none" start_char="1333" end_char="1334">el</TOKEN>
<TOKEN id="token-7-33" pos="word" morph="none" start_char="1336" end_char="1342">sistema</TOKEN>
<TOKEN id="token-7-34" pos="word" morph="none" start_char="1344" end_char="1355">inmunológico</TOKEN>
<TOKEN id="token-7-35" pos="punct" morph="none" start_char="1356" end_char="1356">"</TOKEN>
<TOKEN id="token-7-36" pos="word" morph="none" start_char="1358" end_char="1360">del</TOKEN>
<TOKEN id="token-7-37" pos="word" morph="none" start_char="1362" end_char="1370">infectado</TOKEN>
<TOKEN id="token-7-38" pos="word" morph="none" start_char="1372" end_char="1372">a</TOKEN>
<TOKEN id="token-7-39" pos="word" morph="none" start_char="1374" end_char="1379">través</TOKEN>
<TOKEN id="token-7-40" pos="word" morph="none" start_char="1381" end_char="1382">de</TOKEN>
<TOKEN id="token-7-41" pos="word" morph="none" start_char="1384" end_char="1390">cambios</TOKEN>
<TOKEN id="token-7-42" pos="word" morph="none" start_char="1392" end_char="1392">y</TOKEN>
<TOKEN id="token-7-43" pos="word" morph="none" start_char="1394" end_char="1405">adaptaciones</TOKEN>
<TOKEN id="token-7-44" pos="word" morph="none" start_char="1407" end_char="1409">que</TOKEN>
<TOKEN id="token-7-45" pos="word" morph="none" start_char="1411" end_char="1412">se</TOKEN>
<TOKEN id="token-7-46" pos="word" morph="none" start_char="1414" end_char="1423">encuentran</TOKEN>
<TOKEN id="token-7-47" pos="word" morph="none" start_char="1425" end_char="1426">en</TOKEN>
<TOKEN id="token-7-48" pos="word" morph="none" start_char="1428" end_char="1430">sus</TOKEN>
<TOKEN id="token-7-49" pos="word" morph="none" start_char="1432" end_char="1438">genomas</TOKEN>
<TOKEN id="token-7-50" pos="punct" morph="none" start_char="1439" end_char="1439">,</TOKEN>
<TOKEN id="token-7-51" pos="word" morph="none" start_char="1441" end_char="1447">detalla</TOKEN>
<TOKEN id="token-7-52" pos="word" morph="none" start_char="1449" end_char="1450">la</TOKEN>
<TOKEN id="token-7-53" pos="word" morph="none" start_char="1452" end_char="1458">agencia</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1461" end_char="1463">
<ORIGINAL_TEXT>Efe</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="1461" end_char="1463">Efe</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1466" end_char="1466">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="punct" morph="none" start_char="1466" end_char="1466">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1469" end_char="1740">
<ORIGINAL_TEXT>Sobre la posibilidad de que los perros pudieran transmitir en este momento el SARS-CoV2, Xian señaló que para ello el animal tendría que tener establecida una población de coronavirus en un tejido en contacto con el exterior, pero por el momento no hay evidencias de ello.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1469" end_char="1473">Sobre</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1475" end_char="1476">la</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1478" end_char="1488">posibilidad</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1490" end_char="1491">de</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1493" end_char="1495">que</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1497" end_char="1499">los</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1501" end_char="1506">perros</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1508" end_char="1515">pudieran</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1517" end_char="1526">transmitir</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1528" end_char="1529">en</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1531" end_char="1534">este</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1536" end_char="1542">momento</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1544" end_char="1545">el</TOKEN>
<TOKEN id="token-10-13" pos="unknown" morph="none" start_char="1547" end_char="1555">SARS-CoV2</TOKEN>
<TOKEN id="token-10-14" pos="punct" morph="none" start_char="1556" end_char="1556">,</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1558" end_char="1561">Xian</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1563" end_char="1568">señaló</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1570" end_char="1572">que</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1574" end_char="1577">para</TOKEN>
<TOKEN id="token-10-19" pos="word" morph="none" start_char="1579" end_char="1582">ello</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="1584" end_char="1585">el</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="1587" end_char="1592">animal</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="1594" end_char="1600">tendría</TOKEN>
<TOKEN id="token-10-23" pos="word" morph="none" start_char="1602" end_char="1604">que</TOKEN>
<TOKEN id="token-10-24" pos="word" morph="none" start_char="1606" end_char="1610">tener</TOKEN>
<TOKEN id="token-10-25" pos="word" morph="none" start_char="1612" end_char="1622">establecida</TOKEN>
<TOKEN id="token-10-26" pos="word" morph="none" start_char="1624" end_char="1626">una</TOKEN>
<TOKEN id="token-10-27" pos="word" morph="none" start_char="1628" end_char="1636">población</TOKEN>
<TOKEN id="token-10-28" pos="word" morph="none" start_char="1638" end_char="1639">de</TOKEN>
<TOKEN id="token-10-29" pos="word" morph="none" start_char="1641" end_char="1651">coronavirus</TOKEN>
<TOKEN id="token-10-30" pos="word" morph="none" start_char="1653" end_char="1654">en</TOKEN>
<TOKEN id="token-10-31" pos="word" morph="none" start_char="1656" end_char="1657">un</TOKEN>
<TOKEN id="token-10-32" pos="word" morph="none" start_char="1659" end_char="1664">tejido</TOKEN>
<TOKEN id="token-10-33" pos="word" morph="none" start_char="1666" end_char="1667">en</TOKEN>
<TOKEN id="token-10-34" pos="word" morph="none" start_char="1669" end_char="1676">contacto</TOKEN>
<TOKEN id="token-10-35" pos="word" morph="none" start_char="1678" end_char="1680">con</TOKEN>
<TOKEN id="token-10-36" pos="word" morph="none" start_char="1682" end_char="1683">el</TOKEN>
<TOKEN id="token-10-37" pos="word" morph="none" start_char="1685" end_char="1692">exterior</TOKEN>
<TOKEN id="token-10-38" pos="punct" morph="none" start_char="1693" end_char="1693">,</TOKEN>
<TOKEN id="token-10-39" pos="word" morph="none" start_char="1695" end_char="1698">pero</TOKEN>
<TOKEN id="token-10-40" pos="word" morph="none" start_char="1700" end_char="1702">por</TOKEN>
<TOKEN id="token-10-41" pos="word" morph="none" start_char="1704" end_char="1705">el</TOKEN>
<TOKEN id="token-10-42" pos="word" morph="none" start_char="1707" end_char="1713">momento</TOKEN>
<TOKEN id="token-10-43" pos="word" morph="none" start_char="1715" end_char="1716">no</TOKEN>
<TOKEN id="token-10-44" pos="word" morph="none" start_char="1718" end_char="1720">hay</TOKEN>
<TOKEN id="token-10-45" pos="word" morph="none" start_char="1722" end_char="1731">evidencias</TOKEN>
<TOKEN id="token-10-46" pos="word" morph="none" start_char="1733" end_char="1734">de</TOKEN>
<TOKEN id="token-10-47" pos="word" morph="none" start_char="1736" end_char="1739">ello</TOKEN>
<TOKEN id="token-10-48" pos="punct" morph="none" start_char="1740" end_char="1740">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1743" end_char="1765">
<ORIGINAL_TEXT>Estudio recibe críticas</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1743" end_char="1749">Estudio</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1751" end_char="1756">recibe</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1758" end_char="1765">críticas</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1769" end_char="1818">
<ORIGINAL_TEXT>La investigación de Xia tiene sus voces en contra.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1769" end_char="1770">La</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1772" end_char="1784">investigación</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1786" end_char="1787">de</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1789" end_char="1791">Xia</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1793" end_char="1797">tiene</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1799" end_char="1801">sus</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1803" end_char="1807">voces</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1809" end_char="1810">en</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1812" end_char="1817">contra</TOKEN>
<TOKEN id="token-12-9" pos="punct" morph="none" start_char="1818" end_char="1818">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1820" end_char="2157">
<ORIGINAL_TEXT>''Me resulta difícil entender cómo el autor ha podido concluir de este estudio, o hacer una hipótesis, que el virus que causa COVID-19 puede haber evolucionado a través de los perros'', señala James Wood, jefe del Departamento de Medicina Veterinaria e investigador de infecciones y control de enfermedades de la Universidad de Cambridge.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="punct" morph="none" start_char="1820" end_char="1821">''</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1822" end_char="1823">Me</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1825" end_char="1831">resulta</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1833" end_char="1839">difícil</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1841" end_char="1848">entender</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1850" end_char="1853">cómo</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1855" end_char="1856">el</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1858" end_char="1862">autor</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1864" end_char="1865">ha</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1867" end_char="1872">podido</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1874" end_char="1881">concluir</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1883" end_char="1884">de</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1886" end_char="1889">este</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="1891" end_char="1897">estudio</TOKEN>
<TOKEN id="token-13-14" pos="punct" morph="none" start_char="1898" end_char="1898">,</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="1900" end_char="1900">o</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="1902" end_char="1906">hacer</TOKEN>
<TOKEN id="token-13-17" pos="word" morph="none" start_char="1908" end_char="1910">una</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="1912" end_char="1920">hipótesis</TOKEN>
<TOKEN id="token-13-19" pos="punct" morph="none" start_char="1921" end_char="1921">,</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="1923" end_char="1925">que</TOKEN>
<TOKEN id="token-13-21" pos="word" morph="none" start_char="1927" end_char="1928">el</TOKEN>
<TOKEN id="token-13-22" pos="word" morph="none" start_char="1930" end_char="1934">virus</TOKEN>
<TOKEN id="token-13-23" pos="word" morph="none" start_char="1936" end_char="1938">que</TOKEN>
<TOKEN id="token-13-24" pos="word" morph="none" start_char="1940" end_char="1944">causa</TOKEN>
<TOKEN id="token-13-25" pos="unknown" morph="none" start_char="1946" end_char="1953">COVID-19</TOKEN>
<TOKEN id="token-13-26" pos="word" morph="none" start_char="1955" end_char="1959">puede</TOKEN>
<TOKEN id="token-13-27" pos="word" morph="none" start_char="1961" end_char="1965">haber</TOKEN>
<TOKEN id="token-13-28" pos="word" morph="none" start_char="1967" end_char="1978">evolucionado</TOKEN>
<TOKEN id="token-13-29" pos="word" morph="none" start_char="1980" end_char="1980">a</TOKEN>
<TOKEN id="token-13-30" pos="word" morph="none" start_char="1982" end_char="1987">través</TOKEN>
<TOKEN id="token-13-31" pos="word" morph="none" start_char="1989" end_char="1990">de</TOKEN>
<TOKEN id="token-13-32" pos="word" morph="none" start_char="1992" end_char="1994">los</TOKEN>
<TOKEN id="token-13-33" pos="word" morph="none" start_char="1996" end_char="2001">perros</TOKEN>
<TOKEN id="token-13-34" pos="punct" morph="none" start_char="2002" end_char="2004">'',</TOKEN>
<TOKEN id="token-13-35" pos="word" morph="none" start_char="2006" end_char="2011">señala</TOKEN>
<TOKEN id="token-13-36" pos="word" morph="none" start_char="2013" end_char="2017">James</TOKEN>
<TOKEN id="token-13-37" pos="word" morph="none" start_char="2019" end_char="2022">Wood</TOKEN>
<TOKEN id="token-13-38" pos="punct" morph="none" start_char="2023" end_char="2023">,</TOKEN>
<TOKEN id="token-13-39" pos="word" morph="none" start_char="2025" end_char="2028">jefe</TOKEN>
<TOKEN id="token-13-40" pos="word" morph="none" start_char="2030" end_char="2032">del</TOKEN>
<TOKEN id="token-13-41" pos="word" morph="none" start_char="2034" end_char="2045">Departamento</TOKEN>
<TOKEN id="token-13-42" pos="word" morph="none" start_char="2047" end_char="2048">de</TOKEN>
<TOKEN id="token-13-43" pos="word" morph="none" start_char="2050" end_char="2057">Medicina</TOKEN>
<TOKEN id="token-13-44" pos="word" morph="none" start_char="2059" end_char="2069">Veterinaria</TOKEN>
<TOKEN id="token-13-45" pos="word" morph="none" start_char="2071" end_char="2071">e</TOKEN>
<TOKEN id="token-13-46" pos="word" morph="none" start_char="2073" end_char="2084">investigador</TOKEN>
<TOKEN id="token-13-47" pos="word" morph="none" start_char="2086" end_char="2087">de</TOKEN>
<TOKEN id="token-13-48" pos="word" morph="none" start_char="2089" end_char="2099">infecciones</TOKEN>
<TOKEN id="token-13-49" pos="word" morph="none" start_char="2101" end_char="2101">y</TOKEN>
<TOKEN id="token-13-50" pos="word" morph="none" start_char="2103" end_char="2109">control</TOKEN>
<TOKEN id="token-13-51" pos="word" morph="none" start_char="2111" end_char="2112">de</TOKEN>
<TOKEN id="token-13-52" pos="word" morph="none" start_char="2114" end_char="2125">enfermedades</TOKEN>
<TOKEN id="token-13-53" pos="word" morph="none" start_char="2127" end_char="2128">de</TOKEN>
<TOKEN id="token-13-54" pos="word" morph="none" start_char="2130" end_char="2131">la</TOKEN>
<TOKEN id="token-13-55" pos="word" morph="none" start_char="2133" end_char="2143">Universidad</TOKEN>
<TOKEN id="token-13-56" pos="word" morph="none" start_char="2145" end_char="2146">de</TOKEN>
<TOKEN id="token-13-57" pos="word" morph="none" start_char="2148" end_char="2156">Cambridge</TOKEN>
<TOKEN id="token-13-58" pos="punct" morph="none" start_char="2157" end_char="2157">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="2160" end_char="2230">
<ORIGINAL_TEXT>Wood considera que hay demasiada inferencia y muy pocos datos directos.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="2160" end_char="2163">Wood</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="2165" end_char="2173">considera</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="2175" end_char="2177">que</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="2179" end_char="2181">hay</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="2183" end_char="2191">demasiada</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="2193" end_char="2202">inferencia</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="2204" end_char="2204">y</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="2206" end_char="2208">muy</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="2210" end_char="2214">pocos</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="2216" end_char="2220">datos</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="2222" end_char="2229">directos</TOKEN>
<TOKEN id="token-14-11" pos="punct" morph="none" start_char="2230" end_char="2230">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="2232" end_char="2361">
<ORIGINAL_TEXT>''No veo nada en este documento que respalde esta suposición y me preocupa que este documento haya sido publicado en esta revista.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="punct" morph="none" start_char="2232" end_char="2233">''</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="2234" end_char="2235">No</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="2237" end_char="2239">veo</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="2241" end_char="2244">nada</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="2246" end_char="2247">en</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="2249" end_char="2252">este</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="2254" end_char="2262">documento</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="2264" end_char="2266">que</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="2268" end_char="2275">respalde</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="2277" end_char="2280">esta</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="2282" end_char="2291">suposición</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="2293" end_char="2293">y</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="2295" end_char="2296">me</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="2298" end_char="2305">preocupa</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="2307" end_char="2309">que</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="2311" end_char="2314">este</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="2316" end_char="2324">documento</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="2326" end_char="2329">haya</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="2331" end_char="2334">sido</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="2336" end_char="2344">publicado</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="2346" end_char="2347">en</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="2349" end_char="2352">esta</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="2354" end_char="2360">revista</TOKEN>
<TOKEN id="token-15-23" pos="punct" morph="none" start_char="2361" end_char="2361">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="2363" end_char="2467">
<ORIGINAL_TEXT>No creo que ningún dueño de perro deba preocuparse como resultado de este trabajo'', agrega el experto al</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="2363" end_char="2364">No</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="2366" end_char="2369">creo</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="2371" end_char="2373">que</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="2375" end_char="2380">ningún</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="2382" end_char="2386">dueño</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="2388" end_char="2389">de</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="2391" end_char="2395">perro</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="2397" end_char="2400">deba</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="2402" end_char="2412">preocuparse</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="2414" end_char="2417">como</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="2419" end_char="2427">resultado</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="2429" end_char="2430">de</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="2432" end_char="2435">este</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2437" end_char="2443">trabajo</TOKEN>
<TOKEN id="token-16-14" pos="punct" morph="none" start_char="2444" end_char="2446">'',</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="2448" end_char="2453">agrega</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="2455" end_char="2456">el</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="2458" end_char="2464">experto</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="2466" end_char="2467">al</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2470" end_char="2479">
<ORIGINAL_TEXT>Daily Mail</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2470" end_char="2474">Daily</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2476" end_char="2479">Mail</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2482" end_char="2482">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="punct" morph="none" start_char="2482" end_char="2482">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2485" end_char="2820">
<ORIGINAL_TEXT>El profesor de Virología del Roslin Institute de la Universidad de Edimburgo, Paul Digard, apunta que la investigación de Xia adopta un enfoque muy limitado para examinar la secuencia del SARS-CoV-2 en busca de pistas sobre su origen que no brindan un respaldo convincente para la hipótesis de que los perros fueron la fuente del virus.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2485" end_char="2486">El</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2488" end_char="2495">profesor</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2497" end_char="2498">de</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="2500" end_char="2508">Virología</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2510" end_char="2512">del</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="2514" end_char="2519">Roslin</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2521" end_char="2529">Institute</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="2531" end_char="2532">de</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="2534" end_char="2535">la</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="2537" end_char="2547">Universidad</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="2549" end_char="2550">de</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="2552" end_char="2560">Edimburgo</TOKEN>
<TOKEN id="token-19-12" pos="punct" morph="none" start_char="2561" end_char="2561">,</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="2563" end_char="2566">Paul</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="2568" end_char="2573">Digard</TOKEN>
<TOKEN id="token-19-15" pos="punct" morph="none" start_char="2574" end_char="2574">,</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="2576" end_char="2581">apunta</TOKEN>
<TOKEN id="token-19-17" pos="word" morph="none" start_char="2583" end_char="2585">que</TOKEN>
<TOKEN id="token-19-18" pos="word" morph="none" start_char="2587" end_char="2588">la</TOKEN>
<TOKEN id="token-19-19" pos="word" morph="none" start_char="2590" end_char="2602">investigación</TOKEN>
<TOKEN id="token-19-20" pos="word" morph="none" start_char="2604" end_char="2605">de</TOKEN>
<TOKEN id="token-19-21" pos="word" morph="none" start_char="2607" end_char="2609">Xia</TOKEN>
<TOKEN id="token-19-22" pos="word" morph="none" start_char="2611" end_char="2616">adopta</TOKEN>
<TOKEN id="token-19-23" pos="word" morph="none" start_char="2618" end_char="2619">un</TOKEN>
<TOKEN id="token-19-24" pos="word" morph="none" start_char="2621" end_char="2627">enfoque</TOKEN>
<TOKEN id="token-19-25" pos="word" morph="none" start_char="2629" end_char="2631">muy</TOKEN>
<TOKEN id="token-19-26" pos="word" morph="none" start_char="2633" end_char="2640">limitado</TOKEN>
<TOKEN id="token-19-27" pos="word" morph="none" start_char="2642" end_char="2645">para</TOKEN>
<TOKEN id="token-19-28" pos="word" morph="none" start_char="2647" end_char="2654">examinar</TOKEN>
<TOKEN id="token-19-29" pos="word" morph="none" start_char="2656" end_char="2657">la</TOKEN>
<TOKEN id="token-19-30" pos="word" morph="none" start_char="2659" end_char="2667">secuencia</TOKEN>
<TOKEN id="token-19-31" pos="word" morph="none" start_char="2669" end_char="2671">del</TOKEN>
<TOKEN id="token-19-32" pos="unknown" morph="none" start_char="2673" end_char="2682">SARS-CoV-2</TOKEN>
<TOKEN id="token-19-33" pos="word" morph="none" start_char="2684" end_char="2685">en</TOKEN>
<TOKEN id="token-19-34" pos="word" morph="none" start_char="2687" end_char="2691">busca</TOKEN>
<TOKEN id="token-19-35" pos="word" morph="none" start_char="2693" end_char="2694">de</TOKEN>
<TOKEN id="token-19-36" pos="word" morph="none" start_char="2696" end_char="2701">pistas</TOKEN>
<TOKEN id="token-19-37" pos="word" morph="none" start_char="2703" end_char="2707">sobre</TOKEN>
<TOKEN id="token-19-38" pos="word" morph="none" start_char="2709" end_char="2710">su</TOKEN>
<TOKEN id="token-19-39" pos="word" morph="none" start_char="2712" end_char="2717">origen</TOKEN>
<TOKEN id="token-19-40" pos="word" morph="none" start_char="2719" end_char="2721">que</TOKEN>
<TOKEN id="token-19-41" pos="word" morph="none" start_char="2723" end_char="2724">no</TOKEN>
<TOKEN id="token-19-42" pos="word" morph="none" start_char="2726" end_char="2732">brindan</TOKEN>
<TOKEN id="token-19-43" pos="word" morph="none" start_char="2734" end_char="2735">un</TOKEN>
<TOKEN id="token-19-44" pos="word" morph="none" start_char="2737" end_char="2744">respaldo</TOKEN>
<TOKEN id="token-19-45" pos="word" morph="none" start_char="2746" end_char="2756">convincente</TOKEN>
<TOKEN id="token-19-46" pos="word" morph="none" start_char="2758" end_char="2761">para</TOKEN>
<TOKEN id="token-19-47" pos="word" morph="none" start_char="2763" end_char="2764">la</TOKEN>
<TOKEN id="token-19-48" pos="word" morph="none" start_char="2766" end_char="2774">hipótesis</TOKEN>
<TOKEN id="token-19-49" pos="word" morph="none" start_char="2776" end_char="2777">de</TOKEN>
<TOKEN id="token-19-50" pos="word" morph="none" start_char="2779" end_char="2781">que</TOKEN>
<TOKEN id="token-19-51" pos="word" morph="none" start_char="2783" end_char="2785">los</TOKEN>
<TOKEN id="token-19-52" pos="word" morph="none" start_char="2787" end_char="2792">perros</TOKEN>
<TOKEN id="token-19-53" pos="word" morph="none" start_char="2794" end_char="2799">fueron</TOKEN>
<TOKEN id="token-19-54" pos="word" morph="none" start_char="2801" end_char="2802">la</TOKEN>
<TOKEN id="token-19-55" pos="word" morph="none" start_char="2804" end_char="2809">fuente</TOKEN>
<TOKEN id="token-19-56" pos="word" morph="none" start_char="2811" end_char="2813">del</TOKEN>
<TOKEN id="token-19-57" pos="word" morph="none" start_char="2815" end_char="2819">virus</TOKEN>
<TOKEN id="token-19-58" pos="punct" morph="none" start_char="2820" end_char="2820">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2823" end_char="3121">
<ORIGINAL_TEXT>Daniella Dos Santos, presidenta de la Asociación Británica de Veterinaria (BVA, por sus siglas en inglés), indica que esta investigación es puramente teórica y que ''el autor mismo señala que hasta la fecha no hay evidencia de que los perros puedan replicar o eliminar el virus que causa COVID-19''.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2823" end_char="2830">Daniella</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="2832" end_char="2834">Dos</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="2836" end_char="2841">Santos</TOKEN>
<TOKEN id="token-20-3" pos="punct" morph="none" start_char="2842" end_char="2842">,</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2844" end_char="2853">presidenta</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2855" end_char="2856">de</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2858" end_char="2859">la</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="2861" end_char="2870">Asociación</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="2872" end_char="2880">Británica</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="2882" end_char="2883">de</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="2885" end_char="2895">Veterinaria</TOKEN>
<TOKEN id="token-20-11" pos="punct" morph="none" start_char="2897" end_char="2897">(</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="2898" end_char="2900">BVA</TOKEN>
<TOKEN id="token-20-13" pos="punct" morph="none" start_char="2901" end_char="2901">,</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="2903" end_char="2905">por</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="2907" end_char="2909">sus</TOKEN>
<TOKEN id="token-20-16" pos="word" morph="none" start_char="2911" end_char="2916">siglas</TOKEN>
<TOKEN id="token-20-17" pos="word" morph="none" start_char="2918" end_char="2919">en</TOKEN>
<TOKEN id="token-20-18" pos="word" morph="none" start_char="2921" end_char="2926">inglés</TOKEN>
<TOKEN id="token-20-19" pos="punct" morph="none" start_char="2927" end_char="2928">),</TOKEN>
<TOKEN id="token-20-20" pos="word" morph="none" start_char="2930" end_char="2935">indica</TOKEN>
<TOKEN id="token-20-21" pos="word" morph="none" start_char="2937" end_char="2939">que</TOKEN>
<TOKEN id="token-20-22" pos="word" morph="none" start_char="2941" end_char="2944">esta</TOKEN>
<TOKEN id="token-20-23" pos="word" morph="none" start_char="2946" end_char="2958">investigación</TOKEN>
<TOKEN id="token-20-24" pos="word" morph="none" start_char="2960" end_char="2961">es</TOKEN>
<TOKEN id="token-20-25" pos="word" morph="none" start_char="2963" end_char="2971">puramente</TOKEN>
<TOKEN id="token-20-26" pos="word" morph="none" start_char="2973" end_char="2979">teórica</TOKEN>
<TOKEN id="token-20-27" pos="word" morph="none" start_char="2981" end_char="2981">y</TOKEN>
<TOKEN id="token-20-28" pos="word" morph="none" start_char="2983" end_char="2985">que</TOKEN>
<TOKEN id="token-20-29" pos="punct" morph="none" start_char="2987" end_char="2988">''</TOKEN>
<TOKEN id="token-20-30" pos="word" morph="none" start_char="2989" end_char="2990">el</TOKEN>
<TOKEN id="token-20-31" pos="word" morph="none" start_char="2992" end_char="2996">autor</TOKEN>
<TOKEN id="token-20-32" pos="word" morph="none" start_char="2998" end_char="3002">mismo</TOKEN>
<TOKEN id="token-20-33" pos="word" morph="none" start_char="3004" end_char="3009">señala</TOKEN>
<TOKEN id="token-20-34" pos="word" morph="none" start_char="3011" end_char="3013">que</TOKEN>
<TOKEN id="token-20-35" pos="word" morph="none" start_char="3015" end_char="3019">hasta</TOKEN>
<TOKEN id="token-20-36" pos="word" morph="none" start_char="3021" end_char="3022">la</TOKEN>
<TOKEN id="token-20-37" pos="word" morph="none" start_char="3024" end_char="3028">fecha</TOKEN>
<TOKEN id="token-20-38" pos="word" morph="none" start_char="3030" end_char="3031">no</TOKEN>
<TOKEN id="token-20-39" pos="word" morph="none" start_char="3033" end_char="3035">hay</TOKEN>
<TOKEN id="token-20-40" pos="word" morph="none" start_char="3037" end_char="3045">evidencia</TOKEN>
<TOKEN id="token-20-41" pos="word" morph="none" start_char="3047" end_char="3048">de</TOKEN>
<TOKEN id="token-20-42" pos="word" morph="none" start_char="3050" end_char="3052">que</TOKEN>
<TOKEN id="token-20-43" pos="word" morph="none" start_char="3054" end_char="3056">los</TOKEN>
<TOKEN id="token-20-44" pos="word" morph="none" start_char="3058" end_char="3063">perros</TOKEN>
<TOKEN id="token-20-45" pos="word" morph="none" start_char="3065" end_char="3070">puedan</TOKEN>
<TOKEN id="token-20-46" pos="word" morph="none" start_char="3072" end_char="3079">replicar</TOKEN>
<TOKEN id="token-20-47" pos="word" morph="none" start_char="3081" end_char="3081">o</TOKEN>
<TOKEN id="token-20-48" pos="word" morph="none" start_char="3083" end_char="3090">eliminar</TOKEN>
<TOKEN id="token-20-49" pos="word" morph="none" start_char="3092" end_char="3093">el</TOKEN>
<TOKEN id="token-20-50" pos="word" morph="none" start_char="3095" end_char="3099">virus</TOKEN>
<TOKEN id="token-20-51" pos="word" morph="none" start_char="3101" end_char="3103">que</TOKEN>
<TOKEN id="token-20-52" pos="word" morph="none" start_char="3105" end_char="3109">causa</TOKEN>
<TOKEN id="token-20-53" pos="unknown" morph="none" start_char="3111" end_char="3118">COVID-19</TOKEN>
<TOKEN id="token-20-54" pos="punct" morph="none" start_char="3119" end_char="3121">''.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="3123" end_char="3286">
<ORIGINAL_TEXT>''Instamos a la extrema precaución al interpretarlo como algo más que un recordatorio de que se está trabajando para considerar los orígenes del virus", puntualizó.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="punct" morph="none" start_char="3123" end_char="3124">''</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="3125" end_char="3132">Instamos</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="3134" end_char="3134">a</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="3136" end_char="3137">la</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="3139" end_char="3145">extrema</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="3147" end_char="3156">precaución</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="3158" end_char="3159">al</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="3161" end_char="3173">interpretarlo</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="3175" end_char="3178">como</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="3180" end_char="3183">algo</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="3185" end_char="3187">más</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="3189" end_char="3191">que</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="3193" end_char="3194">un</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="3196" end_char="3207">recordatorio</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="3209" end_char="3210">de</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="3212" end_char="3214">que</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="3216" end_char="3217">se</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="3219" end_char="3222">está</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="3224" end_char="3233">trabajando</TOKEN>
<TOKEN id="token-21-19" pos="word" morph="none" start_char="3235" end_char="3238">para</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="3240" end_char="3249">considerar</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="3251" end_char="3253">los</TOKEN>
<TOKEN id="token-21-22" pos="word" morph="none" start_char="3255" end_char="3262">orígenes</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="3264" end_char="3266">del</TOKEN>
<TOKEN id="token-21-24" pos="word" morph="none" start_char="3268" end_char="3272">virus</TOKEN>
<TOKEN id="token-21-25" pos="punct" morph="none" start_char="3273" end_char="3274">",</TOKEN>
<TOKEN id="token-21-26" pos="word" morph="none" start_char="3276" end_char="3285">puntualizó</TOKEN>
<TOKEN id="token-21-27" pos="punct" morph="none" start_char="3286" end_char="3286">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="3288" end_char="3290">
<ORIGINAL_TEXT>(I)</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="punct" morph="none" start_char="3288" end_char="3288">(</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="3289" end_char="3289">I</TOKEN>
<TOKEN id="token-22-2" pos="punct" morph="none" start_char="3290" end_char="3290">)</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
