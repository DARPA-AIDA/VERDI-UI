<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="ukr">
<DOC id="L0C049PDP" lang="ukr" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="12799" raw_text_md5="0e61747cfa4ea1307553feac6bf2bd77">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="89">
<ORIGINAL_TEXT>Israeli Startup Claims SARS-Cov-2 Escaped from Wuhan Lab During Gain-of-Function Research</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="7">Israeli</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="9" end_char="15">Startup</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="17" end_char="22">Claims</TOKEN>
<TOKEN id="token-0-3" pos="unknown" morph="none" start_char="24" end_char="33">SARS-Cov-2</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="35" end_char="41">Escaped</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="43" end_char="46">from</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="48" end_char="52">Wuhan</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="54" end_char="56">Lab</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="58" end_char="63">During</TOKEN>
<TOKEN id="token-0-9" pos="unknown" morph="none" start_char="65" end_char="80">Gain-of-Function</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="82" end_char="89">Research</TOKEN>
</SEG>
<SEG id="segment-1" start_char="93" end_char="287">
<ORIGINAL_TEXT>An Israeli startup named Rootclaim has determined that there is an 81% chance that the SARS-cov-2 was accidentally released from the Wuhan bioresearch laboratory during gain-of-function research.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="93" end_char="94">An</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="96" end_char="102">Israeli</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="104" end_char="110">startup</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="112" end_char="116">named</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="118" end_char="126">Rootclaim</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="128" end_char="130">has</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="132" end_char="141">determined</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="143" end_char="146">that</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="148" end_char="152">there</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="154" end_char="155">is</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="157" end_char="158">an</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="160" end_char="161">81</TOKEN>
<TOKEN id="token-1-12" pos="punct" morph="none" start_char="162" end_char="162">%</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="164" end_char="169">chance</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="171" end_char="174">that</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="176" end_char="178">the</TOKEN>
<TOKEN id="token-1-16" pos="unknown" morph="none" start_char="180" end_char="189">SARS-cov-2</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="191" end_char="193">was</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="195" end_char="206">accidentally</TOKEN>
<TOKEN id="token-1-19" pos="word" morph="none" start_char="208" end_char="215">released</TOKEN>
<TOKEN id="token-1-20" pos="word" morph="none" start_char="217" end_char="220">from</TOKEN>
<TOKEN id="token-1-21" pos="word" morph="none" start_char="222" end_char="224">the</TOKEN>
<TOKEN id="token-1-22" pos="word" morph="none" start_char="226" end_char="230">Wuhan</TOKEN>
<TOKEN id="token-1-23" pos="word" morph="none" start_char="232" end_char="242">bioresearch</TOKEN>
<TOKEN id="token-1-24" pos="word" morph="none" start_char="244" end_char="253">laboratory</TOKEN>
<TOKEN id="token-1-25" pos="word" morph="none" start_char="255" end_char="260">during</TOKEN>
<TOKEN id="token-1-26" pos="unknown" morph="none" start_char="262" end_char="277">gain-of-function</TOKEN>
<TOKEN id="token-1-27" pos="word" morph="none" start_char="279" end_char="286">research</TOKEN>
<TOKEN id="token-1-28" pos="punct" morph="none" start_char="287" end_char="287">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="290" end_char="353">
<ORIGINAL_TEXT>Rootclaim is, i'm assuming, an ai driven service that does this:</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="290" end_char="298">Rootclaim</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="300" end_char="301">is</TOKEN>
<TOKEN id="token-2-2" pos="punct" morph="none" start_char="302" end_char="302">,</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="304" end_char="306">i'm</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="308" end_char="315">assuming</TOKEN>
<TOKEN id="token-2-5" pos="punct" morph="none" start_char="316" end_char="316">,</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="318" end_char="319">an</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="321" end_char="322">ai</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="324" end_char="329">driven</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="331" end_char="337">service</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="339" end_char="342">that</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="344" end_char="347">does</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="349" end_char="352">this</TOKEN>
<TOKEN id="token-2-13" pos="punct" morph="none" start_char="353" end_char="353">:</TOKEN>
</SEG>
<SEG id="segment-3" start_char="356" end_char="451">
<ORIGINAL_TEXT>Rootclaim outperforms human reasoning by correcting for the biases and flaws of human intuition.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="356" end_char="364">Rootclaim</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="366" end_char="376">outperforms</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="378" end_char="382">human</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="384" end_char="392">reasoning</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="394" end_char="395">by</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="397" end_char="406">correcting</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="408" end_char="410">for</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="412" end_char="414">the</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="416" end_char="421">biases</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="423" end_char="425">and</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="427" end_char="431">flaws</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="433" end_char="434">of</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="436" end_char="440">human</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="442" end_char="450">intuition</TOKEN>
<TOKEN id="token-3-14" pos="punct" morph="none" start_char="451" end_char="451">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="453" end_char="624">
<ORIGINAL_TEXT>The platform integrates all available evidence, assesses it for credibility and uses probabilistic models to reach conclusions about the likelihood of competing hypotheses.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="453" end_char="455">The</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="457" end_char="464">platform</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="466" end_char="475">integrates</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="477" end_char="479">all</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="481" end_char="489">available</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="491" end_char="498">evidence</TOKEN>
<TOKEN id="token-4-6" pos="punct" morph="none" start_char="499" end_char="499">,</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="501" end_char="508">assesses</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="510" end_char="511">it</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="513" end_char="515">for</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="517" end_char="527">credibility</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="529" end_char="531">and</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="533" end_char="536">uses</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="538" end_char="550">probabilistic</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="552" end_char="557">models</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="559" end_char="560">to</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="562" end_char="566">reach</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="568" end_char="578">conclusions</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="580" end_char="584">about</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="586" end_char="588">the</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="590" end_char="599">likelihood</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="601" end_char="602">of</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="604" end_char="612">competing</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="614" end_char="623">hypotheses</TOKEN>
<TOKEN id="token-4-24" pos="punct" morph="none" start_char="624" end_char="624">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="626" end_char="730">
<ORIGINAL_TEXT>Its conclusions represent the best available understanding of the complexity and uncertainty in our world</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="626" end_char="628">Its</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="630" end_char="640">conclusions</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="642" end_char="650">represent</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="652" end_char="654">the</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="656" end_char="659">best</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="661" end_char="669">available</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="671" end_char="683">understanding</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="685" end_char="686">of</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="688" end_char="690">the</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="692" end_char="701">complexity</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="703" end_char="705">and</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="707" end_char="717">uncertainty</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="719" end_char="720">in</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="722" end_char="724">our</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="726" end_char="730">world</TOKEN>
</SEG>
<SEG id="segment-6" start_char="733" end_char="754">
<ORIGINAL_TEXT>According to rootclaim</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="733" end_char="741">According</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="743" end_char="744">to</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="746" end_char="754">rootclaim</TOKEN>
</SEG>
<SEG id="segment-7" start_char="757" end_char="776">
<ORIGINAL_TEXT>www.rootclaim.com...</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="url" morph="none" start_char="757" end_char="776">www.rootclaim.com...</TOKEN>
</SEG>
<SEG id="segment-8" start_char="779" end_char="864">
<ORIGINAL_TEXT>The virus was developed during gain-of-function research and was released by accident.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="779" end_char="781">The</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="783" end_char="787">virus</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="789" end_char="791">was</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="793" end_char="801">developed</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="803" end_char="808">during</TOKEN>
<TOKEN id="token-8-5" pos="unknown" morph="none" start_char="810" end_char="825">gain-of-function</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="827" end_char="834">research</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="836" end_char="838">and</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="840" end_char="842">was</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="844" end_char="851">released</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="853" end_char="854">by</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="856" end_char="863">accident</TOKEN>
<TOKEN id="token-8-12" pos="punct" morph="none" start_char="864" end_char="864">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="866" end_char="1009">
<ORIGINAL_TEXT>(81% probability) Hypotheses Considered 1 81% Lab escape: The virus was developed during gain-of-function research and was released by accident.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="punct" morph="none" start_char="866" end_char="866">(</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="867" end_char="868">81</TOKEN>
<TOKEN id="token-9-2" pos="punct" morph="none" start_char="869" end_char="869">%</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="871" end_char="881">probability</TOKEN>
<TOKEN id="token-9-4" pos="punct" morph="none" start_char="882" end_char="882">)</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="884" end_char="893">Hypotheses</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="895" end_char="904">Considered</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="906" end_char="906">1</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="908" end_char="909">81</TOKEN>
<TOKEN id="token-9-9" pos="punct" morph="none" start_char="910" end_char="910">%</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="912" end_char="914">Lab</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="916" end_char="921">escape</TOKEN>
<TOKEN id="token-9-12" pos="punct" morph="none" start_char="922" end_char="922">:</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="924" end_char="926">The</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="928" end_char="932">virus</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="934" end_char="936">was</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="938" end_char="946">developed</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="948" end_char="953">during</TOKEN>
<TOKEN id="token-9-18" pos="unknown" morph="none" start_char="955" end_char="970">gain-of-function</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="972" end_char="979">research</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="981" end_char="983">and</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="985" end_char="987">was</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="989" end_char="996">released</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="998" end_char="999">by</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1001" end_char="1008">accident</TOKEN>
<TOKEN id="token-9-25" pos="punct" morph="none" start_char="1009" end_char="1009">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1011" end_char="1097">
<ORIGINAL_TEXT>2 16% Zoonotic: The virus evolved in nature and was transmitted to humans zoonotically.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1011" end_char="1011">2</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1013" end_char="1014">16</TOKEN>
<TOKEN id="token-10-2" pos="punct" morph="none" start_char="1015" end_char="1015">%</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1017" end_char="1024">Zoonotic</TOKEN>
<TOKEN id="token-10-4" pos="punct" morph="none" start_char="1025" end_char="1025">:</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1027" end_char="1029">The</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1031" end_char="1035">virus</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1037" end_char="1043">evolved</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1045" end_char="1046">in</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1048" end_char="1053">nature</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1055" end_char="1057">and</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1059" end_char="1061">was</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1063" end_char="1073">transmitted</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1075" end_char="1076">to</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1078" end_char="1083">humans</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1085" end_char="1096">zoonotically</TOKEN>
<TOKEN id="token-10-16" pos="punct" morph="none" start_char="1097" end_char="1097">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1099" end_char="1198">
<ORIGINAL_TEXT>3 2.8% Bioweapon: The virus was genetically engineered as a bioweapon and was deliberately released.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1099" end_char="1099">3</TOKEN>
<TOKEN id="token-11-1" pos="unknown" morph="none" start_char="1101" end_char="1103">2.8</TOKEN>
<TOKEN id="token-11-2" pos="punct" morph="none" start_char="1104" end_char="1104">%</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1106" end_char="1114">Bioweapon</TOKEN>
<TOKEN id="token-11-4" pos="punct" morph="none" start_char="1115" end_char="1115">:</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1117" end_char="1119">The</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1121" end_char="1125">virus</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1127" end_char="1129">was</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1131" end_char="1141">genetically</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1143" end_char="1152">engineered</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1154" end_char="1155">as</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1157" end_char="1157">a</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1159" end_char="1167">bioweapon</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1169" end_char="1171">and</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1173" end_char="1175">was</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1177" end_char="1188">deliberately</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1190" end_char="1197">released</TOKEN>
<TOKEN id="token-11-17" pos="punct" morph="none" start_char="1198" end_char="1198">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1200" end_char="1333">
<ORIGINAL_TEXT>When a novel coronavirus was first identified in late 2019, the assumption was that, like most epidemics, it was of a zoonotic source.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1200" end_char="1203">When</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1205" end_char="1205">a</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1207" end_char="1211">novel</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1213" end_char="1223">coronavirus</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1225" end_char="1227">was</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1229" end_char="1233">first</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1235" end_char="1244">identified</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1246" end_char="1247">in</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1249" end_char="1252">late</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1254" end_char="1257">2019</TOKEN>
<TOKEN id="token-12-10" pos="punct" morph="none" start_char="1258" end_char="1258">,</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1260" end_char="1262">the</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1264" end_char="1273">assumption</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1275" end_char="1277">was</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1279" end_char="1282">that</TOKEN>
<TOKEN id="token-12-15" pos="punct" morph="none" start_char="1283" end_char="1283">,</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1285" end_char="1288">like</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1290" end_char="1293">most</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="1295" end_char="1303">epidemics</TOKEN>
<TOKEN id="token-12-19" pos="punct" morph="none" start_char="1304" end_char="1304">,</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1306" end_char="1307">it</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1309" end_char="1311">was</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1313" end_char="1314">of</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1316" end_char="1316">a</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1318" end_char="1325">zoonotic</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1327" end_char="1332">source</TOKEN>
<TOKEN id="token-12-26" pos="punct" morph="none" start_char="1333" end_char="1333">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1335" end_char="1464">
<ORIGINAL_TEXT>A few studies, including one published in the prestigious Nature magazine, concluded that the virus is not a laboratory construct.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1335" end_char="1335">A</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1337" end_char="1339">few</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1341" end_char="1347">studies</TOKEN>
<TOKEN id="token-13-3" pos="punct" morph="none" start_char="1348" end_char="1348">,</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1350" end_char="1358">including</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1360" end_char="1362">one</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1364" end_char="1372">published</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1374" end_char="1375">in</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1377" end_char="1379">the</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1381" end_char="1391">prestigious</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1393" end_char="1398">Nature</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1400" end_char="1407">magazine</TOKEN>
<TOKEN id="token-13-12" pos="punct" morph="none" start_char="1408" end_char="1408">,</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="1410" end_char="1418">concluded</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="1420" end_char="1423">that</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="1425" end_char="1427">the</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="1429" end_char="1433">virus</TOKEN>
<TOKEN id="token-13-17" pos="word" morph="none" start_char="1435" end_char="1436">is</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="1438" end_char="1440">not</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="1442" end_char="1442">a</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="1444" end_char="1453">laboratory</TOKEN>
<TOKEN id="token-13-21" pos="word" morph="none" start_char="1455" end_char="1463">construct</TOKEN>
<TOKEN id="token-13-22" pos="punct" morph="none" start_char="1464" end_char="1464">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1466" end_char="1636">
<ORIGINAL_TEXT>Today, claiming a non-zoonotic origin is widely considered a conspiracy theory, and indeed many such claims are easily refutable without requiring probabilistic inference.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1466" end_char="1470">Today</TOKEN>
<TOKEN id="token-14-1" pos="punct" morph="none" start_char="1471" end_char="1471">,</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1473" end_char="1480">claiming</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1482" end_char="1482">a</TOKEN>
<TOKEN id="token-14-4" pos="unknown" morph="none" start_char="1484" end_char="1495">non-zoonotic</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1497" end_char="1502">origin</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1504" end_char="1505">is</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1507" end_char="1512">widely</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1514" end_char="1523">considered</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1525" end_char="1525">a</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1527" end_char="1536">conspiracy</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1538" end_char="1543">theory</TOKEN>
<TOKEN id="token-14-12" pos="punct" morph="none" start_char="1544" end_char="1544">,</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="1546" end_char="1548">and</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="1550" end_char="1555">indeed</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="1557" end_char="1560">many</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="1562" end_char="1565">such</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="1567" end_char="1572">claims</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="1574" end_char="1576">are</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="1578" end_char="1583">easily</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="1585" end_char="1593">refutable</TOKEN>
<TOKEN id="token-14-21" pos="word" morph="none" start_char="1595" end_char="1601">without</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="1603" end_char="1611">requiring</TOKEN>
<TOKEN id="token-14-23" pos="word" morph="none" start_char="1613" end_char="1625">probabilistic</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="1627" end_char="1635">inference</TOKEN>
<TOKEN id="token-14-25" pos="punct" morph="none" start_char="1636" end_char="1636">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1638" end_char="1852">
<ORIGINAL_TEXT>However, the possibility of a lab escape does require serious examination, especially when considering the proximity of the source to a major coronavirus lab and several unusual findings in the genome of SARS-CoV-2.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1638" end_char="1644">However</TOKEN>
<TOKEN id="token-15-1" pos="punct" morph="none" start_char="1645" end_char="1645">,</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1647" end_char="1649">the</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="1651" end_char="1661">possibility</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1663" end_char="1664">of</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1666" end_char="1666">a</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="1668" end_char="1670">lab</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1672" end_char="1677">escape</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1679" end_char="1682">does</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="1684" end_char="1690">require</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1692" end_char="1698">serious</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="1700" end_char="1710">examination</TOKEN>
<TOKEN id="token-15-12" pos="punct" morph="none" start_char="1711" end_char="1711">,</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="1713" end_char="1722">especially</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="1724" end_char="1727">when</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="1729" end_char="1739">considering</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="1741" end_char="1743">the</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="1745" end_char="1753">proximity</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="1755" end_char="1756">of</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="1758" end_char="1760">the</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="1762" end_char="1767">source</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="1769" end_char="1770">to</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="1772" end_char="1772">a</TOKEN>
<TOKEN id="token-15-23" pos="word" morph="none" start_char="1774" end_char="1778">major</TOKEN>
<TOKEN id="token-15-24" pos="word" morph="none" start_char="1780" end_char="1790">coronavirus</TOKEN>
<TOKEN id="token-15-25" pos="word" morph="none" start_char="1792" end_char="1794">lab</TOKEN>
<TOKEN id="token-15-26" pos="word" morph="none" start_char="1796" end_char="1798">and</TOKEN>
<TOKEN id="token-15-27" pos="word" morph="none" start_char="1800" end_char="1806">several</TOKEN>
<TOKEN id="token-15-28" pos="word" morph="none" start_char="1808" end_char="1814">unusual</TOKEN>
<TOKEN id="token-15-29" pos="word" morph="none" start_char="1816" end_char="1823">findings</TOKEN>
<TOKEN id="token-15-30" pos="word" morph="none" start_char="1825" end_char="1826">in</TOKEN>
<TOKEN id="token-15-31" pos="word" morph="none" start_char="1828" end_char="1830">the</TOKEN>
<TOKEN id="token-15-32" pos="word" morph="none" start_char="1832" end_char="1837">genome</TOKEN>
<TOKEN id="token-15-33" pos="word" morph="none" start_char="1839" end_char="1840">of</TOKEN>
<TOKEN id="token-15-34" pos="unknown" morph="none" start_char="1842" end_char="1851">SARS-CoV-2</TOKEN>
<TOKEN id="token-15-35" pos="punct" morph="none" start_char="1852" end_char="1852">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1854" end_char="2005">
<ORIGINAL_TEXT>Due to the complexities of weighing an unlikely lab origin against findings that are unlikely for a zoonotic source, a probabilistic analysis is needed.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1854" end_char="1856">Due</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1858" end_char="1859">to</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1861" end_char="1863">the</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1865" end_char="1876">complexities</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1878" end_char="1879">of</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1881" end_char="1888">weighing</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="1890" end_char="1891">an</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="1893" end_char="1900">unlikely</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="1902" end_char="1904">lab</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="1906" end_char="1911">origin</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="1913" end_char="1919">against</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="1921" end_char="1928">findings</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="1930" end_char="1933">that</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="1935" end_char="1937">are</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="1939" end_char="1946">unlikely</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="1948" end_char="1950">for</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="1952" end_char="1952">a</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="1954" end_char="1961">zoonotic</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="1963" end_char="1968">source</TOKEN>
<TOKEN id="token-16-19" pos="punct" morph="none" start_char="1969" end_char="1969">,</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="1971" end_char="1971">a</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="1973" end_char="1985">probabilistic</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="1987" end_char="1994">analysis</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="1996" end_char="1997">is</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="1999" end_char="2004">needed</TOKEN>
<TOKEN id="token-16-25" pos="punct" morph="none" start_char="2005" end_char="2005">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2008" end_char="2087">
<ORIGINAL_TEXT>I'm sure this will likely be dismissed by the majority of media and many people.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2008" end_char="2010">I'm</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2012" end_char="2015">sure</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2017" end_char="2020">this</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2022" end_char="2025">will</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2027" end_char="2032">likely</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2034" end_char="2035">be</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2037" end_char="2045">dismissed</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="2047" end_char="2048">by</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="2050" end_char="2052">the</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="2054" end_char="2061">majority</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="2063" end_char="2064">of</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="2066" end_char="2070">media</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="2072" end_char="2074">and</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="2076" end_char="2079">many</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="2081" end_char="2086">people</TOKEN>
<TOKEN id="token-17-15" pos="punct" morph="none" start_char="2087" end_char="2087">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2089" end_char="2135">
<ORIGINAL_TEXT>But it seems like it's worth looking more into.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2089" end_char="2091">But</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2093" end_char="2094">it</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="2096" end_char="2100">seems</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="2102" end_char="2105">like</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2107" end_char="2110">it's</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="2112" end_char="2116">worth</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2118" end_char="2124">looking</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2126" end_char="2129">more</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2131" end_char="2134">into</TOKEN>
<TOKEN id="token-18-9" pos="punct" morph="none" start_char="2135" end_char="2135">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2137" end_char="2202">
<ORIGINAL_TEXT>The link goes more into the evidence behind the eventual decision.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2137" end_char="2139">The</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2141" end_char="2144">link</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2146" end_char="2149">goes</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="2151" end_char="2154">more</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2156" end_char="2159">into</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="2161" end_char="2163">the</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2165" end_char="2172">evidence</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="2174" end_char="2179">behind</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="2181" end_char="2183">the</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="2185" end_char="2192">eventual</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="2194" end_char="2201">decision</TOKEN>
<TOKEN id="token-19-11" pos="punct" morph="none" start_char="2202" end_char="2202">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2205" end_char="2252">
<ORIGINAL_TEXT>ETA: a bit of info on gain of function research.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2205" end_char="2207">ETA</TOKEN>
<TOKEN id="token-20-1" pos="punct" morph="none" start_char="2208" end_char="2208">:</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="2210" end_char="2210">a</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="2212" end_char="2214">bit</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2216" end_char="2217">of</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2219" end_char="2222">info</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2224" end_char="2225">on</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="2227" end_char="2230">gain</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="2232" end_char="2233">of</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="2235" end_char="2242">function</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="2244" end_char="2251">research</TOKEN>
<TOKEN id="token-20-11" pos="punct" morph="none" start_char="2252" end_char="2252">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="2255" end_char="2277">
<ORIGINAL_TEXT>www.ncbi.nlm.nih.gov...</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="url" morph="none" start_char="2255" end_char="2277">www.ncbi.nlm.nih.gov...</TOKEN>
</SEG>
<SEG id="segment-22" start_char="2280" end_char="2561">
<ORIGINAL_TEXT>Subbarao explained that routine virological methods involve experiments that aim to produce a gain of a desired function, such as higher yields for vaccine strains, but often also lead to loss of function, such as loss of the ability for a virus to replicate well, as a consequence.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="2280" end_char="2287">Subbarao</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="2289" end_char="2297">explained</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="2299" end_char="2302">that</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="2304" end_char="2310">routine</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="2312" end_char="2322">virological</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="2324" end_char="2330">methods</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="2332" end_char="2338">involve</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="2340" end_char="2350">experiments</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="2352" end_char="2355">that</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="2357" end_char="2359">aim</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="2361" end_char="2362">to</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="2364" end_char="2370">produce</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="2372" end_char="2372">a</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="2374" end_char="2377">gain</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="2379" end_char="2380">of</TOKEN>
<TOKEN id="token-22-15" pos="word" morph="none" start_char="2382" end_char="2382">a</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="2384" end_char="2390">desired</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="2392" end_char="2399">function</TOKEN>
<TOKEN id="token-22-18" pos="punct" morph="none" start_char="2400" end_char="2400">,</TOKEN>
<TOKEN id="token-22-19" pos="word" morph="none" start_char="2402" end_char="2405">such</TOKEN>
<TOKEN id="token-22-20" pos="word" morph="none" start_char="2407" end_char="2408">as</TOKEN>
<TOKEN id="token-22-21" pos="word" morph="none" start_char="2410" end_char="2415">higher</TOKEN>
<TOKEN id="token-22-22" pos="word" morph="none" start_char="2417" end_char="2422">yields</TOKEN>
<TOKEN id="token-22-23" pos="word" morph="none" start_char="2424" end_char="2426">for</TOKEN>
<TOKEN id="token-22-24" pos="word" morph="none" start_char="2428" end_char="2434">vaccine</TOKEN>
<TOKEN id="token-22-25" pos="word" morph="none" start_char="2436" end_char="2442">strains</TOKEN>
<TOKEN id="token-22-26" pos="punct" morph="none" start_char="2443" end_char="2443">,</TOKEN>
<TOKEN id="token-22-27" pos="word" morph="none" start_char="2445" end_char="2447">but</TOKEN>
<TOKEN id="token-22-28" pos="word" morph="none" start_char="2449" end_char="2453">often</TOKEN>
<TOKEN id="token-22-29" pos="word" morph="none" start_char="2455" end_char="2458">also</TOKEN>
<TOKEN id="token-22-30" pos="word" morph="none" start_char="2460" end_char="2463">lead</TOKEN>
<TOKEN id="token-22-31" pos="word" morph="none" start_char="2465" end_char="2466">to</TOKEN>
<TOKEN id="token-22-32" pos="word" morph="none" start_char="2468" end_char="2471">loss</TOKEN>
<TOKEN id="token-22-33" pos="word" morph="none" start_char="2473" end_char="2474">of</TOKEN>
<TOKEN id="token-22-34" pos="word" morph="none" start_char="2476" end_char="2483">function</TOKEN>
<TOKEN id="token-22-35" pos="punct" morph="none" start_char="2484" end_char="2484">,</TOKEN>
<TOKEN id="token-22-36" pos="word" morph="none" start_char="2486" end_char="2489">such</TOKEN>
<TOKEN id="token-22-37" pos="word" morph="none" start_char="2491" end_char="2492">as</TOKEN>
<TOKEN id="token-22-38" pos="word" morph="none" start_char="2494" end_char="2497">loss</TOKEN>
<TOKEN id="token-22-39" pos="word" morph="none" start_char="2499" end_char="2500">of</TOKEN>
<TOKEN id="token-22-40" pos="word" morph="none" start_char="2502" end_char="2504">the</TOKEN>
<TOKEN id="token-22-41" pos="word" morph="none" start_char="2506" end_char="2512">ability</TOKEN>
<TOKEN id="token-22-42" pos="word" morph="none" start_char="2514" end_char="2516">for</TOKEN>
<TOKEN id="token-22-43" pos="word" morph="none" start_char="2518" end_char="2518">a</TOKEN>
<TOKEN id="token-22-44" pos="word" morph="none" start_char="2520" end_char="2524">virus</TOKEN>
<TOKEN id="token-22-45" pos="word" morph="none" start_char="2526" end_char="2527">to</TOKEN>
<TOKEN id="token-22-46" pos="word" morph="none" start_char="2529" end_char="2537">replicate</TOKEN>
<TOKEN id="token-22-47" pos="word" morph="none" start_char="2539" end_char="2542">well</TOKEN>
<TOKEN id="token-22-48" pos="punct" morph="none" start_char="2543" end_char="2543">,</TOKEN>
<TOKEN id="token-22-49" pos="word" morph="none" start_char="2545" end_char="2546">as</TOKEN>
<TOKEN id="token-22-50" pos="word" morph="none" start_char="2548" end_char="2548">a</TOKEN>
<TOKEN id="token-22-51" pos="word" morph="none" start_char="2550" end_char="2560">consequence</TOKEN>
<TOKEN id="token-22-52" pos="punct" morph="none" start_char="2561" end_char="2561">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="2563" end_char="2806">
<ORIGINAL_TEXT>In other words, any selection process involving an alteration of genotypes and their resulting phenotypes is considered a type of Gain-of-Function (GoF) research, even if the U.S. policy is intended to apply to only a small subset of such work.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="2563" end_char="2564">In</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="2566" end_char="2570">other</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="2572" end_char="2576">words</TOKEN>
<TOKEN id="token-23-3" pos="punct" morph="none" start_char="2577" end_char="2577">,</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="2579" end_char="2581">any</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="2583" end_char="2591">selection</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="2593" end_char="2599">process</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="2601" end_char="2609">involving</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="2611" end_char="2612">an</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="2614" end_char="2623">alteration</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="2625" end_char="2626">of</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="2628" end_char="2636">genotypes</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="2638" end_char="2640">and</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="2642" end_char="2646">their</TOKEN>
<TOKEN id="token-23-14" pos="word" morph="none" start_char="2648" end_char="2656">resulting</TOKEN>
<TOKEN id="token-23-15" pos="word" morph="none" start_char="2658" end_char="2667">phenotypes</TOKEN>
<TOKEN id="token-23-16" pos="word" morph="none" start_char="2669" end_char="2670">is</TOKEN>
<TOKEN id="token-23-17" pos="word" morph="none" start_char="2672" end_char="2681">considered</TOKEN>
<TOKEN id="token-23-18" pos="word" morph="none" start_char="2683" end_char="2683">a</TOKEN>
<TOKEN id="token-23-19" pos="word" morph="none" start_char="2685" end_char="2688">type</TOKEN>
<TOKEN id="token-23-20" pos="word" morph="none" start_char="2690" end_char="2691">of</TOKEN>
<TOKEN id="token-23-21" pos="unknown" morph="none" start_char="2693" end_char="2708">Gain-of-Function</TOKEN>
<TOKEN id="token-23-22" pos="punct" morph="none" start_char="2710" end_char="2710">(</TOKEN>
<TOKEN id="token-23-23" pos="word" morph="none" start_char="2711" end_char="2713">GoF</TOKEN>
<TOKEN id="token-23-24" pos="punct" morph="none" start_char="2714" end_char="2714">)</TOKEN>
<TOKEN id="token-23-25" pos="word" morph="none" start_char="2716" end_char="2723">research</TOKEN>
<TOKEN id="token-23-26" pos="punct" morph="none" start_char="2724" end_char="2724">,</TOKEN>
<TOKEN id="token-23-27" pos="word" morph="none" start_char="2726" end_char="2729">even</TOKEN>
<TOKEN id="token-23-28" pos="word" morph="none" start_char="2731" end_char="2732">if</TOKEN>
<TOKEN id="token-23-29" pos="word" morph="none" start_char="2734" end_char="2736">the</TOKEN>
<TOKEN id="token-23-30" pos="unknown" morph="none" start_char="2738" end_char="2740">U.S</TOKEN>
<TOKEN id="token-23-31" pos="punct" morph="none" start_char="2741" end_char="2741">.</TOKEN>
<TOKEN id="token-23-32" pos="word" morph="none" start_char="2743" end_char="2748">policy</TOKEN>
<TOKEN id="token-23-33" pos="word" morph="none" start_char="2750" end_char="2751">is</TOKEN>
<TOKEN id="token-23-34" pos="word" morph="none" start_char="2753" end_char="2760">intended</TOKEN>
<TOKEN id="token-23-35" pos="word" morph="none" start_char="2762" end_char="2763">to</TOKEN>
<TOKEN id="token-23-36" pos="word" morph="none" start_char="2765" end_char="2769">apply</TOKEN>
<TOKEN id="token-23-37" pos="word" morph="none" start_char="2771" end_char="2772">to</TOKEN>
<TOKEN id="token-23-38" pos="word" morph="none" start_char="2774" end_char="2777">only</TOKEN>
<TOKEN id="token-23-39" pos="word" morph="none" start_char="2779" end_char="2779">a</TOKEN>
<TOKEN id="token-23-40" pos="word" morph="none" start_char="2781" end_char="2785">small</TOKEN>
<TOKEN id="token-23-41" pos="word" morph="none" start_char="2787" end_char="2792">subset</TOKEN>
<TOKEN id="token-23-42" pos="word" morph="none" start_char="2794" end_char="2795">of</TOKEN>
<TOKEN id="token-23-43" pos="word" morph="none" start_char="2797" end_char="2800">such</TOKEN>
<TOKEN id="token-23-44" pos="word" morph="none" start_char="2802" end_char="2805">work</TOKEN>
<TOKEN id="token-23-45" pos="punct" morph="none" start_char="2806" end_char="2806">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="2808" end_char="3023">
<ORIGINAL_TEXT>Subbarao emphasized that such experiments in virology are fundamental to understanding the biology, ecology, and pathogenesis of viruses and added that much basic knowledge is still lacking for SARS-CoV and MERS-CoV.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="2808" end_char="2815">Subbarao</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="2817" end_char="2826">emphasized</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="2828" end_char="2831">that</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="2833" end_char="2836">such</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="2838" end_char="2848">experiments</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="2850" end_char="2851">in</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="2853" end_char="2860">virology</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="2862" end_char="2864">are</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="2866" end_char="2876">fundamental</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="2878" end_char="2879">to</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="2881" end_char="2893">understanding</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="2895" end_char="2897">the</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="2899" end_char="2905">biology</TOKEN>
<TOKEN id="token-24-13" pos="punct" morph="none" start_char="2906" end_char="2906">,</TOKEN>
<TOKEN id="token-24-14" pos="word" morph="none" start_char="2908" end_char="2914">ecology</TOKEN>
<TOKEN id="token-24-15" pos="punct" morph="none" start_char="2915" end_char="2915">,</TOKEN>
<TOKEN id="token-24-16" pos="word" morph="none" start_char="2917" end_char="2919">and</TOKEN>
<TOKEN id="token-24-17" pos="word" morph="none" start_char="2921" end_char="2932">pathogenesis</TOKEN>
<TOKEN id="token-24-18" pos="word" morph="none" start_char="2934" end_char="2935">of</TOKEN>
<TOKEN id="token-24-19" pos="word" morph="none" start_char="2937" end_char="2943">viruses</TOKEN>
<TOKEN id="token-24-20" pos="word" morph="none" start_char="2945" end_char="2947">and</TOKEN>
<TOKEN id="token-24-21" pos="word" morph="none" start_char="2949" end_char="2953">added</TOKEN>
<TOKEN id="token-24-22" pos="word" morph="none" start_char="2955" end_char="2958">that</TOKEN>
<TOKEN id="token-24-23" pos="word" morph="none" start_char="2960" end_char="2963">much</TOKEN>
<TOKEN id="token-24-24" pos="word" morph="none" start_char="2965" end_char="2969">basic</TOKEN>
<TOKEN id="token-24-25" pos="word" morph="none" start_char="2971" end_char="2979">knowledge</TOKEN>
<TOKEN id="token-24-26" pos="word" morph="none" start_char="2981" end_char="2982">is</TOKEN>
<TOKEN id="token-24-27" pos="word" morph="none" start_char="2984" end_char="2988">still</TOKEN>
<TOKEN id="token-24-28" pos="word" morph="none" start_char="2990" end_char="2996">lacking</TOKEN>
<TOKEN id="token-24-29" pos="word" morph="none" start_char="2998" end_char="3000">for</TOKEN>
<TOKEN id="token-24-30" pos="unknown" morph="none" start_char="3002" end_char="3009">SARS-CoV</TOKEN>
<TOKEN id="token-24-31" pos="word" morph="none" start_char="3011" end_char="3013">and</TOKEN>
<TOKEN id="token-24-32" pos="unknown" morph="none" start_char="3015" end_char="3022">MERS-CoV</TOKEN>
<TOKEN id="token-24-33" pos="punct" morph="none" start_char="3023" end_char="3023">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="3025" end_char="3266">
<ORIGINAL_TEXT>Subbarao introduced the key questions that virologists ask at all stages of research on the emergence or re-emergence of a virus and specifically adapted these general questions to the three viruses of interest in the symposium (see Box 3-1).</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="3025" end_char="3032">Subbarao</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="3034" end_char="3043">introduced</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="3045" end_char="3047">the</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="3049" end_char="3051">key</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="3053" end_char="3061">questions</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="3063" end_char="3066">that</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="3068" end_char="3078">virologists</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="3080" end_char="3082">ask</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="3084" end_char="3085">at</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="3087" end_char="3089">all</TOKEN>
<TOKEN id="token-25-10" pos="word" morph="none" start_char="3091" end_char="3096">stages</TOKEN>
<TOKEN id="token-25-11" pos="word" morph="none" start_char="3098" end_char="3099">of</TOKEN>
<TOKEN id="token-25-12" pos="word" morph="none" start_char="3101" end_char="3108">research</TOKEN>
<TOKEN id="token-25-13" pos="word" morph="none" start_char="3110" end_char="3111">on</TOKEN>
<TOKEN id="token-25-14" pos="word" morph="none" start_char="3113" end_char="3115">the</TOKEN>
<TOKEN id="token-25-15" pos="word" morph="none" start_char="3117" end_char="3125">emergence</TOKEN>
<TOKEN id="token-25-16" pos="word" morph="none" start_char="3127" end_char="3128">or</TOKEN>
<TOKEN id="token-25-17" pos="unknown" morph="none" start_char="3130" end_char="3141">re-emergence</TOKEN>
<TOKEN id="token-25-18" pos="word" morph="none" start_char="3143" end_char="3144">of</TOKEN>
<TOKEN id="token-25-19" pos="word" morph="none" start_char="3146" end_char="3146">a</TOKEN>
<TOKEN id="token-25-20" pos="word" morph="none" start_char="3148" end_char="3152">virus</TOKEN>
<TOKEN id="token-25-21" pos="word" morph="none" start_char="3154" end_char="3156">and</TOKEN>
<TOKEN id="token-25-22" pos="word" morph="none" start_char="3158" end_char="3169">specifically</TOKEN>
<TOKEN id="token-25-23" pos="word" morph="none" start_char="3171" end_char="3177">adapted</TOKEN>
<TOKEN id="token-25-24" pos="word" morph="none" start_char="3179" end_char="3183">these</TOKEN>
<TOKEN id="token-25-25" pos="word" morph="none" start_char="3185" end_char="3191">general</TOKEN>
<TOKEN id="token-25-26" pos="word" morph="none" start_char="3193" end_char="3201">questions</TOKEN>
<TOKEN id="token-25-27" pos="word" morph="none" start_char="3203" end_char="3204">to</TOKEN>
<TOKEN id="token-25-28" pos="word" morph="none" start_char="3206" end_char="3208">the</TOKEN>
<TOKEN id="token-25-29" pos="word" morph="none" start_char="3210" end_char="3214">three</TOKEN>
<TOKEN id="token-25-30" pos="word" morph="none" start_char="3216" end_char="3222">viruses</TOKEN>
<TOKEN id="token-25-31" pos="word" morph="none" start_char="3224" end_char="3225">of</TOKEN>
<TOKEN id="token-25-32" pos="word" morph="none" start_char="3227" end_char="3234">interest</TOKEN>
<TOKEN id="token-25-33" pos="word" morph="none" start_char="3236" end_char="3237">in</TOKEN>
<TOKEN id="token-25-34" pos="word" morph="none" start_char="3239" end_char="3241">the</TOKEN>
<TOKEN id="token-25-35" pos="word" morph="none" start_char="3243" end_char="3251">symposium</TOKEN>
<TOKEN id="token-25-36" pos="punct" morph="none" start_char="3253" end_char="3253">(</TOKEN>
<TOKEN id="token-25-37" pos="word" morph="none" start_char="3254" end_char="3256">see</TOKEN>
<TOKEN id="token-25-38" pos="word" morph="none" start_char="3258" end_char="3260">Box</TOKEN>
<TOKEN id="token-25-39" pos="unknown" morph="none" start_char="3262" end_char="3264">3-1</TOKEN>
<TOKEN id="token-25-40" pos="punct" morph="none" start_char="3265" end_char="3266">).</TOKEN>
</SEG>
<SEG id="segment-26" start_char="3268" end_char="3437">
<ORIGINAL_TEXT>To answer these questions, virologists use gain- and loss-of-function experiments to understand the genetic makeup of viruses and the specifics of virus-host interaction.</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="3268" end_char="3269">To</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="3271" end_char="3276">answer</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="3278" end_char="3282">these</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="3284" end_char="3292">questions</TOKEN>
<TOKEN id="token-26-4" pos="punct" morph="none" start_char="3293" end_char="3293">,</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="3295" end_char="3305">virologists</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="3307" end_char="3309">use</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="3311" end_char="3314">gain</TOKEN>
<TOKEN id="token-26-8" pos="punct" morph="none" start_char="3315" end_char="3315">-</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="3317" end_char="3319">and</TOKEN>
<TOKEN id="token-26-10" pos="unknown" morph="none" start_char="3321" end_char="3336">loss-of-function</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="3338" end_char="3348">experiments</TOKEN>
<TOKEN id="token-26-12" pos="word" morph="none" start_char="3350" end_char="3351">to</TOKEN>
<TOKEN id="token-26-13" pos="word" morph="none" start_char="3353" end_char="3362">understand</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="3364" end_char="3366">the</TOKEN>
<TOKEN id="token-26-15" pos="word" morph="none" start_char="3368" end_char="3374">genetic</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="3376" end_char="3381">makeup</TOKEN>
<TOKEN id="token-26-17" pos="word" morph="none" start_char="3383" end_char="3384">of</TOKEN>
<TOKEN id="token-26-18" pos="word" morph="none" start_char="3386" end_char="3392">viruses</TOKEN>
<TOKEN id="token-26-19" pos="word" morph="none" start_char="3394" end_char="3396">and</TOKEN>
<TOKEN id="token-26-20" pos="word" morph="none" start_char="3398" end_char="3400">the</TOKEN>
<TOKEN id="token-26-21" pos="word" morph="none" start_char="3402" end_char="3410">specifics</TOKEN>
<TOKEN id="token-26-22" pos="word" morph="none" start_char="3412" end_char="3413">of</TOKEN>
<TOKEN id="token-26-23" pos="unknown" morph="none" start_char="3415" end_char="3424">virus-host</TOKEN>
<TOKEN id="token-26-24" pos="word" morph="none" start_char="3426" end_char="3436">interaction</TOKEN>
<TOKEN id="token-26-25" pos="punct" morph="none" start_char="3437" end_char="3437">.</TOKEN>
</SEG>
<SEG id="segment-27" start_char="3439" end_char="3722">
<ORIGINAL_TEXT>For instance, researchers now have advanced molecular technologies, such as reverse genetics, which allow them to produce de novo recombinant viruses from cloned cDNA, and deep sequencing that are critical for studying how viruses escape the host immune system and antiviral controls.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="3439" end_char="3441">For</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="3443" end_char="3450">instance</TOKEN>
<TOKEN id="token-27-2" pos="punct" morph="none" start_char="3451" end_char="3451">,</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="3453" end_char="3463">researchers</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="3465" end_char="3467">now</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="3469" end_char="3472">have</TOKEN>
<TOKEN id="token-27-6" pos="word" morph="none" start_char="3474" end_char="3481">advanced</TOKEN>
<TOKEN id="token-27-7" pos="word" morph="none" start_char="3483" end_char="3491">molecular</TOKEN>
<TOKEN id="token-27-8" pos="word" morph="none" start_char="3493" end_char="3504">technologies</TOKEN>
<TOKEN id="token-27-9" pos="punct" morph="none" start_char="3505" end_char="3505">,</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="3507" end_char="3510">such</TOKEN>
<TOKEN id="token-27-11" pos="word" morph="none" start_char="3512" end_char="3513">as</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="3515" end_char="3521">reverse</TOKEN>
<TOKEN id="token-27-13" pos="word" morph="none" start_char="3523" end_char="3530">genetics</TOKEN>
<TOKEN id="token-27-14" pos="punct" morph="none" start_char="3531" end_char="3531">,</TOKEN>
<TOKEN id="token-27-15" pos="word" morph="none" start_char="3533" end_char="3537">which</TOKEN>
<TOKEN id="token-27-16" pos="word" morph="none" start_char="3539" end_char="3543">allow</TOKEN>
<TOKEN id="token-27-17" pos="word" morph="none" start_char="3545" end_char="3548">them</TOKEN>
<TOKEN id="token-27-18" pos="word" morph="none" start_char="3550" end_char="3551">to</TOKEN>
<TOKEN id="token-27-19" pos="word" morph="none" start_char="3553" end_char="3559">produce</TOKEN>
<TOKEN id="token-27-20" pos="word" morph="none" start_char="3561" end_char="3562">de</TOKEN>
<TOKEN id="token-27-21" pos="word" morph="none" start_char="3564" end_char="3567">novo</TOKEN>
<TOKEN id="token-27-22" pos="word" morph="none" start_char="3569" end_char="3579">recombinant</TOKEN>
<TOKEN id="token-27-23" pos="word" morph="none" start_char="3581" end_char="3587">viruses</TOKEN>
<TOKEN id="token-27-24" pos="word" morph="none" start_char="3589" end_char="3592">from</TOKEN>
<TOKEN id="token-27-25" pos="word" morph="none" start_char="3594" end_char="3599">cloned</TOKEN>
<TOKEN id="token-27-26" pos="word" morph="none" start_char="3601" end_char="3604">cDNA</TOKEN>
<TOKEN id="token-27-27" pos="punct" morph="none" start_char="3605" end_char="3605">,</TOKEN>
<TOKEN id="token-27-28" pos="word" morph="none" start_char="3607" end_char="3609">and</TOKEN>
<TOKEN id="token-27-29" pos="word" morph="none" start_char="3611" end_char="3614">deep</TOKEN>
<TOKEN id="token-27-30" pos="word" morph="none" start_char="3616" end_char="3625">sequencing</TOKEN>
<TOKEN id="token-27-31" pos="word" morph="none" start_char="3627" end_char="3630">that</TOKEN>
<TOKEN id="token-27-32" pos="word" morph="none" start_char="3632" end_char="3634">are</TOKEN>
<TOKEN id="token-27-33" pos="word" morph="none" start_char="3636" end_char="3643">critical</TOKEN>
<TOKEN id="token-27-34" pos="word" morph="none" start_char="3645" end_char="3647">for</TOKEN>
<TOKEN id="token-27-35" pos="word" morph="none" start_char="3649" end_char="3656">studying</TOKEN>
<TOKEN id="token-27-36" pos="word" morph="none" start_char="3658" end_char="3660">how</TOKEN>
<TOKEN id="token-27-37" pos="word" morph="none" start_char="3662" end_char="3668">viruses</TOKEN>
<TOKEN id="token-27-38" pos="word" morph="none" start_char="3670" end_char="3675">escape</TOKEN>
<TOKEN id="token-27-39" pos="word" morph="none" start_char="3677" end_char="3679">the</TOKEN>
<TOKEN id="token-27-40" pos="word" morph="none" start_char="3681" end_char="3684">host</TOKEN>
<TOKEN id="token-27-41" pos="word" morph="none" start_char="3686" end_char="3691">immune</TOKEN>
<TOKEN id="token-27-42" pos="word" morph="none" start_char="3693" end_char="3698">system</TOKEN>
<TOKEN id="token-27-43" pos="word" morph="none" start_char="3700" end_char="3702">and</TOKEN>
<TOKEN id="token-27-44" pos="word" morph="none" start_char="3704" end_char="3712">antiviral</TOKEN>
<TOKEN id="token-27-45" pos="word" morph="none" start_char="3714" end_char="3721">controls</TOKEN>
<TOKEN id="token-27-46" pos="punct" morph="none" start_char="3722" end_char="3722">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="3724" end_char="3888">
<ORIGINAL_TEXT>Researchers also use targeted host or viral genome modification using small interfering RNA or the bacterial CRISPR-associated protein-9 nuclease as an editing tool.</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="3724" end_char="3734">Researchers</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="3736" end_char="3739">also</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="3741" end_char="3743">use</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="3745" end_char="3752">targeted</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="3754" end_char="3757">host</TOKEN>
<TOKEN id="token-28-5" pos="word" morph="none" start_char="3759" end_char="3760">or</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="3762" end_char="3766">viral</TOKEN>
<TOKEN id="token-28-7" pos="word" morph="none" start_char="3768" end_char="3773">genome</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="3775" end_char="3786">modification</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="3788" end_char="3792">using</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="3794" end_char="3798">small</TOKEN>
<TOKEN id="token-28-11" pos="word" morph="none" start_char="3800" end_char="3810">interfering</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="3812" end_char="3814">RNA</TOKEN>
<TOKEN id="token-28-13" pos="word" morph="none" start_char="3816" end_char="3817">or</TOKEN>
<TOKEN id="token-28-14" pos="word" morph="none" start_char="3819" end_char="3821">the</TOKEN>
<TOKEN id="token-28-15" pos="word" morph="none" start_char="3823" end_char="3831">bacterial</TOKEN>
<TOKEN id="token-28-16" pos="unknown" morph="none" start_char="3833" end_char="3849">CRISPR-associated</TOKEN>
<TOKEN id="token-28-17" pos="unknown" morph="none" start_char="3851" end_char="3859">protein-9</TOKEN>
<TOKEN id="token-28-18" pos="word" morph="none" start_char="3861" end_char="3868">nuclease</TOKEN>
<TOKEN id="token-28-19" pos="word" morph="none" start_char="3870" end_char="3871">as</TOKEN>
<TOKEN id="token-28-20" pos="word" morph="none" start_char="3873" end_char="3874">an</TOKEN>
<TOKEN id="token-28-21" pos="word" morph="none" start_char="3876" end_char="3882">editing</TOKEN>
<TOKEN id="token-28-22" pos="word" morph="none" start_char="3884" end_char="3887">tool</TOKEN>
<TOKEN id="token-28-23" pos="punct" morph="none" start_char="3888" end_char="3888">.</TOKEN>
</SEG>
<SEG id="segment-29" start_char="3891" end_char="3944">
<ORIGINAL_TEXT>edit on 30/12/2020 by dug88 because: (no reason given)</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="3891" end_char="3894">edit</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="3896" end_char="3897">on</TOKEN>
<TOKEN id="token-29-2" pos="unknown" morph="none" start_char="3899" end_char="3908">30/12/2020</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="3910" end_char="3911">by</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="3913" end_char="3917">dug88</TOKEN>
<TOKEN id="token-29-5" pos="word" morph="none" start_char="3919" end_char="3925">because</TOKEN>
<TOKEN id="token-29-6" pos="punct" morph="none" start_char="3926" end_char="3926">:</TOKEN>
<TOKEN id="token-29-7" pos="punct" morph="none" start_char="3928" end_char="3928">(</TOKEN>
<TOKEN id="token-29-8" pos="word" morph="none" start_char="3929" end_char="3930">no</TOKEN>
<TOKEN id="token-29-9" pos="word" morph="none" start_char="3932" end_char="3937">reason</TOKEN>
<TOKEN id="token-29-10" pos="word" morph="none" start_char="3939" end_char="3943">given</TOKEN>
<TOKEN id="token-29-11" pos="punct" morph="none" start_char="3944" end_char="3944">)</TOKEN>
</SEG>
<SEG id="segment-30" start_char="3949" end_char="4175">
<ORIGINAL_TEXT>originally posted by: dug88 An Israeli startup named Rootclaim has determined that there is an 81% chance that the SARS-cov-2 was accidentally released from the Wuhan bioresearch laboratory during gain-of-function research. ...</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="3949" end_char="3958">originally</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="3960" end_char="3965">posted</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="3967" end_char="3968">by</TOKEN>
<TOKEN id="token-30-3" pos="punct" morph="none" start_char="3969" end_char="3969">:</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="3971" end_char="3975">dug88</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="3977" end_char="3978">An</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="3980" end_char="3986">Israeli</TOKEN>
<TOKEN id="token-30-7" pos="word" morph="none" start_char="3988" end_char="3994">startup</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="3996" end_char="4000">named</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="4002" end_char="4010">Rootclaim</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="4012" end_char="4014">has</TOKEN>
<TOKEN id="token-30-11" pos="word" morph="none" start_char="4016" end_char="4025">determined</TOKEN>
<TOKEN id="token-30-12" pos="word" morph="none" start_char="4027" end_char="4030">that</TOKEN>
<TOKEN id="token-30-13" pos="word" morph="none" start_char="4032" end_char="4036">there</TOKEN>
<TOKEN id="token-30-14" pos="word" morph="none" start_char="4038" end_char="4039">is</TOKEN>
<TOKEN id="token-30-15" pos="word" morph="none" start_char="4041" end_char="4042">an</TOKEN>
<TOKEN id="token-30-16" pos="word" morph="none" start_char="4044" end_char="4045">81</TOKEN>
<TOKEN id="token-30-17" pos="punct" morph="none" start_char="4046" end_char="4046">%</TOKEN>
<TOKEN id="token-30-18" pos="word" morph="none" start_char="4048" end_char="4053">chance</TOKEN>
<TOKEN id="token-30-19" pos="word" morph="none" start_char="4055" end_char="4058">that</TOKEN>
<TOKEN id="token-30-20" pos="word" morph="none" start_char="4060" end_char="4062">the</TOKEN>
<TOKEN id="token-30-21" pos="unknown" morph="none" start_char="4064" end_char="4073">SARS-cov-2</TOKEN>
<TOKEN id="token-30-22" pos="word" morph="none" start_char="4075" end_char="4077">was</TOKEN>
<TOKEN id="token-30-23" pos="word" morph="none" start_char="4079" end_char="4090">accidentally</TOKEN>
<TOKEN id="token-30-24" pos="word" morph="none" start_char="4092" end_char="4099">released</TOKEN>
<TOKEN id="token-30-25" pos="word" morph="none" start_char="4101" end_char="4104">from</TOKEN>
<TOKEN id="token-30-26" pos="word" morph="none" start_char="4106" end_char="4108">the</TOKEN>
<TOKEN id="token-30-27" pos="word" morph="none" start_char="4110" end_char="4114">Wuhan</TOKEN>
<TOKEN id="token-30-28" pos="word" morph="none" start_char="4116" end_char="4126">bioresearch</TOKEN>
<TOKEN id="token-30-29" pos="word" morph="none" start_char="4128" end_char="4137">laboratory</TOKEN>
<TOKEN id="token-30-30" pos="word" morph="none" start_char="4139" end_char="4144">during</TOKEN>
<TOKEN id="token-30-31" pos="unknown" morph="none" start_char="4146" end_char="4161">gain-of-function</TOKEN>
<TOKEN id="token-30-32" pos="word" morph="none" start_char="4163" end_char="4170">research</TOKEN>
<TOKEN id="token-30-33" pos="punct" morph="none" start_char="4171" end_char="4171">.</TOKEN>
<TOKEN id="token-30-34" pos="punct" morph="none" start_char="4173" end_char="4175">...</TOKEN>
</SEG>
<SEG id="segment-31" start_char="4178" end_char="4216">
<ORIGINAL_TEXT>Didn't need no damn AI to tell me that!</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="word" morph="none" start_char="4178" end_char="4183">Didn't</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="4185" end_char="4188">need</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="4190" end_char="4191">no</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="4193" end_char="4196">damn</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="4198" end_char="4199">AI</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="4201" end_char="4202">to</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="4204" end_char="4207">tell</TOKEN>
<TOKEN id="token-31-7" pos="word" morph="none" start_char="4209" end_char="4210">me</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="4212" end_char="4215">that</TOKEN>
<TOKEN id="token-31-9" pos="punct" morph="none" start_char="4216" end_char="4216">!</TOKEN>
</SEG>
<SEG id="segment-32" start_char="4219" end_char="4283">
<ORIGINAL_TEXT>That is, if it's actually all as terrible as they claim it to be.</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="4219" end_char="4222">That</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="4224" end_char="4225">is</TOKEN>
<TOKEN id="token-32-2" pos="punct" morph="none" start_char="4226" end_char="4226">,</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="4228" end_char="4229">if</TOKEN>
<TOKEN id="token-32-4" pos="word" morph="none" start_char="4231" end_char="4234">it's</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="4236" end_char="4243">actually</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="4245" end_char="4247">all</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="4249" end_char="4250">as</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="4252" end_char="4259">terrible</TOKEN>
<TOKEN id="token-32-9" pos="word" morph="none" start_char="4261" end_char="4262">as</TOKEN>
<TOKEN id="token-32-10" pos="word" morph="none" start_char="4264" end_char="4267">they</TOKEN>
<TOKEN id="token-32-11" pos="word" morph="none" start_char="4269" end_char="4273">claim</TOKEN>
<TOKEN id="token-32-12" pos="word" morph="none" start_char="4275" end_char="4276">it</TOKEN>
<TOKEN id="token-32-13" pos="word" morph="none" start_char="4278" end_char="4279">to</TOKEN>
<TOKEN id="token-32-14" pos="word" morph="none" start_char="4281" end_char="4282">be</TOKEN>
<TOKEN id="token-32-15" pos="punct" morph="none" start_char="4283" end_char="4283">.</TOKEN>
</SEG>
<SEG id="segment-33" start_char="4285" end_char="4285">
<ORIGINAL_TEXT>:</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="punct" morph="none" start_char="4285" end_char="4285">:</TOKEN>
</SEG>
<SEG id="segment-34" start_char="4288" end_char="4341">
<ORIGINAL_TEXT>edit on 2020 12 30 by incoserv because: clarification.</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="4288" end_char="4291">edit</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="4293" end_char="4294">on</TOKEN>
<TOKEN id="token-34-2" pos="word" morph="none" start_char="4296" end_char="4299">2020</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="4301" end_char="4302">12</TOKEN>
<TOKEN id="token-34-4" pos="word" morph="none" start_char="4304" end_char="4305">30</TOKEN>
<TOKEN id="token-34-5" pos="word" morph="none" start_char="4307" end_char="4308">by</TOKEN>
<TOKEN id="token-34-6" pos="word" morph="none" start_char="4310" end_char="4317">incoserv</TOKEN>
<TOKEN id="token-34-7" pos="word" morph="none" start_char="4319" end_char="4325">because</TOKEN>
<TOKEN id="token-34-8" pos="punct" morph="none" start_char="4326" end_char="4326">:</TOKEN>
<TOKEN id="token-34-9" pos="word" morph="none" start_char="4328" end_char="4340">clarification</TOKEN>
<TOKEN id="token-34-10" pos="punct" morph="none" start_char="4341" end_char="4341">.</TOKEN>
</SEG>
<SEG id="segment-35" start_char="4346" end_char="4362">
<ORIGINAL_TEXT>a reply to: dug88</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="4346" end_char="4346">a</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="4348" end_char="4352">reply</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="4354" end_char="4355">to</TOKEN>
<TOKEN id="token-35-3" pos="punct" morph="none" start_char="4356" end_char="4356">:</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="4358" end_char="4362">dug88</TOKEN>
</SEG>
<SEG id="segment-36" start_char="4365" end_char="4434">
<ORIGINAL_TEXT>To be honest dug88, humans got there first....and quite some time ago.</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="4365" end_char="4366">To</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="4368" end_char="4369">be</TOKEN>
<TOKEN id="token-36-2" pos="word" morph="none" start_char="4371" end_char="4376">honest</TOKEN>
<TOKEN id="token-36-3" pos="word" morph="none" start_char="4378" end_char="4382">dug88</TOKEN>
<TOKEN id="token-36-4" pos="punct" morph="none" start_char="4383" end_char="4383">,</TOKEN>
<TOKEN id="token-36-5" pos="word" morph="none" start_char="4385" end_char="4390">humans</TOKEN>
<TOKEN id="token-36-6" pos="word" morph="none" start_char="4392" end_char="4394">got</TOKEN>
<TOKEN id="token-36-7" pos="word" morph="none" start_char="4396" end_char="4400">there</TOKEN>
<TOKEN id="token-36-8" pos="unknown" morph="none" start_char="4402" end_char="4413">first....and</TOKEN>
<TOKEN id="token-36-9" pos="word" morph="none" start_char="4415" end_char="4419">quite</TOKEN>
<TOKEN id="token-36-10" pos="word" morph="none" start_char="4421" end_char="4424">some</TOKEN>
<TOKEN id="token-36-11" pos="word" morph="none" start_char="4426" end_char="4429">time</TOKEN>
<TOKEN id="token-36-12" pos="word" morph="none" start_char="4431" end_char="4433">ago</TOKEN>
<TOKEN id="token-36-13" pos="punct" morph="none" start_char="4434" end_char="4434">.</TOKEN>
</SEG>
<SEG id="segment-37" start_char="4437" end_char="4614">
<ORIGINAL_TEXT>Nonetheless, this is significant and very important in demonstrating that when Bill Gates (of hell) says there will be more pandemics, we know where to start pointing the finger.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="4437" end_char="4447">Nonetheless</TOKEN>
<TOKEN id="token-37-1" pos="punct" morph="none" start_char="4448" end_char="4448">,</TOKEN>
<TOKEN id="token-37-2" pos="word" morph="none" start_char="4450" end_char="4453">this</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="4455" end_char="4456">is</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="4458" end_char="4468">significant</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="4470" end_char="4472">and</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="4474" end_char="4477">very</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="4479" end_char="4487">important</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="4489" end_char="4490">in</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="4492" end_char="4504">demonstrating</TOKEN>
<TOKEN id="token-37-10" pos="word" morph="none" start_char="4506" end_char="4509">that</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="4511" end_char="4514">when</TOKEN>
<TOKEN id="token-37-12" pos="word" morph="none" start_char="4516" end_char="4519">Bill</TOKEN>
<TOKEN id="token-37-13" pos="word" morph="none" start_char="4521" end_char="4525">Gates</TOKEN>
<TOKEN id="token-37-14" pos="punct" morph="none" start_char="4527" end_char="4527">(</TOKEN>
<TOKEN id="token-37-15" pos="word" morph="none" start_char="4528" end_char="4529">of</TOKEN>
<TOKEN id="token-37-16" pos="word" morph="none" start_char="4531" end_char="4534">hell</TOKEN>
<TOKEN id="token-37-17" pos="punct" morph="none" start_char="4535" end_char="4535">)</TOKEN>
<TOKEN id="token-37-18" pos="word" morph="none" start_char="4537" end_char="4540">says</TOKEN>
<TOKEN id="token-37-19" pos="word" morph="none" start_char="4542" end_char="4546">there</TOKEN>
<TOKEN id="token-37-20" pos="word" morph="none" start_char="4548" end_char="4551">will</TOKEN>
<TOKEN id="token-37-21" pos="word" morph="none" start_char="4553" end_char="4554">be</TOKEN>
<TOKEN id="token-37-22" pos="word" morph="none" start_char="4556" end_char="4559">more</TOKEN>
<TOKEN id="token-37-23" pos="word" morph="none" start_char="4561" end_char="4569">pandemics</TOKEN>
<TOKEN id="token-37-24" pos="punct" morph="none" start_char="4570" end_char="4570">,</TOKEN>
<TOKEN id="token-37-25" pos="word" morph="none" start_char="4572" end_char="4573">we</TOKEN>
<TOKEN id="token-37-26" pos="word" morph="none" start_char="4575" end_char="4578">know</TOKEN>
<TOKEN id="token-37-27" pos="word" morph="none" start_char="4580" end_char="4584">where</TOKEN>
<TOKEN id="token-37-28" pos="word" morph="none" start_char="4586" end_char="4587">to</TOKEN>
<TOKEN id="token-37-29" pos="word" morph="none" start_char="4589" end_char="4593">start</TOKEN>
<TOKEN id="token-37-30" pos="word" morph="none" start_char="4595" end_char="4602">pointing</TOKEN>
<TOKEN id="token-37-31" pos="word" morph="none" start_char="4604" end_char="4606">the</TOKEN>
<TOKEN id="token-37-32" pos="word" morph="none" start_char="4608" end_char="4613">finger</TOKEN>
<TOKEN id="token-37-33" pos="punct" morph="none" start_char="4614" end_char="4614">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="4617" end_char="4666">
<ORIGINAL_TEXT>Never forget Sars-CoV-2 Is The Tip Of The Iceberg.</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="4617" end_char="4621">Never</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="4623" end_char="4628">forget</TOKEN>
<TOKEN id="token-38-2" pos="unknown" morph="none" start_char="4630" end_char="4639">Sars-CoV-2</TOKEN>
<TOKEN id="token-38-3" pos="word" morph="none" start_char="4641" end_char="4642">Is</TOKEN>
<TOKEN id="token-38-4" pos="word" morph="none" start_char="4644" end_char="4646">The</TOKEN>
<TOKEN id="token-38-5" pos="word" morph="none" start_char="4648" end_char="4650">Tip</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="4652" end_char="4653">Of</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="4655" end_char="4657">The</TOKEN>
<TOKEN id="token-38-8" pos="word" morph="none" start_char="4659" end_char="4665">Iceberg</TOKEN>
<TOKEN id="token-38-9" pos="punct" morph="none" start_char="4666" end_char="4666">.</TOKEN>
</SEG>
<SEG id="segment-39" start_char="4670" end_char="4786">
<ORIGINAL_TEXT>If it accidently leaked from China, they owe the rest of the world for the losses that will set us all back a decade.</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="word" morph="none" start_char="4670" end_char="4671">If</TOKEN>
<TOKEN id="token-39-1" pos="word" morph="none" start_char="4673" end_char="4674">it</TOKEN>
<TOKEN id="token-39-2" pos="word" morph="none" start_char="4676" end_char="4685">accidently</TOKEN>
<TOKEN id="token-39-3" pos="word" morph="none" start_char="4687" end_char="4692">leaked</TOKEN>
<TOKEN id="token-39-4" pos="word" morph="none" start_char="4694" end_char="4697">from</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="4699" end_char="4703">China</TOKEN>
<TOKEN id="token-39-6" pos="punct" morph="none" start_char="4704" end_char="4704">,</TOKEN>
<TOKEN id="token-39-7" pos="word" morph="none" start_char="4706" end_char="4709">they</TOKEN>
<TOKEN id="token-39-8" pos="word" morph="none" start_char="4711" end_char="4713">owe</TOKEN>
<TOKEN id="token-39-9" pos="word" morph="none" start_char="4715" end_char="4717">the</TOKEN>
<TOKEN id="token-39-10" pos="word" morph="none" start_char="4719" end_char="4722">rest</TOKEN>
<TOKEN id="token-39-11" pos="word" morph="none" start_char="4724" end_char="4725">of</TOKEN>
<TOKEN id="token-39-12" pos="word" morph="none" start_char="4727" end_char="4729">the</TOKEN>
<TOKEN id="token-39-13" pos="word" morph="none" start_char="4731" end_char="4735">world</TOKEN>
<TOKEN id="token-39-14" pos="word" morph="none" start_char="4737" end_char="4739">for</TOKEN>
<TOKEN id="token-39-15" pos="word" morph="none" start_char="4741" end_char="4743">the</TOKEN>
<TOKEN id="token-39-16" pos="word" morph="none" start_char="4745" end_char="4750">losses</TOKEN>
<TOKEN id="token-39-17" pos="word" morph="none" start_char="4752" end_char="4755">that</TOKEN>
<TOKEN id="token-39-18" pos="word" morph="none" start_char="4757" end_char="4760">will</TOKEN>
<TOKEN id="token-39-19" pos="word" morph="none" start_char="4762" end_char="4764">set</TOKEN>
<TOKEN id="token-39-20" pos="word" morph="none" start_char="4766" end_char="4767">us</TOKEN>
<TOKEN id="token-39-21" pos="word" morph="none" start_char="4769" end_char="4771">all</TOKEN>
<TOKEN id="token-39-22" pos="word" morph="none" start_char="4773" end_char="4776">back</TOKEN>
<TOKEN id="token-39-23" pos="word" morph="none" start_char="4778" end_char="4778">a</TOKEN>
<TOKEN id="token-39-24" pos="word" morph="none" start_char="4780" end_char="4785">decade</TOKEN>
<TOKEN id="token-39-25" pos="punct" morph="none" start_char="4786" end_char="4786">.</TOKEN>
</SEG>
<SEG id="segment-40" start_char="4790" end_char="4927">
<ORIGINAL_TEXT>The idea that a bat 800 miles away from Wuhan caused a outbreak of a novel corona virus at a wet market and not the Bio lab is just insane</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="word" morph="none" start_char="4790" end_char="4792">The</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="4794" end_char="4797">idea</TOKEN>
<TOKEN id="token-40-2" pos="word" morph="none" start_char="4799" end_char="4802">that</TOKEN>
<TOKEN id="token-40-3" pos="word" morph="none" start_char="4804" end_char="4804">a</TOKEN>
<TOKEN id="token-40-4" pos="word" morph="none" start_char="4806" end_char="4808">bat</TOKEN>
<TOKEN id="token-40-5" pos="word" morph="none" start_char="4810" end_char="4812">800</TOKEN>
<TOKEN id="token-40-6" pos="word" morph="none" start_char="4814" end_char="4818">miles</TOKEN>
<TOKEN id="token-40-7" pos="word" morph="none" start_char="4820" end_char="4823">away</TOKEN>
<TOKEN id="token-40-8" pos="word" morph="none" start_char="4825" end_char="4828">from</TOKEN>
<TOKEN id="token-40-9" pos="word" morph="none" start_char="4830" end_char="4834">Wuhan</TOKEN>
<TOKEN id="token-40-10" pos="word" morph="none" start_char="4836" end_char="4841">caused</TOKEN>
<TOKEN id="token-40-11" pos="word" morph="none" start_char="4843" end_char="4843">a</TOKEN>
<TOKEN id="token-40-12" pos="word" morph="none" start_char="4845" end_char="4852">outbreak</TOKEN>
<TOKEN id="token-40-13" pos="word" morph="none" start_char="4854" end_char="4855">of</TOKEN>
<TOKEN id="token-40-14" pos="word" morph="none" start_char="4857" end_char="4857">a</TOKEN>
<TOKEN id="token-40-15" pos="word" morph="none" start_char="4859" end_char="4863">novel</TOKEN>
<TOKEN id="token-40-16" pos="word" morph="none" start_char="4865" end_char="4870">corona</TOKEN>
<TOKEN id="token-40-17" pos="word" morph="none" start_char="4872" end_char="4876">virus</TOKEN>
<TOKEN id="token-40-18" pos="word" morph="none" start_char="4878" end_char="4879">at</TOKEN>
<TOKEN id="token-40-19" pos="word" morph="none" start_char="4881" end_char="4881">a</TOKEN>
<TOKEN id="token-40-20" pos="word" morph="none" start_char="4883" end_char="4885">wet</TOKEN>
<TOKEN id="token-40-21" pos="word" morph="none" start_char="4887" end_char="4892">market</TOKEN>
<TOKEN id="token-40-22" pos="word" morph="none" start_char="4894" end_char="4896">and</TOKEN>
<TOKEN id="token-40-23" pos="word" morph="none" start_char="4898" end_char="4900">not</TOKEN>
<TOKEN id="token-40-24" pos="word" morph="none" start_char="4902" end_char="4904">the</TOKEN>
<TOKEN id="token-40-25" pos="word" morph="none" start_char="4906" end_char="4908">Bio</TOKEN>
<TOKEN id="token-40-26" pos="word" morph="none" start_char="4910" end_char="4912">lab</TOKEN>
<TOKEN id="token-40-27" pos="word" morph="none" start_char="4914" end_char="4915">is</TOKEN>
<TOKEN id="token-40-28" pos="word" morph="none" start_char="4917" end_char="4920">just</TOKEN>
<TOKEN id="token-40-29" pos="word" morph="none" start_char="4922" end_char="4927">insane</TOKEN>
</SEG>
<SEG id="segment-41" start_char="4930" end_char="5127">
<ORIGINAL_TEXT>So much points towds a accadental release from the Bio lab there is just no way a natraly occurring virus made that trip without leaving a trail of infected citys just as bad if not worse than Wuhan</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="4930" end_char="4931">So</TOKEN>
<TOKEN id="token-41-1" pos="word" morph="none" start_char="4933" end_char="4936">much</TOKEN>
<TOKEN id="token-41-2" pos="word" morph="none" start_char="4938" end_char="4943">points</TOKEN>
<TOKEN id="token-41-3" pos="word" morph="none" start_char="4945" end_char="4949">towds</TOKEN>
<TOKEN id="token-41-4" pos="word" morph="none" start_char="4951" end_char="4951">a</TOKEN>
<TOKEN id="token-41-5" pos="word" morph="none" start_char="4953" end_char="4962">accadental</TOKEN>
<TOKEN id="token-41-6" pos="word" morph="none" start_char="4964" end_char="4970">release</TOKEN>
<TOKEN id="token-41-7" pos="word" morph="none" start_char="4972" end_char="4975">from</TOKEN>
<TOKEN id="token-41-8" pos="word" morph="none" start_char="4977" end_char="4979">the</TOKEN>
<TOKEN id="token-41-9" pos="word" morph="none" start_char="4981" end_char="4983">Bio</TOKEN>
<TOKEN id="token-41-10" pos="word" morph="none" start_char="4985" end_char="4987">lab</TOKEN>
<TOKEN id="token-41-11" pos="word" morph="none" start_char="4989" end_char="4993">there</TOKEN>
<TOKEN id="token-41-12" pos="word" morph="none" start_char="4995" end_char="4996">is</TOKEN>
<TOKEN id="token-41-13" pos="word" morph="none" start_char="4998" end_char="5001">just</TOKEN>
<TOKEN id="token-41-14" pos="word" morph="none" start_char="5003" end_char="5004">no</TOKEN>
<TOKEN id="token-41-15" pos="word" morph="none" start_char="5006" end_char="5008">way</TOKEN>
<TOKEN id="token-41-16" pos="word" morph="none" start_char="5010" end_char="5010">a</TOKEN>
<TOKEN id="token-41-17" pos="word" morph="none" start_char="5012" end_char="5018">natraly</TOKEN>
<TOKEN id="token-41-18" pos="word" morph="none" start_char="5020" end_char="5028">occurring</TOKEN>
<TOKEN id="token-41-19" pos="word" morph="none" start_char="5030" end_char="5034">virus</TOKEN>
<TOKEN id="token-41-20" pos="word" morph="none" start_char="5036" end_char="5039">made</TOKEN>
<TOKEN id="token-41-21" pos="word" morph="none" start_char="5041" end_char="5044">that</TOKEN>
<TOKEN id="token-41-22" pos="word" morph="none" start_char="5046" end_char="5049">trip</TOKEN>
<TOKEN id="token-41-23" pos="word" morph="none" start_char="5051" end_char="5057">without</TOKEN>
<TOKEN id="token-41-24" pos="word" morph="none" start_char="5059" end_char="5065">leaving</TOKEN>
<TOKEN id="token-41-25" pos="word" morph="none" start_char="5067" end_char="5067">a</TOKEN>
<TOKEN id="token-41-26" pos="word" morph="none" start_char="5069" end_char="5073">trail</TOKEN>
<TOKEN id="token-41-27" pos="word" morph="none" start_char="5075" end_char="5076">of</TOKEN>
<TOKEN id="token-41-28" pos="word" morph="none" start_char="5078" end_char="5085">infected</TOKEN>
<TOKEN id="token-41-29" pos="word" morph="none" start_char="5087" end_char="5091">citys</TOKEN>
<TOKEN id="token-41-30" pos="word" morph="none" start_char="5093" end_char="5096">just</TOKEN>
<TOKEN id="token-41-31" pos="word" morph="none" start_char="5098" end_char="5099">as</TOKEN>
<TOKEN id="token-41-32" pos="word" morph="none" start_char="5101" end_char="5103">bad</TOKEN>
<TOKEN id="token-41-33" pos="word" morph="none" start_char="5105" end_char="5106">if</TOKEN>
<TOKEN id="token-41-34" pos="word" morph="none" start_char="5108" end_char="5110">not</TOKEN>
<TOKEN id="token-41-35" pos="word" morph="none" start_char="5112" end_char="5116">worse</TOKEN>
<TOKEN id="token-41-36" pos="word" morph="none" start_char="5118" end_char="5121">than</TOKEN>
<TOKEN id="token-41-37" pos="word" morph="none" start_char="5123" end_char="5127">Wuhan</TOKEN>
</SEG>
<SEG id="segment-42" start_char="5130" end_char="5188">
<ORIGINAL_TEXT>It came from that lab there is no other logical explanation</ORIGINAL_TEXT>
<TOKEN id="token-42-0" pos="word" morph="none" start_char="5130" end_char="5131">It</TOKEN>
<TOKEN id="token-42-1" pos="word" morph="none" start_char="5133" end_char="5136">came</TOKEN>
<TOKEN id="token-42-2" pos="word" morph="none" start_char="5138" end_char="5141">from</TOKEN>
<TOKEN id="token-42-3" pos="word" morph="none" start_char="5143" end_char="5146">that</TOKEN>
<TOKEN id="token-42-4" pos="word" morph="none" start_char="5148" end_char="5150">lab</TOKEN>
<TOKEN id="token-42-5" pos="word" morph="none" start_char="5152" end_char="5156">there</TOKEN>
<TOKEN id="token-42-6" pos="word" morph="none" start_char="5158" end_char="5159">is</TOKEN>
<TOKEN id="token-42-7" pos="word" morph="none" start_char="5161" end_char="5162">no</TOKEN>
<TOKEN id="token-42-8" pos="word" morph="none" start_char="5164" end_char="5168">other</TOKEN>
<TOKEN id="token-42-9" pos="word" morph="none" start_char="5170" end_char="5176">logical</TOKEN>
<TOKEN id="token-42-10" pos="word" morph="none" start_char="5178" end_char="5188">explanation</TOKEN>
</SEG>
<SEG id="segment-43" start_char="5191" end_char="5321">
<ORIGINAL_TEXT>I personally don't think what escaped was modified but I would not rule out the possibility tho I doubt we will ever know for shure</ORIGINAL_TEXT>
<TOKEN id="token-43-0" pos="word" morph="none" start_char="5191" end_char="5191">I</TOKEN>
<TOKEN id="token-43-1" pos="word" morph="none" start_char="5193" end_char="5202">personally</TOKEN>
<TOKEN id="token-43-2" pos="word" morph="none" start_char="5204" end_char="5208">don't</TOKEN>
<TOKEN id="token-43-3" pos="word" morph="none" start_char="5210" end_char="5214">think</TOKEN>
<TOKEN id="token-43-4" pos="word" morph="none" start_char="5216" end_char="5219">what</TOKEN>
<TOKEN id="token-43-5" pos="word" morph="none" start_char="5221" end_char="5227">escaped</TOKEN>
<TOKEN id="token-43-6" pos="word" morph="none" start_char="5229" end_char="5231">was</TOKEN>
<TOKEN id="token-43-7" pos="word" morph="none" start_char="5233" end_char="5240">modified</TOKEN>
<TOKEN id="token-43-8" pos="word" morph="none" start_char="5242" end_char="5244">but</TOKEN>
<TOKEN id="token-43-9" pos="word" morph="none" start_char="5246" end_char="5246">I</TOKEN>
<TOKEN id="token-43-10" pos="word" morph="none" start_char="5248" end_char="5252">would</TOKEN>
<TOKEN id="token-43-11" pos="word" morph="none" start_char="5254" end_char="5256">not</TOKEN>
<TOKEN id="token-43-12" pos="word" morph="none" start_char="5258" end_char="5261">rule</TOKEN>
<TOKEN id="token-43-13" pos="word" morph="none" start_char="5263" end_char="5265">out</TOKEN>
<TOKEN id="token-43-14" pos="word" morph="none" start_char="5267" end_char="5269">the</TOKEN>
<TOKEN id="token-43-15" pos="word" morph="none" start_char="5271" end_char="5281">possibility</TOKEN>
<TOKEN id="token-43-16" pos="word" morph="none" start_char="5283" end_char="5285">tho</TOKEN>
<TOKEN id="token-43-17" pos="word" morph="none" start_char="5287" end_char="5287">I</TOKEN>
<TOKEN id="token-43-18" pos="word" morph="none" start_char="5289" end_char="5293">doubt</TOKEN>
<TOKEN id="token-43-19" pos="word" morph="none" start_char="5295" end_char="5296">we</TOKEN>
<TOKEN id="token-43-20" pos="word" morph="none" start_char="5298" end_char="5301">will</TOKEN>
<TOKEN id="token-43-21" pos="word" morph="none" start_char="5303" end_char="5306">ever</TOKEN>
<TOKEN id="token-43-22" pos="word" morph="none" start_char="5308" end_char="5311">know</TOKEN>
<TOKEN id="token-43-23" pos="word" morph="none" start_char="5313" end_char="5315">for</TOKEN>
<TOKEN id="token-43-24" pos="word" morph="none" start_char="5317" end_char="5321">shure</TOKEN>
</SEG>
<SEG id="segment-44" start_char="5325" end_char="5341">
<ORIGINAL_TEXT>a reply to: dug88</ORIGINAL_TEXT>
<TOKEN id="token-44-0" pos="word" morph="none" start_char="5325" end_char="5325">a</TOKEN>
<TOKEN id="token-44-1" pos="word" morph="none" start_char="5327" end_char="5331">reply</TOKEN>
<TOKEN id="token-44-2" pos="word" morph="none" start_char="5333" end_char="5334">to</TOKEN>
<TOKEN id="token-44-3" pos="punct" morph="none" start_char="5335" end_char="5335">:</TOKEN>
<TOKEN id="token-44-4" pos="word" morph="none" start_char="5337" end_char="5341">dug88</TOKEN>
</SEG>
<SEG id="segment-45" start_char="5344" end_char="5399">
<ORIGINAL_TEXT>Thanks for posting this dug, but pardon my skepticism...</ORIGINAL_TEXT>
<TOKEN id="token-45-0" pos="word" morph="none" start_char="5344" end_char="5349">Thanks</TOKEN>
<TOKEN id="token-45-1" pos="word" morph="none" start_char="5351" end_char="5353">for</TOKEN>
<TOKEN id="token-45-2" pos="word" morph="none" start_char="5355" end_char="5361">posting</TOKEN>
<TOKEN id="token-45-3" pos="word" morph="none" start_char="5363" end_char="5366">this</TOKEN>
<TOKEN id="token-45-4" pos="word" morph="none" start_char="5368" end_char="5370">dug</TOKEN>
<TOKEN id="token-45-5" pos="punct" morph="none" start_char="5371" end_char="5371">,</TOKEN>
<TOKEN id="token-45-6" pos="word" morph="none" start_char="5373" end_char="5375">but</TOKEN>
<TOKEN id="token-45-7" pos="word" morph="none" start_char="5377" end_char="5382">pardon</TOKEN>
<TOKEN id="token-45-8" pos="word" morph="none" start_char="5384" end_char="5385">my</TOKEN>
<TOKEN id="token-45-9" pos="word" morph="none" start_char="5387" end_char="5396">skepticism</TOKEN>
<TOKEN id="token-45-10" pos="punct" morph="none" start_char="5397" end_char="5399">...</TOKEN>
</SEG>
<SEG id="segment-46" start_char="5402" end_char="5474">
<ORIGINAL_TEXT>An AI, that's gonna legitimatize our whack-a-doodle conspiracy-theories ?</ORIGINAL_TEXT>
<TOKEN id="token-46-0" pos="word" morph="none" start_char="5402" end_char="5403">An</TOKEN>
<TOKEN id="token-46-1" pos="word" morph="none" start_char="5405" end_char="5406">AI</TOKEN>
<TOKEN id="token-46-2" pos="punct" morph="none" start_char="5407" end_char="5407">,</TOKEN>
<TOKEN id="token-46-3" pos="word" morph="none" start_char="5409" end_char="5414">that's</TOKEN>
<TOKEN id="token-46-4" pos="word" morph="none" start_char="5416" end_char="5420">gonna</TOKEN>
<TOKEN id="token-46-5" pos="word" morph="none" start_char="5422" end_char="5433">legitimatize</TOKEN>
<TOKEN id="token-46-6" pos="word" morph="none" start_char="5435" end_char="5437">our</TOKEN>
<TOKEN id="token-46-7" pos="unknown" morph="none" start_char="5439" end_char="5452">whack-a-doodle</TOKEN>
<TOKEN id="token-46-8" pos="unknown" morph="none" start_char="5454" end_char="5472">conspiracy-theories</TOKEN>
<TOKEN id="token-46-9" pos="punct" morph="none" start_char="5474" end_char="5474">?</TOKEN>
</SEG>
<SEG id="segment-47" start_char="5476" end_char="5483">
<ORIGINAL_TEXT>Really ?</ORIGINAL_TEXT>
<TOKEN id="token-47-0" pos="word" morph="none" start_char="5476" end_char="5481">Really</TOKEN>
<TOKEN id="token-47-1" pos="punct" morph="none" start_char="5483" end_char="5483">?</TOKEN>
</SEG>
<SEG id="segment-48" start_char="5486" end_char="5567">
<ORIGINAL_TEXT>From the land, where many tech startups are fronted by 'former' Moss-head agents ?</ORIGINAL_TEXT>
<TOKEN id="token-48-0" pos="word" morph="none" start_char="5486" end_char="5489">From</TOKEN>
<TOKEN id="token-48-1" pos="word" morph="none" start_char="5491" end_char="5493">the</TOKEN>
<TOKEN id="token-48-2" pos="word" morph="none" start_char="5495" end_char="5498">land</TOKEN>
<TOKEN id="token-48-3" pos="punct" morph="none" start_char="5499" end_char="5499">,</TOKEN>
<TOKEN id="token-48-4" pos="word" morph="none" start_char="5501" end_char="5505">where</TOKEN>
<TOKEN id="token-48-5" pos="word" morph="none" start_char="5507" end_char="5510">many</TOKEN>
<TOKEN id="token-48-6" pos="word" morph="none" start_char="5512" end_char="5515">tech</TOKEN>
<TOKEN id="token-48-7" pos="word" morph="none" start_char="5517" end_char="5524">startups</TOKEN>
<TOKEN id="token-48-8" pos="word" morph="none" start_char="5526" end_char="5528">are</TOKEN>
<TOKEN id="token-48-9" pos="word" morph="none" start_char="5530" end_char="5536">fronted</TOKEN>
<TOKEN id="token-48-10" pos="word" morph="none" start_char="5538" end_char="5539">by</TOKEN>
<TOKEN id="token-48-11" pos="punct" morph="none" start_char="5541" end_char="5541">'</TOKEN>
<TOKEN id="token-48-12" pos="word" morph="none" start_char="5542" end_char="5547">former</TOKEN>
<TOKEN id="token-48-13" pos="punct" morph="none" start_char="5548" end_char="5548">'</TOKEN>
<TOKEN id="token-48-14" pos="unknown" morph="none" start_char="5550" end_char="5558">Moss-head</TOKEN>
<TOKEN id="token-48-15" pos="word" morph="none" start_char="5560" end_char="5565">agents</TOKEN>
<TOKEN id="token-48-16" pos="punct" morph="none" start_char="5567" end_char="5567">?</TOKEN>
</SEG>
<SEG id="segment-49" start_char="5569" end_char="5576">
<ORIGINAL_TEXT>Really ?</ORIGINAL_TEXT>
<TOKEN id="token-49-0" pos="word" morph="none" start_char="5569" end_char="5574">Really</TOKEN>
<TOKEN id="token-49-1" pos="punct" morph="none" start_char="5576" end_char="5576">?</TOKEN>
</SEG>
<SEG id="segment-50" start_char="5579" end_char="5637">
<ORIGINAL_TEXT>How soon until they use this, to justify attacks, or wars ?</ORIGINAL_TEXT>
<TOKEN id="token-50-0" pos="word" morph="none" start_char="5579" end_char="5581">How</TOKEN>
<TOKEN id="token-50-1" pos="word" morph="none" start_char="5583" end_char="5586">soon</TOKEN>
<TOKEN id="token-50-2" pos="word" morph="none" start_char="5588" end_char="5592">until</TOKEN>
<TOKEN id="token-50-3" pos="word" morph="none" start_char="5594" end_char="5597">they</TOKEN>
<TOKEN id="token-50-4" pos="word" morph="none" start_char="5599" end_char="5601">use</TOKEN>
<TOKEN id="token-50-5" pos="word" morph="none" start_char="5603" end_char="5606">this</TOKEN>
<TOKEN id="token-50-6" pos="punct" morph="none" start_char="5607" end_char="5607">,</TOKEN>
<TOKEN id="token-50-7" pos="word" morph="none" start_char="5609" end_char="5610">to</TOKEN>
<TOKEN id="token-50-8" pos="word" morph="none" start_char="5612" end_char="5618">justify</TOKEN>
<TOKEN id="token-50-9" pos="word" morph="none" start_char="5620" end_char="5626">attacks</TOKEN>
<TOKEN id="token-50-10" pos="punct" morph="none" start_char="5627" end_char="5627">,</TOKEN>
<TOKEN id="token-50-11" pos="word" morph="none" start_char="5629" end_char="5630">or</TOKEN>
<TOKEN id="token-50-12" pos="word" morph="none" start_char="5632" end_char="5635">wars</TOKEN>
<TOKEN id="token-50-13" pos="punct" morph="none" start_char="5637" end_char="5637">?</TOKEN>
</SEG>
<SEG id="segment-51" start_char="5640" end_char="5665">
<ORIGINAL_TEXT>Just doesn't feel right...</ORIGINAL_TEXT>
<TOKEN id="token-51-0" pos="word" morph="none" start_char="5640" end_char="5643">Just</TOKEN>
<TOKEN id="token-51-1" pos="word" morph="none" start_char="5645" end_char="5651">doesn't</TOKEN>
<TOKEN id="token-51-2" pos="word" morph="none" start_char="5653" end_char="5656">feel</TOKEN>
<TOKEN id="token-51-3" pos="word" morph="none" start_char="5658" end_char="5662">right</TOKEN>
<TOKEN id="token-51-4" pos="punct" morph="none" start_char="5663" end_char="5665">...</TOKEN>
</SEG>
<SEG id="segment-52" start_char="5668" end_char="5707">
<ORIGINAL_TEXT>edit on 30-12-2020 by Nothin because: sp</ORIGINAL_TEXT>
<TOKEN id="token-52-0" pos="word" morph="none" start_char="5668" end_char="5671">edit</TOKEN>
<TOKEN id="token-52-1" pos="word" morph="none" start_char="5673" end_char="5674">on</TOKEN>
<TOKEN id="token-52-2" pos="unknown" morph="none" start_char="5676" end_char="5685">30-12-2020</TOKEN>
<TOKEN id="token-52-3" pos="word" morph="none" start_char="5687" end_char="5688">by</TOKEN>
<TOKEN id="token-52-4" pos="word" morph="none" start_char="5690" end_char="5695">Nothin</TOKEN>
<TOKEN id="token-52-5" pos="word" morph="none" start_char="5697" end_char="5703">because</TOKEN>
<TOKEN id="token-52-6" pos="punct" morph="none" start_char="5704" end_char="5704">:</TOKEN>
<TOKEN id="token-52-7" pos="word" morph="none" start_char="5706" end_char="5707">sp</TOKEN>
</SEG>
<SEG id="segment-53" start_char="5712" end_char="5884">
<ORIGINAL_TEXT>originally posted by: CriticalStinker a reply to: dug88 If it accidently leaked from China, they owe the rest of the world for the losses that will set us all back a decade.</ORIGINAL_TEXT>
<TOKEN id="token-53-0" pos="word" morph="none" start_char="5712" end_char="5721">originally</TOKEN>
<TOKEN id="token-53-1" pos="word" morph="none" start_char="5723" end_char="5728">posted</TOKEN>
<TOKEN id="token-53-2" pos="word" morph="none" start_char="5730" end_char="5731">by</TOKEN>
<TOKEN id="token-53-3" pos="punct" morph="none" start_char="5732" end_char="5732">:</TOKEN>
<TOKEN id="token-53-4" pos="word" morph="none" start_char="5734" end_char="5748">CriticalStinker</TOKEN>
<TOKEN id="token-53-5" pos="word" morph="none" start_char="5750" end_char="5750">a</TOKEN>
<TOKEN id="token-53-6" pos="word" morph="none" start_char="5752" end_char="5756">reply</TOKEN>
<TOKEN id="token-53-7" pos="word" morph="none" start_char="5758" end_char="5759">to</TOKEN>
<TOKEN id="token-53-8" pos="punct" morph="none" start_char="5760" end_char="5760">:</TOKEN>
<TOKEN id="token-53-9" pos="word" morph="none" start_char="5762" end_char="5766">dug88</TOKEN>
<TOKEN id="token-53-10" pos="word" morph="none" start_char="5768" end_char="5769">If</TOKEN>
<TOKEN id="token-53-11" pos="word" morph="none" start_char="5771" end_char="5772">it</TOKEN>
<TOKEN id="token-53-12" pos="word" morph="none" start_char="5774" end_char="5783">accidently</TOKEN>
<TOKEN id="token-53-13" pos="word" morph="none" start_char="5785" end_char="5790">leaked</TOKEN>
<TOKEN id="token-53-14" pos="word" morph="none" start_char="5792" end_char="5795">from</TOKEN>
<TOKEN id="token-53-15" pos="word" morph="none" start_char="5797" end_char="5801">China</TOKEN>
<TOKEN id="token-53-16" pos="punct" morph="none" start_char="5802" end_char="5802">,</TOKEN>
<TOKEN id="token-53-17" pos="word" morph="none" start_char="5804" end_char="5807">they</TOKEN>
<TOKEN id="token-53-18" pos="word" morph="none" start_char="5809" end_char="5811">owe</TOKEN>
<TOKEN id="token-53-19" pos="word" morph="none" start_char="5813" end_char="5815">the</TOKEN>
<TOKEN id="token-53-20" pos="word" morph="none" start_char="5817" end_char="5820">rest</TOKEN>
<TOKEN id="token-53-21" pos="word" morph="none" start_char="5822" end_char="5823">of</TOKEN>
<TOKEN id="token-53-22" pos="word" morph="none" start_char="5825" end_char="5827">the</TOKEN>
<TOKEN id="token-53-23" pos="word" morph="none" start_char="5829" end_char="5833">world</TOKEN>
<TOKEN id="token-53-24" pos="word" morph="none" start_char="5835" end_char="5837">for</TOKEN>
<TOKEN id="token-53-25" pos="word" morph="none" start_char="5839" end_char="5841">the</TOKEN>
<TOKEN id="token-53-26" pos="word" morph="none" start_char="5843" end_char="5848">losses</TOKEN>
<TOKEN id="token-53-27" pos="word" morph="none" start_char="5850" end_char="5853">that</TOKEN>
<TOKEN id="token-53-28" pos="word" morph="none" start_char="5855" end_char="5858">will</TOKEN>
<TOKEN id="token-53-29" pos="word" morph="none" start_char="5860" end_char="5862">set</TOKEN>
<TOKEN id="token-53-30" pos="word" morph="none" start_char="5864" end_char="5865">us</TOKEN>
<TOKEN id="token-53-31" pos="word" morph="none" start_char="5867" end_char="5869">all</TOKEN>
<TOKEN id="token-53-32" pos="word" morph="none" start_char="5871" end_char="5874">back</TOKEN>
<TOKEN id="token-53-33" pos="word" morph="none" start_char="5876" end_char="5876">a</TOKEN>
<TOKEN id="token-53-34" pos="word" morph="none" start_char="5878" end_char="5883">decade</TOKEN>
<TOKEN id="token-53-35" pos="punct" morph="none" start_char="5884" end_char="5884">.</TOKEN>
</SEG>
<SEG id="segment-54" start_char="5887" end_char="5901">
<ORIGINAL_TEXT>Hold up though.</ORIGINAL_TEXT>
<TOKEN id="token-54-0" pos="word" morph="none" start_char="5887" end_char="5890">Hold</TOKEN>
<TOKEN id="token-54-1" pos="word" morph="none" start_char="5892" end_char="5893">up</TOKEN>
<TOKEN id="token-54-2" pos="word" morph="none" start_char="5895" end_char="5900">though</TOKEN>
<TOKEN id="token-54-3" pos="punct" morph="none" start_char="5901" end_char="5901">.</TOKEN>
</SEG>
<SEG id="segment-55" start_char="5903" end_char="5985">
<ORIGINAL_TEXT>If it was being studied at UNC Chapel Hills R lab for GoF capacity (circa 2014-16?</ORIGINAL_TEXT>
<TOKEN id="token-55-0" pos="word" morph="none" start_char="5903" end_char="5904">If</TOKEN>
<TOKEN id="token-55-1" pos="word" morph="none" start_char="5906" end_char="5907">it</TOKEN>
<TOKEN id="token-55-2" pos="word" morph="none" start_char="5909" end_char="5911">was</TOKEN>
<TOKEN id="token-55-3" pos="word" morph="none" start_char="5913" end_char="5917">being</TOKEN>
<TOKEN id="token-55-4" pos="word" morph="none" start_char="5919" end_char="5925">studied</TOKEN>
<TOKEN id="token-55-5" pos="word" morph="none" start_char="5927" end_char="5928">at</TOKEN>
<TOKEN id="token-55-6" pos="word" morph="none" start_char="5930" end_char="5932">UNC</TOKEN>
<TOKEN id="token-55-7" pos="word" morph="none" start_char="5934" end_char="5939">Chapel</TOKEN>
<TOKEN id="token-55-8" pos="word" morph="none" start_char="5941" end_char="5946">Hills</TOKEN>
<TOKEN id="token-55-9" pos="word" morph="none" start_char="5948" end_char="5948">R</TOKEN>
<TOKEN id="token-55-10" pos="word" morph="none" start_char="5950" end_char="5952">lab</TOKEN>
<TOKEN id="token-55-11" pos="word" morph="none" start_char="5954" end_char="5956">for</TOKEN>
<TOKEN id="token-55-12" pos="word" morph="none" start_char="5958" end_char="5960">GoF</TOKEN>
<TOKEN id="token-55-13" pos="word" morph="none" start_char="5962" end_char="5969">capacity</TOKEN>
<TOKEN id="token-55-14" pos="punct" morph="none" start_char="5971" end_char="5971">(</TOKEN>
<TOKEN id="token-55-15" pos="word" morph="none" start_char="5972" end_char="5976">circa</TOKEN>
<TOKEN id="token-55-16" pos="unknown" morph="none" start_char="5978" end_char="5984">2014-16</TOKEN>
<TOKEN id="token-55-17" pos="punct" morph="none" start_char="5985" end_char="5985">?</TOKEN>
</SEG>
<SEG id="segment-56" start_char="5987" end_char="6070">
<ORIGINAL_TEXT>)and when it was learned of the intention and capability, it was shut down there....</ORIGINAL_TEXT>
<TOKEN id="token-56-0" pos="punct" morph="none" start_char="5987" end_char="5987">)</TOKEN>
<TOKEN id="token-56-1" pos="word" morph="none" start_char="5988" end_char="5990">and</TOKEN>
<TOKEN id="token-56-2" pos="word" morph="none" start_char="5992" end_char="5995">when</TOKEN>
<TOKEN id="token-56-3" pos="word" morph="none" start_char="5997" end_char="5998">it</TOKEN>
<TOKEN id="token-56-4" pos="word" morph="none" start_char="6000" end_char="6002">was</TOKEN>
<TOKEN id="token-56-5" pos="word" morph="none" start_char="6004" end_char="6010">learned</TOKEN>
<TOKEN id="token-56-6" pos="word" morph="none" start_char="6012" end_char="6013">of</TOKEN>
<TOKEN id="token-56-7" pos="word" morph="none" start_char="6015" end_char="6017">the</TOKEN>
<TOKEN id="token-56-8" pos="word" morph="none" start_char="6019" end_char="6027">intention</TOKEN>
<TOKEN id="token-56-9" pos="word" morph="none" start_char="6029" end_char="6031">and</TOKEN>
<TOKEN id="token-56-10" pos="word" morph="none" start_char="6033" end_char="6042">capability</TOKEN>
<TOKEN id="token-56-11" pos="punct" morph="none" start_char="6043" end_char="6043">,</TOKEN>
<TOKEN id="token-56-12" pos="word" morph="none" start_char="6045" end_char="6046">it</TOKEN>
<TOKEN id="token-56-13" pos="word" morph="none" start_char="6048" end_char="6050">was</TOKEN>
<TOKEN id="token-56-14" pos="word" morph="none" start_char="6052" end_char="6055">shut</TOKEN>
<TOKEN id="token-56-15" pos="word" morph="none" start_char="6057" end_char="6060">down</TOKEN>
<TOKEN id="token-56-16" pos="word" morph="none" start_char="6062" end_char="6066">there</TOKEN>
<TOKEN id="token-56-17" pos="punct" morph="none" start_char="6067" end_char="6070">....</TOKEN>
</SEG>
<SEG id="segment-57" start_char="6073" end_char="6161">
<ORIGINAL_TEXT>So did we outsource our low-key Geneva List Bio-No-No hot potato to A BSL-4 lab in Wuhan?</ORIGINAL_TEXT>
<TOKEN id="token-57-0" pos="word" morph="none" start_char="6073" end_char="6074">So</TOKEN>
<TOKEN id="token-57-1" pos="word" morph="none" start_char="6076" end_char="6078">did</TOKEN>
<TOKEN id="token-57-2" pos="word" morph="none" start_char="6080" end_char="6081">we</TOKEN>
<TOKEN id="token-57-3" pos="word" morph="none" start_char="6083" end_char="6091">outsource</TOKEN>
<TOKEN id="token-57-4" pos="word" morph="none" start_char="6093" end_char="6095">our</TOKEN>
<TOKEN id="token-57-5" pos="unknown" morph="none" start_char="6097" end_char="6103">low-key</TOKEN>
<TOKEN id="token-57-6" pos="word" morph="none" start_char="6105" end_char="6110">Geneva</TOKEN>
<TOKEN id="token-57-7" pos="word" morph="none" start_char="6112" end_char="6115">List</TOKEN>
<TOKEN id="token-57-8" pos="unknown" morph="none" start_char="6117" end_char="6125">Bio-No-No</TOKEN>
<TOKEN id="token-57-9" pos="word" morph="none" start_char="6127" end_char="6129">hot</TOKEN>
<TOKEN id="token-57-10" pos="word" morph="none" start_char="6131" end_char="6136">potato</TOKEN>
<TOKEN id="token-57-11" pos="word" morph="none" start_char="6138" end_char="6139">to</TOKEN>
<TOKEN id="token-57-12" pos="word" morph="none" start_char="6141" end_char="6141">A</TOKEN>
<TOKEN id="token-57-13" pos="unknown" morph="none" start_char="6143" end_char="6147">BSL-4</TOKEN>
<TOKEN id="token-57-14" pos="word" morph="none" start_char="6149" end_char="6151">lab</TOKEN>
<TOKEN id="token-57-15" pos="word" morph="none" start_char="6153" end_char="6154">in</TOKEN>
<TOKEN id="token-57-16" pos="word" morph="none" start_char="6156" end_char="6160">Wuhan</TOKEN>
<TOKEN id="token-57-17" pos="punct" morph="none" start_char="6161" end_char="6161">?</TOKEN>
</SEG>
<SEG id="segment-58" start_char="6163" end_char="6192">
<ORIGINAL_TEXT>Plausible deniability and all?</ORIGINAL_TEXT>
<TOKEN id="token-58-0" pos="word" morph="none" start_char="6163" end_char="6171">Plausible</TOKEN>
<TOKEN id="token-58-1" pos="word" morph="none" start_char="6173" end_char="6183">deniability</TOKEN>
<TOKEN id="token-58-2" pos="word" morph="none" start_char="6185" end_char="6187">and</TOKEN>
<TOKEN id="token-58-3" pos="word" morph="none" start_char="6189" end_char="6191">all</TOKEN>
<TOKEN id="token-58-4" pos="punct" morph="none" start_char="6192" end_char="6192">?</TOKEN>
</SEG>
<SEG id="segment-59" start_char="6195" end_char="6246">
<ORIGINAL_TEXT>How many licks to the center of the lollipop indeed.</ORIGINAL_TEXT>
<TOKEN id="token-59-0" pos="word" morph="none" start_char="6195" end_char="6197">How</TOKEN>
<TOKEN id="token-59-1" pos="word" morph="none" start_char="6199" end_char="6202">many</TOKEN>
<TOKEN id="token-59-2" pos="word" morph="none" start_char="6204" end_char="6208">licks</TOKEN>
<TOKEN id="token-59-3" pos="word" morph="none" start_char="6210" end_char="6211">to</TOKEN>
<TOKEN id="token-59-4" pos="word" morph="none" start_char="6213" end_char="6215">the</TOKEN>
<TOKEN id="token-59-5" pos="word" morph="none" start_char="6217" end_char="6222">center</TOKEN>
<TOKEN id="token-59-6" pos="word" morph="none" start_char="6224" end_char="6225">of</TOKEN>
<TOKEN id="token-59-7" pos="word" morph="none" start_char="6227" end_char="6229">the</TOKEN>
<TOKEN id="token-59-8" pos="word" morph="none" start_char="6231" end_char="6238">lollipop</TOKEN>
<TOKEN id="token-59-9" pos="word" morph="none" start_char="6240" end_char="6245">indeed</TOKEN>
<TOKEN id="token-59-10" pos="punct" morph="none" start_char="6246" end_char="6246">.</TOKEN>
</SEG>
<SEG id="segment-60" start_char="6249" end_char="6292">
<ORIGINAL_TEXT>Also: dont lick lollipop, it may have Covid</ORIGINAL_TEXT>
<TOKEN id="token-60-0" pos="word" morph="none" start_char="6249" end_char="6252">Also</TOKEN>
<TOKEN id="token-60-1" pos="punct" morph="none" start_char="6253" end_char="6253">:</TOKEN>
<TOKEN id="token-60-2" pos="word" morph="none" start_char="6255" end_char="6259">dont</TOKEN>
<TOKEN id="token-60-3" pos="word" morph="none" start_char="6261" end_char="6264">lick</TOKEN>
<TOKEN id="token-60-4" pos="word" morph="none" start_char="6266" end_char="6273">lollipop</TOKEN>
<TOKEN id="token-60-5" pos="punct" morph="none" start_char="6274" end_char="6274">,</TOKEN>
<TOKEN id="token-60-6" pos="word" morph="none" start_char="6276" end_char="6277">it</TOKEN>
<TOKEN id="token-60-7" pos="word" morph="none" start_char="6279" end_char="6281">may</TOKEN>
<TOKEN id="token-60-8" pos="word" morph="none" start_char="6283" end_char="6286">have</TOKEN>
<TOKEN id="token-60-9" pos="word" morph="none" start_char="6288" end_char="6292">Covid</TOKEN>
</SEG>
<SEG id="segment-61" start_char="6295" end_char="6353">
<ORIGINAL_TEXT>edit on 30-12-2020 by slatesteam because: (no reason given)</ORIGINAL_TEXT>
<TOKEN id="token-61-0" pos="word" morph="none" start_char="6295" end_char="6298">edit</TOKEN>
<TOKEN id="token-61-1" pos="word" morph="none" start_char="6300" end_char="6301">on</TOKEN>
<TOKEN id="token-61-2" pos="unknown" morph="none" start_char="6303" end_char="6312">30-12-2020</TOKEN>
<TOKEN id="token-61-3" pos="word" morph="none" start_char="6314" end_char="6315">by</TOKEN>
<TOKEN id="token-61-4" pos="word" morph="none" start_char="6317" end_char="6326">slatesteam</TOKEN>
<TOKEN id="token-61-5" pos="word" morph="none" start_char="6328" end_char="6334">because</TOKEN>
<TOKEN id="token-61-6" pos="punct" morph="none" start_char="6335" end_char="6335">:</TOKEN>
<TOKEN id="token-61-7" pos="punct" morph="none" start_char="6337" end_char="6337">(</TOKEN>
<TOKEN id="token-61-8" pos="word" morph="none" start_char="6338" end_char="6339">no</TOKEN>
<TOKEN id="token-61-9" pos="word" morph="none" start_char="6341" end_char="6346">reason</TOKEN>
<TOKEN id="token-61-10" pos="word" morph="none" start_char="6348" end_char="6352">given</TOKEN>
<TOKEN id="token-61-11" pos="punct" morph="none" start_char="6353" end_char="6353">)</TOKEN>
</SEG>
<SEG id="segment-62" start_char="6356" end_char="6414">
<ORIGINAL_TEXT>edit on 30-12-2020 by slatesteam because: (no reason given)</ORIGINAL_TEXT>
<TOKEN id="token-62-0" pos="word" morph="none" start_char="6356" end_char="6359">edit</TOKEN>
<TOKEN id="token-62-1" pos="word" morph="none" start_char="6361" end_char="6362">on</TOKEN>
<TOKEN id="token-62-2" pos="unknown" morph="none" start_char="6364" end_char="6373">30-12-2020</TOKEN>
<TOKEN id="token-62-3" pos="word" morph="none" start_char="6375" end_char="6376">by</TOKEN>
<TOKEN id="token-62-4" pos="word" morph="none" start_char="6378" end_char="6387">slatesteam</TOKEN>
<TOKEN id="token-62-5" pos="word" morph="none" start_char="6389" end_char="6395">because</TOKEN>
<TOKEN id="token-62-6" pos="punct" morph="none" start_char="6396" end_char="6396">:</TOKEN>
<TOKEN id="token-62-7" pos="punct" morph="none" start_char="6398" end_char="6398">(</TOKEN>
<TOKEN id="token-62-8" pos="word" morph="none" start_char="6399" end_char="6400">no</TOKEN>
<TOKEN id="token-62-9" pos="word" morph="none" start_char="6402" end_char="6407">reason</TOKEN>
<TOKEN id="token-62-10" pos="word" morph="none" start_char="6409" end_char="6413">given</TOKEN>
<TOKEN id="token-62-11" pos="punct" morph="none" start_char="6414" end_char="6414">)</TOKEN>
</SEG>
<SEG id="segment-63" start_char="6419" end_char="6477">
<ORIGINAL_TEXT>edit on 30-12-2020 by slatesteam because: (no reason given)</ORIGINAL_TEXT>
<TOKEN id="token-63-0" pos="word" morph="none" start_char="6419" end_char="6422">edit</TOKEN>
<TOKEN id="token-63-1" pos="word" morph="none" start_char="6424" end_char="6425">on</TOKEN>
<TOKEN id="token-63-2" pos="unknown" morph="none" start_char="6427" end_char="6436">30-12-2020</TOKEN>
<TOKEN id="token-63-3" pos="word" morph="none" start_char="6438" end_char="6439">by</TOKEN>
<TOKEN id="token-63-4" pos="word" morph="none" start_char="6441" end_char="6450">slatesteam</TOKEN>
<TOKEN id="token-63-5" pos="word" morph="none" start_char="6452" end_char="6458">because</TOKEN>
<TOKEN id="token-63-6" pos="punct" morph="none" start_char="6459" end_char="6459">:</TOKEN>
<TOKEN id="token-63-7" pos="punct" morph="none" start_char="6461" end_char="6461">(</TOKEN>
<TOKEN id="token-63-8" pos="word" morph="none" start_char="6462" end_char="6463">no</TOKEN>
<TOKEN id="token-63-9" pos="word" morph="none" start_char="6465" end_char="6470">reason</TOKEN>
<TOKEN id="token-63-10" pos="word" morph="none" start_char="6472" end_char="6476">given</TOKEN>
<TOKEN id="token-63-11" pos="punct" morph="none" start_char="6477" end_char="6477">)</TOKEN>
</SEG>
<SEG id="segment-64" start_char="6481" end_char="6498">
<ORIGINAL_TEXT>a reply to: Nothin</ORIGINAL_TEXT>
<TOKEN id="token-64-0" pos="word" morph="none" start_char="6481" end_char="6481">a</TOKEN>
<TOKEN id="token-64-1" pos="word" morph="none" start_char="6483" end_char="6487">reply</TOKEN>
<TOKEN id="token-64-2" pos="word" morph="none" start_char="6489" end_char="6490">to</TOKEN>
<TOKEN id="token-64-3" pos="punct" morph="none" start_char="6491" end_char="6491">:</TOKEN>
<TOKEN id="token-64-4" pos="word" morph="none" start_char="6493" end_char="6498">Nothin</TOKEN>
</SEG>
<SEG id="segment-65" start_char="6501" end_char="6597">
<ORIGINAL_TEXT>They do have their evidence and reasoning available for scrutiny should you choose to look at it.</ORIGINAL_TEXT>
<TOKEN id="token-65-0" pos="word" morph="none" start_char="6501" end_char="6504">They</TOKEN>
<TOKEN id="token-65-1" pos="word" morph="none" start_char="6506" end_char="6507">do</TOKEN>
<TOKEN id="token-65-2" pos="word" morph="none" start_char="6509" end_char="6512">have</TOKEN>
<TOKEN id="token-65-3" pos="word" morph="none" start_char="6514" end_char="6518">their</TOKEN>
<TOKEN id="token-65-4" pos="word" morph="none" start_char="6520" end_char="6527">evidence</TOKEN>
<TOKEN id="token-65-5" pos="word" morph="none" start_char="6529" end_char="6531">and</TOKEN>
<TOKEN id="token-65-6" pos="word" morph="none" start_char="6533" end_char="6541">reasoning</TOKEN>
<TOKEN id="token-65-7" pos="word" morph="none" start_char="6543" end_char="6551">available</TOKEN>
<TOKEN id="token-65-8" pos="word" morph="none" start_char="6553" end_char="6555">for</TOKEN>
<TOKEN id="token-65-9" pos="word" morph="none" start_char="6557" end_char="6564">scrutiny</TOKEN>
<TOKEN id="token-65-10" pos="word" morph="none" start_char="6566" end_char="6571">should</TOKEN>
<TOKEN id="token-65-11" pos="word" morph="none" start_char="6573" end_char="6575">you</TOKEN>
<TOKEN id="token-65-12" pos="word" morph="none" start_char="6577" end_char="6582">choose</TOKEN>
<TOKEN id="token-65-13" pos="word" morph="none" start_char="6584" end_char="6585">to</TOKEN>
<TOKEN id="token-65-14" pos="word" morph="none" start_char="6587" end_char="6590">look</TOKEN>
<TOKEN id="token-65-15" pos="word" morph="none" start_char="6592" end_char="6593">at</TOKEN>
<TOKEN id="token-65-16" pos="word" morph="none" start_char="6595" end_char="6596">it</TOKEN>
<TOKEN id="token-65-17" pos="punct" morph="none" start_char="6597" end_char="6597">.</TOKEN>
</SEG>
<SEG id="segment-66" start_char="6599" end_char="6665">
<ORIGINAL_TEXT>I wouldn't bother waiting for the cnn summary, I doubt it's coming.</ORIGINAL_TEXT>
<TOKEN id="token-66-0" pos="word" morph="none" start_char="6599" end_char="6599">I</TOKEN>
<TOKEN id="token-66-1" pos="word" morph="none" start_char="6601" end_char="6608">wouldn't</TOKEN>
<TOKEN id="token-66-2" pos="word" morph="none" start_char="6610" end_char="6615">bother</TOKEN>
<TOKEN id="token-66-3" pos="word" morph="none" start_char="6617" end_char="6623">waiting</TOKEN>
<TOKEN id="token-66-4" pos="word" morph="none" start_char="6625" end_char="6627">for</TOKEN>
<TOKEN id="token-66-5" pos="word" morph="none" start_char="6629" end_char="6631">the</TOKEN>
<TOKEN id="token-66-6" pos="word" morph="none" start_char="6633" end_char="6635">cnn</TOKEN>
<TOKEN id="token-66-7" pos="word" morph="none" start_char="6637" end_char="6643">summary</TOKEN>
<TOKEN id="token-66-8" pos="punct" morph="none" start_char="6644" end_char="6644">,</TOKEN>
<TOKEN id="token-66-9" pos="word" morph="none" start_char="6646" end_char="6646">I</TOKEN>
<TOKEN id="token-66-10" pos="word" morph="none" start_char="6648" end_char="6652">doubt</TOKEN>
<TOKEN id="token-66-11" pos="word" morph="none" start_char="6654" end_char="6657">it's</TOKEN>
<TOKEN id="token-66-12" pos="word" morph="none" start_char="6659" end_char="6664">coming</TOKEN>
<TOKEN id="token-66-13" pos="punct" morph="none" start_char="6665" end_char="6665">.</TOKEN>
</SEG>
<SEG id="segment-67" start_char="6668" end_char="6721">
<ORIGINAL_TEXT>edit on 30/12/2020 by dug88 because: (no reason given)</ORIGINAL_TEXT>
<TOKEN id="token-67-0" pos="word" morph="none" start_char="6668" end_char="6671">edit</TOKEN>
<TOKEN id="token-67-1" pos="word" morph="none" start_char="6673" end_char="6674">on</TOKEN>
<TOKEN id="token-67-2" pos="unknown" morph="none" start_char="6676" end_char="6685">30/12/2020</TOKEN>
<TOKEN id="token-67-3" pos="word" morph="none" start_char="6687" end_char="6688">by</TOKEN>
<TOKEN id="token-67-4" pos="word" morph="none" start_char="6690" end_char="6694">dug88</TOKEN>
<TOKEN id="token-67-5" pos="word" morph="none" start_char="6696" end_char="6702">because</TOKEN>
<TOKEN id="token-67-6" pos="punct" morph="none" start_char="6703" end_char="6703">:</TOKEN>
<TOKEN id="token-67-7" pos="punct" morph="none" start_char="6705" end_char="6705">(</TOKEN>
<TOKEN id="token-67-8" pos="word" morph="none" start_char="6706" end_char="6707">no</TOKEN>
<TOKEN id="token-67-9" pos="word" morph="none" start_char="6709" end_char="6714">reason</TOKEN>
<TOKEN id="token-67-10" pos="word" morph="none" start_char="6716" end_char="6720">given</TOKEN>
<TOKEN id="token-67-11" pos="punct" morph="none" start_char="6721" end_char="6721">)</TOKEN>
</SEG>
<SEG id="segment-68" start_char="6725" end_char="6741">
<ORIGINAL_TEXT>a reply to: dug88</ORIGINAL_TEXT>
<TOKEN id="token-68-0" pos="word" morph="none" start_char="6725" end_char="6725">a</TOKEN>
<TOKEN id="token-68-1" pos="word" morph="none" start_char="6727" end_char="6731">reply</TOKEN>
<TOKEN id="token-68-2" pos="word" morph="none" start_char="6733" end_char="6734">to</TOKEN>
<TOKEN id="token-68-3" pos="punct" morph="none" start_char="6735" end_char="6735">:</TOKEN>
<TOKEN id="token-68-4" pos="word" morph="none" start_char="6737" end_char="6741">dug88</TOKEN>
</SEG>
<SEG id="segment-69" start_char="6744" end_char="6890">
<ORIGINAL_TEXT>We could perhaps find the evidence and reasoning that Alexa uses as well, but still wouldn't trust her for the recipe to boil-water, or make toast.</ORIGINAL_TEXT>
<TOKEN id="token-69-0" pos="word" morph="none" start_char="6744" end_char="6745">We</TOKEN>
<TOKEN id="token-69-1" pos="word" morph="none" start_char="6747" end_char="6751">could</TOKEN>
<TOKEN id="token-69-2" pos="word" morph="none" start_char="6753" end_char="6759">perhaps</TOKEN>
<TOKEN id="token-69-3" pos="word" morph="none" start_char="6761" end_char="6764">find</TOKEN>
<TOKEN id="token-69-4" pos="word" morph="none" start_char="6766" end_char="6768">the</TOKEN>
<TOKEN id="token-69-5" pos="word" morph="none" start_char="6770" end_char="6777">evidence</TOKEN>
<TOKEN id="token-69-6" pos="word" morph="none" start_char="6779" end_char="6781">and</TOKEN>
<TOKEN id="token-69-7" pos="word" morph="none" start_char="6783" end_char="6791">reasoning</TOKEN>
<TOKEN id="token-69-8" pos="word" morph="none" start_char="6793" end_char="6796">that</TOKEN>
<TOKEN id="token-69-9" pos="word" morph="none" start_char="6798" end_char="6802">Alexa</TOKEN>
<TOKEN id="token-69-10" pos="word" morph="none" start_char="6804" end_char="6807">uses</TOKEN>
<TOKEN id="token-69-11" pos="word" morph="none" start_char="6809" end_char="6810">as</TOKEN>
<TOKEN id="token-69-12" pos="word" morph="none" start_char="6812" end_char="6815">well</TOKEN>
<TOKEN id="token-69-13" pos="punct" morph="none" start_char="6816" end_char="6816">,</TOKEN>
<TOKEN id="token-69-14" pos="word" morph="none" start_char="6818" end_char="6820">but</TOKEN>
<TOKEN id="token-69-15" pos="word" morph="none" start_char="6822" end_char="6826">still</TOKEN>
<TOKEN id="token-69-16" pos="word" morph="none" start_char="6828" end_char="6835">wouldn't</TOKEN>
<TOKEN id="token-69-17" pos="word" morph="none" start_char="6837" end_char="6841">trust</TOKEN>
<TOKEN id="token-69-18" pos="word" morph="none" start_char="6843" end_char="6845">her</TOKEN>
<TOKEN id="token-69-19" pos="word" morph="none" start_char="6847" end_char="6849">for</TOKEN>
<TOKEN id="token-69-20" pos="word" morph="none" start_char="6851" end_char="6853">the</TOKEN>
<TOKEN id="token-69-21" pos="word" morph="none" start_char="6855" end_char="6860">recipe</TOKEN>
<TOKEN id="token-69-22" pos="word" morph="none" start_char="6862" end_char="6863">to</TOKEN>
<TOKEN id="token-69-23" pos="unknown" morph="none" start_char="6865" end_char="6874">boil-water</TOKEN>
<TOKEN id="token-69-24" pos="punct" morph="none" start_char="6875" end_char="6875">,</TOKEN>
<TOKEN id="token-69-25" pos="word" morph="none" start_char="6877" end_char="6878">or</TOKEN>
<TOKEN id="token-69-26" pos="word" morph="none" start_char="6880" end_char="6883">make</TOKEN>
<TOKEN id="token-69-27" pos="word" morph="none" start_char="6885" end_char="6889">toast</TOKEN>
<TOKEN id="token-69-28" pos="punct" morph="none" start_char="6890" end_char="6890">.</TOKEN>
</SEG>
<SEG id="segment-70" start_char="6893" end_char="6919">
<ORIGINAL_TEXT>It just don't feel right...</ORIGINAL_TEXT>
<TOKEN id="token-70-0" pos="word" morph="none" start_char="6893" end_char="6894">It</TOKEN>
<TOKEN id="token-70-1" pos="word" morph="none" start_char="6896" end_char="6899">just</TOKEN>
<TOKEN id="token-70-2" pos="word" morph="none" start_char="6901" end_char="6905">don't</TOKEN>
<TOKEN id="token-70-3" pos="word" morph="none" start_char="6907" end_char="6910">feel</TOKEN>
<TOKEN id="token-70-4" pos="word" morph="none" start_char="6912" end_char="6916">right</TOKEN>
<TOKEN id="token-70-5" pos="punct" morph="none" start_char="6917" end_char="6919">...</TOKEN>
</SEG>
<SEG id="segment-71" start_char="6924" end_char="7146">
<ORIGINAL_TEXT>originally posted by: dug88 An Israeli startup named Rootclaim has determined that there is an 81% chance that the SARS-cov-2 was accidentally released from the Wuhan bioresearch laboratory during gain-of-function research.</ORIGINAL_TEXT>
<TOKEN id="token-71-0" pos="word" morph="none" start_char="6924" end_char="6933">originally</TOKEN>
<TOKEN id="token-71-1" pos="word" morph="none" start_char="6935" end_char="6940">posted</TOKEN>
<TOKEN id="token-71-2" pos="word" morph="none" start_char="6942" end_char="6943">by</TOKEN>
<TOKEN id="token-71-3" pos="punct" morph="none" start_char="6944" end_char="6944">:</TOKEN>
<TOKEN id="token-71-4" pos="word" morph="none" start_char="6946" end_char="6950">dug88</TOKEN>
<TOKEN id="token-71-5" pos="word" morph="none" start_char="6952" end_char="6953">An</TOKEN>
<TOKEN id="token-71-6" pos="word" morph="none" start_char="6955" end_char="6961">Israeli</TOKEN>
<TOKEN id="token-71-7" pos="word" morph="none" start_char="6963" end_char="6969">startup</TOKEN>
<TOKEN id="token-71-8" pos="word" morph="none" start_char="6971" end_char="6975">named</TOKEN>
<TOKEN id="token-71-9" pos="word" morph="none" start_char="6977" end_char="6985">Rootclaim</TOKEN>
<TOKEN id="token-71-10" pos="word" morph="none" start_char="6987" end_char="6989">has</TOKEN>
<TOKEN id="token-71-11" pos="word" morph="none" start_char="6991" end_char="7000">determined</TOKEN>
<TOKEN id="token-71-12" pos="word" morph="none" start_char="7002" end_char="7005">that</TOKEN>
<TOKEN id="token-71-13" pos="word" morph="none" start_char="7007" end_char="7011">there</TOKEN>
<TOKEN id="token-71-14" pos="word" morph="none" start_char="7013" end_char="7014">is</TOKEN>
<TOKEN id="token-71-15" pos="word" morph="none" start_char="7016" end_char="7017">an</TOKEN>
<TOKEN id="token-71-16" pos="word" morph="none" start_char="7019" end_char="7020">81</TOKEN>
<TOKEN id="token-71-17" pos="punct" morph="none" start_char="7021" end_char="7021">%</TOKEN>
<TOKEN id="token-71-18" pos="word" morph="none" start_char="7023" end_char="7028">chance</TOKEN>
<TOKEN id="token-71-19" pos="word" morph="none" start_char="7030" end_char="7033">that</TOKEN>
<TOKEN id="token-71-20" pos="word" morph="none" start_char="7035" end_char="7037">the</TOKEN>
<TOKEN id="token-71-21" pos="unknown" morph="none" start_char="7039" end_char="7048">SARS-cov-2</TOKEN>
<TOKEN id="token-71-22" pos="word" morph="none" start_char="7050" end_char="7052">was</TOKEN>
<TOKEN id="token-71-23" pos="word" morph="none" start_char="7054" end_char="7065">accidentally</TOKEN>
<TOKEN id="token-71-24" pos="word" morph="none" start_char="7067" end_char="7074">released</TOKEN>
<TOKEN id="token-71-25" pos="word" morph="none" start_char="7076" end_char="7079">from</TOKEN>
<TOKEN id="token-71-26" pos="word" morph="none" start_char="7081" end_char="7083">the</TOKEN>
<TOKEN id="token-71-27" pos="word" morph="none" start_char="7085" end_char="7089">Wuhan</TOKEN>
<TOKEN id="token-71-28" pos="word" morph="none" start_char="7091" end_char="7101">bioresearch</TOKEN>
<TOKEN id="token-71-29" pos="word" morph="none" start_char="7103" end_char="7112">laboratory</TOKEN>
<TOKEN id="token-71-30" pos="word" morph="none" start_char="7114" end_char="7119">during</TOKEN>
<TOKEN id="token-71-31" pos="unknown" morph="none" start_char="7121" end_char="7136">gain-of-function</TOKEN>
<TOKEN id="token-71-32" pos="word" morph="none" start_char="7138" end_char="7145">research</TOKEN>
<TOKEN id="token-71-33" pos="punct" morph="none" start_char="7146" end_char="7146">.</TOKEN>
</SEG>
<SEG id="segment-72" start_char="7148" end_char="7255">
<ORIGINAL_TEXT>Rootclaim is, i'm assuming, an ai driven service that does this: According to rootclaim www.rootclaim.com...</ORIGINAL_TEXT>
<TOKEN id="token-72-0" pos="word" morph="none" start_char="7148" end_char="7156">Rootclaim</TOKEN>
<TOKEN id="token-72-1" pos="word" morph="none" start_char="7158" end_char="7159">is</TOKEN>
<TOKEN id="token-72-2" pos="punct" morph="none" start_char="7160" end_char="7160">,</TOKEN>
<TOKEN id="token-72-3" pos="word" morph="none" start_char="7162" end_char="7164">i'm</TOKEN>
<TOKEN id="token-72-4" pos="word" morph="none" start_char="7166" end_char="7173">assuming</TOKEN>
<TOKEN id="token-72-5" pos="punct" morph="none" start_char="7174" end_char="7174">,</TOKEN>
<TOKEN id="token-72-6" pos="word" morph="none" start_char="7176" end_char="7177">an</TOKEN>
<TOKEN id="token-72-7" pos="word" morph="none" start_char="7179" end_char="7180">ai</TOKEN>
<TOKEN id="token-72-8" pos="word" morph="none" start_char="7182" end_char="7187">driven</TOKEN>
<TOKEN id="token-72-9" pos="word" morph="none" start_char="7189" end_char="7195">service</TOKEN>
<TOKEN id="token-72-10" pos="word" morph="none" start_char="7197" end_char="7200">that</TOKEN>
<TOKEN id="token-72-11" pos="word" morph="none" start_char="7202" end_char="7205">does</TOKEN>
<TOKEN id="token-72-12" pos="word" morph="none" start_char="7207" end_char="7210">this</TOKEN>
<TOKEN id="token-72-13" pos="punct" morph="none" start_char="7211" end_char="7211">:</TOKEN>
<TOKEN id="token-72-14" pos="word" morph="none" start_char="7213" end_char="7221">According</TOKEN>
<TOKEN id="token-72-15" pos="word" morph="none" start_char="7223" end_char="7224">to</TOKEN>
<TOKEN id="token-72-16" pos="word" morph="none" start_char="7226" end_char="7234">rootclaim</TOKEN>
<TOKEN id="token-72-17" pos="url" morph="none" start_char="7236" end_char="7255">www.rootclaim.com...</TOKEN>
</SEG>
<SEG id="segment-73" start_char="7257" end_char="7336">
<ORIGINAL_TEXT>I'm sure this will likely be dismissed by the majority of media and many people.</ORIGINAL_TEXT>
<TOKEN id="token-73-0" pos="word" morph="none" start_char="7257" end_char="7259">I'm</TOKEN>
<TOKEN id="token-73-1" pos="word" morph="none" start_char="7261" end_char="7264">sure</TOKEN>
<TOKEN id="token-73-2" pos="word" morph="none" start_char="7266" end_char="7269">this</TOKEN>
<TOKEN id="token-73-3" pos="word" morph="none" start_char="7271" end_char="7274">will</TOKEN>
<TOKEN id="token-73-4" pos="word" morph="none" start_char="7276" end_char="7281">likely</TOKEN>
<TOKEN id="token-73-5" pos="word" morph="none" start_char="7283" end_char="7284">be</TOKEN>
<TOKEN id="token-73-6" pos="word" morph="none" start_char="7286" end_char="7294">dismissed</TOKEN>
<TOKEN id="token-73-7" pos="word" morph="none" start_char="7296" end_char="7297">by</TOKEN>
<TOKEN id="token-73-8" pos="word" morph="none" start_char="7299" end_char="7301">the</TOKEN>
<TOKEN id="token-73-9" pos="word" morph="none" start_char="7303" end_char="7310">majority</TOKEN>
<TOKEN id="token-73-10" pos="word" morph="none" start_char="7312" end_char="7313">of</TOKEN>
<TOKEN id="token-73-11" pos="word" morph="none" start_char="7315" end_char="7319">media</TOKEN>
<TOKEN id="token-73-12" pos="word" morph="none" start_char="7321" end_char="7323">and</TOKEN>
<TOKEN id="token-73-13" pos="word" morph="none" start_char="7325" end_char="7328">many</TOKEN>
<TOKEN id="token-73-14" pos="word" morph="none" start_char="7330" end_char="7335">people</TOKEN>
<TOKEN id="token-73-15" pos="punct" morph="none" start_char="7336" end_char="7336">.</TOKEN>
</SEG>
<SEG id="segment-74" start_char="7338" end_char="7384">
<ORIGINAL_TEXT>But it seems like it's worth looking more into.</ORIGINAL_TEXT>
<TOKEN id="token-74-0" pos="word" morph="none" start_char="7338" end_char="7340">But</TOKEN>
<TOKEN id="token-74-1" pos="word" morph="none" start_char="7342" end_char="7343">it</TOKEN>
<TOKEN id="token-74-2" pos="word" morph="none" start_char="7345" end_char="7349">seems</TOKEN>
<TOKEN id="token-74-3" pos="word" morph="none" start_char="7351" end_char="7354">like</TOKEN>
<TOKEN id="token-74-4" pos="word" morph="none" start_char="7356" end_char="7359">it's</TOKEN>
<TOKEN id="token-74-5" pos="word" morph="none" start_char="7361" end_char="7365">worth</TOKEN>
<TOKEN id="token-74-6" pos="word" morph="none" start_char="7367" end_char="7373">looking</TOKEN>
<TOKEN id="token-74-7" pos="word" morph="none" start_char="7375" end_char="7378">more</TOKEN>
<TOKEN id="token-74-8" pos="word" morph="none" start_char="7380" end_char="7383">into</TOKEN>
<TOKEN id="token-74-9" pos="punct" morph="none" start_char="7384" end_char="7384">.</TOKEN>
</SEG>
<SEG id="segment-75" start_char="7386" end_char="7451">
<ORIGINAL_TEXT>The link goes more into the evidence behind the eventual decision.</ORIGINAL_TEXT>
<TOKEN id="token-75-0" pos="word" morph="none" start_char="7386" end_char="7388">The</TOKEN>
<TOKEN id="token-75-1" pos="word" morph="none" start_char="7390" end_char="7393">link</TOKEN>
<TOKEN id="token-75-2" pos="word" morph="none" start_char="7395" end_char="7398">goes</TOKEN>
<TOKEN id="token-75-3" pos="word" morph="none" start_char="7400" end_char="7403">more</TOKEN>
<TOKEN id="token-75-4" pos="word" morph="none" start_char="7405" end_char="7408">into</TOKEN>
<TOKEN id="token-75-5" pos="word" morph="none" start_char="7410" end_char="7412">the</TOKEN>
<TOKEN id="token-75-6" pos="word" morph="none" start_char="7414" end_char="7421">evidence</TOKEN>
<TOKEN id="token-75-7" pos="word" morph="none" start_char="7423" end_char="7428">behind</TOKEN>
<TOKEN id="token-75-8" pos="word" morph="none" start_char="7430" end_char="7432">the</TOKEN>
<TOKEN id="token-75-9" pos="word" morph="none" start_char="7434" end_char="7441">eventual</TOKEN>
<TOKEN id="token-75-10" pos="word" morph="none" start_char="7443" end_char="7450">decision</TOKEN>
<TOKEN id="token-75-11" pos="punct" morph="none" start_char="7451" end_char="7451">.</TOKEN>
</SEG>
<SEG id="segment-76" start_char="7453" end_char="7500">
<ORIGINAL_TEXT>ETA: a bit of info on gain of function research.</ORIGINAL_TEXT>
<TOKEN id="token-76-0" pos="word" morph="none" start_char="7453" end_char="7455">ETA</TOKEN>
<TOKEN id="token-76-1" pos="punct" morph="none" start_char="7456" end_char="7456">:</TOKEN>
<TOKEN id="token-76-2" pos="word" morph="none" start_char="7458" end_char="7458">a</TOKEN>
<TOKEN id="token-76-3" pos="word" morph="none" start_char="7460" end_char="7462">bit</TOKEN>
<TOKEN id="token-76-4" pos="word" morph="none" start_char="7464" end_char="7465">of</TOKEN>
<TOKEN id="token-76-5" pos="word" morph="none" start_char="7467" end_char="7470">info</TOKEN>
<TOKEN id="token-76-6" pos="word" morph="none" start_char="7472" end_char="7473">on</TOKEN>
<TOKEN id="token-76-7" pos="word" morph="none" start_char="7475" end_char="7478">gain</TOKEN>
<TOKEN id="token-76-8" pos="word" morph="none" start_char="7480" end_char="7481">of</TOKEN>
<TOKEN id="token-76-9" pos="word" morph="none" start_char="7483" end_char="7490">function</TOKEN>
<TOKEN id="token-76-10" pos="word" morph="none" start_char="7492" end_char="7499">research</TOKEN>
<TOKEN id="token-76-11" pos="punct" morph="none" start_char="7500" end_char="7500">.</TOKEN>
</SEG>
<SEG id="segment-77" start_char="7502" end_char="7524">
<ORIGINAL_TEXT>www.ncbi.nlm.nih.gov...</ORIGINAL_TEXT>
<TOKEN id="token-77-0" pos="url" morph="none" start_char="7502" end_char="7524">www.ncbi.nlm.nih.gov...</TOKEN>
</SEG>
<SEG id="segment-78" start_char="7527" end_char="7609">
<ORIGINAL_TEXT>Most of us who have any damn sense or ability to reason knew this about a year ago.</ORIGINAL_TEXT>
<TOKEN id="token-78-0" pos="word" morph="none" start_char="7527" end_char="7530">Most</TOKEN>
<TOKEN id="token-78-1" pos="word" morph="none" start_char="7532" end_char="7533">of</TOKEN>
<TOKEN id="token-78-2" pos="word" morph="none" start_char="7535" end_char="7536">us</TOKEN>
<TOKEN id="token-78-3" pos="word" morph="none" start_char="7538" end_char="7540">who</TOKEN>
<TOKEN id="token-78-4" pos="word" morph="none" start_char="7542" end_char="7545">have</TOKEN>
<TOKEN id="token-78-5" pos="word" morph="none" start_char="7547" end_char="7549">any</TOKEN>
<TOKEN id="token-78-6" pos="word" morph="none" start_char="7551" end_char="7554">damn</TOKEN>
<TOKEN id="token-78-7" pos="word" morph="none" start_char="7556" end_char="7560">sense</TOKEN>
<TOKEN id="token-78-8" pos="word" morph="none" start_char="7562" end_char="7563">or</TOKEN>
<TOKEN id="token-78-9" pos="word" morph="none" start_char="7565" end_char="7571">ability</TOKEN>
<TOKEN id="token-78-10" pos="word" morph="none" start_char="7573" end_char="7574">to</TOKEN>
<TOKEN id="token-78-11" pos="word" morph="none" start_char="7576" end_char="7581">reason</TOKEN>
<TOKEN id="token-78-12" pos="word" morph="none" start_char="7583" end_char="7586">knew</TOKEN>
<TOKEN id="token-78-13" pos="word" morph="none" start_char="7588" end_char="7591">this</TOKEN>
<TOKEN id="token-78-14" pos="word" morph="none" start_char="7593" end_char="7597">about</TOKEN>
<TOKEN id="token-78-15" pos="word" morph="none" start_char="7599" end_char="7599">a</TOKEN>
<TOKEN id="token-78-16" pos="word" morph="none" start_char="7601" end_char="7604">year</TOKEN>
<TOKEN id="token-78-17" pos="word" morph="none" start_char="7606" end_char="7608">ago</TOKEN>
<TOKEN id="token-78-18" pos="punct" morph="none" start_char="7609" end_char="7609">.</TOKEN>
</SEG>
<SEG id="segment-79" start_char="7614" end_char="7635">
<ORIGINAL_TEXT>a reply to: slatesteam</ORIGINAL_TEXT>
<TOKEN id="token-79-0" pos="word" morph="none" start_char="7614" end_char="7614">a</TOKEN>
<TOKEN id="token-79-1" pos="word" morph="none" start_char="7616" end_char="7620">reply</TOKEN>
<TOKEN id="token-79-2" pos="word" morph="none" start_char="7622" end_char="7623">to</TOKEN>
<TOKEN id="token-79-3" pos="punct" morph="none" start_char="7624" end_char="7624">:</TOKEN>
<TOKEN id="token-79-4" pos="word" morph="none" start_char="7626" end_char="7635">slatesteam</TOKEN>
</SEG>
<SEG id="segment-80" start_char="7638" end_char="7670">
<ORIGINAL_TEXT>Your point is purely speculative.</ORIGINAL_TEXT>
<TOKEN id="token-80-0" pos="word" morph="none" start_char="7638" end_char="7641">Your</TOKEN>
<TOKEN id="token-80-1" pos="word" morph="none" start_char="7643" end_char="7647">point</TOKEN>
<TOKEN id="token-80-2" pos="word" morph="none" start_char="7649" end_char="7650">is</TOKEN>
<TOKEN id="token-80-3" pos="word" morph="none" start_char="7652" end_char="7657">purely</TOKEN>
<TOKEN id="token-80-4" pos="word" morph="none" start_char="7659" end_char="7669">speculative</TOKEN>
<TOKEN id="token-80-5" pos="punct" morph="none" start_char="7670" end_char="7670">.</TOKEN>
</SEG>
<SEG id="segment-81" start_char="7673" end_char="7730">
<ORIGINAL_TEXT>If that was the case, I'd expect more rhetoric from China.</ORIGINAL_TEXT>
<TOKEN id="token-81-0" pos="word" morph="none" start_char="7673" end_char="7674">If</TOKEN>
<TOKEN id="token-81-1" pos="word" morph="none" start_char="7676" end_char="7679">that</TOKEN>
<TOKEN id="token-81-2" pos="word" morph="none" start_char="7681" end_char="7683">was</TOKEN>
<TOKEN id="token-81-3" pos="word" morph="none" start_char="7685" end_char="7687">the</TOKEN>
<TOKEN id="token-81-4" pos="word" morph="none" start_char="7689" end_char="7692">case</TOKEN>
<TOKEN id="token-81-5" pos="punct" morph="none" start_char="7693" end_char="7693">,</TOKEN>
<TOKEN id="token-81-6" pos="word" morph="none" start_char="7695" end_char="7697">I'd</TOKEN>
<TOKEN id="token-81-7" pos="word" morph="none" start_char="7699" end_char="7704">expect</TOKEN>
<TOKEN id="token-81-8" pos="word" morph="none" start_char="7706" end_char="7709">more</TOKEN>
<TOKEN id="token-81-9" pos="word" morph="none" start_char="7711" end_char="7718">rhetoric</TOKEN>
<TOKEN id="token-81-10" pos="word" morph="none" start_char="7720" end_char="7723">from</TOKEN>
<TOKEN id="token-81-11" pos="word" morph="none" start_char="7725" end_char="7729">China</TOKEN>
<TOKEN id="token-81-12" pos="punct" morph="none" start_char="7730" end_char="7730">.</TOKEN>
</SEG>
<SEG id="segment-82" start_char="7735" end_char="7854">
<ORIGINAL_TEXT>originally posted by: Nothin a reply to: dug88 An AI, that's gonna legitimatize our whack-a-doodle conspiracy-theories ?</ORIGINAL_TEXT>
<TOKEN id="token-82-0" pos="word" morph="none" start_char="7735" end_char="7744">originally</TOKEN>
<TOKEN id="token-82-1" pos="word" morph="none" start_char="7746" end_char="7751">posted</TOKEN>
<TOKEN id="token-82-2" pos="word" morph="none" start_char="7753" end_char="7754">by</TOKEN>
<TOKEN id="token-82-3" pos="punct" morph="none" start_char="7755" end_char="7755">:</TOKEN>
<TOKEN id="token-82-4" pos="word" morph="none" start_char="7757" end_char="7762">Nothin</TOKEN>
<TOKEN id="token-82-5" pos="word" morph="none" start_char="7764" end_char="7764">a</TOKEN>
<TOKEN id="token-82-6" pos="word" morph="none" start_char="7766" end_char="7770">reply</TOKEN>
<TOKEN id="token-82-7" pos="word" morph="none" start_char="7772" end_char="7773">to</TOKEN>
<TOKEN id="token-82-8" pos="punct" morph="none" start_char="7774" end_char="7774">:</TOKEN>
<TOKEN id="token-82-9" pos="word" morph="none" start_char="7776" end_char="7780">dug88</TOKEN>
<TOKEN id="token-82-10" pos="word" morph="none" start_char="7782" end_char="7783">An</TOKEN>
<TOKEN id="token-82-11" pos="word" morph="none" start_char="7785" end_char="7786">AI</TOKEN>
<TOKEN id="token-82-12" pos="punct" morph="none" start_char="7787" end_char="7787">,</TOKEN>
<TOKEN id="token-82-13" pos="word" morph="none" start_char="7789" end_char="7794">that's</TOKEN>
<TOKEN id="token-82-14" pos="word" morph="none" start_char="7796" end_char="7800">gonna</TOKEN>
<TOKEN id="token-82-15" pos="word" morph="none" start_char="7802" end_char="7813">legitimatize</TOKEN>
<TOKEN id="token-82-16" pos="word" morph="none" start_char="7815" end_char="7817">our</TOKEN>
<TOKEN id="token-82-17" pos="unknown" morph="none" start_char="7819" end_char="7832">whack-a-doodle</TOKEN>
<TOKEN id="token-82-18" pos="unknown" morph="none" start_char="7834" end_char="7852">conspiracy-theories</TOKEN>
<TOKEN id="token-82-19" pos="punct" morph="none" start_char="7854" end_char="7854">?</TOKEN>
</SEG>
<SEG id="segment-83" start_char="7856" end_char="7863">
<ORIGINAL_TEXT>Really ?</ORIGINAL_TEXT>
<TOKEN id="token-83-0" pos="word" morph="none" start_char="7856" end_char="7861">Really</TOKEN>
<TOKEN id="token-83-1" pos="punct" morph="none" start_char="7863" end_char="7863">?</TOKEN>
</SEG>
<SEG id="segment-84" start_char="7866" end_char="7960">
<ORIGINAL_TEXT>Well AI are good at giving probabilities but we can't reach conclusions based on probabilities.</ORIGINAL_TEXT>
<TOKEN id="token-84-0" pos="word" morph="none" start_char="7866" end_char="7869">Well</TOKEN>
<TOKEN id="token-84-1" pos="word" morph="none" start_char="7871" end_char="7872">AI</TOKEN>
<TOKEN id="token-84-2" pos="word" morph="none" start_char="7874" end_char="7876">are</TOKEN>
<TOKEN id="token-84-3" pos="word" morph="none" start_char="7878" end_char="7881">good</TOKEN>
<TOKEN id="token-84-4" pos="word" morph="none" start_char="7883" end_char="7884">at</TOKEN>
<TOKEN id="token-84-5" pos="word" morph="none" start_char="7886" end_char="7891">giving</TOKEN>
<TOKEN id="token-84-6" pos="word" morph="none" start_char="7893" end_char="7905">probabilities</TOKEN>
<TOKEN id="token-84-7" pos="word" morph="none" start_char="7907" end_char="7909">but</TOKEN>
<TOKEN id="token-84-8" pos="word" morph="none" start_char="7911" end_char="7912">we</TOKEN>
<TOKEN id="token-84-9" pos="word" morph="none" start_char="7914" end_char="7918">can't</TOKEN>
<TOKEN id="token-84-10" pos="word" morph="none" start_char="7920" end_char="7924">reach</TOKEN>
<TOKEN id="token-84-11" pos="word" morph="none" start_char="7926" end_char="7936">conclusions</TOKEN>
<TOKEN id="token-84-12" pos="word" morph="none" start_char="7938" end_char="7942">based</TOKEN>
<TOKEN id="token-84-13" pos="word" morph="none" start_char="7944" end_char="7945">on</TOKEN>
<TOKEN id="token-84-14" pos="word" morph="none" start_char="7947" end_char="7959">probabilities</TOKEN>
<TOKEN id="token-84-15" pos="punct" morph="none" start_char="7960" end_char="7960">.</TOKEN>
</SEG>
<SEG id="segment-85" start_char="7962" end_char="8137">
<ORIGINAL_TEXT>However I do find it much more probable that Covid-19 was released from a lab considering that the Wuhan lab already created a SARS-Cov hybrid capable of infecting human cells.</ORIGINAL_TEXT>
<TOKEN id="token-85-0" pos="word" morph="none" start_char="7962" end_char="7968">However</TOKEN>
<TOKEN id="token-85-1" pos="word" morph="none" start_char="7970" end_char="7970">I</TOKEN>
<TOKEN id="token-85-2" pos="word" morph="none" start_char="7972" end_char="7973">do</TOKEN>
<TOKEN id="token-85-3" pos="word" morph="none" start_char="7975" end_char="7978">find</TOKEN>
<TOKEN id="token-85-4" pos="word" morph="none" start_char="7980" end_char="7981">it</TOKEN>
<TOKEN id="token-85-5" pos="word" morph="none" start_char="7983" end_char="7986">much</TOKEN>
<TOKEN id="token-85-6" pos="word" morph="none" start_char="7988" end_char="7991">more</TOKEN>
<TOKEN id="token-85-7" pos="word" morph="none" start_char="7993" end_char="8000">probable</TOKEN>
<TOKEN id="token-85-8" pos="word" morph="none" start_char="8002" end_char="8005">that</TOKEN>
<TOKEN id="token-85-9" pos="unknown" morph="none" start_char="8007" end_char="8014">Covid-19</TOKEN>
<TOKEN id="token-85-10" pos="word" morph="none" start_char="8016" end_char="8018">was</TOKEN>
<TOKEN id="token-85-11" pos="word" morph="none" start_char="8020" end_char="8027">released</TOKEN>
<TOKEN id="token-85-12" pos="word" morph="none" start_char="8029" end_char="8032">from</TOKEN>
<TOKEN id="token-85-13" pos="word" morph="none" start_char="8034" end_char="8034">a</TOKEN>
<TOKEN id="token-85-14" pos="word" morph="none" start_char="8036" end_char="8038">lab</TOKEN>
<TOKEN id="token-85-15" pos="word" morph="none" start_char="8040" end_char="8050">considering</TOKEN>
<TOKEN id="token-85-16" pos="word" morph="none" start_char="8052" end_char="8055">that</TOKEN>
<TOKEN id="token-85-17" pos="word" morph="none" start_char="8057" end_char="8059">the</TOKEN>
<TOKEN id="token-85-18" pos="word" morph="none" start_char="8061" end_char="8065">Wuhan</TOKEN>
<TOKEN id="token-85-19" pos="word" morph="none" start_char="8067" end_char="8069">lab</TOKEN>
<TOKEN id="token-85-20" pos="word" morph="none" start_char="8071" end_char="8077">already</TOKEN>
<TOKEN id="token-85-21" pos="word" morph="none" start_char="8079" end_char="8085">created</TOKEN>
<TOKEN id="token-85-22" pos="word" morph="none" start_char="8087" end_char="8087">a</TOKEN>
<TOKEN id="token-85-23" pos="unknown" morph="none" start_char="8089" end_char="8096">SARS-Cov</TOKEN>
<TOKEN id="token-85-24" pos="word" morph="none" start_char="8098" end_char="8103">hybrid</TOKEN>
<TOKEN id="token-85-25" pos="word" morph="none" start_char="8105" end_char="8111">capable</TOKEN>
<TOKEN id="token-85-26" pos="word" morph="none" start_char="8113" end_char="8114">of</TOKEN>
<TOKEN id="token-85-27" pos="word" morph="none" start_char="8116" end_char="8124">infecting</TOKEN>
<TOKEN id="token-85-28" pos="word" morph="none" start_char="8126" end_char="8130">human</TOKEN>
<TOKEN id="token-85-29" pos="word" morph="none" start_char="8132" end_char="8136">cells</TOKEN>
<TOKEN id="token-85-30" pos="punct" morph="none" start_char="8137" end_char="8137">.</TOKEN>
</SEG>
<SEG id="segment-86" start_char="8139" end_char="8285">
<ORIGINAL_TEXT>The mainstream theories about how it jumped to humans make little sense and there is very little evidence to actually show how it jumped to humans.</ORIGINAL_TEXT>
<TOKEN id="token-86-0" pos="word" morph="none" start_char="8139" end_char="8141">The</TOKEN>
<TOKEN id="token-86-1" pos="word" morph="none" start_char="8143" end_char="8152">mainstream</TOKEN>
<TOKEN id="token-86-2" pos="word" morph="none" start_char="8154" end_char="8161">theories</TOKEN>
<TOKEN id="token-86-3" pos="word" morph="none" start_char="8163" end_char="8167">about</TOKEN>
<TOKEN id="token-86-4" pos="word" morph="none" start_char="8169" end_char="8171">how</TOKEN>
<TOKEN id="token-86-5" pos="word" morph="none" start_char="8173" end_char="8174">it</TOKEN>
<TOKEN id="token-86-6" pos="word" morph="none" start_char="8176" end_char="8181">jumped</TOKEN>
<TOKEN id="token-86-7" pos="word" morph="none" start_char="8183" end_char="8184">to</TOKEN>
<TOKEN id="token-86-8" pos="word" morph="none" start_char="8186" end_char="8191">humans</TOKEN>
<TOKEN id="token-86-9" pos="word" morph="none" start_char="8193" end_char="8196">make</TOKEN>
<TOKEN id="token-86-10" pos="word" morph="none" start_char="8198" end_char="8203">little</TOKEN>
<TOKEN id="token-86-11" pos="word" morph="none" start_char="8205" end_char="8209">sense</TOKEN>
<TOKEN id="token-86-12" pos="word" morph="none" start_char="8211" end_char="8213">and</TOKEN>
<TOKEN id="token-86-13" pos="word" morph="none" start_char="8215" end_char="8219">there</TOKEN>
<TOKEN id="token-86-14" pos="word" morph="none" start_char="8221" end_char="8222">is</TOKEN>
<TOKEN id="token-86-15" pos="word" morph="none" start_char="8224" end_char="8227">very</TOKEN>
<TOKEN id="token-86-16" pos="word" morph="none" start_char="8229" end_char="8234">little</TOKEN>
<TOKEN id="token-86-17" pos="word" morph="none" start_char="8236" end_char="8243">evidence</TOKEN>
<TOKEN id="token-86-18" pos="word" morph="none" start_char="8245" end_char="8246">to</TOKEN>
<TOKEN id="token-86-19" pos="word" morph="none" start_char="8248" end_char="8255">actually</TOKEN>
<TOKEN id="token-86-20" pos="word" morph="none" start_char="8257" end_char="8260">show</TOKEN>
<TOKEN id="token-86-21" pos="word" morph="none" start_char="8262" end_char="8264">how</TOKEN>
<TOKEN id="token-86-22" pos="word" morph="none" start_char="8266" end_char="8267">it</TOKEN>
<TOKEN id="token-86-23" pos="word" morph="none" start_char="8269" end_char="8274">jumped</TOKEN>
<TOKEN id="token-86-24" pos="word" morph="none" start_char="8276" end_char="8277">to</TOKEN>
<TOKEN id="token-86-25" pos="word" morph="none" start_char="8279" end_char="8284">humans</TOKEN>
<TOKEN id="token-86-26" pos="punct" morph="none" start_char="8285" end_char="8285">.</TOKEN>
</SEG>
<SEG id="segment-87" start_char="8287" end_char="8520">
<ORIGINAL_TEXT>The origin point of the virus would suggest a very high probability that the virus was accidentally leaked from the Wuhan lab, which isn't hard to imagine considering how effectively Covid-19 spreads regardless of all safety measures.</ORIGINAL_TEXT>
<TOKEN id="token-87-0" pos="word" morph="none" start_char="8287" end_char="8289">The</TOKEN>
<TOKEN id="token-87-1" pos="word" morph="none" start_char="8291" end_char="8296">origin</TOKEN>
<TOKEN id="token-87-2" pos="word" morph="none" start_char="8298" end_char="8302">point</TOKEN>
<TOKEN id="token-87-3" pos="word" morph="none" start_char="8304" end_char="8305">of</TOKEN>
<TOKEN id="token-87-4" pos="word" morph="none" start_char="8307" end_char="8309">the</TOKEN>
<TOKEN id="token-87-5" pos="word" morph="none" start_char="8311" end_char="8315">virus</TOKEN>
<TOKEN id="token-87-6" pos="word" morph="none" start_char="8317" end_char="8321">would</TOKEN>
<TOKEN id="token-87-7" pos="word" morph="none" start_char="8323" end_char="8329">suggest</TOKEN>
<TOKEN id="token-87-8" pos="word" morph="none" start_char="8331" end_char="8331">a</TOKEN>
<TOKEN id="token-87-9" pos="word" morph="none" start_char="8333" end_char="8336">very</TOKEN>
<TOKEN id="token-87-10" pos="word" morph="none" start_char="8338" end_char="8341">high</TOKEN>
<TOKEN id="token-87-11" pos="word" morph="none" start_char="8343" end_char="8353">probability</TOKEN>
<TOKEN id="token-87-12" pos="word" morph="none" start_char="8355" end_char="8358">that</TOKEN>
<TOKEN id="token-87-13" pos="word" morph="none" start_char="8360" end_char="8362">the</TOKEN>
<TOKEN id="token-87-14" pos="word" morph="none" start_char="8364" end_char="8368">virus</TOKEN>
<TOKEN id="token-87-15" pos="word" morph="none" start_char="8370" end_char="8372">was</TOKEN>
<TOKEN id="token-87-16" pos="word" morph="none" start_char="8374" end_char="8385">accidentally</TOKEN>
<TOKEN id="token-87-17" pos="word" morph="none" start_char="8387" end_char="8392">leaked</TOKEN>
<TOKEN id="token-87-18" pos="word" morph="none" start_char="8394" end_char="8397">from</TOKEN>
<TOKEN id="token-87-19" pos="word" morph="none" start_char="8399" end_char="8401">the</TOKEN>
<TOKEN id="token-87-20" pos="word" morph="none" start_char="8403" end_char="8407">Wuhan</TOKEN>
<TOKEN id="token-87-21" pos="word" morph="none" start_char="8409" end_char="8411">lab</TOKEN>
<TOKEN id="token-87-22" pos="punct" morph="none" start_char="8412" end_char="8412">,</TOKEN>
<TOKEN id="token-87-23" pos="word" morph="none" start_char="8414" end_char="8418">which</TOKEN>
<TOKEN id="token-87-24" pos="word" morph="none" start_char="8420" end_char="8424">isn't</TOKEN>
<TOKEN id="token-87-25" pos="word" morph="none" start_char="8426" end_char="8429">hard</TOKEN>
<TOKEN id="token-87-26" pos="word" morph="none" start_char="8431" end_char="8432">to</TOKEN>
<TOKEN id="token-87-27" pos="word" morph="none" start_char="8434" end_char="8440">imagine</TOKEN>
<TOKEN id="token-87-28" pos="word" morph="none" start_char="8442" end_char="8452">considering</TOKEN>
<TOKEN id="token-87-29" pos="word" morph="none" start_char="8454" end_char="8456">how</TOKEN>
<TOKEN id="token-87-30" pos="word" morph="none" start_char="8458" end_char="8468">effectively</TOKEN>
<TOKEN id="token-87-31" pos="unknown" morph="none" start_char="8470" end_char="8477">Covid-19</TOKEN>
<TOKEN id="token-87-32" pos="word" morph="none" start_char="8479" end_char="8485">spreads</TOKEN>
<TOKEN id="token-87-33" pos="word" morph="none" start_char="8487" end_char="8496">regardless</TOKEN>
<TOKEN id="token-87-34" pos="word" morph="none" start_char="8498" end_char="8499">of</TOKEN>
<TOKEN id="token-87-35" pos="word" morph="none" start_char="8501" end_char="8503">all</TOKEN>
<TOKEN id="token-87-36" pos="word" morph="none" start_char="8505" end_char="8510">safety</TOKEN>
<TOKEN id="token-87-37" pos="word" morph="none" start_char="8512" end_char="8519">measures</TOKEN>
<TOKEN id="token-87-38" pos="punct" morph="none" start_char="8520" end_char="8520">.</TOKEN>
</SEG>
<SEG id="segment-88" start_char="8523" end_char="8744">
<ORIGINAL_TEXT>In 2005, a group including researchers from the Wuhan Institute of Virology published research into the origin of the SARS coronavirus, finding that China's horseshoe bats are natural reservoirs of SARS-like coronaviruses.</ORIGINAL_TEXT>
<TOKEN id="token-88-0" pos="word" morph="none" start_char="8523" end_char="8524">In</TOKEN>
<TOKEN id="token-88-1" pos="word" morph="none" start_char="8526" end_char="8529">2005</TOKEN>
<TOKEN id="token-88-2" pos="punct" morph="none" start_char="8530" end_char="8530">,</TOKEN>
<TOKEN id="token-88-3" pos="word" morph="none" start_char="8532" end_char="8532">a</TOKEN>
<TOKEN id="token-88-4" pos="word" morph="none" start_char="8534" end_char="8538">group</TOKEN>
<TOKEN id="token-88-5" pos="word" morph="none" start_char="8540" end_char="8548">including</TOKEN>
<TOKEN id="token-88-6" pos="word" morph="none" start_char="8550" end_char="8560">researchers</TOKEN>
<TOKEN id="token-88-7" pos="word" morph="none" start_char="8562" end_char="8565">from</TOKEN>
<TOKEN id="token-88-8" pos="word" morph="none" start_char="8567" end_char="8569">the</TOKEN>
<TOKEN id="token-88-9" pos="word" morph="none" start_char="8571" end_char="8575">Wuhan</TOKEN>
<TOKEN id="token-88-10" pos="word" morph="none" start_char="8577" end_char="8585">Institute</TOKEN>
<TOKEN id="token-88-11" pos="word" morph="none" start_char="8587" end_char="8588">of</TOKEN>
<TOKEN id="token-88-12" pos="word" morph="none" start_char="8590" end_char="8597">Virology</TOKEN>
<TOKEN id="token-88-13" pos="word" morph="none" start_char="8599" end_char="8607">published</TOKEN>
<TOKEN id="token-88-14" pos="word" morph="none" start_char="8609" end_char="8616">research</TOKEN>
<TOKEN id="token-88-15" pos="word" morph="none" start_char="8618" end_char="8621">into</TOKEN>
<TOKEN id="token-88-16" pos="word" morph="none" start_char="8623" end_char="8625">the</TOKEN>
<TOKEN id="token-88-17" pos="word" morph="none" start_char="8627" end_char="8632">origin</TOKEN>
<TOKEN id="token-88-18" pos="word" morph="none" start_char="8634" end_char="8635">of</TOKEN>
<TOKEN id="token-88-19" pos="word" morph="none" start_char="8637" end_char="8639">the</TOKEN>
<TOKEN id="token-88-20" pos="word" morph="none" start_char="8641" end_char="8644">SARS</TOKEN>
<TOKEN id="token-88-21" pos="word" morph="none" start_char="8646" end_char="8656">coronavirus</TOKEN>
<TOKEN id="token-88-22" pos="punct" morph="none" start_char="8657" end_char="8657">,</TOKEN>
<TOKEN id="token-88-23" pos="word" morph="none" start_char="8659" end_char="8665">finding</TOKEN>
<TOKEN id="token-88-24" pos="word" morph="none" start_char="8667" end_char="8670">that</TOKEN>
<TOKEN id="token-88-25" pos="word" morph="none" start_char="8672" end_char="8678">China's</TOKEN>
<TOKEN id="token-88-26" pos="word" morph="none" start_char="8680" end_char="8688">horseshoe</TOKEN>
<TOKEN id="token-88-27" pos="word" morph="none" start_char="8690" end_char="8693">bats</TOKEN>
<TOKEN id="token-88-28" pos="word" morph="none" start_char="8695" end_char="8697">are</TOKEN>
<TOKEN id="token-88-29" pos="word" morph="none" start_char="8699" end_char="8705">natural</TOKEN>
<TOKEN id="token-88-30" pos="word" morph="none" start_char="8707" end_char="8716">reservoirs</TOKEN>
<TOKEN id="token-88-31" pos="word" morph="none" start_char="8718" end_char="8719">of</TOKEN>
<TOKEN id="token-88-32" pos="unknown" morph="none" start_char="8721" end_char="8729">SARS-like</TOKEN>
<TOKEN id="token-88-33" pos="word" morph="none" start_char="8731" end_char="8743">coronaviruses</TOKEN>
<TOKEN id="token-88-34" pos="punct" morph="none" start_char="8744" end_char="8744">.</TOKEN>
</SEG>
<SEG id="segment-89" start_char="8746" end_char="8933">
<ORIGINAL_TEXT>[6] Continuing this work over a period of years, researchers from the Institute sampled thousands of horseshoe bats in locations across China, isolating over 300 bat coronavirus sequences.</ORIGINAL_TEXT>
<TOKEN id="token-89-0" pos="punct" morph="none" start_char="8746" end_char="8746">[</TOKEN>
<TOKEN id="token-89-1" pos="word" morph="none" start_char="8747" end_char="8747">6</TOKEN>
<TOKEN id="token-89-2" pos="punct" morph="none" start_char="8748" end_char="8748">]</TOKEN>
<TOKEN id="token-89-3" pos="word" morph="none" start_char="8750" end_char="8759">Continuing</TOKEN>
<TOKEN id="token-89-4" pos="word" morph="none" start_char="8761" end_char="8764">this</TOKEN>
<TOKEN id="token-89-5" pos="word" morph="none" start_char="8766" end_char="8769">work</TOKEN>
<TOKEN id="token-89-6" pos="word" morph="none" start_char="8771" end_char="8774">over</TOKEN>
<TOKEN id="token-89-7" pos="word" morph="none" start_char="8776" end_char="8776">a</TOKEN>
<TOKEN id="token-89-8" pos="word" morph="none" start_char="8778" end_char="8783">period</TOKEN>
<TOKEN id="token-89-9" pos="word" morph="none" start_char="8785" end_char="8786">of</TOKEN>
<TOKEN id="token-89-10" pos="word" morph="none" start_char="8788" end_char="8792">years</TOKEN>
<TOKEN id="token-89-11" pos="punct" morph="none" start_char="8793" end_char="8793">,</TOKEN>
<TOKEN id="token-89-12" pos="word" morph="none" start_char="8795" end_char="8805">researchers</TOKEN>
<TOKEN id="token-89-13" pos="word" morph="none" start_char="8807" end_char="8810">from</TOKEN>
<TOKEN id="token-89-14" pos="word" morph="none" start_char="8812" end_char="8814">the</TOKEN>
<TOKEN id="token-89-15" pos="word" morph="none" start_char="8816" end_char="8824">Institute</TOKEN>
<TOKEN id="token-89-16" pos="word" morph="none" start_char="8826" end_char="8832">sampled</TOKEN>
<TOKEN id="token-89-17" pos="word" morph="none" start_char="8834" end_char="8842">thousands</TOKEN>
<TOKEN id="token-89-18" pos="word" morph="none" start_char="8844" end_char="8845">of</TOKEN>
<TOKEN id="token-89-19" pos="word" morph="none" start_char="8847" end_char="8855">horseshoe</TOKEN>
<TOKEN id="token-89-20" pos="word" morph="none" start_char="8857" end_char="8860">bats</TOKEN>
<TOKEN id="token-89-21" pos="word" morph="none" start_char="8862" end_char="8863">in</TOKEN>
<TOKEN id="token-89-22" pos="word" morph="none" start_char="8865" end_char="8873">locations</TOKEN>
<TOKEN id="token-89-23" pos="word" morph="none" start_char="8875" end_char="8880">across</TOKEN>
<TOKEN id="token-89-24" pos="word" morph="none" start_char="8882" end_char="8886">China</TOKEN>
<TOKEN id="token-89-25" pos="punct" morph="none" start_char="8887" end_char="8887">,</TOKEN>
<TOKEN id="token-89-26" pos="word" morph="none" start_char="8889" end_char="8897">isolating</TOKEN>
<TOKEN id="token-89-27" pos="word" morph="none" start_char="8899" end_char="8902">over</TOKEN>
<TOKEN id="token-89-28" pos="word" morph="none" start_char="8904" end_char="8906">300</TOKEN>
<TOKEN id="token-89-29" pos="word" morph="none" start_char="8908" end_char="8910">bat</TOKEN>
<TOKEN id="token-89-30" pos="word" morph="none" start_char="8912" end_char="8922">coronavirus</TOKEN>
<TOKEN id="token-89-31" pos="word" morph="none" start_char="8924" end_char="8932">sequences</TOKEN>
<TOKEN id="token-89-32" pos="punct" morph="none" start_char="8933" end_char="8933">.</TOKEN>
</SEG>
<SEG id="segment-90" start_char="8935" end_char="9049">
<ORIGINAL_TEXT>[7] In 2015, the Institute published successful research on whether a bat coronavirus could be made to infect HeLa.</ORIGINAL_TEXT>
<TOKEN id="token-90-0" pos="punct" morph="none" start_char="8935" end_char="8935">[</TOKEN>
<TOKEN id="token-90-1" pos="word" morph="none" start_char="8936" end_char="8936">7</TOKEN>
<TOKEN id="token-90-2" pos="punct" morph="none" start_char="8937" end_char="8937">]</TOKEN>
<TOKEN id="token-90-3" pos="word" morph="none" start_char="8939" end_char="8940">In</TOKEN>
<TOKEN id="token-90-4" pos="word" morph="none" start_char="8942" end_char="8945">2015</TOKEN>
<TOKEN id="token-90-5" pos="punct" morph="none" start_char="8946" end_char="8946">,</TOKEN>
<TOKEN id="token-90-6" pos="word" morph="none" start_char="8948" end_char="8950">the</TOKEN>
<TOKEN id="token-90-7" pos="word" morph="none" start_char="8952" end_char="8960">Institute</TOKEN>
<TOKEN id="token-90-8" pos="word" morph="none" start_char="8962" end_char="8970">published</TOKEN>
<TOKEN id="token-90-9" pos="word" morph="none" start_char="8972" end_char="8981">successful</TOKEN>
<TOKEN id="token-90-10" pos="word" morph="none" start_char="8983" end_char="8990">research</TOKEN>
<TOKEN id="token-90-11" pos="word" morph="none" start_char="8992" end_char="8993">on</TOKEN>
<TOKEN id="token-90-12" pos="word" morph="none" start_char="8995" end_char="9001">whether</TOKEN>
<TOKEN id="token-90-13" pos="word" morph="none" start_char="9003" end_char="9003">a</TOKEN>
<TOKEN id="token-90-14" pos="word" morph="none" start_char="9005" end_char="9007">bat</TOKEN>
<TOKEN id="token-90-15" pos="word" morph="none" start_char="9009" end_char="9019">coronavirus</TOKEN>
<TOKEN id="token-90-16" pos="word" morph="none" start_char="9021" end_char="9025">could</TOKEN>
<TOKEN id="token-90-17" pos="word" morph="none" start_char="9027" end_char="9028">be</TOKEN>
<TOKEN id="token-90-18" pos="word" morph="none" start_char="9030" end_char="9033">made</TOKEN>
<TOKEN id="token-90-19" pos="word" morph="none" start_char="9035" end_char="9036">to</TOKEN>
<TOKEN id="token-90-20" pos="word" morph="none" start_char="9038" end_char="9043">infect</TOKEN>
<TOKEN id="token-90-21" pos="word" morph="none" start_char="9045" end_char="9048">HeLa</TOKEN>
<TOKEN id="token-90-22" pos="punct" morph="none" start_char="9049" end_char="9049">.</TOKEN>
</SEG>
<SEG id="segment-91" start_char="9051" end_char="9211">
<ORIGINAL_TEXT>A team from the Institute engineered a hybrid virus, combining a bat coronavirus with a SARS virus that had been adapted to grow in mice and mimic human disease.</ORIGINAL_TEXT>
<TOKEN id="token-91-0" pos="word" morph="none" start_char="9051" end_char="9051">A</TOKEN>
<TOKEN id="token-91-1" pos="word" morph="none" start_char="9053" end_char="9056">team</TOKEN>
<TOKEN id="token-91-2" pos="word" morph="none" start_char="9058" end_char="9061">from</TOKEN>
<TOKEN id="token-91-3" pos="word" morph="none" start_char="9063" end_char="9065">the</TOKEN>
<TOKEN id="token-91-4" pos="word" morph="none" start_char="9067" end_char="9075">Institute</TOKEN>
<TOKEN id="token-91-5" pos="word" morph="none" start_char="9077" end_char="9086">engineered</TOKEN>
<TOKEN id="token-91-6" pos="word" morph="none" start_char="9088" end_char="9088">a</TOKEN>
<TOKEN id="token-91-7" pos="word" morph="none" start_char="9090" end_char="9095">hybrid</TOKEN>
<TOKEN id="token-91-8" pos="word" morph="none" start_char="9097" end_char="9101">virus</TOKEN>
<TOKEN id="token-91-9" pos="punct" morph="none" start_char="9102" end_char="9102">,</TOKEN>
<TOKEN id="token-91-10" pos="word" morph="none" start_char="9104" end_char="9112">combining</TOKEN>
<TOKEN id="token-91-11" pos="word" morph="none" start_char="9114" end_char="9114">a</TOKEN>
<TOKEN id="token-91-12" pos="word" morph="none" start_char="9116" end_char="9118">bat</TOKEN>
<TOKEN id="token-91-13" pos="word" morph="none" start_char="9120" end_char="9130">coronavirus</TOKEN>
<TOKEN id="token-91-14" pos="word" morph="none" start_char="9132" end_char="9135">with</TOKEN>
<TOKEN id="token-91-15" pos="word" morph="none" start_char="9137" end_char="9137">a</TOKEN>
<TOKEN id="token-91-16" pos="word" morph="none" start_char="9139" end_char="9142">SARS</TOKEN>
<TOKEN id="token-91-17" pos="word" morph="none" start_char="9144" end_char="9148">virus</TOKEN>
<TOKEN id="token-91-18" pos="word" morph="none" start_char="9150" end_char="9153">that</TOKEN>
<TOKEN id="token-91-19" pos="word" morph="none" start_char="9155" end_char="9157">had</TOKEN>
<TOKEN id="token-91-20" pos="word" morph="none" start_char="9159" end_char="9162">been</TOKEN>
<TOKEN id="token-91-21" pos="word" morph="none" start_char="9164" end_char="9170">adapted</TOKEN>
<TOKEN id="token-91-22" pos="word" morph="none" start_char="9172" end_char="9173">to</TOKEN>
<TOKEN id="token-91-23" pos="word" morph="none" start_char="9175" end_char="9178">grow</TOKEN>
<TOKEN id="token-91-24" pos="word" morph="none" start_char="9180" end_char="9181">in</TOKEN>
<TOKEN id="token-91-25" pos="word" morph="none" start_char="9183" end_char="9186">mice</TOKEN>
<TOKEN id="token-91-26" pos="word" morph="none" start_char="9188" end_char="9190">and</TOKEN>
<TOKEN id="token-91-27" pos="word" morph="none" start_char="9192" end_char="9196">mimic</TOKEN>
<TOKEN id="token-91-28" pos="word" morph="none" start_char="9198" end_char="9202">human</TOKEN>
<TOKEN id="token-91-29" pos="word" morph="none" start_char="9204" end_char="9210">disease</TOKEN>
<TOKEN id="token-91-30" pos="punct" morph="none" start_char="9211" end_char="9211">.</TOKEN>
</SEG>
<SEG id="segment-92" start_char="9213" end_char="9260">
<ORIGINAL_TEXT>The hybrid virus was able to infect human cells.</ORIGINAL_TEXT>
<TOKEN id="token-92-0" pos="word" morph="none" start_char="9213" end_char="9215">The</TOKEN>
<TOKEN id="token-92-1" pos="word" morph="none" start_char="9217" end_char="9222">hybrid</TOKEN>
<TOKEN id="token-92-2" pos="word" morph="none" start_char="9224" end_char="9228">virus</TOKEN>
<TOKEN id="token-92-3" pos="word" morph="none" start_char="9230" end_char="9232">was</TOKEN>
<TOKEN id="token-92-4" pos="word" morph="none" start_char="9234" end_char="9237">able</TOKEN>
<TOKEN id="token-92-5" pos="word" morph="none" start_char="9239" end_char="9240">to</TOKEN>
<TOKEN id="token-92-6" pos="word" morph="none" start_char="9242" end_char="9247">infect</TOKEN>
<TOKEN id="token-92-7" pos="word" morph="none" start_char="9249" end_char="9253">human</TOKEN>
<TOKEN id="token-92-8" pos="word" morph="none" start_char="9255" end_char="9259">cells</TOKEN>
<TOKEN id="token-92-9" pos="punct" morph="none" start_char="9260" end_char="9260">.</TOKEN>
</SEG>
<SEG id="segment-93" start_char="9262" end_char="9318">
<ORIGINAL_TEXT>[8][9] Wuhan Institute of Virology - Coronavirus research</ORIGINAL_TEXT>
<TOKEN id="token-93-0" pos="punct" morph="none" start_char="9262" end_char="9262">[</TOKEN>
<TOKEN id="token-93-1" pos="unknown" morph="none" start_char="9263" end_char="9266">8][9</TOKEN>
<TOKEN id="token-93-2" pos="punct" morph="none" start_char="9267" end_char="9267">]</TOKEN>
<TOKEN id="token-93-3" pos="word" morph="none" start_char="9269" end_char="9273">Wuhan</TOKEN>
<TOKEN id="token-93-4" pos="word" morph="none" start_char="9275" end_char="9283">Institute</TOKEN>
<TOKEN id="token-93-5" pos="word" morph="none" start_char="9285" end_char="9286">of</TOKEN>
<TOKEN id="token-93-6" pos="word" morph="none" start_char="9288" end_char="9295">Virology</TOKEN>
<TOKEN id="token-93-7" pos="punct" morph="none" start_char="9297" end_char="9297">-</TOKEN>
<TOKEN id="token-93-8" pos="word" morph="none" start_char="9299" end_char="9309">Coronavirus</TOKEN>
<TOKEN id="token-93-9" pos="word" morph="none" start_char="9311" end_char="9318">research</TOKEN>
</SEG>
<SEG id="segment-94" start_char="9321" end_char="9381">
<ORIGINAL_TEXT>edit on 30/12/2020 by ChaoticOrder because: (no reason given)</ORIGINAL_TEXT>
<TOKEN id="token-94-0" pos="word" morph="none" start_char="9321" end_char="9324">edit</TOKEN>
<TOKEN id="token-94-1" pos="word" morph="none" start_char="9326" end_char="9327">on</TOKEN>
<TOKEN id="token-94-2" pos="unknown" morph="none" start_char="9329" end_char="9338">30/12/2020</TOKEN>
<TOKEN id="token-94-3" pos="word" morph="none" start_char="9340" end_char="9341">by</TOKEN>
<TOKEN id="token-94-4" pos="word" morph="none" start_char="9343" end_char="9354">ChaoticOrder</TOKEN>
<TOKEN id="token-94-5" pos="word" morph="none" start_char="9356" end_char="9362">because</TOKEN>
<TOKEN id="token-94-6" pos="punct" morph="none" start_char="9363" end_char="9363">:</TOKEN>
<TOKEN id="token-94-7" pos="punct" morph="none" start_char="9365" end_char="9365">(</TOKEN>
<TOKEN id="token-94-8" pos="word" morph="none" start_char="9366" end_char="9367">no</TOKEN>
<TOKEN id="token-94-9" pos="word" morph="none" start_char="9369" end_char="9374">reason</TOKEN>
<TOKEN id="token-94-10" pos="word" morph="none" start_char="9376" end_char="9380">given</TOKEN>
<TOKEN id="token-94-11" pos="punct" morph="none" start_char="9381" end_char="9381">)</TOKEN>
</SEG>
<SEG id="segment-95" start_char="9386" end_char="9593">
<ORIGINAL_TEXT>pretty sure i remember reading about this before the virus spread outside of china after watching a video about it from a youtuber that used to live in china and still had contacts he talked to about it with.</ORIGINAL_TEXT>
<TOKEN id="token-95-0" pos="word" morph="none" start_char="9386" end_char="9391">pretty</TOKEN>
<TOKEN id="token-95-1" pos="word" morph="none" start_char="9393" end_char="9396">sure</TOKEN>
<TOKEN id="token-95-2" pos="word" morph="none" start_char="9398" end_char="9398">i</TOKEN>
<TOKEN id="token-95-3" pos="word" morph="none" start_char="9400" end_char="9407">remember</TOKEN>
<TOKEN id="token-95-4" pos="word" morph="none" start_char="9409" end_char="9415">reading</TOKEN>
<TOKEN id="token-95-5" pos="word" morph="none" start_char="9417" end_char="9421">about</TOKEN>
<TOKEN id="token-95-6" pos="word" morph="none" start_char="9423" end_char="9426">this</TOKEN>
<TOKEN id="token-95-7" pos="word" morph="none" start_char="9428" end_char="9433">before</TOKEN>
<TOKEN id="token-95-8" pos="word" morph="none" start_char="9435" end_char="9437">the</TOKEN>
<TOKEN id="token-95-9" pos="word" morph="none" start_char="9439" end_char="9443">virus</TOKEN>
<TOKEN id="token-95-10" pos="word" morph="none" start_char="9445" end_char="9450">spread</TOKEN>
<TOKEN id="token-95-11" pos="word" morph="none" start_char="9452" end_char="9458">outside</TOKEN>
<TOKEN id="token-95-12" pos="word" morph="none" start_char="9460" end_char="9461">of</TOKEN>
<TOKEN id="token-95-13" pos="word" morph="none" start_char="9463" end_char="9467">china</TOKEN>
<TOKEN id="token-95-14" pos="word" morph="none" start_char="9469" end_char="9473">after</TOKEN>
<TOKEN id="token-95-15" pos="word" morph="none" start_char="9475" end_char="9482">watching</TOKEN>
<TOKEN id="token-95-16" pos="word" morph="none" start_char="9484" end_char="9484">a</TOKEN>
<TOKEN id="token-95-17" pos="word" morph="none" start_char="9486" end_char="9490">video</TOKEN>
<TOKEN id="token-95-18" pos="word" morph="none" start_char="9492" end_char="9496">about</TOKEN>
<TOKEN id="token-95-19" pos="word" morph="none" start_char="9498" end_char="9499">it</TOKEN>
<TOKEN id="token-95-20" pos="word" morph="none" start_char="9501" end_char="9504">from</TOKEN>
<TOKEN id="token-95-21" pos="word" morph="none" start_char="9506" end_char="9506">a</TOKEN>
<TOKEN id="token-95-22" pos="word" morph="none" start_char="9508" end_char="9515">youtuber</TOKEN>
<TOKEN id="token-95-23" pos="word" morph="none" start_char="9517" end_char="9520">that</TOKEN>
<TOKEN id="token-95-24" pos="word" morph="none" start_char="9522" end_char="9525">used</TOKEN>
<TOKEN id="token-95-25" pos="word" morph="none" start_char="9527" end_char="9528">to</TOKEN>
<TOKEN id="token-95-26" pos="word" morph="none" start_char="9530" end_char="9533">live</TOKEN>
<TOKEN id="token-95-27" pos="word" morph="none" start_char="9535" end_char="9536">in</TOKEN>
<TOKEN id="token-95-28" pos="word" morph="none" start_char="9538" end_char="9542">china</TOKEN>
<TOKEN id="token-95-29" pos="word" morph="none" start_char="9544" end_char="9546">and</TOKEN>
<TOKEN id="token-95-30" pos="word" morph="none" start_char="9548" end_char="9552">still</TOKEN>
<TOKEN id="token-95-31" pos="word" morph="none" start_char="9554" end_char="9556">had</TOKEN>
<TOKEN id="token-95-32" pos="word" morph="none" start_char="9558" end_char="9565">contacts</TOKEN>
<TOKEN id="token-95-33" pos="word" morph="none" start_char="9567" end_char="9568">he</TOKEN>
<TOKEN id="token-95-34" pos="word" morph="none" start_char="9570" end_char="9575">talked</TOKEN>
<TOKEN id="token-95-35" pos="word" morph="none" start_char="9577" end_char="9578">to</TOKEN>
<TOKEN id="token-95-36" pos="word" morph="none" start_char="9580" end_char="9584">about</TOKEN>
<TOKEN id="token-95-37" pos="word" morph="none" start_char="9586" end_char="9587">it</TOKEN>
<TOKEN id="token-95-38" pos="word" morph="none" start_char="9589" end_char="9592">with</TOKEN>
<TOKEN id="token-95-39" pos="punct" morph="none" start_char="9593" end_char="9593">.</TOKEN>
</SEG>
<SEG id="segment-96" start_char="9595" end_char="9760">
<ORIGINAL_TEXT>dont remember the details but it was pretty compelling information indicating a coverup and researchers disappearing or being reassigned before the virus became news.</ORIGINAL_TEXT>
<TOKEN id="token-96-0" pos="word" morph="none" start_char="9595" end_char="9598">dont</TOKEN>
<TOKEN id="token-96-1" pos="word" morph="none" start_char="9600" end_char="9607">remember</TOKEN>
<TOKEN id="token-96-2" pos="word" morph="none" start_char="9609" end_char="9611">the</TOKEN>
<TOKEN id="token-96-3" pos="word" morph="none" start_char="9613" end_char="9619">details</TOKEN>
<TOKEN id="token-96-4" pos="word" morph="none" start_char="9621" end_char="9623">but</TOKEN>
<TOKEN id="token-96-5" pos="word" morph="none" start_char="9625" end_char="9626">it</TOKEN>
<TOKEN id="token-96-6" pos="word" morph="none" start_char="9628" end_char="9630">was</TOKEN>
<TOKEN id="token-96-7" pos="word" morph="none" start_char="9632" end_char="9637">pretty</TOKEN>
<TOKEN id="token-96-8" pos="word" morph="none" start_char="9639" end_char="9648">compelling</TOKEN>
<TOKEN id="token-96-9" pos="word" morph="none" start_char="9650" end_char="9660">information</TOKEN>
<TOKEN id="token-96-10" pos="word" morph="none" start_char="9662" end_char="9671">indicating</TOKEN>
<TOKEN id="token-96-11" pos="word" morph="none" start_char="9673" end_char="9673">a</TOKEN>
<TOKEN id="token-96-12" pos="word" morph="none" start_char="9675" end_char="9681">coverup</TOKEN>
<TOKEN id="token-96-13" pos="word" morph="none" start_char="9683" end_char="9685">and</TOKEN>
<TOKEN id="token-96-14" pos="word" morph="none" start_char="9687" end_char="9697">researchers</TOKEN>
<TOKEN id="token-96-15" pos="word" morph="none" start_char="9699" end_char="9710">disappearing</TOKEN>
<TOKEN id="token-96-16" pos="word" morph="none" start_char="9712" end_char="9713">or</TOKEN>
<TOKEN id="token-96-17" pos="word" morph="none" start_char="9715" end_char="9719">being</TOKEN>
<TOKEN id="token-96-18" pos="word" morph="none" start_char="9721" end_char="9730">reassigned</TOKEN>
<TOKEN id="token-96-19" pos="word" morph="none" start_char="9732" end_char="9737">before</TOKEN>
<TOKEN id="token-96-20" pos="word" morph="none" start_char="9739" end_char="9741">the</TOKEN>
<TOKEN id="token-96-21" pos="word" morph="none" start_char="9743" end_char="9747">virus</TOKEN>
<TOKEN id="token-96-22" pos="word" morph="none" start_char="9749" end_char="9754">became</TOKEN>
<TOKEN id="token-96-23" pos="word" morph="none" start_char="9756" end_char="9759">news</TOKEN>
<TOKEN id="token-96-24" pos="punct" morph="none" start_char="9760" end_char="9760">.</TOKEN>
</SEG>
<SEG id="segment-97" start_char="9764" end_char="9787">
<ORIGINAL_TEXT>a reply to: ChaoticOrder</ORIGINAL_TEXT>
<TOKEN id="token-97-0" pos="word" morph="none" start_char="9764" end_char="9764">a</TOKEN>
<TOKEN id="token-97-1" pos="word" morph="none" start_char="9766" end_char="9770">reply</TOKEN>
<TOKEN id="token-97-2" pos="word" morph="none" start_char="9772" end_char="9773">to</TOKEN>
<TOKEN id="token-97-3" pos="punct" morph="none" start_char="9774" end_char="9774">:</TOKEN>
<TOKEN id="token-97-4" pos="word" morph="none" start_char="9776" end_char="9787">ChaoticOrder</TOKEN>
</SEG>
<SEG id="segment-98" start_char="9790" end_char="9863">
<ORIGINAL_TEXT>Yes : can see those possibilities, with significant levels of probability.</ORIGINAL_TEXT>
<TOKEN id="token-98-0" pos="word" morph="none" start_char="9790" end_char="9792">Yes</TOKEN>
<TOKEN id="token-98-1" pos="punct" morph="none" start_char="9794" end_char="9794">:</TOKEN>
<TOKEN id="token-98-2" pos="word" morph="none" start_char="9796" end_char="9798">can</TOKEN>
<TOKEN id="token-98-3" pos="word" morph="none" start_char="9800" end_char="9802">see</TOKEN>
<TOKEN id="token-98-4" pos="word" morph="none" start_char="9804" end_char="9808">those</TOKEN>
<TOKEN id="token-98-5" pos="word" morph="none" start_char="9810" end_char="9822">possibilities</TOKEN>
<TOKEN id="token-98-6" pos="punct" morph="none" start_char="9823" end_char="9823">,</TOKEN>
<TOKEN id="token-98-7" pos="word" morph="none" start_char="9825" end_char="9828">with</TOKEN>
<TOKEN id="token-98-8" pos="word" morph="none" start_char="9830" end_char="9840">significant</TOKEN>
<TOKEN id="token-98-9" pos="word" morph="none" start_char="9842" end_char="9847">levels</TOKEN>
<TOKEN id="token-98-10" pos="word" morph="none" start_char="9849" end_char="9850">of</TOKEN>
<TOKEN id="token-98-11" pos="word" morph="none" start_char="9852" end_char="9862">probability</TOKEN>
<TOKEN id="token-98-12" pos="punct" morph="none" start_char="9863" end_char="9863">.</TOKEN>
</SEG>
<SEG id="segment-99" start_char="9866" end_char="9898">
<ORIGINAL_TEXT>My problem is in trusting the AI.</ORIGINAL_TEXT>
<TOKEN id="token-99-0" pos="word" morph="none" start_char="9866" end_char="9867">My</TOKEN>
<TOKEN id="token-99-1" pos="word" morph="none" start_char="9869" end_char="9875">problem</TOKEN>
<TOKEN id="token-99-2" pos="word" morph="none" start_char="9877" end_char="9878">is</TOKEN>
<TOKEN id="token-99-3" pos="word" morph="none" start_char="9880" end_char="9881">in</TOKEN>
<TOKEN id="token-99-4" pos="word" morph="none" start_char="9883" end_char="9890">trusting</TOKEN>
<TOKEN id="token-99-5" pos="word" morph="none" start_char="9892" end_char="9894">the</TOKEN>
<TOKEN id="token-99-6" pos="word" morph="none" start_char="9896" end_char="9897">AI</TOKEN>
<TOKEN id="token-99-7" pos="punct" morph="none" start_char="9898" end_char="9898">.</TOKEN>
</SEG>
<SEG id="segment-100" start_char="9900" end_char="9938">
<ORIGINAL_TEXT>Or rather : the AI, that isn't neutral.</ORIGINAL_TEXT>
<TOKEN id="token-100-0" pos="word" morph="none" start_char="9900" end_char="9901">Or</TOKEN>
<TOKEN id="token-100-1" pos="word" morph="none" start_char="9903" end_char="9908">rather</TOKEN>
<TOKEN id="token-100-2" pos="punct" morph="none" start_char="9910" end_char="9910">:</TOKEN>
<TOKEN id="token-100-3" pos="word" morph="none" start_char="9912" end_char="9914">the</TOKEN>
<TOKEN id="token-100-4" pos="word" morph="none" start_char="9916" end_char="9917">AI</TOKEN>
<TOKEN id="token-100-5" pos="punct" morph="none" start_char="9918" end_char="9918">,</TOKEN>
<TOKEN id="token-100-6" pos="word" morph="none" start_char="9920" end_char="9923">that</TOKEN>
<TOKEN id="token-100-7" pos="word" morph="none" start_char="9925" end_char="9929">isn't</TOKEN>
<TOKEN id="token-100-8" pos="word" morph="none" start_char="9931" end_char="9937">neutral</TOKEN>
<TOKEN id="token-100-9" pos="punct" morph="none" start_char="9938" end_char="9938">.</TOKEN>
</SEG>
<SEG id="segment-101" start_char="9940" end_char="10021">
<ORIGINAL_TEXT>The AI : that is owned, financed, and programmed by someone, or some organization.</ORIGINAL_TEXT>
<TOKEN id="token-101-0" pos="word" morph="none" start_char="9940" end_char="9942">The</TOKEN>
<TOKEN id="token-101-1" pos="word" morph="none" start_char="9944" end_char="9945">AI</TOKEN>
<TOKEN id="token-101-2" pos="punct" morph="none" start_char="9947" end_char="9947">:</TOKEN>
<TOKEN id="token-101-3" pos="word" morph="none" start_char="9949" end_char="9952">that</TOKEN>
<TOKEN id="token-101-4" pos="word" morph="none" start_char="9954" end_char="9955">is</TOKEN>
<TOKEN id="token-101-5" pos="word" morph="none" start_char="9957" end_char="9961">owned</TOKEN>
<TOKEN id="token-101-6" pos="punct" morph="none" start_char="9962" end_char="9962">,</TOKEN>
<TOKEN id="token-101-7" pos="word" morph="none" start_char="9964" end_char="9971">financed</TOKEN>
<TOKEN id="token-101-8" pos="punct" morph="none" start_char="9972" end_char="9972">,</TOKEN>
<TOKEN id="token-101-9" pos="word" morph="none" start_char="9974" end_char="9976">and</TOKEN>
<TOKEN id="token-101-10" pos="word" morph="none" start_char="9978" end_char="9987">programmed</TOKEN>
<TOKEN id="token-101-11" pos="word" morph="none" start_char="9989" end_char="9990">by</TOKEN>
<TOKEN id="token-101-12" pos="word" morph="none" start_char="9992" end_char="9998">someone</TOKEN>
<TOKEN id="token-101-13" pos="punct" morph="none" start_char="9999" end_char="9999">,</TOKEN>
<TOKEN id="token-101-14" pos="word" morph="none" start_char="10001" end_char="10002">or</TOKEN>
<TOKEN id="token-101-15" pos="word" morph="none" start_char="10004" end_char="10007">some</TOKEN>
<TOKEN id="token-101-16" pos="word" morph="none" start_char="10009" end_char="10020">organization</TOKEN>
<TOKEN id="token-101-17" pos="punct" morph="none" start_char="10021" end_char="10021">.</TOKEN>
</SEG>
<SEG id="segment-102" start_char="10024" end_char="10051">
<ORIGINAL_TEXT>What might their agenda be ?</ORIGINAL_TEXT>
<TOKEN id="token-102-0" pos="word" morph="none" start_char="10024" end_char="10027">What</TOKEN>
<TOKEN id="token-102-1" pos="word" morph="none" start_char="10029" end_char="10033">might</TOKEN>
<TOKEN id="token-102-2" pos="word" morph="none" start_char="10035" end_char="10039">their</TOKEN>
<TOKEN id="token-102-3" pos="word" morph="none" start_char="10041" end_char="10046">agenda</TOKEN>
<TOKEN id="token-102-4" pos="word" morph="none" start_char="10048" end_char="10049">be</TOKEN>
<TOKEN id="token-102-5" pos="punct" morph="none" start_char="10051" end_char="10051">?</TOKEN>
</SEG>
<SEG id="segment-103" start_char="10054" end_char="10150">
<ORIGINAL_TEXT>Will it become more and more difficult, to spot the narrative, the further that AI tech evolves ?</ORIGINAL_TEXT>
<TOKEN id="token-103-0" pos="word" morph="none" start_char="10054" end_char="10057">Will</TOKEN>
<TOKEN id="token-103-1" pos="word" morph="none" start_char="10059" end_char="10060">it</TOKEN>
<TOKEN id="token-103-2" pos="word" morph="none" start_char="10062" end_char="10067">become</TOKEN>
<TOKEN id="token-103-3" pos="word" morph="none" start_char="10069" end_char="10072">more</TOKEN>
<TOKEN id="token-103-4" pos="word" morph="none" start_char="10074" end_char="10076">and</TOKEN>
<TOKEN id="token-103-5" pos="word" morph="none" start_char="10078" end_char="10081">more</TOKEN>
<TOKEN id="token-103-6" pos="word" morph="none" start_char="10083" end_char="10091">difficult</TOKEN>
<TOKEN id="token-103-7" pos="punct" morph="none" start_char="10092" end_char="10092">,</TOKEN>
<TOKEN id="token-103-8" pos="word" morph="none" start_char="10094" end_char="10095">to</TOKEN>
<TOKEN id="token-103-9" pos="word" morph="none" start_char="10097" end_char="10100">spot</TOKEN>
<TOKEN id="token-103-10" pos="word" morph="none" start_char="10102" end_char="10104">the</TOKEN>
<TOKEN id="token-103-11" pos="word" morph="none" start_char="10106" end_char="10114">narrative</TOKEN>
<TOKEN id="token-103-12" pos="punct" morph="none" start_char="10115" end_char="10115">,</TOKEN>
<TOKEN id="token-103-13" pos="word" morph="none" start_char="10117" end_char="10119">the</TOKEN>
<TOKEN id="token-103-14" pos="word" morph="none" start_char="10121" end_char="10127">further</TOKEN>
<TOKEN id="token-103-15" pos="word" morph="none" start_char="10129" end_char="10132">that</TOKEN>
<TOKEN id="token-103-16" pos="word" morph="none" start_char="10134" end_char="10135">AI</TOKEN>
<TOKEN id="token-103-17" pos="word" morph="none" start_char="10137" end_char="10140">tech</TOKEN>
<TOKEN id="token-103-18" pos="word" morph="none" start_char="10142" end_char="10148">evolves</TOKEN>
<TOKEN id="token-103-19" pos="punct" morph="none" start_char="10150" end_char="10150">?</TOKEN>
</SEG>
<SEG id="segment-104" start_char="10153" end_char="10201">
<ORIGINAL_TEXT>( Ooops : that almost sounds like : AI evolves...</ORIGINAL_TEXT>
<TOKEN id="token-104-0" pos="punct" morph="none" start_char="10153" end_char="10153">(</TOKEN>
<TOKEN id="token-104-1" pos="word" morph="none" start_char="10155" end_char="10159">Ooops</TOKEN>
<TOKEN id="token-104-2" pos="punct" morph="none" start_char="10161" end_char="10161">:</TOKEN>
<TOKEN id="token-104-3" pos="word" morph="none" start_char="10163" end_char="10166">that</TOKEN>
<TOKEN id="token-104-4" pos="word" morph="none" start_char="10168" end_char="10173">almost</TOKEN>
<TOKEN id="token-104-5" pos="word" morph="none" start_char="10175" end_char="10180">sounds</TOKEN>
<TOKEN id="token-104-6" pos="word" morph="none" start_char="10182" end_char="10185">like</TOKEN>
<TOKEN id="token-104-7" pos="punct" morph="none" start_char="10187" end_char="10187">:</TOKEN>
<TOKEN id="token-104-8" pos="word" morph="none" start_char="10189" end_char="10190">AI</TOKEN>
<TOKEN id="token-104-9" pos="word" morph="none" start_char="10192" end_char="10198">evolves</TOKEN>
<TOKEN id="token-104-10" pos="punct" morph="none" start_char="10199" end_char="10201">...</TOKEN>
</SEG>
<SEG id="segment-105" start_char="10203" end_char="10210">
<ORIGINAL_TEXT>LoL !! )</ORIGINAL_TEXT>
<TOKEN id="token-105-0" pos="word" morph="none" start_char="10203" end_char="10205">LoL</TOKEN>
<TOKEN id="token-105-1" pos="punct" morph="none" start_char="10207" end_char="10208">!!</TOKEN>
<TOKEN id="token-105-2" pos="punct" morph="none" start_char="10210" end_char="10210">)</TOKEN>
</SEG>
<SEG id="segment-106" start_char="10214" end_char="10313">
<ORIGINAL_TEXT>One thing I will add is that Fauci used tax payer dollars to pay for that gain of function research.</ORIGINAL_TEXT>
<TOKEN id="token-106-0" pos="word" morph="none" start_char="10214" end_char="10216">One</TOKEN>
<TOKEN id="token-106-1" pos="word" morph="none" start_char="10218" end_char="10222">thing</TOKEN>
<TOKEN id="token-106-2" pos="word" morph="none" start_char="10224" end_char="10224">I</TOKEN>
<TOKEN id="token-106-3" pos="word" morph="none" start_char="10226" end_char="10229">will</TOKEN>
<TOKEN id="token-106-4" pos="word" morph="none" start_char="10231" end_char="10233">add</TOKEN>
<TOKEN id="token-106-5" pos="word" morph="none" start_char="10235" end_char="10236">is</TOKEN>
<TOKEN id="token-106-6" pos="word" morph="none" start_char="10238" end_char="10241">that</TOKEN>
<TOKEN id="token-106-7" pos="word" morph="none" start_char="10243" end_char="10247">Fauci</TOKEN>
<TOKEN id="token-106-8" pos="word" morph="none" start_char="10249" end_char="10252">used</TOKEN>
<TOKEN id="token-106-9" pos="word" morph="none" start_char="10254" end_char="10256">tax</TOKEN>
<TOKEN id="token-106-10" pos="word" morph="none" start_char="10258" end_char="10262">payer</TOKEN>
<TOKEN id="token-106-11" pos="word" morph="none" start_char="10264" end_char="10270">dollars</TOKEN>
<TOKEN id="token-106-12" pos="word" morph="none" start_char="10272" end_char="10273">to</TOKEN>
<TOKEN id="token-106-13" pos="word" morph="none" start_char="10275" end_char="10277">pay</TOKEN>
<TOKEN id="token-106-14" pos="word" morph="none" start_char="10279" end_char="10281">for</TOKEN>
<TOKEN id="token-106-15" pos="word" morph="none" start_char="10283" end_char="10286">that</TOKEN>
<TOKEN id="token-106-16" pos="word" morph="none" start_char="10288" end_char="10291">gain</TOKEN>
<TOKEN id="token-106-17" pos="word" morph="none" start_char="10293" end_char="10294">of</TOKEN>
<TOKEN id="token-106-18" pos="word" morph="none" start_char="10296" end_char="10303">function</TOKEN>
<TOKEN id="token-106-19" pos="word" morph="none" start_char="10305" end_char="10312">research</TOKEN>
<TOKEN id="token-106-20" pos="punct" morph="none" start_char="10313" end_char="10313">.</TOKEN>
</SEG>
<SEG id="segment-107" start_char="10316" end_char="10398">
<ORIGINAL_TEXT>This has all been known by those who followed the bread crumbs since the beginning.</ORIGINAL_TEXT>
<TOKEN id="token-107-0" pos="word" morph="none" start_char="10316" end_char="10319">This</TOKEN>
<TOKEN id="token-107-1" pos="word" morph="none" start_char="10321" end_char="10323">has</TOKEN>
<TOKEN id="token-107-2" pos="word" morph="none" start_char="10325" end_char="10327">all</TOKEN>
<TOKEN id="token-107-3" pos="word" morph="none" start_char="10329" end_char="10332">been</TOKEN>
<TOKEN id="token-107-4" pos="word" morph="none" start_char="10334" end_char="10338">known</TOKEN>
<TOKEN id="token-107-5" pos="word" morph="none" start_char="10340" end_char="10341">by</TOKEN>
<TOKEN id="token-107-6" pos="word" morph="none" start_char="10343" end_char="10347">those</TOKEN>
<TOKEN id="token-107-7" pos="word" morph="none" start_char="10349" end_char="10351">who</TOKEN>
<TOKEN id="token-107-8" pos="word" morph="none" start_char="10353" end_char="10360">followed</TOKEN>
<TOKEN id="token-107-9" pos="word" morph="none" start_char="10362" end_char="10364">the</TOKEN>
<TOKEN id="token-107-10" pos="word" morph="none" start_char="10366" end_char="10370">bread</TOKEN>
<TOKEN id="token-107-11" pos="word" morph="none" start_char="10372" end_char="10377">crumbs</TOKEN>
<TOKEN id="token-107-12" pos="word" morph="none" start_char="10379" end_char="10383">since</TOKEN>
<TOKEN id="token-107-13" pos="word" morph="none" start_char="10385" end_char="10387">the</TOKEN>
<TOKEN id="token-107-14" pos="word" morph="none" start_char="10389" end_char="10397">beginning</TOKEN>
<TOKEN id="token-107-15" pos="punct" morph="none" start_char="10398" end_char="10398">.</TOKEN>
</SEG>
<SEG id="segment-108" start_char="10401" end_char="10531">
<ORIGINAL_TEXT>The release in my opinion was not accidental, but rushed out prematurely when Trump's election forced them to bump up their timing.</ORIGINAL_TEXT>
<TOKEN id="token-108-0" pos="word" morph="none" start_char="10401" end_char="10403">The</TOKEN>
<TOKEN id="token-108-1" pos="word" morph="none" start_char="10405" end_char="10411">release</TOKEN>
<TOKEN id="token-108-2" pos="word" morph="none" start_char="10413" end_char="10414">in</TOKEN>
<TOKEN id="token-108-3" pos="word" morph="none" start_char="10416" end_char="10417">my</TOKEN>
<TOKEN id="token-108-4" pos="word" morph="none" start_char="10419" end_char="10425">opinion</TOKEN>
<TOKEN id="token-108-5" pos="word" morph="none" start_char="10427" end_char="10429">was</TOKEN>
<TOKEN id="token-108-6" pos="word" morph="none" start_char="10431" end_char="10433">not</TOKEN>
<TOKEN id="token-108-7" pos="word" morph="none" start_char="10435" end_char="10444">accidental</TOKEN>
<TOKEN id="token-108-8" pos="punct" morph="none" start_char="10445" end_char="10445">,</TOKEN>
<TOKEN id="token-108-9" pos="word" morph="none" start_char="10447" end_char="10449">but</TOKEN>
<TOKEN id="token-108-10" pos="word" morph="none" start_char="10451" end_char="10456">rushed</TOKEN>
<TOKEN id="token-108-11" pos="word" morph="none" start_char="10458" end_char="10460">out</TOKEN>
<TOKEN id="token-108-12" pos="word" morph="none" start_char="10462" end_char="10472">prematurely</TOKEN>
<TOKEN id="token-108-13" pos="word" morph="none" start_char="10474" end_char="10477">when</TOKEN>
<TOKEN id="token-108-14" pos="word" morph="none" start_char="10479" end_char="10485">Trump's</TOKEN>
<TOKEN id="token-108-15" pos="word" morph="none" start_char="10487" end_char="10494">election</TOKEN>
<TOKEN id="token-108-16" pos="word" morph="none" start_char="10496" end_char="10501">forced</TOKEN>
<TOKEN id="token-108-17" pos="word" morph="none" start_char="10503" end_char="10506">them</TOKEN>
<TOKEN id="token-108-18" pos="word" morph="none" start_char="10508" end_char="10509">to</TOKEN>
<TOKEN id="token-108-19" pos="word" morph="none" start_char="10511" end_char="10514">bump</TOKEN>
<TOKEN id="token-108-20" pos="word" morph="none" start_char="10516" end_char="10517">up</TOKEN>
<TOKEN id="token-108-21" pos="word" morph="none" start_char="10519" end_char="10523">their</TOKEN>
<TOKEN id="token-108-22" pos="word" morph="none" start_char="10525" end_char="10530">timing</TOKEN>
<TOKEN id="token-108-23" pos="punct" morph="none" start_char="10531" end_char="10531">.</TOKEN>
</SEG>
<SEG id="segment-109" start_char="10534" end_char="10632">
<ORIGINAL_TEXT>My best bet is the plan is now to continue the hype while continuing work on the real deal eos bug.</ORIGINAL_TEXT>
<TOKEN id="token-109-0" pos="word" morph="none" start_char="10534" end_char="10535">My</TOKEN>
<TOKEN id="token-109-1" pos="word" morph="none" start_char="10537" end_char="10540">best</TOKEN>
<TOKEN id="token-109-2" pos="word" morph="none" start_char="10542" end_char="10544">bet</TOKEN>
<TOKEN id="token-109-3" pos="word" morph="none" start_char="10546" end_char="10547">is</TOKEN>
<TOKEN id="token-109-4" pos="word" morph="none" start_char="10549" end_char="10551">the</TOKEN>
<TOKEN id="token-109-5" pos="word" morph="none" start_char="10553" end_char="10556">plan</TOKEN>
<TOKEN id="token-109-6" pos="word" morph="none" start_char="10558" end_char="10559">is</TOKEN>
<TOKEN id="token-109-7" pos="word" morph="none" start_char="10561" end_char="10563">now</TOKEN>
<TOKEN id="token-109-8" pos="word" morph="none" start_char="10565" end_char="10566">to</TOKEN>
<TOKEN id="token-109-9" pos="word" morph="none" start_char="10568" end_char="10575">continue</TOKEN>
<TOKEN id="token-109-10" pos="word" morph="none" start_char="10577" end_char="10579">the</TOKEN>
<TOKEN id="token-109-11" pos="word" morph="none" start_char="10581" end_char="10584">hype</TOKEN>
<TOKEN id="token-109-12" pos="word" morph="none" start_char="10586" end_char="10590">while</TOKEN>
<TOKEN id="token-109-13" pos="word" morph="none" start_char="10592" end_char="10601">continuing</TOKEN>
<TOKEN id="token-109-14" pos="word" morph="none" start_char="10603" end_char="10606">work</TOKEN>
<TOKEN id="token-109-15" pos="word" morph="none" start_char="10608" end_char="10609">on</TOKEN>
<TOKEN id="token-109-16" pos="word" morph="none" start_char="10611" end_char="10613">the</TOKEN>
<TOKEN id="token-109-17" pos="word" morph="none" start_char="10615" end_char="10618">real</TOKEN>
<TOKEN id="token-109-18" pos="word" morph="none" start_char="10620" end_char="10623">deal</TOKEN>
<TOKEN id="token-109-19" pos="word" morph="none" start_char="10625" end_char="10627">eos</TOKEN>
<TOKEN id="token-109-20" pos="word" morph="none" start_char="10629" end_char="10631">bug</TOKEN>
<TOKEN id="token-109-21" pos="punct" morph="none" start_char="10632" end_char="10632">.</TOKEN>
</SEG>
<SEG id="segment-110" start_char="10634" end_char="10928">
<ORIGINAL_TEXT>Once that one gets unleashed people will be begging to be chipped and let into sterile FEMA work camps, and those who can't will still beg to have their children taken from them and put in one, so their children can at least have a chance at life even if it is in a FEMA slave camp or work farm.</ORIGINAL_TEXT>
<TOKEN id="token-110-0" pos="word" morph="none" start_char="10634" end_char="10637">Once</TOKEN>
<TOKEN id="token-110-1" pos="word" morph="none" start_char="10639" end_char="10642">that</TOKEN>
<TOKEN id="token-110-2" pos="word" morph="none" start_char="10644" end_char="10646">one</TOKEN>
<TOKEN id="token-110-3" pos="word" morph="none" start_char="10648" end_char="10651">gets</TOKEN>
<TOKEN id="token-110-4" pos="word" morph="none" start_char="10653" end_char="10661">unleashed</TOKEN>
<TOKEN id="token-110-5" pos="word" morph="none" start_char="10663" end_char="10668">people</TOKEN>
<TOKEN id="token-110-6" pos="word" morph="none" start_char="10670" end_char="10673">will</TOKEN>
<TOKEN id="token-110-7" pos="word" morph="none" start_char="10675" end_char="10676">be</TOKEN>
<TOKEN id="token-110-8" pos="word" morph="none" start_char="10678" end_char="10684">begging</TOKEN>
<TOKEN id="token-110-9" pos="word" morph="none" start_char="10686" end_char="10687">to</TOKEN>
<TOKEN id="token-110-10" pos="word" morph="none" start_char="10689" end_char="10690">be</TOKEN>
<TOKEN id="token-110-11" pos="word" morph="none" start_char="10692" end_char="10698">chipped</TOKEN>
<TOKEN id="token-110-12" pos="word" morph="none" start_char="10700" end_char="10702">and</TOKEN>
<TOKEN id="token-110-13" pos="word" morph="none" start_char="10704" end_char="10706">let</TOKEN>
<TOKEN id="token-110-14" pos="word" morph="none" start_char="10708" end_char="10711">into</TOKEN>
<TOKEN id="token-110-15" pos="word" morph="none" start_char="10713" end_char="10719">sterile</TOKEN>
<TOKEN id="token-110-16" pos="word" morph="none" start_char="10721" end_char="10724">FEMA</TOKEN>
<TOKEN id="token-110-17" pos="word" morph="none" start_char="10726" end_char="10729">work</TOKEN>
<TOKEN id="token-110-18" pos="word" morph="none" start_char="10731" end_char="10735">camps</TOKEN>
<TOKEN id="token-110-19" pos="punct" morph="none" start_char="10736" end_char="10736">,</TOKEN>
<TOKEN id="token-110-20" pos="word" morph="none" start_char="10738" end_char="10740">and</TOKEN>
<TOKEN id="token-110-21" pos="word" morph="none" start_char="10742" end_char="10746">those</TOKEN>
<TOKEN id="token-110-22" pos="word" morph="none" start_char="10748" end_char="10750">who</TOKEN>
<TOKEN id="token-110-23" pos="word" morph="none" start_char="10752" end_char="10756">can't</TOKEN>
<TOKEN id="token-110-24" pos="word" morph="none" start_char="10758" end_char="10761">will</TOKEN>
<TOKEN id="token-110-25" pos="word" morph="none" start_char="10763" end_char="10767">still</TOKEN>
<TOKEN id="token-110-26" pos="word" morph="none" start_char="10769" end_char="10771">beg</TOKEN>
<TOKEN id="token-110-27" pos="word" morph="none" start_char="10773" end_char="10774">to</TOKEN>
<TOKEN id="token-110-28" pos="word" morph="none" start_char="10776" end_char="10779">have</TOKEN>
<TOKEN id="token-110-29" pos="word" morph="none" start_char="10781" end_char="10785">their</TOKEN>
<TOKEN id="token-110-30" pos="word" morph="none" start_char="10787" end_char="10794">children</TOKEN>
<TOKEN id="token-110-31" pos="word" morph="none" start_char="10796" end_char="10800">taken</TOKEN>
<TOKEN id="token-110-32" pos="word" morph="none" start_char="10802" end_char="10805">from</TOKEN>
<TOKEN id="token-110-33" pos="word" morph="none" start_char="10807" end_char="10810">them</TOKEN>
<TOKEN id="token-110-34" pos="word" morph="none" start_char="10812" end_char="10814">and</TOKEN>
<TOKEN id="token-110-35" pos="word" morph="none" start_char="10816" end_char="10818">put</TOKEN>
<TOKEN id="token-110-36" pos="word" morph="none" start_char="10820" end_char="10821">in</TOKEN>
<TOKEN id="token-110-37" pos="word" morph="none" start_char="10823" end_char="10825">one</TOKEN>
<TOKEN id="token-110-38" pos="punct" morph="none" start_char="10826" end_char="10826">,</TOKEN>
<TOKEN id="token-110-39" pos="word" morph="none" start_char="10828" end_char="10829">so</TOKEN>
<TOKEN id="token-110-40" pos="word" morph="none" start_char="10831" end_char="10835">their</TOKEN>
<TOKEN id="token-110-41" pos="word" morph="none" start_char="10837" end_char="10844">children</TOKEN>
<TOKEN id="token-110-42" pos="word" morph="none" start_char="10846" end_char="10848">can</TOKEN>
<TOKEN id="token-110-43" pos="word" morph="none" start_char="10850" end_char="10851">at</TOKEN>
<TOKEN id="token-110-44" pos="word" morph="none" start_char="10853" end_char="10857">least</TOKEN>
<TOKEN id="token-110-45" pos="word" morph="none" start_char="10859" end_char="10862">have</TOKEN>
<TOKEN id="token-110-46" pos="word" morph="none" start_char="10864" end_char="10864">a</TOKEN>
<TOKEN id="token-110-47" pos="word" morph="none" start_char="10866" end_char="10871">chance</TOKEN>
<TOKEN id="token-110-48" pos="word" morph="none" start_char="10873" end_char="10874">at</TOKEN>
<TOKEN id="token-110-49" pos="word" morph="none" start_char="10876" end_char="10879">life</TOKEN>
<TOKEN id="token-110-50" pos="word" morph="none" start_char="10881" end_char="10884">even</TOKEN>
<TOKEN id="token-110-51" pos="word" morph="none" start_char="10886" end_char="10887">if</TOKEN>
<TOKEN id="token-110-52" pos="word" morph="none" start_char="10889" end_char="10890">it</TOKEN>
<TOKEN id="token-110-53" pos="word" morph="none" start_char="10892" end_char="10893">is</TOKEN>
<TOKEN id="token-110-54" pos="word" morph="none" start_char="10895" end_char="10896">in</TOKEN>
<TOKEN id="token-110-55" pos="word" morph="none" start_char="10898" end_char="10898">a</TOKEN>
<TOKEN id="token-110-56" pos="word" morph="none" start_char="10900" end_char="10903">FEMA</TOKEN>
<TOKEN id="token-110-57" pos="word" morph="none" start_char="10905" end_char="10909">slave</TOKEN>
<TOKEN id="token-110-58" pos="word" morph="none" start_char="10911" end_char="10914">camp</TOKEN>
<TOKEN id="token-110-59" pos="word" morph="none" start_char="10916" end_char="10917">or</TOKEN>
<TOKEN id="token-110-60" pos="word" morph="none" start_char="10919" end_char="10922">work</TOKEN>
<TOKEN id="token-110-61" pos="word" morph="none" start_char="10924" end_char="10927">farm</TOKEN>
<TOKEN id="token-110-62" pos="punct" morph="none" start_char="10928" end_char="10928">.</TOKEN>
</SEG>
<SEG id="segment-111" start_char="10931" end_char="11009">
<ORIGINAL_TEXT>Think of covid as the foreplay that will tell them how far they can go with us.</ORIGINAL_TEXT>
<TOKEN id="token-111-0" pos="word" morph="none" start_char="10931" end_char="10935">Think</TOKEN>
<TOKEN id="token-111-1" pos="word" morph="none" start_char="10937" end_char="10938">of</TOKEN>
<TOKEN id="token-111-2" pos="word" morph="none" start_char="10940" end_char="10944">covid</TOKEN>
<TOKEN id="token-111-3" pos="word" morph="none" start_char="10946" end_char="10947">as</TOKEN>
<TOKEN id="token-111-4" pos="word" morph="none" start_char="10949" end_char="10951">the</TOKEN>
<TOKEN id="token-111-5" pos="word" morph="none" start_char="10953" end_char="10960">foreplay</TOKEN>
<TOKEN id="token-111-6" pos="word" morph="none" start_char="10962" end_char="10965">that</TOKEN>
<TOKEN id="token-111-7" pos="word" morph="none" start_char="10967" end_char="10970">will</TOKEN>
<TOKEN id="token-111-8" pos="word" morph="none" start_char="10972" end_char="10975">tell</TOKEN>
<TOKEN id="token-111-9" pos="word" morph="none" start_char="10977" end_char="10980">them</TOKEN>
<TOKEN id="token-111-10" pos="word" morph="none" start_char="10982" end_char="10984">how</TOKEN>
<TOKEN id="token-111-11" pos="word" morph="none" start_char="10986" end_char="10988">far</TOKEN>
<TOKEN id="token-111-12" pos="word" morph="none" start_char="10990" end_char="10993">they</TOKEN>
<TOKEN id="token-111-13" pos="word" morph="none" start_char="10995" end_char="10997">can</TOKEN>
<TOKEN id="token-111-14" pos="word" morph="none" start_char="10999" end_char="11000">go</TOKEN>
<TOKEN id="token-111-15" pos="word" morph="none" start_char="11002" end_char="11005">with</TOKEN>
<TOKEN id="token-111-16" pos="word" morph="none" start_char="11007" end_char="11008">us</TOKEN>
<TOKEN id="token-111-17" pos="punct" morph="none" start_char="11009" end_char="11009">.</TOKEN>
</SEG>
<SEG id="segment-112" start_char="11011" end_char="11188">
<ORIGINAL_TEXT>Once mandated vaccines, lock downs, and mask mandates, and ubi have become fully normalized for the seasonal flue come covid, the other shoe should be just getting ready to drop.</ORIGINAL_TEXT>
<TOKEN id="token-112-0" pos="word" morph="none" start_char="11011" end_char="11014">Once</TOKEN>
<TOKEN id="token-112-1" pos="word" morph="none" start_char="11016" end_char="11023">mandated</TOKEN>
<TOKEN id="token-112-2" pos="word" morph="none" start_char="11025" end_char="11032">vaccines</TOKEN>
<TOKEN id="token-112-3" pos="punct" morph="none" start_char="11033" end_char="11033">,</TOKEN>
<TOKEN id="token-112-4" pos="word" morph="none" start_char="11035" end_char="11038">lock</TOKEN>
<TOKEN id="token-112-5" pos="word" morph="none" start_char="11040" end_char="11044">downs</TOKEN>
<TOKEN id="token-112-6" pos="punct" morph="none" start_char="11045" end_char="11045">,</TOKEN>
<TOKEN id="token-112-7" pos="word" morph="none" start_char="11047" end_char="11049">and</TOKEN>
<TOKEN id="token-112-8" pos="word" morph="none" start_char="11051" end_char="11054">mask</TOKEN>
<TOKEN id="token-112-9" pos="word" morph="none" start_char="11056" end_char="11063">mandates</TOKEN>
<TOKEN id="token-112-10" pos="punct" morph="none" start_char="11064" end_char="11064">,</TOKEN>
<TOKEN id="token-112-11" pos="word" morph="none" start_char="11066" end_char="11068">and</TOKEN>
<TOKEN id="token-112-12" pos="word" morph="none" start_char="11070" end_char="11072">ubi</TOKEN>
<TOKEN id="token-112-13" pos="word" morph="none" start_char="11074" end_char="11077">have</TOKEN>
<TOKEN id="token-112-14" pos="word" morph="none" start_char="11079" end_char="11084">become</TOKEN>
<TOKEN id="token-112-15" pos="word" morph="none" start_char="11086" end_char="11090">fully</TOKEN>
<TOKEN id="token-112-16" pos="word" morph="none" start_char="11092" end_char="11101">normalized</TOKEN>
<TOKEN id="token-112-17" pos="word" morph="none" start_char="11103" end_char="11105">for</TOKEN>
<TOKEN id="token-112-18" pos="word" morph="none" start_char="11107" end_char="11109">the</TOKEN>
<TOKEN id="token-112-19" pos="word" morph="none" start_char="11111" end_char="11118">seasonal</TOKEN>
<TOKEN id="token-112-20" pos="word" morph="none" start_char="11120" end_char="11123">flue</TOKEN>
<TOKEN id="token-112-21" pos="word" morph="none" start_char="11125" end_char="11128">come</TOKEN>
<TOKEN id="token-112-22" pos="word" morph="none" start_char="11130" end_char="11134">covid</TOKEN>
<TOKEN id="token-112-23" pos="punct" morph="none" start_char="11135" end_char="11135">,</TOKEN>
<TOKEN id="token-112-24" pos="word" morph="none" start_char="11137" end_char="11139">the</TOKEN>
<TOKEN id="token-112-25" pos="word" morph="none" start_char="11141" end_char="11145">other</TOKEN>
<TOKEN id="token-112-26" pos="word" morph="none" start_char="11147" end_char="11150">shoe</TOKEN>
<TOKEN id="token-112-27" pos="word" morph="none" start_char="11152" end_char="11157">should</TOKEN>
<TOKEN id="token-112-28" pos="word" morph="none" start_char="11159" end_char="11160">be</TOKEN>
<TOKEN id="token-112-29" pos="word" morph="none" start_char="11162" end_char="11165">just</TOKEN>
<TOKEN id="token-112-30" pos="word" morph="none" start_char="11167" end_char="11173">getting</TOKEN>
<TOKEN id="token-112-31" pos="word" morph="none" start_char="11175" end_char="11179">ready</TOKEN>
<TOKEN id="token-112-32" pos="word" morph="none" start_char="11181" end_char="11182">to</TOKEN>
<TOKEN id="token-112-33" pos="word" morph="none" start_char="11184" end_char="11187">drop</TOKEN>
<TOKEN id="token-112-34" pos="punct" morph="none" start_char="11188" end_char="11188">.</TOKEN>
</SEG>
<SEG id="segment-113" start_char="11193" end_char="11286">
<ORIGINAL_TEXT>originally posted by: CriticalStinker a reply to: slatesteam Your point is purely speculative.</ORIGINAL_TEXT>
<TOKEN id="token-113-0" pos="word" morph="none" start_char="11193" end_char="11202">originally</TOKEN>
<TOKEN id="token-113-1" pos="word" morph="none" start_char="11204" end_char="11209">posted</TOKEN>
<TOKEN id="token-113-2" pos="word" morph="none" start_char="11211" end_char="11212">by</TOKEN>
<TOKEN id="token-113-3" pos="punct" morph="none" start_char="11213" end_char="11213">:</TOKEN>
<TOKEN id="token-113-4" pos="word" morph="none" start_char="11215" end_char="11229">CriticalStinker</TOKEN>
<TOKEN id="token-113-5" pos="word" morph="none" start_char="11231" end_char="11231">a</TOKEN>
<TOKEN id="token-113-6" pos="word" morph="none" start_char="11233" end_char="11237">reply</TOKEN>
<TOKEN id="token-113-7" pos="word" morph="none" start_char="11239" end_char="11240">to</TOKEN>
<TOKEN id="token-113-8" pos="punct" morph="none" start_char="11241" end_char="11241">:</TOKEN>
<TOKEN id="token-113-9" pos="word" morph="none" start_char="11243" end_char="11252">slatesteam</TOKEN>
<TOKEN id="token-113-10" pos="word" morph="none" start_char="11254" end_char="11257">Your</TOKEN>
<TOKEN id="token-113-11" pos="word" morph="none" start_char="11259" end_char="11263">point</TOKEN>
<TOKEN id="token-113-12" pos="word" morph="none" start_char="11265" end_char="11266">is</TOKEN>
<TOKEN id="token-113-13" pos="word" morph="none" start_char="11268" end_char="11273">purely</TOKEN>
<TOKEN id="token-113-14" pos="word" morph="none" start_char="11275" end_char="11285">speculative</TOKEN>
<TOKEN id="token-113-15" pos="punct" morph="none" start_char="11286" end_char="11286">.</TOKEN>
</SEG>
<SEG id="segment-114" start_char="11288" end_char="11345">
<ORIGINAL_TEXT>If that was the case, I'd expect more rhetoric from China.</ORIGINAL_TEXT>
<TOKEN id="token-114-0" pos="word" morph="none" start_char="11288" end_char="11289">If</TOKEN>
<TOKEN id="token-114-1" pos="word" morph="none" start_char="11291" end_char="11294">that</TOKEN>
<TOKEN id="token-114-2" pos="word" morph="none" start_char="11296" end_char="11298">was</TOKEN>
<TOKEN id="token-114-3" pos="word" morph="none" start_char="11300" end_char="11302">the</TOKEN>
<TOKEN id="token-114-4" pos="word" morph="none" start_char="11304" end_char="11307">case</TOKEN>
<TOKEN id="token-114-5" pos="punct" morph="none" start_char="11308" end_char="11308">,</TOKEN>
<TOKEN id="token-114-6" pos="word" morph="none" start_char="11310" end_char="11312">I'd</TOKEN>
<TOKEN id="token-114-7" pos="word" morph="none" start_char="11314" end_char="11319">expect</TOKEN>
<TOKEN id="token-114-8" pos="word" morph="none" start_char="11321" end_char="11324">more</TOKEN>
<TOKEN id="token-114-9" pos="word" morph="none" start_char="11326" end_char="11333">rhetoric</TOKEN>
<TOKEN id="token-114-10" pos="word" morph="none" start_char="11335" end_char="11338">from</TOKEN>
<TOKEN id="token-114-11" pos="word" morph="none" start_char="11340" end_char="11344">China</TOKEN>
<TOKEN id="token-114-12" pos="punct" morph="none" start_char="11345" end_char="11345">.</TOKEN>
</SEG>
<SEG id="segment-115" start_char="11348" end_char="11393">
<ORIGINAL_TEXT>Define more.... thats all the CCP is good for</ORIGINAL_TEXT>
<TOKEN id="token-115-0" pos="word" morph="none" start_char="11348" end_char="11353">Define</TOKEN>
<TOKEN id="token-115-1" pos="word" morph="none" start_char="11355" end_char="11358">more</TOKEN>
<TOKEN id="token-115-2" pos="punct" morph="none" start_char="11359" end_char="11362">....</TOKEN>
<TOKEN id="token-115-3" pos="word" morph="none" start_char="11364" end_char="11369">thats</TOKEN>
<TOKEN id="token-115-4" pos="word" morph="none" start_char="11371" end_char="11373">all</TOKEN>
<TOKEN id="token-115-5" pos="word" morph="none" start_char="11375" end_char="11377">the</TOKEN>
<TOKEN id="token-115-6" pos="word" morph="none" start_char="11379" end_char="11381">CCP</TOKEN>
<TOKEN id="token-115-7" pos="word" morph="none" start_char="11383" end_char="11384">is</TOKEN>
<TOKEN id="token-115-8" pos="word" morph="none" start_char="11386" end_char="11389">good</TOKEN>
<TOKEN id="token-115-9" pos="word" morph="none" start_char="11391" end_char="11393">for</TOKEN>
</SEG>
<SEG id="segment-116" start_char="11396" end_char="11407">
<ORIGINAL_TEXT>And so what.</ORIGINAL_TEXT>
<TOKEN id="token-116-0" pos="word" morph="none" start_char="11396" end_char="11398">And</TOKEN>
<TOKEN id="token-116-1" pos="word" morph="none" start_char="11400" end_char="11401">so</TOKEN>
<TOKEN id="token-116-2" pos="word" morph="none" start_char="11403" end_char="11406">what</TOKEN>
<TOKEN id="token-116-3" pos="punct" morph="none" start_char="11407" end_char="11407">.</TOKEN>
</SEG>
<SEG id="segment-117" start_char="11409" end_char="11540">
<ORIGINAL_TEXT>It cant be refuted and its just food for thought.... all this is confirmable btw, with the exception this is EXACTLY what happened</ORIGINAL_TEXT>
<TOKEN id="token-117-0" pos="word" morph="none" start_char="11409" end_char="11410">It</TOKEN>
<TOKEN id="token-117-1" pos="word" morph="none" start_char="11412" end_char="11416">cant</TOKEN>
<TOKEN id="token-117-2" pos="word" morph="none" start_char="11418" end_char="11419">be</TOKEN>
<TOKEN id="token-117-3" pos="word" morph="none" start_char="11421" end_char="11427">refuted</TOKEN>
<TOKEN id="token-117-4" pos="word" morph="none" start_char="11429" end_char="11431">and</TOKEN>
<TOKEN id="token-117-5" pos="word" morph="none" start_char="11433" end_char="11436">its</TOKEN>
<TOKEN id="token-117-6" pos="word" morph="none" start_char="11438" end_char="11441">just</TOKEN>
<TOKEN id="token-117-7" pos="word" morph="none" start_char="11443" end_char="11446">food</TOKEN>
<TOKEN id="token-117-8" pos="word" morph="none" start_char="11448" end_char="11450">for</TOKEN>
<TOKEN id="token-117-9" pos="word" morph="none" start_char="11452" end_char="11458">thought</TOKEN>
<TOKEN id="token-117-10" pos="punct" morph="none" start_char="11459" end_char="11462">....</TOKEN>
<TOKEN id="token-117-11" pos="word" morph="none" start_char="11464" end_char="11466">all</TOKEN>
<TOKEN id="token-117-12" pos="word" morph="none" start_char="11468" end_char="11471">this</TOKEN>
<TOKEN id="token-117-13" pos="word" morph="none" start_char="11473" end_char="11474">is</TOKEN>
<TOKEN id="token-117-14" pos="word" morph="none" start_char="11476" end_char="11486">confirmable</TOKEN>
<TOKEN id="token-117-15" pos="word" morph="none" start_char="11488" end_char="11490">btw</TOKEN>
<TOKEN id="token-117-16" pos="punct" morph="none" start_char="11491" end_char="11491">,</TOKEN>
<TOKEN id="token-117-17" pos="word" morph="none" start_char="11493" end_char="11496">with</TOKEN>
<TOKEN id="token-117-18" pos="word" morph="none" start_char="11498" end_char="11500">the</TOKEN>
<TOKEN id="token-117-19" pos="word" morph="none" start_char="11502" end_char="11510">exception</TOKEN>
<TOKEN id="token-117-20" pos="word" morph="none" start_char="11512" end_char="11515">this</TOKEN>
<TOKEN id="token-117-21" pos="word" morph="none" start_char="11517" end_char="11518">is</TOKEN>
<TOKEN id="token-117-22" pos="word" morph="none" start_char="11520" end_char="11526">EXACTLY</TOKEN>
<TOKEN id="token-117-23" pos="word" morph="none" start_char="11528" end_char="11531">what</TOKEN>
<TOKEN id="token-117-24" pos="word" morph="none" start_char="11533" end_char="11540">happened</TOKEN>
</SEG>
<SEG id="segment-118" start_char="11545" end_char="11561">
<ORIGINAL_TEXT>a reply to: dug88</ORIGINAL_TEXT>
<TOKEN id="token-118-0" pos="word" morph="none" start_char="11545" end_char="11545">a</TOKEN>
<TOKEN id="token-118-1" pos="word" morph="none" start_char="11547" end_char="11551">reply</TOKEN>
<TOKEN id="token-118-2" pos="word" morph="none" start_char="11553" end_char="11554">to</TOKEN>
<TOKEN id="token-118-3" pos="punct" morph="none" start_char="11555" end_char="11555">:</TOKEN>
<TOKEN id="token-118-4" pos="word" morph="none" start_char="11557" end_char="11561">dug88</TOKEN>
</SEG>
<SEG id="segment-119" start_char="11564" end_char="11693">
<ORIGINAL_TEXT>The problem with systems like this is that if you put bias data into them you merely get out a conclusion that supports this bias.</ORIGINAL_TEXT>
<TOKEN id="token-119-0" pos="word" morph="none" start_char="11564" end_char="11566">The</TOKEN>
<TOKEN id="token-119-1" pos="word" morph="none" start_char="11568" end_char="11574">problem</TOKEN>
<TOKEN id="token-119-2" pos="word" morph="none" start_char="11576" end_char="11579">with</TOKEN>
<TOKEN id="token-119-3" pos="word" morph="none" start_char="11581" end_char="11587">systems</TOKEN>
<TOKEN id="token-119-4" pos="word" morph="none" start_char="11589" end_char="11592">like</TOKEN>
<TOKEN id="token-119-5" pos="word" morph="none" start_char="11594" end_char="11597">this</TOKEN>
<TOKEN id="token-119-6" pos="word" morph="none" start_char="11599" end_char="11600">is</TOKEN>
<TOKEN id="token-119-7" pos="word" morph="none" start_char="11602" end_char="11605">that</TOKEN>
<TOKEN id="token-119-8" pos="word" morph="none" start_char="11607" end_char="11608">if</TOKEN>
<TOKEN id="token-119-9" pos="word" morph="none" start_char="11610" end_char="11612">you</TOKEN>
<TOKEN id="token-119-10" pos="word" morph="none" start_char="11614" end_char="11616">put</TOKEN>
<TOKEN id="token-119-11" pos="word" morph="none" start_char="11618" end_char="11621">bias</TOKEN>
<TOKEN id="token-119-12" pos="word" morph="none" start_char="11623" end_char="11626">data</TOKEN>
<TOKEN id="token-119-13" pos="word" morph="none" start_char="11628" end_char="11631">into</TOKEN>
<TOKEN id="token-119-14" pos="word" morph="none" start_char="11633" end_char="11636">them</TOKEN>
<TOKEN id="token-119-15" pos="word" morph="none" start_char="11638" end_char="11640">you</TOKEN>
<TOKEN id="token-119-16" pos="word" morph="none" start_char="11642" end_char="11647">merely</TOKEN>
<TOKEN id="token-119-17" pos="word" morph="none" start_char="11649" end_char="11651">get</TOKEN>
<TOKEN id="token-119-18" pos="word" morph="none" start_char="11653" end_char="11655">out</TOKEN>
<TOKEN id="token-119-19" pos="word" morph="none" start_char="11657" end_char="11657">a</TOKEN>
<TOKEN id="token-119-20" pos="word" morph="none" start_char="11659" end_char="11668">conclusion</TOKEN>
<TOKEN id="token-119-21" pos="word" morph="none" start_char="11670" end_char="11673">that</TOKEN>
<TOKEN id="token-119-22" pos="word" morph="none" start_char="11675" end_char="11682">supports</TOKEN>
<TOKEN id="token-119-23" pos="word" morph="none" start_char="11684" end_char="11687">this</TOKEN>
<TOKEN id="token-119-24" pos="word" morph="none" start_char="11689" end_char="11692">bias</TOKEN>
<TOKEN id="token-119-25" pos="punct" morph="none" start_char="11693" end_char="11693">.</TOKEN>
</SEG>
<SEG id="segment-120" start_char="11696" end_char="11848">
<ORIGINAL_TEXT>Like those sentencing algorithms that jailed young black men for longer than normal based on court documents where human judges did the exact same thing.</ORIGINAL_TEXT>
<TOKEN id="token-120-0" pos="word" morph="none" start_char="11696" end_char="11699">Like</TOKEN>
<TOKEN id="token-120-1" pos="word" morph="none" start_char="11701" end_char="11705">those</TOKEN>
<TOKEN id="token-120-2" pos="word" morph="none" start_char="11707" end_char="11716">sentencing</TOKEN>
<TOKEN id="token-120-3" pos="word" morph="none" start_char="11718" end_char="11727">algorithms</TOKEN>
<TOKEN id="token-120-4" pos="word" morph="none" start_char="11729" end_char="11732">that</TOKEN>
<TOKEN id="token-120-5" pos="word" morph="none" start_char="11734" end_char="11739">jailed</TOKEN>
<TOKEN id="token-120-6" pos="word" morph="none" start_char="11741" end_char="11745">young</TOKEN>
<TOKEN id="token-120-7" pos="word" morph="none" start_char="11747" end_char="11751">black</TOKEN>
<TOKEN id="token-120-8" pos="word" morph="none" start_char="11753" end_char="11755">men</TOKEN>
<TOKEN id="token-120-9" pos="word" morph="none" start_char="11757" end_char="11759">for</TOKEN>
<TOKEN id="token-120-10" pos="word" morph="none" start_char="11761" end_char="11766">longer</TOKEN>
<TOKEN id="token-120-11" pos="word" morph="none" start_char="11768" end_char="11771">than</TOKEN>
<TOKEN id="token-120-12" pos="word" morph="none" start_char="11773" end_char="11778">normal</TOKEN>
<TOKEN id="token-120-13" pos="word" morph="none" start_char="11780" end_char="11784">based</TOKEN>
<TOKEN id="token-120-14" pos="word" morph="none" start_char="11786" end_char="11787">on</TOKEN>
<TOKEN id="token-120-15" pos="word" morph="none" start_char="11789" end_char="11793">court</TOKEN>
<TOKEN id="token-120-16" pos="word" morph="none" start_char="11795" end_char="11803">documents</TOKEN>
<TOKEN id="token-120-17" pos="word" morph="none" start_char="11805" end_char="11809">where</TOKEN>
<TOKEN id="token-120-18" pos="word" morph="none" start_char="11811" end_char="11815">human</TOKEN>
<TOKEN id="token-120-19" pos="word" morph="none" start_char="11817" end_char="11822">judges</TOKEN>
<TOKEN id="token-120-20" pos="word" morph="none" start_char="11824" end_char="11826">did</TOKEN>
<TOKEN id="token-120-21" pos="word" morph="none" start_char="11828" end_char="11830">the</TOKEN>
<TOKEN id="token-120-22" pos="word" morph="none" start_char="11832" end_char="11836">exact</TOKEN>
<TOKEN id="token-120-23" pos="word" morph="none" start_char="11838" end_char="11841">same</TOKEN>
<TOKEN id="token-120-24" pos="word" morph="none" start_char="11843" end_char="11847">thing</TOKEN>
<TOKEN id="token-120-25" pos="punct" morph="none" start_char="11848" end_char="11848">.</TOKEN>
</SEG>
<SEG id="segment-121" start_char="11851" end_char="12003">
<ORIGINAL_TEXT>Until they submit their source data and algorithms for peer review I'm going to look at this as being a bias ai confirming the bias if it's bias creator.</ORIGINAL_TEXT>
<TOKEN id="token-121-0" pos="word" morph="none" start_char="11851" end_char="11855">Until</TOKEN>
<TOKEN id="token-121-1" pos="word" morph="none" start_char="11857" end_char="11860">they</TOKEN>
<TOKEN id="token-121-2" pos="word" morph="none" start_char="11862" end_char="11867">submit</TOKEN>
<TOKEN id="token-121-3" pos="word" morph="none" start_char="11869" end_char="11873">their</TOKEN>
<TOKEN id="token-121-4" pos="word" morph="none" start_char="11875" end_char="11880">source</TOKEN>
<TOKEN id="token-121-5" pos="word" morph="none" start_char="11882" end_char="11885">data</TOKEN>
<TOKEN id="token-121-6" pos="word" morph="none" start_char="11887" end_char="11889">and</TOKEN>
<TOKEN id="token-121-7" pos="word" morph="none" start_char="11891" end_char="11900">algorithms</TOKEN>
<TOKEN id="token-121-8" pos="word" morph="none" start_char="11902" end_char="11904">for</TOKEN>
<TOKEN id="token-121-9" pos="word" morph="none" start_char="11906" end_char="11909">peer</TOKEN>
<TOKEN id="token-121-10" pos="word" morph="none" start_char="11911" end_char="11916">review</TOKEN>
<TOKEN id="token-121-11" pos="word" morph="none" start_char="11918" end_char="11920">I'm</TOKEN>
<TOKEN id="token-121-12" pos="word" morph="none" start_char="11922" end_char="11926">going</TOKEN>
<TOKEN id="token-121-13" pos="word" morph="none" start_char="11928" end_char="11929">to</TOKEN>
<TOKEN id="token-121-14" pos="word" morph="none" start_char="11931" end_char="11934">look</TOKEN>
<TOKEN id="token-121-15" pos="word" morph="none" start_char="11936" end_char="11937">at</TOKEN>
<TOKEN id="token-121-16" pos="word" morph="none" start_char="11939" end_char="11942">this</TOKEN>
<TOKEN id="token-121-17" pos="word" morph="none" start_char="11944" end_char="11945">as</TOKEN>
<TOKEN id="token-121-18" pos="word" morph="none" start_char="11947" end_char="11951">being</TOKEN>
<TOKEN id="token-121-19" pos="word" morph="none" start_char="11953" end_char="11953">a</TOKEN>
<TOKEN id="token-121-20" pos="word" morph="none" start_char="11955" end_char="11958">bias</TOKEN>
<TOKEN id="token-121-21" pos="word" morph="none" start_char="11960" end_char="11961">ai</TOKEN>
<TOKEN id="token-121-22" pos="word" morph="none" start_char="11963" end_char="11972">confirming</TOKEN>
<TOKEN id="token-121-23" pos="word" morph="none" start_char="11974" end_char="11976">the</TOKEN>
<TOKEN id="token-121-24" pos="word" morph="none" start_char="11978" end_char="11981">bias</TOKEN>
<TOKEN id="token-121-25" pos="word" morph="none" start_char="11983" end_char="11984">if</TOKEN>
<TOKEN id="token-121-26" pos="word" morph="none" start_char="11986" end_char="11989">it's</TOKEN>
<TOKEN id="token-121-27" pos="word" morph="none" start_char="11991" end_char="11994">bias</TOKEN>
<TOKEN id="token-121-28" pos="word" morph="none" start_char="11996" end_char="12002">creator</TOKEN>
<TOKEN id="token-121-29" pos="punct" morph="none" start_char="12003" end_char="12003">.</TOKEN>
</SEG>
<SEG id="segment-122" start_char="12007" end_char="12095">
<ORIGINAL_TEXT>edit on 12/31/20 by Gothmog because: Not following with off-topic and ignorant statements</ORIGINAL_TEXT>
<TOKEN id="token-122-0" pos="word" morph="none" start_char="12007" end_char="12010">edit</TOKEN>
<TOKEN id="token-122-1" pos="word" morph="none" start_char="12012" end_char="12013">on</TOKEN>
<TOKEN id="token-122-2" pos="unknown" morph="none" start_char="12015" end_char="12022">12/31/20</TOKEN>
<TOKEN id="token-122-3" pos="word" morph="none" start_char="12024" end_char="12025">by</TOKEN>
<TOKEN id="token-122-4" pos="word" morph="none" start_char="12027" end_char="12033">Gothmog</TOKEN>
<TOKEN id="token-122-5" pos="word" morph="none" start_char="12035" end_char="12041">because</TOKEN>
<TOKEN id="token-122-6" pos="punct" morph="none" start_char="12042" end_char="12042">:</TOKEN>
<TOKEN id="token-122-7" pos="word" morph="none" start_char="12044" end_char="12046">Not</TOKEN>
<TOKEN id="token-122-8" pos="word" morph="none" start_char="12048" end_char="12056">following</TOKEN>
<TOKEN id="token-122-9" pos="word" morph="none" start_char="12058" end_char="12061">with</TOKEN>
<TOKEN id="token-122-10" pos="unknown" morph="none" start_char="12063" end_char="12071">off-topic</TOKEN>
<TOKEN id="token-122-11" pos="word" morph="none" start_char="12073" end_char="12075">and</TOKEN>
<TOKEN id="token-122-12" pos="word" morph="none" start_char="12077" end_char="12084">ignorant</TOKEN>
<TOKEN id="token-122-13" pos="word" morph="none" start_char="12086" end_char="12095">statements</TOKEN>
</SEG>
<SEG id="segment-123" start_char="12099" end_char="12179">
<ORIGINAL_TEXT>The case for lab origins is much stronger than zoonotic from all I have reviewed.</ORIGINAL_TEXT>
<TOKEN id="token-123-0" pos="word" morph="none" start_char="12099" end_char="12101">The</TOKEN>
<TOKEN id="token-123-1" pos="word" morph="none" start_char="12103" end_char="12106">case</TOKEN>
<TOKEN id="token-123-2" pos="word" morph="none" start_char="12108" end_char="12110">for</TOKEN>
<TOKEN id="token-123-3" pos="word" morph="none" start_char="12112" end_char="12114">lab</TOKEN>
<TOKEN id="token-123-4" pos="word" morph="none" start_char="12116" end_char="12122">origins</TOKEN>
<TOKEN id="token-123-5" pos="word" morph="none" start_char="12124" end_char="12125">is</TOKEN>
<TOKEN id="token-123-6" pos="word" morph="none" start_char="12127" end_char="12130">much</TOKEN>
<TOKEN id="token-123-7" pos="word" morph="none" start_char="12132" end_char="12139">stronger</TOKEN>
<TOKEN id="token-123-8" pos="word" morph="none" start_char="12141" end_char="12144">than</TOKEN>
<TOKEN id="token-123-9" pos="word" morph="none" start_char="12146" end_char="12153">zoonotic</TOKEN>
<TOKEN id="token-123-10" pos="word" morph="none" start_char="12155" end_char="12158">from</TOKEN>
<TOKEN id="token-123-11" pos="word" morph="none" start_char="12160" end_char="12162">all</TOKEN>
<TOKEN id="token-123-12" pos="word" morph="none" start_char="12164" end_char="12164">I</TOKEN>
<TOKEN id="token-123-13" pos="word" morph="none" start_char="12166" end_char="12169">have</TOKEN>
<TOKEN id="token-123-14" pos="word" morph="none" start_char="12171" end_char="12178">reviewed</TOKEN>
<TOKEN id="token-123-15" pos="punct" morph="none" start_char="12179" end_char="12179">.</TOKEN>
</SEG>
<SEG id="segment-124" start_char="12182" end_char="12296">
<ORIGINAL_TEXT>As being an accidental or intentional release, Event 201 does provide a compelling case for an intentional release.</ORIGINAL_TEXT>
<TOKEN id="token-124-0" pos="word" morph="none" start_char="12182" end_char="12183">As</TOKEN>
<TOKEN id="token-124-1" pos="word" morph="none" start_char="12185" end_char="12189">being</TOKEN>
<TOKEN id="token-124-2" pos="word" morph="none" start_char="12191" end_char="12192">an</TOKEN>
<TOKEN id="token-124-3" pos="word" morph="none" start_char="12194" end_char="12203">accidental</TOKEN>
<TOKEN id="token-124-4" pos="word" morph="none" start_char="12205" end_char="12206">or</TOKEN>
<TOKEN id="token-124-5" pos="word" morph="none" start_char="12208" end_char="12218">intentional</TOKEN>
<TOKEN id="token-124-6" pos="word" morph="none" start_char="12220" end_char="12226">release</TOKEN>
<TOKEN id="token-124-7" pos="punct" morph="none" start_char="12227" end_char="12227">,</TOKEN>
<TOKEN id="token-124-8" pos="word" morph="none" start_char="12229" end_char="12233">Event</TOKEN>
<TOKEN id="token-124-9" pos="word" morph="none" start_char="12235" end_char="12237">201</TOKEN>
<TOKEN id="token-124-10" pos="word" morph="none" start_char="12239" end_char="12242">does</TOKEN>
<TOKEN id="token-124-11" pos="word" morph="none" start_char="12244" end_char="12250">provide</TOKEN>
<TOKEN id="token-124-12" pos="word" morph="none" start_char="12252" end_char="12252">a</TOKEN>
<TOKEN id="token-124-13" pos="word" morph="none" start_char="12254" end_char="12263">compelling</TOKEN>
<TOKEN id="token-124-14" pos="word" morph="none" start_char="12265" end_char="12268">case</TOKEN>
<TOKEN id="token-124-15" pos="word" morph="none" start_char="12270" end_char="12272">for</TOKEN>
<TOKEN id="token-124-16" pos="word" morph="none" start_char="12274" end_char="12275">an</TOKEN>
<TOKEN id="token-124-17" pos="word" morph="none" start_char="12277" end_char="12287">intentional</TOKEN>
<TOKEN id="token-124-18" pos="word" morph="none" start_char="12289" end_char="12295">release</TOKEN>
<TOKEN id="token-124-19" pos="punct" morph="none" start_char="12296" end_char="12296">.</TOKEN>
</SEG>
<SEG id="segment-125" start_char="12298" end_char="12408">
<ORIGINAL_TEXT>Those involved in the pandemic planning stages are still in positions of power and influence in covid response.</ORIGINAL_TEXT>
<TOKEN id="token-125-0" pos="word" morph="none" start_char="12298" end_char="12302">Those</TOKEN>
<TOKEN id="token-125-1" pos="word" morph="none" start_char="12304" end_char="12311">involved</TOKEN>
<TOKEN id="token-125-2" pos="word" morph="none" start_char="12313" end_char="12314">in</TOKEN>
<TOKEN id="token-125-3" pos="word" morph="none" start_char="12316" end_char="12318">the</TOKEN>
<TOKEN id="token-125-4" pos="word" morph="none" start_char="12320" end_char="12327">pandemic</TOKEN>
<TOKEN id="token-125-5" pos="word" morph="none" start_char="12329" end_char="12336">planning</TOKEN>
<TOKEN id="token-125-6" pos="word" morph="none" start_char="12338" end_char="12343">stages</TOKEN>
<TOKEN id="token-125-7" pos="word" morph="none" start_char="12345" end_char="12347">are</TOKEN>
<TOKEN id="token-125-8" pos="word" morph="none" start_char="12349" end_char="12353">still</TOKEN>
<TOKEN id="token-125-9" pos="word" morph="none" start_char="12355" end_char="12356">in</TOKEN>
<TOKEN id="token-125-10" pos="word" morph="none" start_char="12358" end_char="12366">positions</TOKEN>
<TOKEN id="token-125-11" pos="word" morph="none" start_char="12368" end_char="12369">of</TOKEN>
<TOKEN id="token-125-12" pos="word" morph="none" start_char="12371" end_char="12375">power</TOKEN>
<TOKEN id="token-125-13" pos="word" morph="none" start_char="12377" end_char="12379">and</TOKEN>
<TOKEN id="token-125-14" pos="word" morph="none" start_char="12381" end_char="12389">influence</TOKEN>
<TOKEN id="token-125-15" pos="word" morph="none" start_char="12391" end_char="12392">in</TOKEN>
<TOKEN id="token-125-16" pos="word" morph="none" start_char="12394" end_char="12398">covid</TOKEN>
<TOKEN id="token-125-17" pos="word" morph="none" start_char="12400" end_char="12407">response</TOKEN>
<TOKEN id="token-125-18" pos="punct" morph="none" start_char="12408" end_char="12408">.</TOKEN>
</SEG>
<SEG id="segment-126" start_char="12410" end_char="12508">
<ORIGINAL_TEXT>Suppressing treatments while promoting lock downs, masks, social distancing and vaccination policy.</ORIGINAL_TEXT>
<TOKEN id="token-126-0" pos="word" morph="none" start_char="12410" end_char="12420">Suppressing</TOKEN>
<TOKEN id="token-126-1" pos="word" morph="none" start_char="12422" end_char="12431">treatments</TOKEN>
<TOKEN id="token-126-2" pos="word" morph="none" start_char="12433" end_char="12437">while</TOKEN>
<TOKEN id="token-126-3" pos="word" morph="none" start_char="12439" end_char="12447">promoting</TOKEN>
<TOKEN id="token-126-4" pos="word" morph="none" start_char="12449" end_char="12452">lock</TOKEN>
<TOKEN id="token-126-5" pos="word" morph="none" start_char="12454" end_char="12458">downs</TOKEN>
<TOKEN id="token-126-6" pos="punct" morph="none" start_char="12459" end_char="12459">,</TOKEN>
<TOKEN id="token-126-7" pos="word" morph="none" start_char="12461" end_char="12465">masks</TOKEN>
<TOKEN id="token-126-8" pos="punct" morph="none" start_char="12466" end_char="12466">,</TOKEN>
<TOKEN id="token-126-9" pos="word" morph="none" start_char="12468" end_char="12473">social</TOKEN>
<TOKEN id="token-126-10" pos="word" morph="none" start_char="12475" end_char="12484">distancing</TOKEN>
<TOKEN id="token-126-11" pos="word" morph="none" start_char="12486" end_char="12488">and</TOKEN>
<TOKEN id="token-126-12" pos="word" morph="none" start_char="12490" end_char="12500">vaccination</TOKEN>
<TOKEN id="token-126-13" pos="word" morph="none" start_char="12502" end_char="12507">policy</TOKEN>
<TOKEN id="token-126-14" pos="punct" morph="none" start_char="12508" end_char="12508">.</TOKEN>
</SEG>
<SEG id="segment-127" start_char="12511" end_char="12558">
<ORIGINAL_TEXT>It does look like a biological attack from here.</ORIGINAL_TEXT>
<TOKEN id="token-127-0" pos="word" morph="none" start_char="12511" end_char="12512">It</TOKEN>
<TOKEN id="token-127-1" pos="word" morph="none" start_char="12514" end_char="12517">does</TOKEN>
<TOKEN id="token-127-2" pos="word" morph="none" start_char="12519" end_char="12522">look</TOKEN>
<TOKEN id="token-127-3" pos="word" morph="none" start_char="12524" end_char="12527">like</TOKEN>
<TOKEN id="token-127-4" pos="word" morph="none" start_char="12529" end_char="12529">a</TOKEN>
<TOKEN id="token-127-5" pos="word" morph="none" start_char="12531" end_char="12540">biological</TOKEN>
<TOKEN id="token-127-6" pos="word" morph="none" start_char="12542" end_char="12547">attack</TOKEN>
<TOKEN id="token-127-7" pos="word" morph="none" start_char="12549" end_char="12552">from</TOKEN>
<TOKEN id="token-127-8" pos="word" morph="none" start_char="12554" end_char="12557">here</TOKEN>
<TOKEN id="token-127-9" pos="punct" morph="none" start_char="12558" end_char="12558">.</TOKEN>
</SEG>
<SEG id="segment-128" start_char="12560" end_char="12643">
<ORIGINAL_TEXT>Some are motivated by money as the government coffers got pillaged during the panic.</ORIGINAL_TEXT>
<TOKEN id="token-128-0" pos="word" morph="none" start_char="12560" end_char="12563">Some</TOKEN>
<TOKEN id="token-128-1" pos="word" morph="none" start_char="12565" end_char="12567">are</TOKEN>
<TOKEN id="token-128-2" pos="word" morph="none" start_char="12569" end_char="12577">motivated</TOKEN>
<TOKEN id="token-128-3" pos="word" morph="none" start_char="12579" end_char="12580">by</TOKEN>
<TOKEN id="token-128-4" pos="word" morph="none" start_char="12582" end_char="12586">money</TOKEN>
<TOKEN id="token-128-5" pos="word" morph="none" start_char="12588" end_char="12589">as</TOKEN>
<TOKEN id="token-128-6" pos="word" morph="none" start_char="12591" end_char="12593">the</TOKEN>
<TOKEN id="token-128-7" pos="word" morph="none" start_char="12595" end_char="12604">government</TOKEN>
<TOKEN id="token-128-8" pos="word" morph="none" start_char="12606" end_char="12612">coffers</TOKEN>
<TOKEN id="token-128-9" pos="word" morph="none" start_char="12614" end_char="12616">got</TOKEN>
<TOKEN id="token-128-10" pos="word" morph="none" start_char="12618" end_char="12625">pillaged</TOKEN>
<TOKEN id="token-128-11" pos="word" morph="none" start_char="12627" end_char="12632">during</TOKEN>
<TOKEN id="token-128-12" pos="word" morph="none" start_char="12634" end_char="12636">the</TOKEN>
<TOKEN id="token-128-13" pos="word" morph="none" start_char="12638" end_char="12642">panic</TOKEN>
<TOKEN id="token-128-14" pos="punct" morph="none" start_char="12643" end_char="12643">.</TOKEN>
</SEG>
<SEG id="segment-129" start_char="12645" end_char="12751">
<ORIGINAL_TEXT>Most are trying to do the right thing, just a bit lost with all the censorship and misinformation going on.</ORIGINAL_TEXT>
<TOKEN id="token-129-0" pos="word" morph="none" start_char="12645" end_char="12648">Most</TOKEN>
<TOKEN id="token-129-1" pos="word" morph="none" start_char="12650" end_char="12652">are</TOKEN>
<TOKEN id="token-129-2" pos="word" morph="none" start_char="12654" end_char="12659">trying</TOKEN>
<TOKEN id="token-129-3" pos="word" morph="none" start_char="12661" end_char="12662">to</TOKEN>
<TOKEN id="token-129-4" pos="word" morph="none" start_char="12664" end_char="12665">do</TOKEN>
<TOKEN id="token-129-5" pos="word" morph="none" start_char="12667" end_char="12669">the</TOKEN>
<TOKEN id="token-129-6" pos="word" morph="none" start_char="12671" end_char="12675">right</TOKEN>
<TOKEN id="token-129-7" pos="word" morph="none" start_char="12677" end_char="12681">thing</TOKEN>
<TOKEN id="token-129-8" pos="punct" morph="none" start_char="12682" end_char="12682">,</TOKEN>
<TOKEN id="token-129-9" pos="word" morph="none" start_char="12684" end_char="12687">just</TOKEN>
<TOKEN id="token-129-10" pos="word" morph="none" start_char="12689" end_char="12689">a</TOKEN>
<TOKEN id="token-129-11" pos="word" morph="none" start_char="12691" end_char="12693">bit</TOKEN>
<TOKEN id="token-129-12" pos="word" morph="none" start_char="12695" end_char="12698">lost</TOKEN>
<TOKEN id="token-129-13" pos="word" morph="none" start_char="12700" end_char="12703">with</TOKEN>
<TOKEN id="token-129-14" pos="word" morph="none" start_char="12705" end_char="12707">all</TOKEN>
<TOKEN id="token-129-15" pos="word" morph="none" start_char="12709" end_char="12711">the</TOKEN>
<TOKEN id="token-129-16" pos="word" morph="none" start_char="12713" end_char="12722">censorship</TOKEN>
<TOKEN id="token-129-17" pos="word" morph="none" start_char="12724" end_char="12726">and</TOKEN>
<TOKEN id="token-129-18" pos="word" morph="none" start_char="12728" end_char="12741">misinformation</TOKEN>
<TOKEN id="token-129-19" pos="word" morph="none" start_char="12743" end_char="12747">going</TOKEN>
<TOKEN id="token-129-20" pos="word" morph="none" start_char="12749" end_char="12750">on</TOKEN>
<TOKEN id="token-129-21" pos="punct" morph="none" start_char="12751" end_char="12751">.</TOKEN>
</SEG>
<SEG id="segment-130" start_char="12753" end_char="12795">
<ORIGINAL_TEXT>A few are trying to kill of the population.</ORIGINAL_TEXT>
<TOKEN id="token-130-0" pos="word" morph="none" start_char="12753" end_char="12753">A</TOKEN>
<TOKEN id="token-130-1" pos="word" morph="none" start_char="12755" end_char="12757">few</TOKEN>
<TOKEN id="token-130-2" pos="word" morph="none" start_char="12759" end_char="12761">are</TOKEN>
<TOKEN id="token-130-3" pos="word" morph="none" start_char="12763" end_char="12768">trying</TOKEN>
<TOKEN id="token-130-4" pos="word" morph="none" start_char="12770" end_char="12771">to</TOKEN>
<TOKEN id="token-130-5" pos="word" morph="none" start_char="12773" end_char="12776">kill</TOKEN>
<TOKEN id="token-130-6" pos="word" morph="none" start_char="12778" end_char="12779">of</TOKEN>
<TOKEN id="token-130-7" pos="word" morph="none" start_char="12781" end_char="12783">the</TOKEN>
<TOKEN id="token-130-8" pos="word" morph="none" start_char="12785" end_char="12794">population</TOKEN>
<TOKEN id="token-130-9" pos="punct" morph="none" start_char="12795" end_char="12795">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
