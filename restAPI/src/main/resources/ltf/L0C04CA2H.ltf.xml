<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CA2H" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="3647" raw_text_md5="b799be5de57353ddd208f78d0c069947">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="87">
<ORIGINAL_TEXT>Fact Check: Were System And Method Of Testing For Coronavirus Testing Patented In 2015?</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="4">Fact</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="6" end_char="10">Check</TOKEN>
<TOKEN id="token-0-2" pos="punct" morph="none" start_char="11" end_char="11">:</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="13" end_char="16">Were</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="18" end_char="23">System</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="25" end_char="27">And</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="29" end_char="34">Method</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="36" end_char="37">Of</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="39" end_char="45">Testing</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="47" end_char="49">For</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="51" end_char="61">Coronavirus</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="63" end_char="69">Testing</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="71" end_char="78">Patented</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="80" end_char="81">In</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="83" end_char="86">2015</TOKEN>
<TOKEN id="token-0-15" pos="punct" morph="none" start_char="87" end_char="87">?</TOKEN>
</SEG>
<SEG id="segment-1" start_char="92" end_char="191">
<ORIGINAL_TEXT>Many conspiracy theories have been doing the rounds on the internet since the inception of Covid-19.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="92" end_char="95">Many</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="97" end_char="106">conspiracy</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="108" end_char="115">theories</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="117" end_char="120">have</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="122" end_char="125">been</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="127" end_char="131">doing</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="133" end_char="135">the</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="137" end_char="142">rounds</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="144" end_char="145">on</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="147" end_char="149">the</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="151" end_char="158">internet</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="160" end_char="164">since</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="166" end_char="168">the</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="170" end_char="178">inception</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="180" end_char="181">of</TOKEN>
<TOKEN id="token-1-15" pos="unknown" morph="none" start_char="183" end_char="190">Covid-19</TOKEN>
<TOKEN id="token-1-16" pos="punct" morph="none" start_char="191" end_char="191">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="193" end_char="289">
<ORIGINAL_TEXT>Many theories have been circulated about how Covid-19 was a scam or how it was already predicted.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="193" end_char="196">Many</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="198" end_char="205">theories</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="207" end_char="210">have</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="212" end_char="215">been</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="217" end_char="226">circulated</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="228" end_char="232">about</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="234" end_char="236">how</TOKEN>
<TOKEN id="token-2-7" pos="unknown" morph="none" start_char="238" end_char="245">Covid-19</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="247" end_char="249">was</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="251" end_char="251">a</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="253" end_char="256">scam</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="258" end_char="259">or</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="261" end_char="263">how</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="265" end_char="266">it</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="268" end_char="270">was</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="272" end_char="278">already</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="280" end_char="288">predicted</TOKEN>
<TOKEN id="token-2-17" pos="punct" morph="none" start_char="289" end_char="289">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="292" end_char="369">
<ORIGINAL_TEXT>In the backdrop of the same context, another social media post has gone viral.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="292" end_char="293">In</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="295" end_char="297">the</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="299" end_char="306">backdrop</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="308" end_char="309">of</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="311" end_char="313">the</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="315" end_char="318">same</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="320" end_char="326">context</TOKEN>
<TOKEN id="token-3-7" pos="punct" morph="none" start_char="327" end_char="327">,</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="329" end_char="335">another</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="337" end_char="342">social</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="344" end_char="348">media</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="350" end_char="353">post</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="355" end_char="357">has</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="359" end_char="362">gone</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="364" end_char="368">viral</TOKEN>
<TOKEN id="token-3-15" pos="punct" morph="none" start_char="369" end_char="369">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="371" end_char="463">
<ORIGINAL_TEXT>This post says that the system and method for testing Covid-19 were already patented in 2015.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="371" end_char="374">This</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="376" end_char="379">post</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="381" end_char="384">says</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="386" end_char="389">that</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="391" end_char="393">the</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="395" end_char="400">system</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="402" end_char="404">and</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="406" end_char="411">method</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="413" end_char="415">for</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="417" end_char="423">testing</TOKEN>
<TOKEN id="token-4-10" pos="unknown" morph="none" start_char="425" end_char="432">Covid-19</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="434" end_char="437">were</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="439" end_char="445">already</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="447" end_char="454">patented</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="456" end_char="457">in</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="459" end_char="462">2015</TOKEN>
<TOKEN id="token-4-16" pos="punct" morph="none" start_char="463" end_char="463">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="465" end_char="630">
<ORIGINAL_TEXT>The claim is being shared with an image that shows a supplemental application that was filed in 2020, followed by the provisional application which was filed in 2015.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="465" end_char="467">The</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="469" end_char="473">claim</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="475" end_char="476">is</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="478" end_char="482">being</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="484" end_char="489">shared</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="491" end_char="494">with</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="496" end_char="497">an</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="499" end_char="503">image</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="505" end_char="508">that</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="510" end_char="514">shows</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="516" end_char="516">a</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="518" end_char="529">supplemental</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="531" end_char="541">application</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="543" end_char="546">that</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="548" end_char="550">was</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="552" end_char="556">filed</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="558" end_char="559">in</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="561" end_char="564">2020</TOKEN>
<TOKEN id="token-5-18" pos="punct" morph="none" start_char="565" end_char="565">,</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="567" end_char="574">followed</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="576" end_char="577">by</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="579" end_char="581">the</TOKEN>
<TOKEN id="token-5-22" pos="word" morph="none" start_char="583" end_char="593">provisional</TOKEN>
<TOKEN id="token-5-23" pos="word" morph="none" start_char="595" end_char="605">application</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="607" end_char="611">which</TOKEN>
<TOKEN id="token-5-25" pos="word" morph="none" start_char="613" end_char="615">was</TOKEN>
<TOKEN id="token-5-26" pos="word" morph="none" start_char="617" end_char="621">filed</TOKEN>
<TOKEN id="token-5-27" pos="word" morph="none" start_char="623" end_char="624">in</TOKEN>
<TOKEN id="token-5-28" pos="word" morph="none" start_char="626" end_char="629">2015</TOKEN>
<TOKEN id="token-5-29" pos="punct" morph="none" start_char="630" end_char="630">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="633" end_char="751">
<ORIGINAL_TEXT>One of the captions says, "For those of you who don't think that this COVID-19 Scam wasn't planned years in advance ...</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="633" end_char="635">One</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="637" end_char="638">of</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="640" end_char="642">the</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="644" end_char="651">captions</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="653" end_char="656">says</TOKEN>
<TOKEN id="token-6-5" pos="punct" morph="none" start_char="657" end_char="657">,</TOKEN>
<TOKEN id="token-6-6" pos="punct" morph="none" start_char="659" end_char="659">"</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="660" end_char="662">For</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="664" end_char="668">those</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="670" end_char="671">of</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="673" end_char="675">you</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="677" end_char="679">who</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="681" end_char="685">don't</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="687" end_char="691">think</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="693" end_char="696">that</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="698" end_char="701">this</TOKEN>
<TOKEN id="token-6-16" pos="unknown" morph="none" start_char="703" end_char="710">COVID-19</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="712" end_char="715">Scam</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="717" end_char="722">wasn't</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="724" end_char="730">planned</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="732" end_char="736">years</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="738" end_char="739">in</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="741" end_char="747">advance</TOKEN>
<TOKEN id="token-6-23" pos="punct" morph="none" start_char="749" end_char="751">...</TOKEN>
</SEG>
<SEG id="segment-7" start_char="753" end_char="831">
<ORIGINAL_TEXT>PLEASE explain how a Rothchild filed for a COVID-19 patent BACK IN 2015 ?!?! ..</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="753" end_char="758">PLEASE</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="760" end_char="766">explain</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="768" end_char="770">how</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="772" end_char="772">a</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="774" end_char="782">Rothchild</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="784" end_char="788">filed</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="790" end_char="792">for</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="794" end_char="794">a</TOKEN>
<TOKEN id="token-7-8" pos="unknown" morph="none" start_char="796" end_char="803">COVID-19</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="805" end_char="810">patent</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="812" end_char="815">BACK</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="817" end_char="818">IN</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="820" end_char="823">2015</TOKEN>
<TOKEN id="token-7-13" pos="punct" morph="none" start_char="825" end_char="828">?!?!</TOKEN>
<TOKEN id="token-7-14" pos="punct" morph="none" start_char="830" end_char="831">..</TOKEN>
</SEG>
<SEG id="segment-8" start_char="833" end_char="847">
<ORIGINAL_TEXT>5 YEARS AGO ?!?</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="833" end_char="833">5</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="835" end_char="839">YEARS</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="841" end_char="843">AGO</TOKEN>
<TOKEN id="token-8-3" pos="punct" morph="none" start_char="845" end_char="847">?!?</TOKEN>
</SEG>
<SEG id="segment-9" start_char="849" end_char="897">
<ORIGINAL_TEXT>(Larry Melanchuk is your head still in the sand?"</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="punct" morph="none" start_char="849" end_char="849">(</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="850" end_char="854">Larry</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="856" end_char="864">Melanchuk</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="866" end_char="867">is</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="869" end_char="872">your</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="874" end_char="877">head</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="879" end_char="883">still</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="885" end_char="886">in</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="888" end_char="890">the</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="892" end_char="895">sand</TOKEN>
<TOKEN id="token-9-10" pos="punct" morph="none" start_char="896" end_char="897">?"</TOKEN>
</SEG>
<SEG id="segment-10" start_char="900" end_char="1020">
<ORIGINAL_TEXT>A similar claim was shared by Facebook user Ron Lloyd with the caption, "All the obedient sheeple should check the dates!</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="900" end_char="900">A</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="902" end_char="908">similar</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="910" end_char="914">claim</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="916" end_char="918">was</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="920" end_char="925">shared</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="927" end_char="928">by</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="930" end_char="937">Facebook</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="939" end_char="942">user</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="944" end_char="946">Ron</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="948" end_char="952">Lloyd</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="954" end_char="957">with</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="959" end_char="961">the</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="963" end_char="969">caption</TOKEN>
<TOKEN id="token-10-13" pos="punct" morph="none" start_char="970" end_char="970">,</TOKEN>
<TOKEN id="token-10-14" pos="punct" morph="none" start_char="972" end_char="972">"</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="973" end_char="975">All</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="977" end_char="979">the</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="981" end_char="988">obedient</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="990" end_char="996">sheeple</TOKEN>
<TOKEN id="token-10-19" pos="word" morph="none" start_char="998" end_char="1003">should</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="1005" end_char="1009">check</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="1011" end_char="1013">the</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="1015" end_char="1019">dates</TOKEN>
<TOKEN id="token-10-23" pos="punct" morph="none" start_char="1020" end_char="1020">!</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1022" end_char="1047">
<ORIGINAL_TEXT>You've all been scammed!".</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1022" end_char="1027">You've</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1029" end_char="1031">all</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1033" end_char="1036">been</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1038" end_char="1044">scammed</TOKEN>
<TOKEN id="token-11-4" pos="punct" morph="none" start_char="1045" end_char="1047">!".</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1050" end_char="1174">
<ORIGINAL_TEXT>All the obedient sheeple should check the dates‚ùó Youve all been scammed‚ùó üëáüëáüëáüëáPosted by Ron Lloyd on Wednesday, 7 October 2020</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1050" end_char="1052">All</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1054" end_char="1056">the</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1058" end_char="1065">obedient</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1067" end_char="1073">sheeple</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1075" end_char="1080">should</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1082" end_char="1086">check</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1088" end_char="1090">the</TOKEN>
<TOKEN id="token-12-7" pos="unknown" morph="none" start_char="1092" end_char="1097">dates‚ùó</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1099" end_char="1103">Youve</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1105" end_char="1107">all</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1109" end_char="1112">been</TOKEN>
<TOKEN id="token-12-11" pos="unknown" morph="none" start_char="1114" end_char="1121">scammed‚ùó</TOKEN>
<TOKEN id="token-12-12" pos="unknown" morph="none" start_char="1123" end_char="1132">üëáüëáüëáüëáPosted</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1134" end_char="1135">by</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1137" end_char="1139">Ron</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1141" end_char="1145">Lloyd</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1147" end_char="1148">on</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1150" end_char="1158">Wednesday</TOKEN>
<TOKEN id="token-12-18" pos="punct" morph="none" start_char="1159" end_char="1159">,</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="1161" end_char="1161">7</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1163" end_char="1169">October</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1171" end_char="1174">2020</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1178" end_char="1183">
<ORIGINAL_TEXT>Claim:</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1178" end_char="1182">Claim</TOKEN>
<TOKEN id="token-13-1" pos="punct" morph="none" start_char="1183" end_char="1183">:</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1186" end_char="1319">
<ORIGINAL_TEXT>The provisional application for system and method for testing Covid-19 was already filed in 2015, 5 years before coronavirus pandemic.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1186" end_char="1188">The</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1190" end_char="1200">provisional</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1202" end_char="1212">application</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1214" end_char="1216">for</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1218" end_char="1223">system</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1225" end_char="1227">and</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1229" end_char="1234">method</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1236" end_char="1238">for</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1240" end_char="1246">testing</TOKEN>
<TOKEN id="token-14-9" pos="unknown" morph="none" start_char="1248" end_char="1255">Covid-19</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1257" end_char="1259">was</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1261" end_char="1267">already</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="1269" end_char="1273">filed</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="1275" end_char="1276">in</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="1278" end_char="1281">2015</TOKEN>
<TOKEN id="token-14-15" pos="punct" morph="none" start_char="1282" end_char="1282">,</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="1284" end_char="1284">5</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="1286" end_char="1290">years</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="1292" end_char="1297">before</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="1299" end_char="1309">coronavirus</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="1311" end_char="1318">pandemic</TOKEN>
<TOKEN id="token-14-21" pos="punct" morph="none" start_char="1319" end_char="1319">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1322" end_char="1332">
<ORIGINAL_TEXT>Fact Check:</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1322" end_char="1325">Fact</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1327" end_char="1331">Check</TOKEN>
<TOKEN id="token-15-2" pos="punct" morph="none" start_char="1332" end_char="1332">:</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1336" end_char="1441">
<ORIGINAL_TEXT>The Logical Indian used the keyword 'system and method for testing Covid-19' and found a patent on Google.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1336" end_char="1338">The</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1340" end_char="1346">Logical</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1348" end_char="1353">Indian</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1355" end_char="1358">used</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1360" end_char="1362">the</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1364" end_char="1370">keyword</TOKEN>
<TOKEN id="token-16-6" pos="punct" morph="none" start_char="1372" end_char="1372">'</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="1373" end_char="1378">system</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="1380" end_char="1382">and</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="1384" end_char="1389">method</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="1391" end_char="1393">for</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="1395" end_char="1401">testing</TOKEN>
<TOKEN id="token-16-12" pos="unknown" morph="none" start_char="1403" end_char="1410">Covid-19</TOKEN>
<TOKEN id="token-16-13" pos="punct" morph="none" start_char="1411" end_char="1411">'</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="1413" end_char="1415">and</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="1417" end_char="1421">found</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="1423" end_char="1423">a</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="1425" end_char="1430">patent</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="1432" end_char="1433">on</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="1435" end_char="1440">Google</TOKEN>
<TOKEN id="token-16-20" pos="punct" morph="none" start_char="1441" end_char="1441">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="1443" end_char="1574">
<ORIGINAL_TEXT>The patent was found on the website, with the application, 'System and Method for Testing for COVID-19' filed by Rothschild Richard.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="1443" end_char="1445">The</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="1447" end_char="1452">patent</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="1454" end_char="1456">was</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="1458" end_char="1462">found</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="1464" end_char="1465">on</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="1467" end_char="1469">the</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="1471" end_char="1477">website</TOKEN>
<TOKEN id="token-17-7" pos="punct" morph="none" start_char="1478" end_char="1478">,</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="1480" end_char="1483">with</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="1485" end_char="1487">the</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="1489" end_char="1499">application</TOKEN>
<TOKEN id="token-17-11" pos="punct" morph="none" start_char="1500" end_char="1500">,</TOKEN>
<TOKEN id="token-17-12" pos="punct" morph="none" start_char="1502" end_char="1502">'</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="1503" end_char="1508">System</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="1510" end_char="1512">and</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="1514" end_char="1519">Method</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="1521" end_char="1523">for</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="1525" end_char="1531">Testing</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="1533" end_char="1535">for</TOKEN>
<TOKEN id="token-17-19" pos="unknown" morph="none" start_char="1537" end_char="1544">COVID-19</TOKEN>
<TOKEN id="token-17-20" pos="punct" morph="none" start_char="1545" end_char="1545">'</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="1547" end_char="1551">filed</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="1553" end_char="1554">by</TOKEN>
<TOKEN id="token-17-23" pos="word" morph="none" start_char="1556" end_char="1565">Rothschild</TOKEN>
<TOKEN id="token-17-24" pos="word" morph="none" start_char="1567" end_char="1573">Richard</TOKEN>
<TOKEN id="token-17-25" pos="punct" morph="none" start_char="1574" end_char="1574">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="1577" end_char="1758">
<ORIGINAL_TEXT>The search was also done on Espacenet and found that the filing is a "Continuation in part" (CIP) application for a US patent, i.e; a partial continuation of an existing application.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="1577" end_char="1579">The</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="1581" end_char="1586">search</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="1588" end_char="1590">was</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="1592" end_char="1595">also</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="1597" end_char="1600">done</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="1602" end_char="1603">on</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="1605" end_char="1613">Espacenet</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="1615" end_char="1617">and</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="1619" end_char="1623">found</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="1625" end_char="1628">that</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="1630" end_char="1632">the</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="1634" end_char="1639">filing</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="1641" end_char="1642">is</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="1644" end_char="1644">a</TOKEN>
<TOKEN id="token-18-14" pos="punct" morph="none" start_char="1646" end_char="1646">"</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="1647" end_char="1658">Continuation</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="1660" end_char="1661">in</TOKEN>
<TOKEN id="token-18-17" pos="word" morph="none" start_char="1663" end_char="1666">part</TOKEN>
<TOKEN id="token-18-18" pos="punct" morph="none" start_char="1667" end_char="1667">"</TOKEN>
<TOKEN id="token-18-19" pos="punct" morph="none" start_char="1669" end_char="1669">(</TOKEN>
<TOKEN id="token-18-20" pos="word" morph="none" start_char="1670" end_char="1672">CIP</TOKEN>
<TOKEN id="token-18-21" pos="punct" morph="none" start_char="1673" end_char="1673">)</TOKEN>
<TOKEN id="token-18-22" pos="word" morph="none" start_char="1675" end_char="1685">application</TOKEN>
<TOKEN id="token-18-23" pos="word" morph="none" start_char="1687" end_char="1689">for</TOKEN>
<TOKEN id="token-18-24" pos="word" morph="none" start_char="1691" end_char="1691">a</TOKEN>
<TOKEN id="token-18-25" pos="word" morph="none" start_char="1693" end_char="1694">US</TOKEN>
<TOKEN id="token-18-26" pos="word" morph="none" start_char="1696" end_char="1701">patent</TOKEN>
<TOKEN id="token-18-27" pos="punct" morph="none" start_char="1702" end_char="1702">,</TOKEN>
<TOKEN id="token-18-28" pos="unknown" morph="none" start_char="1704" end_char="1706">i.e</TOKEN>
<TOKEN id="token-18-29" pos="punct" morph="none" start_char="1707" end_char="1707">;</TOKEN>
<TOKEN id="token-18-30" pos="word" morph="none" start_char="1709" end_char="1709">a</TOKEN>
<TOKEN id="token-18-31" pos="word" morph="none" start_char="1711" end_char="1717">partial</TOKEN>
<TOKEN id="token-18-32" pos="word" morph="none" start_char="1719" end_char="1730">continuation</TOKEN>
<TOKEN id="token-18-33" pos="word" morph="none" start_char="1732" end_char="1733">of</TOKEN>
<TOKEN id="token-18-34" pos="word" morph="none" start_char="1735" end_char="1736">an</TOKEN>
<TOKEN id="token-18-35" pos="word" morph="none" start_char="1738" end_char="1745">existing</TOKEN>
<TOKEN id="token-18-36" pos="word" morph="none" start_char="1747" end_char="1757">application</TOKEN>
<TOKEN id="token-18-37" pos="punct" morph="none" start_char="1758" end_char="1758">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="1761" end_char="1873">
<ORIGINAL_TEXT>Espacenet is a website where one can search for patents and applications developed by the European Patent Office.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="1761" end_char="1769">Espacenet</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="1771" end_char="1772">is</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="1774" end_char="1774">a</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="1776" end_char="1782">website</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="1784" end_char="1788">where</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="1790" end_char="1792">one</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="1794" end_char="1796">can</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="1798" end_char="1803">search</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="1805" end_char="1807">for</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="1809" end_char="1815">patents</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="1817" end_char="1819">and</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="1821" end_char="1832">applications</TOKEN>
<TOKEN id="token-19-12" pos="word" morph="none" start_char="1834" end_char="1842">developed</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="1844" end_char="1845">by</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="1847" end_char="1849">the</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="1851" end_char="1858">European</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="1860" end_char="1865">Patent</TOKEN>
<TOKEN id="token-19-17" pos="word" morph="none" start_char="1867" end_char="1872">Office</TOKEN>
<TOKEN id="token-19-18" pos="punct" morph="none" start_char="1873" end_char="1873">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="1875" end_char="2019">
<ORIGINAL_TEXT>CIP allows an inventor to link a new patent with an older license, as long as the old invention is contributed to the development of the new one.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="1875" end_char="1877">CIP</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="1879" end_char="1884">allows</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="1886" end_char="1887">an</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="1889" end_char="1896">inventor</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="1898" end_char="1899">to</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="1901" end_char="1904">link</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="1906" end_char="1906">a</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="1908" end_char="1910">new</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="1912" end_char="1917">patent</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="1919" end_char="1922">with</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="1924" end_char="1925">an</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="1927" end_char="1931">older</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="1933" end_char="1939">license</TOKEN>
<TOKEN id="token-20-13" pos="punct" morph="none" start_char="1940" end_char="1940">,</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="1942" end_char="1943">as</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="1945" end_char="1948">long</TOKEN>
<TOKEN id="token-20-16" pos="word" morph="none" start_char="1950" end_char="1951">as</TOKEN>
<TOKEN id="token-20-17" pos="word" morph="none" start_char="1953" end_char="1955">the</TOKEN>
<TOKEN id="token-20-18" pos="word" morph="none" start_char="1957" end_char="1959">old</TOKEN>
<TOKEN id="token-20-19" pos="word" morph="none" start_char="1961" end_char="1969">invention</TOKEN>
<TOKEN id="token-20-20" pos="word" morph="none" start_char="1971" end_char="1972">is</TOKEN>
<TOKEN id="token-20-21" pos="word" morph="none" start_char="1974" end_char="1984">contributed</TOKEN>
<TOKEN id="token-20-22" pos="word" morph="none" start_char="1986" end_char="1987">to</TOKEN>
<TOKEN id="token-20-23" pos="word" morph="none" start_char="1989" end_char="1991">the</TOKEN>
<TOKEN id="token-20-24" pos="word" morph="none" start_char="1993" end_char="2003">development</TOKEN>
<TOKEN id="token-20-25" pos="word" morph="none" start_char="2005" end_char="2006">of</TOKEN>
<TOKEN id="token-20-26" pos="word" morph="none" start_char="2008" end_char="2010">the</TOKEN>
<TOKEN id="token-20-27" pos="word" morph="none" start_char="2012" end_char="2014">new</TOKEN>
<TOKEN id="token-20-28" pos="word" morph="none" start_char="2016" end_char="2018">one</TOKEN>
<TOKEN id="token-20-29" pos="punct" morph="none" start_char="2019" end_char="2019">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="2022" end_char="2253">
<ORIGINAL_TEXT>Rothschild patent which is being noticed in the screenshot of the misleading Facebook posts, which says it was filed in 2013, is actually of "acquiring and transmitting biometric data" and not related to Covid-19 as claimed by post.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="2022" end_char="2031">Rothschild</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="2033" end_char="2038">patent</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="2040" end_char="2044">which</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="2046" end_char="2047">is</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="2049" end_char="2053">being</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="2055" end_char="2061">noticed</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="2063" end_char="2064">in</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="2066" end_char="2068">the</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="2070" end_char="2079">screenshot</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="2081" end_char="2082">of</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="2084" end_char="2086">the</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="2088" end_char="2097">misleading</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="2099" end_char="2106">Facebook</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="2108" end_char="2112">posts</TOKEN>
<TOKEN id="token-21-14" pos="punct" morph="none" start_char="2113" end_char="2113">,</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="2115" end_char="2119">which</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="2121" end_char="2124">says</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="2126" end_char="2127">it</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="2129" end_char="2131">was</TOKEN>
<TOKEN id="token-21-19" pos="word" morph="none" start_char="2133" end_char="2137">filed</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="2139" end_char="2140">in</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="2142" end_char="2145">2013</TOKEN>
<TOKEN id="token-21-22" pos="punct" morph="none" start_char="2146" end_char="2146">,</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="2148" end_char="2149">is</TOKEN>
<TOKEN id="token-21-24" pos="word" morph="none" start_char="2151" end_char="2158">actually</TOKEN>
<TOKEN id="token-21-25" pos="word" morph="none" start_char="2160" end_char="2161">of</TOKEN>
<TOKEN id="token-21-26" pos="punct" morph="none" start_char="2163" end_char="2163">"</TOKEN>
<TOKEN id="token-21-27" pos="word" morph="none" start_char="2164" end_char="2172">acquiring</TOKEN>
<TOKEN id="token-21-28" pos="word" morph="none" start_char="2174" end_char="2176">and</TOKEN>
<TOKEN id="token-21-29" pos="word" morph="none" start_char="2178" end_char="2189">transmitting</TOKEN>
<TOKEN id="token-21-30" pos="word" morph="none" start_char="2191" end_char="2199">biometric</TOKEN>
<TOKEN id="token-21-31" pos="word" morph="none" start_char="2201" end_char="2204">data</TOKEN>
<TOKEN id="token-21-32" pos="punct" morph="none" start_char="2205" end_char="2205">"</TOKEN>
<TOKEN id="token-21-33" pos="word" morph="none" start_char="2207" end_char="2209">and</TOKEN>
<TOKEN id="token-21-34" pos="word" morph="none" start_char="2211" end_char="2213">not</TOKEN>
<TOKEN id="token-21-35" pos="word" morph="none" start_char="2215" end_char="2221">related</TOKEN>
<TOKEN id="token-21-36" pos="word" morph="none" start_char="2223" end_char="2224">to</TOKEN>
<TOKEN id="token-21-37" pos="unknown" morph="none" start_char="2226" end_char="2233">Covid-19</TOKEN>
<TOKEN id="token-21-38" pos="word" morph="none" start_char="2235" end_char="2236">as</TOKEN>
<TOKEN id="token-21-39" pos="word" morph="none" start_char="2238" end_char="2244">claimed</TOKEN>
<TOKEN id="token-21-40" pos="word" morph="none" start_char="2246" end_char="2247">by</TOKEN>
<TOKEN id="token-21-41" pos="word" morph="none" start_char="2249" end_char="2252">post</TOKEN>
<TOKEN id="token-21-42" pos="punct" morph="none" start_char="2253" end_char="2253">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="2255" end_char="2453">
<ORIGINAL_TEXT>Later, CIP application was submitted in May 2020 because it was claimed that the biometric data can also be used to "determine whether the user is suffering from a viral infection, such as COVID-19."</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="2255" end_char="2259">Later</TOKEN>
<TOKEN id="token-22-1" pos="punct" morph="none" start_char="2260" end_char="2260">,</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="2262" end_char="2264">CIP</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="2266" end_char="2276">application</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="2278" end_char="2280">was</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="2282" end_char="2290">submitted</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="2292" end_char="2293">in</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="2295" end_char="2297">May</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="2299" end_char="2302">2020</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="2304" end_char="2310">because</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="2312" end_char="2313">it</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="2315" end_char="2317">was</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="2319" end_char="2325">claimed</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="2327" end_char="2330">that</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="2332" end_char="2334">the</TOKEN>
<TOKEN id="token-22-15" pos="word" morph="none" start_char="2336" end_char="2344">biometric</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="2346" end_char="2349">data</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="2351" end_char="2353">can</TOKEN>
<TOKEN id="token-22-18" pos="word" morph="none" start_char="2355" end_char="2358">also</TOKEN>
<TOKEN id="token-22-19" pos="word" morph="none" start_char="2360" end_char="2361">be</TOKEN>
<TOKEN id="token-22-20" pos="word" morph="none" start_char="2363" end_char="2366">used</TOKEN>
<TOKEN id="token-22-21" pos="word" morph="none" start_char="2368" end_char="2369">to</TOKEN>
<TOKEN id="token-22-22" pos="punct" morph="none" start_char="2371" end_char="2371">"</TOKEN>
<TOKEN id="token-22-23" pos="word" morph="none" start_char="2372" end_char="2380">determine</TOKEN>
<TOKEN id="token-22-24" pos="word" morph="none" start_char="2382" end_char="2388">whether</TOKEN>
<TOKEN id="token-22-25" pos="word" morph="none" start_char="2390" end_char="2392">the</TOKEN>
<TOKEN id="token-22-26" pos="word" morph="none" start_char="2394" end_char="2397">user</TOKEN>
<TOKEN id="token-22-27" pos="word" morph="none" start_char="2399" end_char="2400">is</TOKEN>
<TOKEN id="token-22-28" pos="word" morph="none" start_char="2402" end_char="2410">suffering</TOKEN>
<TOKEN id="token-22-29" pos="word" morph="none" start_char="2412" end_char="2415">from</TOKEN>
<TOKEN id="token-22-30" pos="word" morph="none" start_char="2417" end_char="2417">a</TOKEN>
<TOKEN id="token-22-31" pos="word" morph="none" start_char="2419" end_char="2423">viral</TOKEN>
<TOKEN id="token-22-32" pos="word" morph="none" start_char="2425" end_char="2433">infection</TOKEN>
<TOKEN id="token-22-33" pos="punct" morph="none" start_char="2434" end_char="2434">,</TOKEN>
<TOKEN id="token-22-34" pos="word" morph="none" start_char="2436" end_char="2439">such</TOKEN>
<TOKEN id="token-22-35" pos="word" morph="none" start_char="2441" end_char="2442">as</TOKEN>
<TOKEN id="token-22-36" pos="unknown" morph="none" start_char="2444" end_char="2451">COVID-19</TOKEN>
<TOKEN id="token-22-37" pos="punct" morph="none" start_char="2452" end_char="2453">."</TOKEN>
</SEG>
<SEG id="segment-23" start_char="2456" end_char="2613">
<ORIGINAL_TEXT>According to an AFP report, a European Patent Office spokesperson, Rainer Osterwalder said, "the patent application had no reference to COVID-19 before 2020."</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="2456" end_char="2464">According</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="2466" end_char="2467">to</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="2469" end_char="2470">an</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="2472" end_char="2474">AFP</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="2476" end_char="2481">report</TOKEN>
<TOKEN id="token-23-5" pos="punct" morph="none" start_char="2482" end_char="2482">,</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="2484" end_char="2484">a</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="2486" end_char="2493">European</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="2495" end_char="2500">Patent</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="2502" end_char="2507">Office</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="2509" end_char="2520">spokesperson</TOKEN>
<TOKEN id="token-23-11" pos="punct" morph="none" start_char="2521" end_char="2521">,</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="2523" end_char="2528">Rainer</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="2530" end_char="2540">Osterwalder</TOKEN>
<TOKEN id="token-23-14" pos="word" morph="none" start_char="2542" end_char="2545">said</TOKEN>
<TOKEN id="token-23-15" pos="punct" morph="none" start_char="2546" end_char="2546">,</TOKEN>
<TOKEN id="token-23-16" pos="punct" morph="none" start_char="2548" end_char="2548">"</TOKEN>
<TOKEN id="token-23-17" pos="word" morph="none" start_char="2549" end_char="2551">the</TOKEN>
<TOKEN id="token-23-18" pos="word" morph="none" start_char="2553" end_char="2558">patent</TOKEN>
<TOKEN id="token-23-19" pos="word" morph="none" start_char="2560" end_char="2570">application</TOKEN>
<TOKEN id="token-23-20" pos="word" morph="none" start_char="2572" end_char="2574">had</TOKEN>
<TOKEN id="token-23-21" pos="word" morph="none" start_char="2576" end_char="2577">no</TOKEN>
<TOKEN id="token-23-22" pos="word" morph="none" start_char="2579" end_char="2587">reference</TOKEN>
<TOKEN id="token-23-23" pos="word" morph="none" start_char="2589" end_char="2590">to</TOKEN>
<TOKEN id="token-23-24" pos="unknown" morph="none" start_char="2592" end_char="2599">COVID-19</TOKEN>
<TOKEN id="token-23-25" pos="word" morph="none" start_char="2601" end_char="2606">before</TOKEN>
<TOKEN id="token-23-26" pos="word" morph="none" start_char="2608" end_char="2611">2020</TOKEN>
<TOKEN id="token-23-27" pos="punct" morph="none" start_char="2612" end_char="2613">."</TOKEN>
</SEG>
<SEG id="segment-24" start_char="2616" end_char="2686">
<ORIGINAL_TEXT>"In the first disclosed registration of 2016, this is about video data.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="punct" morph="none" start_char="2616" end_char="2616">"</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="2617" end_char="2618">In</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="2620" end_char="2622">the</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="2624" end_char="2628">first</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="2630" end_char="2638">disclosed</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="2640" end_char="2651">registration</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="2653" end_char="2654">of</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="2656" end_char="2659">2016</TOKEN>
<TOKEN id="token-24-8" pos="punct" morph="none" start_char="2660" end_char="2660">,</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="2662" end_char="2665">this</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="2667" end_char="2668">is</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="2670" end_char="2674">about</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="2676" end_char="2680">video</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="2682" end_char="2685">data</TOKEN>
<TOKEN id="token-24-14" pos="punct" morph="none" start_char="2686" end_char="2686">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="2688" end_char="2808">
<ORIGINAL_TEXT>In the subsequent applications of 2016/2017, the collection, processing and transmission of biometric data was specified.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="2688" end_char="2689">In</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="2691" end_char="2693">the</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="2695" end_char="2704">subsequent</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="2706" end_char="2717">applications</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="2719" end_char="2720">of</TOKEN>
<TOKEN id="token-25-5" pos="unknown" morph="none" start_char="2722" end_char="2730">2016/2017</TOKEN>
<TOKEN id="token-25-6" pos="punct" morph="none" start_char="2731" end_char="2731">,</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="2733" end_char="2735">the</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="2737" end_char="2746">collection</TOKEN>
<TOKEN id="token-25-9" pos="punct" morph="none" start_char="2747" end_char="2747">,</TOKEN>
<TOKEN id="token-25-10" pos="word" morph="none" start_char="2749" end_char="2758">processing</TOKEN>
<TOKEN id="token-25-11" pos="word" morph="none" start_char="2760" end_char="2762">and</TOKEN>
<TOKEN id="token-25-12" pos="word" morph="none" start_char="2764" end_char="2775">transmission</TOKEN>
<TOKEN id="token-25-13" pos="word" morph="none" start_char="2777" end_char="2778">of</TOKEN>
<TOKEN id="token-25-14" pos="word" morph="none" start_char="2780" end_char="2788">biometric</TOKEN>
<TOKEN id="token-25-15" pos="word" morph="none" start_char="2790" end_char="2793">data</TOKEN>
<TOKEN id="token-25-16" pos="word" morph="none" start_char="2795" end_char="2797">was</TOKEN>
<TOKEN id="token-25-17" pos="word" morph="none" start_char="2799" end_char="2807">specified</TOKEN>
<TOKEN id="token-25-18" pos="punct" morph="none" start_char="2808" end_char="2808">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="2810" end_char="3048">
<ORIGINAL_TEXT>The first application from 2015 that you mentioned was never disclosed, but for patent law reasons it must also have referred to the mentioned inventions (otherwise a 'continuation in part' would not be possible)," Osterwalder said to AFP.</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="2810" end_char="2812">The</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="2814" end_char="2818">first</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="2820" end_char="2830">application</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="2832" end_char="2835">from</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="2837" end_char="2840">2015</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="2842" end_char="2845">that</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="2847" end_char="2849">you</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="2851" end_char="2859">mentioned</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="2861" end_char="2863">was</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="2865" end_char="2869">never</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="2871" end_char="2879">disclosed</TOKEN>
<TOKEN id="token-26-11" pos="punct" morph="none" start_char="2880" end_char="2880">,</TOKEN>
<TOKEN id="token-26-12" pos="word" morph="none" start_char="2882" end_char="2884">but</TOKEN>
<TOKEN id="token-26-13" pos="word" morph="none" start_char="2886" end_char="2888">for</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="2890" end_char="2895">patent</TOKEN>
<TOKEN id="token-26-15" pos="word" morph="none" start_char="2897" end_char="2899">law</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="2901" end_char="2907">reasons</TOKEN>
<TOKEN id="token-26-17" pos="word" morph="none" start_char="2909" end_char="2910">it</TOKEN>
<TOKEN id="token-26-18" pos="word" morph="none" start_char="2912" end_char="2915">must</TOKEN>
<TOKEN id="token-26-19" pos="word" morph="none" start_char="2917" end_char="2920">also</TOKEN>
<TOKEN id="token-26-20" pos="word" morph="none" start_char="2922" end_char="2925">have</TOKEN>
<TOKEN id="token-26-21" pos="word" morph="none" start_char="2927" end_char="2934">referred</TOKEN>
<TOKEN id="token-26-22" pos="word" morph="none" start_char="2936" end_char="2937">to</TOKEN>
<TOKEN id="token-26-23" pos="word" morph="none" start_char="2939" end_char="2941">the</TOKEN>
<TOKEN id="token-26-24" pos="word" morph="none" start_char="2943" end_char="2951">mentioned</TOKEN>
<TOKEN id="token-26-25" pos="word" morph="none" start_char="2953" end_char="2962">inventions</TOKEN>
<TOKEN id="token-26-26" pos="punct" morph="none" start_char="2964" end_char="2964">(</TOKEN>
<TOKEN id="token-26-27" pos="word" morph="none" start_char="2965" end_char="2973">otherwise</TOKEN>
<TOKEN id="token-26-28" pos="word" morph="none" start_char="2975" end_char="2975">a</TOKEN>
<TOKEN id="token-26-29" pos="punct" morph="none" start_char="2977" end_char="2977">'</TOKEN>
<TOKEN id="token-26-30" pos="word" morph="none" start_char="2978" end_char="2989">continuation</TOKEN>
<TOKEN id="token-26-31" pos="word" morph="none" start_char="2991" end_char="2992">in</TOKEN>
<TOKEN id="token-26-32" pos="word" morph="none" start_char="2994" end_char="2997">part</TOKEN>
<TOKEN id="token-26-33" pos="punct" morph="none" start_char="2998" end_char="2998">'</TOKEN>
<TOKEN id="token-26-34" pos="word" morph="none" start_char="3000" end_char="3004">would</TOKEN>
<TOKEN id="token-26-35" pos="word" morph="none" start_char="3006" end_char="3008">not</TOKEN>
<TOKEN id="token-26-36" pos="word" morph="none" start_char="3010" end_char="3011">be</TOKEN>
<TOKEN id="token-26-37" pos="word" morph="none" start_char="3013" end_char="3020">possible</TOKEN>
<TOKEN id="token-26-38" pos="punct" morph="none" start_char="3021" end_char="3023">),"</TOKEN>
<TOKEN id="token-26-39" pos="word" morph="none" start_char="3025" end_char="3035">Osterwalder</TOKEN>
<TOKEN id="token-26-40" pos="word" morph="none" start_char="3037" end_char="3040">said</TOKEN>
<TOKEN id="token-26-41" pos="word" morph="none" start_char="3042" end_char="3043">to</TOKEN>
<TOKEN id="token-26-42" pos="word" morph="none" start_char="3045" end_char="3047">AFP</TOKEN>
<TOKEN id="token-26-43" pos="punct" morph="none" start_char="3048" end_char="3048">.</TOKEN>
</SEG>
<SEG id="segment-27" start_char="3051" end_char="3170">
<ORIGINAL_TEXT>Also, on Google patent, it was observed that the recent application for patent is in pending state and not yet approved.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="3051" end_char="3054">Also</TOKEN>
<TOKEN id="token-27-1" pos="punct" morph="none" start_char="3055" end_char="3055">,</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="3057" end_char="3058">on</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="3060" end_char="3065">Google</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="3067" end_char="3072">patent</TOKEN>
<TOKEN id="token-27-5" pos="punct" morph="none" start_char="3073" end_char="3073">,</TOKEN>
<TOKEN id="token-27-6" pos="word" morph="none" start_char="3075" end_char="3076">it</TOKEN>
<TOKEN id="token-27-7" pos="word" morph="none" start_char="3078" end_char="3080">was</TOKEN>
<TOKEN id="token-27-8" pos="word" morph="none" start_char="3082" end_char="3089">observed</TOKEN>
<TOKEN id="token-27-9" pos="word" morph="none" start_char="3091" end_char="3094">that</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="3096" end_char="3098">the</TOKEN>
<TOKEN id="token-27-11" pos="word" morph="none" start_char="3100" end_char="3105">recent</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="3107" end_char="3117">application</TOKEN>
<TOKEN id="token-27-13" pos="word" morph="none" start_char="3119" end_char="3121">for</TOKEN>
<TOKEN id="token-27-14" pos="word" morph="none" start_char="3123" end_char="3128">patent</TOKEN>
<TOKEN id="token-27-15" pos="word" morph="none" start_char="3130" end_char="3131">is</TOKEN>
<TOKEN id="token-27-16" pos="word" morph="none" start_char="3133" end_char="3134">in</TOKEN>
<TOKEN id="token-27-17" pos="word" morph="none" start_char="3136" end_char="3142">pending</TOKEN>
<TOKEN id="token-27-18" pos="word" morph="none" start_char="3144" end_char="3148">state</TOKEN>
<TOKEN id="token-27-19" pos="word" morph="none" start_char="3150" end_char="3152">and</TOKEN>
<TOKEN id="token-27-20" pos="word" morph="none" start_char="3154" end_char="3156">not</TOKEN>
<TOKEN id="token-27-21" pos="word" morph="none" start_char="3158" end_char="3160">yet</TOKEN>
<TOKEN id="token-27-22" pos="word" morph="none" start_char="3162" end_char="3169">approved</TOKEN>
<TOKEN id="token-27-23" pos="punct" morph="none" start_char="3170" end_char="3170">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="3173" end_char="3323">
<ORIGINAL_TEXT>Thus, it was clear that an old patent by Rothschild is being linked to the current situation of Covid-19 and is being presented with the wrong context.</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="3173" end_char="3176">Thus</TOKEN>
<TOKEN id="token-28-1" pos="punct" morph="none" start_char="3177" end_char="3177">,</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="3179" end_char="3180">it</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="3182" end_char="3184">was</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="3186" end_char="3190">clear</TOKEN>
<TOKEN id="token-28-5" pos="word" morph="none" start_char="3192" end_char="3195">that</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="3197" end_char="3198">an</TOKEN>
<TOKEN id="token-28-7" pos="word" morph="none" start_char="3200" end_char="3202">old</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="3204" end_char="3209">patent</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="3211" end_char="3212">by</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="3214" end_char="3223">Rothschild</TOKEN>
<TOKEN id="token-28-11" pos="word" morph="none" start_char="3225" end_char="3226">is</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="3228" end_char="3232">being</TOKEN>
<TOKEN id="token-28-13" pos="word" morph="none" start_char="3234" end_char="3239">linked</TOKEN>
<TOKEN id="token-28-14" pos="word" morph="none" start_char="3241" end_char="3242">to</TOKEN>
<TOKEN id="token-28-15" pos="word" morph="none" start_char="3244" end_char="3246">the</TOKEN>
<TOKEN id="token-28-16" pos="word" morph="none" start_char="3248" end_char="3254">current</TOKEN>
<TOKEN id="token-28-17" pos="word" morph="none" start_char="3256" end_char="3264">situation</TOKEN>
<TOKEN id="token-28-18" pos="word" morph="none" start_char="3266" end_char="3267">of</TOKEN>
<TOKEN id="token-28-19" pos="unknown" morph="none" start_char="3269" end_char="3276">Covid-19</TOKEN>
<TOKEN id="token-28-20" pos="word" morph="none" start_char="3278" end_char="3280">and</TOKEN>
<TOKEN id="token-28-21" pos="word" morph="none" start_char="3282" end_char="3283">is</TOKEN>
<TOKEN id="token-28-22" pos="word" morph="none" start_char="3285" end_char="3289">being</TOKEN>
<TOKEN id="token-28-23" pos="word" morph="none" start_char="3291" end_char="3299">presented</TOKEN>
<TOKEN id="token-28-24" pos="word" morph="none" start_char="3301" end_char="3304">with</TOKEN>
<TOKEN id="token-28-25" pos="word" morph="none" start_char="3306" end_char="3308">the</TOKEN>
<TOKEN id="token-28-26" pos="word" morph="none" start_char="3310" end_char="3314">wrong</TOKEN>
<TOKEN id="token-28-27" pos="word" morph="none" start_char="3316" end_char="3322">context</TOKEN>
<TOKEN id="token-28-28" pos="punct" morph="none" start_char="3323" end_char="3323">.</TOKEN>
</SEG>
<SEG id="segment-29" start_char="3325" end_char="3349">
<ORIGINAL_TEXT>Thus, the claim is false.</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="3325" end_char="3328">Thus</TOKEN>
<TOKEN id="token-29-1" pos="punct" morph="none" start_char="3329" end_char="3329">,</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="3331" end_char="3333">the</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="3335" end_char="3339">claim</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="3341" end_char="3342">is</TOKEN>
<TOKEN id="token-29-5" pos="word" morph="none" start_char="3344" end_char="3348">false</TOKEN>
<TOKEN id="token-29-6" pos="punct" morph="none" start_char="3349" end_char="3349">.</TOKEN>
</SEG>
<SEG id="segment-30" start_char="3352" end_char="3491">
<ORIGINAL_TEXT>If you have any news that you believe needs to be fact-checked, please email us at factcheck@thelogicalindian.com or WhatsApp at 6364000343.</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="3352" end_char="3353">If</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="3355" end_char="3357">you</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="3359" end_char="3362">have</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="3364" end_char="3366">any</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="3368" end_char="3371">news</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="3373" end_char="3376">that</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="3378" end_char="3380">you</TOKEN>
<TOKEN id="token-30-7" pos="word" morph="none" start_char="3382" end_char="3388">believe</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="3390" end_char="3394">needs</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="3396" end_char="3397">to</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="3399" end_char="3400">be</TOKEN>
<TOKEN id="token-30-11" pos="unknown" morph="none" start_char="3402" end_char="3413">fact-checked</TOKEN>
<TOKEN id="token-30-12" pos="punct" morph="none" start_char="3414" end_char="3414">,</TOKEN>
<TOKEN id="token-30-13" pos="word" morph="none" start_char="3416" end_char="3421">please</TOKEN>
<TOKEN id="token-30-14" pos="word" morph="none" start_char="3423" end_char="3427">email</TOKEN>
<TOKEN id="token-30-15" pos="word" morph="none" start_char="3429" end_char="3430">us</TOKEN>
<TOKEN id="token-30-16" pos="word" morph="none" start_char="3432" end_char="3433">at</TOKEN>
<TOKEN id="token-30-17" pos="unknown" morph="none" start_char="3435" end_char="3464">factcheck@thelogicalindian.com</TOKEN>
<TOKEN id="token-30-18" pos="word" morph="none" start_char="3466" end_char="3467">or</TOKEN>
<TOKEN id="token-30-19" pos="word" morph="none" start_char="3469" end_char="3476">WhatsApp</TOKEN>
<TOKEN id="token-30-20" pos="word" morph="none" start_char="3478" end_char="3479">at</TOKEN>
<TOKEN id="token-30-21" pos="word" morph="none" start_char="3481" end_char="3490">6364000343</TOKEN>
<TOKEN id="token-30-22" pos="punct" morph="none" start_char="3491" end_char="3491">.</TOKEN>
</SEG>
<SEG id="segment-31" start_char="3495" end_char="3643">
<ORIGINAL_TEXT>Claim Review : The provisional application for system and method for testing Covid-19 was already filed in 2015, 5 years before coronavirus pandemic.</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="word" morph="none" start_char="3495" end_char="3499">Claim</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="3501" end_char="3506">Review</TOKEN>
<TOKEN id="token-31-2" pos="punct" morph="none" start_char="3508" end_char="3508">:</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="3510" end_char="3512">The</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="3514" end_char="3524">provisional</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="3526" end_char="3536">application</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="3538" end_char="3540">for</TOKEN>
<TOKEN id="token-31-7" pos="word" morph="none" start_char="3542" end_char="3547">system</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="3549" end_char="3551">and</TOKEN>
<TOKEN id="token-31-9" pos="word" morph="none" start_char="3553" end_char="3558">method</TOKEN>
<TOKEN id="token-31-10" pos="word" morph="none" start_char="3560" end_char="3562">for</TOKEN>
<TOKEN id="token-31-11" pos="word" morph="none" start_char="3564" end_char="3570">testing</TOKEN>
<TOKEN id="token-31-12" pos="unknown" morph="none" start_char="3572" end_char="3579">Covid-19</TOKEN>
<TOKEN id="token-31-13" pos="word" morph="none" start_char="3581" end_char="3583">was</TOKEN>
<TOKEN id="token-31-14" pos="word" morph="none" start_char="3585" end_char="3591">already</TOKEN>
<TOKEN id="token-31-15" pos="word" morph="none" start_char="3593" end_char="3597">filed</TOKEN>
<TOKEN id="token-31-16" pos="word" morph="none" start_char="3599" end_char="3600">in</TOKEN>
<TOKEN id="token-31-17" pos="word" morph="none" start_char="3602" end_char="3605">2015</TOKEN>
<TOKEN id="token-31-18" pos="punct" morph="none" start_char="3606" end_char="3606">,</TOKEN>
<TOKEN id="token-31-19" pos="word" morph="none" start_char="3608" end_char="3608">5</TOKEN>
<TOKEN id="token-31-20" pos="word" morph="none" start_char="3610" end_char="3614">years</TOKEN>
<TOKEN id="token-31-21" pos="word" morph="none" start_char="3616" end_char="3621">before</TOKEN>
<TOKEN id="token-31-22" pos="word" morph="none" start_char="3623" end_char="3633">coronavirus</TOKEN>
<TOKEN id="token-31-23" pos="word" morph="none" start_char="3635" end_char="3642">pandemic</TOKEN>
<TOKEN id="token-31-24" pos="punct" morph="none" start_char="3643" end_char="3643">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
