<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="ukr">
<DOC id="L0C049PEV" lang="ukr" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="1370" raw_text_md5="4944d50f052cc533bc63afe392b8ec10">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="77">
<ORIGINAL_TEXT>No, los pacientes asintomáticos no son inmunes y pueden contagiar coronavirus</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="2">No</TOKEN>
<TOKEN id="token-0-1" pos="punct" morph="none" start_char="3" end_char="3">,</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="5" end_char="7">los</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="9" end_char="17">pacientes</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="19" end_char="31">asintomáticos</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="33" end_char="34">no</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="36" end_char="38">son</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="40" end_char="46">inmunes</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="48" end_char="48">y</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="50" end_char="55">pueden</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="57" end_char="65">contagiar</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="67" end_char="77">coronavirus</TOKEN>
</SEG>
<SEG id="segment-1" start_char="81" end_char="211">
<ORIGINAL_TEXT>you wont be happy until everyone is in a tiny box DEAD OR ALIVE – a miserable existence you are creating with less and less freedom</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="81" end_char="83">you</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="85" end_char="88">wont</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="90" end_char="91">be</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="93" end_char="97">happy</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="99" end_char="103">until</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="105" end_char="112">everyone</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="114" end_char="115">is</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="117" end_char="118">in</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="120" end_char="120">a</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="122" end_char="125">tiny</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="127" end_char="129">box</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="131" end_char="134">DEAD</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="136" end_char="137">OR</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="139" end_char="143">ALIVE</TOKEN>
<TOKEN id="token-1-14" pos="punct" morph="none" start_char="145" end_char="145">–</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="147" end_char="147">a</TOKEN>
<TOKEN id="token-1-16" pos="word" morph="none" start_char="149" end_char="157">miserable</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="159" end_char="167">existence</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="169" end_char="171">you</TOKEN>
<TOKEN id="token-1-19" pos="word" morph="none" start_char="173" end_char="175">are</TOKEN>
<TOKEN id="token-1-20" pos="word" morph="none" start_char="177" end_char="184">creating</TOKEN>
<TOKEN id="token-1-21" pos="word" morph="none" start_char="186" end_char="189">with</TOKEN>
<TOKEN id="token-1-22" pos="word" morph="none" start_char="191" end_char="194">less</TOKEN>
<TOKEN id="token-1-23" pos="word" morph="none" start_char="196" end_char="198">and</TOKEN>
<TOKEN id="token-1-24" pos="word" morph="none" start_char="200" end_char="203">less</TOKEN>
<TOKEN id="token-1-25" pos="word" morph="none" start_char="205" end_char="211">freedom</TOKEN>
</SEG>
<SEG id="segment-2" start_char="215" end_char="274">
<ORIGINAL_TEXT>Muchas gracias por la ayuda, enseñando la información falsa.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="215" end_char="220">Muchas</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="222" end_char="228">gracias</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="230" end_char="232">por</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="234" end_char="235">la</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="237" end_char="241">ayuda</TOKEN>
<TOKEN id="token-2-5" pos="punct" morph="none" start_char="242" end_char="242">,</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="244" end_char="252">enseñando</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="254" end_char="255">la</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="257" end_char="267">información</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="269" end_char="273">falsa</TOKEN>
<TOKEN id="token-2-10" pos="punct" morph="none" start_char="274" end_char="274">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="276" end_char="308">
<ORIGINAL_TEXT>Disculpa y chequeare las próximas</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="276" end_char="283">Disculpa</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="285" end_char="285">y</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="287" end_char="295">chequeare</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="297" end_char="299">las</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="301" end_char="308">próximas</TOKEN>
</SEG>
<SEG id="segment-4" start_char="312" end_char="320">
<ORIGINAL_TEXT>Excelente</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="312" end_char="320">Excelente</TOKEN>
</SEG>
<SEG id="segment-5" start_char="324" end_char="401">
<ORIGINAL_TEXT>Me parece muy bien que tengan controladas las noticias falsas.gcias y adelante</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="324" end_char="325">Me</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="327" end_char="332">parece</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="334" end_char="336">muy</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="338" end_char="341">bien</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="343" end_char="345">que</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="347" end_char="352">tengan</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="354" end_char="364">controladas</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="366" end_char="368">las</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="370" end_char="377">noticias</TOKEN>
<TOKEN id="token-5-9" pos="unknown" morph="none" start_char="379" end_char="390">falsas.gcias</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="392" end_char="392">y</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="394" end_char="401">adelante</TOKEN>
</SEG>
<SEG id="segment-6" start_char="405" end_char="618">
<ORIGINAL_TEXT>Yo siempre he estado condiente de que el paciente asintomatico es el que no presenta ninguno de los síntomas característicos del covid 19, pero que si pueden contagiar, no se porque dicen que yo apoyo lo contrario.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="405" end_char="406">Yo</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="408" end_char="414">siempre</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="416" end_char="417">he</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="419" end_char="424">estado</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="426" end_char="434">condiente</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="436" end_char="437">de</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="439" end_char="441">que</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="443" end_char="444">el</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="446" end_char="453">paciente</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="455" end_char="466">asintomatico</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="468" end_char="469">es</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="471" end_char="472">el</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="474" end_char="476">que</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="478" end_char="479">no</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="481" end_char="488">presenta</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="490" end_char="496">ninguno</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="498" end_char="499">de</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="501" end_char="503">los</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="505" end_char="512">síntomas</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="514" end_char="528">característicos</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="530" end_char="532">del</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="534" end_char="538">covid</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="540" end_char="541">19</TOKEN>
<TOKEN id="token-6-23" pos="punct" morph="none" start_char="542" end_char="542">,</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="544" end_char="547">pero</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="549" end_char="551">que</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="553" end_char="554">si</TOKEN>
<TOKEN id="token-6-27" pos="word" morph="none" start_char="556" end_char="561">pueden</TOKEN>
<TOKEN id="token-6-28" pos="word" morph="none" start_char="563" end_char="571">contagiar</TOKEN>
<TOKEN id="token-6-29" pos="punct" morph="none" start_char="572" end_char="572">,</TOKEN>
<TOKEN id="token-6-30" pos="word" morph="none" start_char="574" end_char="575">no</TOKEN>
<TOKEN id="token-6-31" pos="word" morph="none" start_char="577" end_char="578">se</TOKEN>
<TOKEN id="token-6-32" pos="word" morph="none" start_char="580" end_char="585">porque</TOKEN>
<TOKEN id="token-6-33" pos="word" morph="none" start_char="587" end_char="591">dicen</TOKEN>
<TOKEN id="token-6-34" pos="word" morph="none" start_char="593" end_char="595">que</TOKEN>
<TOKEN id="token-6-35" pos="word" morph="none" start_char="597" end_char="598">yo</TOKEN>
<TOKEN id="token-6-36" pos="word" morph="none" start_char="600" end_char="604">apoyo</TOKEN>
<TOKEN id="token-6-37" pos="word" morph="none" start_char="606" end_char="607">lo</TOKEN>
<TOKEN id="token-6-38" pos="word" morph="none" start_char="609" end_char="617">contrario</TOKEN>
<TOKEN id="token-6-39" pos="punct" morph="none" start_char="618" end_char="618">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="620" end_char="726">
<ORIGINAL_TEXT>No me acuerdo de haber publicado ese reportaje, si lo hice disculpen, a lo mejor lo hice inconscientemente.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="620" end_char="621">No</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="623" end_char="624">me</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="626" end_char="632">acuerdo</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="634" end_char="635">de</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="637" end_char="641">haber</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="643" end_char="651">publicado</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="653" end_char="655">ese</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="657" end_char="665">reportaje</TOKEN>
<TOKEN id="token-7-8" pos="punct" morph="none" start_char="666" end_char="666">,</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="668" end_char="669">si</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="671" end_char="672">lo</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="674" end_char="677">hice</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="679" end_char="687">disculpen</TOKEN>
<TOKEN id="token-7-13" pos="punct" morph="none" start_char="688" end_char="688">,</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="690" end_char="690">a</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="692" end_char="693">lo</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="695" end_char="699">mejor</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="701" end_char="702">lo</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="704" end_char="707">hice</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="709" end_char="725">inconscientemente</TOKEN>
<TOKEN id="token-7-20" pos="punct" morph="none" start_char="726" end_char="726">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="730" end_char="836">
<ORIGINAL_TEXT>SERIA BUENO QUE ESTE MEDIO EXPLIQUE EL CONFLICTO DE INTERESES CON SUS FINANCIADORES COMO GOOGLE Y FACEBOOK.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="730" end_char="734">SERIA</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="736" end_char="740">BUENO</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="742" end_char="744">QUE</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="746" end_char="749">ESTE</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="751" end_char="755">MEDIO</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="757" end_char="764">EXPLIQUE</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="766" end_char="767">EL</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="769" end_char="777">CONFLICTO</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="779" end_char="780">DE</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="782" end_char="790">INTERESES</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="792" end_char="794">CON</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="796" end_char="798">SUS</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="800" end_char="812">FINANCIADORES</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="814" end_char="817">COMO</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="819" end_char="824">GOOGLE</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="826" end_char="826">Y</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="828" end_char="835">FACEBOOK</TOKEN>
<TOKEN id="token-8-17" pos="punct" morph="none" start_char="836" end_char="836">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="838" end_char="1004">
<ORIGINAL_TEXT>ADEMAS DEBERIA PRESENTAR EVIDENCIA CIENTIFICA PARA DESMENTIR ALGO ….EN LA REVISTA NATURE DEL DIA 20 DE NOVIEMBRE UN ESTUDIO DEMOSTRO QUE LOS ASINTOMATICOS NO CONTAGIAN</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="838" end_char="843">ADEMAS</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="845" end_char="851">DEBERIA</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="853" end_char="861">PRESENTAR</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="863" end_char="871">EVIDENCIA</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="873" end_char="882">CIENTIFICA</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="884" end_char="887">PARA</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="889" end_char="897">DESMENTIR</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="899" end_char="902">ALGO</TOKEN>
<TOKEN id="token-9-8" pos="punct" morph="none" start_char="904" end_char="905">….</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="906" end_char="907">EN</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="909" end_char="910">LA</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="912" end_char="918">REVISTA</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="920" end_char="925">NATURE</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="927" end_char="929">DEL</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="931" end_char="933">DIA</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="935" end_char="936">20</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="938" end_char="939">DE</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="941" end_char="949">NOVIEMBRE</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="951" end_char="952">UN</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="954" end_char="960">ESTUDIO</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="962" end_char="969">DEMOSTRO</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="971" end_char="973">QUE</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="975" end_char="977">LOS</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="979" end_char="991">ASINTOMATICOS</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="993" end_char="994">NO</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="996" end_char="1004">CONTAGIAN</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1008" end_char="1147">
<ORIGINAL_TEXT>No hay argumentos de rigor científico q confirmen q los asintomáticos transmiten el virus, salvó los mosquitos c vectores de una enfermedad.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1008" end_char="1009">No</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1011" end_char="1013">hay</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1015" end_char="1024">argumentos</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1026" end_char="1027">de</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1029" end_char="1033">rigor</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1035" end_char="1044">científico</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1046" end_char="1046">q</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1048" end_char="1056">confirmen</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1058" end_char="1058">q</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1060" end_char="1062">los</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1064" end_char="1076">asintomáticos</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1078" end_char="1087">transmiten</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1089" end_char="1090">el</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1092" end_char="1096">virus</TOKEN>
<TOKEN id="token-10-14" pos="punct" morph="none" start_char="1097" end_char="1097">,</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1099" end_char="1103">salvó</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1105" end_char="1107">los</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1109" end_char="1117">mosquitos</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1119" end_char="1119">c</TOKEN>
<TOKEN id="token-10-19" pos="word" morph="none" start_char="1121" end_char="1128">vectores</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="1130" end_char="1131">de</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="1133" end_char="1135">una</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="1137" end_char="1146">enfermedad</TOKEN>
<TOKEN id="token-10-23" pos="punct" morph="none" start_char="1147" end_char="1147">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1151" end_char="1244">
<ORIGINAL_TEXT>Y hay un par preguntando cómo se prueba que el asintomático sí transmite el virus.. increíble.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1151" end_char="1151">Y</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1153" end_char="1155">hay</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1157" end_char="1158">un</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1160" end_char="1162">par</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1164" end_char="1174">preguntando</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1176" end_char="1179">cómo</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1181" end_char="1182">se</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1184" end_char="1189">prueba</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1191" end_char="1193">que</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1195" end_char="1196">el</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1198" end_char="1209">asintomático</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1211" end_char="1212">sí</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1214" end_char="1222">transmite</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1224" end_char="1225">el</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1227" end_char="1231">virus</TOKEN>
<TOKEN id="token-11-15" pos="punct" morph="none" start_char="1232" end_char="1233">..</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1235" end_char="1243">increíble</TOKEN>
<TOKEN id="token-11-17" pos="punct" morph="none" start_char="1244" end_char="1244">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1246" end_char="1366">
<ORIGINAL_TEXT>Quizás porque hace un año vivimos los miles de contagios por día de gente que tuvo contacto con asintomáticos, no sé digo</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1246" end_char="1251">Quizás</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1253" end_char="1258">porque</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1260" end_char="1263">hace</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1265" end_char="1266">un</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1268" end_char="1270">año</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1272" end_char="1278">vivimos</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1280" end_char="1282">los</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1284" end_char="1288">miles</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1290" end_char="1291">de</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1293" end_char="1301">contagios</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1303" end_char="1305">por</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1307" end_char="1309">día</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1311" end_char="1312">de</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1314" end_char="1318">gente</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1320" end_char="1322">que</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1324" end_char="1327">tuvo</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1329" end_char="1336">contacto</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1338" end_char="1340">con</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="1342" end_char="1354">asintomáticos</TOKEN>
<TOKEN id="token-12-19" pos="punct" morph="none" start_char="1355" end_char="1355">,</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1357" end_char="1358">no</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1360" end_char="1361">sé</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1363" end_char="1366">digo</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
