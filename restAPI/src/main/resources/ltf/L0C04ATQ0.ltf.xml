<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATQ0" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="2025" raw_text_md5="6d13e487898a53495e0572c5ca9744dc">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="96">
<ORIGINAL_TEXT>China rechaza versión de que el coronavirus surgió en agosto y acusa intento de "desinformación"</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="5">China</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="7" end_char="13">rechaza</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="15" end_char="21">versión</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="23" end_char="24">de</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="26" end_char="28">que</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="30" end_char="31">el</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="33" end_char="43">coronavirus</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="45" end_char="50">surgió</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="52" end_char="53">en</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="55" end_char="60">agosto</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="62" end_char="62">y</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="64" end_char="68">acusa</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="70" end_char="76">intento</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="78" end_char="79">de</TOKEN>
<TOKEN id="token-0-14" pos="punct" morph="none" start_char="81" end_char="81">"</TOKEN>
<TOKEN id="token-0-15" pos="word" morph="none" start_char="82" end_char="95">desinformación</TOKEN>
<TOKEN id="token-0-16" pos="punct" morph="none" start_char="96" end_char="96">"</TOKEN>
</SEG>
<SEG id="segment-1" start_char="101" end_char="150">
<ORIGINAL_TEXT>Todo Avisos, para publicar sus avisos clasificados</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="101" end_char="104">Todo</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="106" end_char="111">Avisos</TOKEN>
<TOKEN id="token-1-2" pos="punct" morph="none" start_char="112" end_char="112">,</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="114" end_char="117">para</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="119" end_char="126">publicar</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="128" end_char="130">sus</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="132" end_char="137">avisos</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="139" end_char="150">clasificados</TOKEN>
</SEG>
<SEG id="segment-2" start_char="156" end_char="385">
<ORIGINAL_TEXT>El estudio preliminar de investigadores de las prestigiosas universidades de Boston y Harvard deja entrever que la epidemia pudo surgir en el verano de 2019 en Wuhan, metrópoli del centro de China que quedó en cuarentena en enero.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="156" end_char="157">El</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="159" end_char="165">estudio</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="167" end_char="176">preliminar</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="178" end_char="179">de</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="181" end_char="194">investigadores</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="196" end_char="197">de</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="199" end_char="201">las</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="203" end_char="214">prestigiosas</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="216" end_char="228">universidades</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="230" end_char="231">de</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="233" end_char="238">Boston</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="240" end_char="240">y</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="242" end_char="248">Harvard</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="250" end_char="253">deja</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="255" end_char="262">entrever</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="264" end_char="266">que</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="268" end_char="269">la</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="271" end_char="278">epidemia</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="280" end_char="283">pudo</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="285" end_char="290">surgir</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="292" end_char="293">en</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="295" end_char="296">el</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="298" end_char="303">verano</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="305" end_char="306">de</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="308" end_char="311">2019</TOKEN>
<TOKEN id="token-2-25" pos="word" morph="none" start_char="313" end_char="314">en</TOKEN>
<TOKEN id="token-2-26" pos="word" morph="none" start_char="316" end_char="320">Wuhan</TOKEN>
<TOKEN id="token-2-27" pos="punct" morph="none" start_char="321" end_char="321">,</TOKEN>
<TOKEN id="token-2-28" pos="word" morph="none" start_char="323" end_char="331">metrópoli</TOKEN>
<TOKEN id="token-2-29" pos="word" morph="none" start_char="333" end_char="335">del</TOKEN>
<TOKEN id="token-2-30" pos="word" morph="none" start_char="337" end_char="342">centro</TOKEN>
<TOKEN id="token-2-31" pos="word" morph="none" start_char="344" end_char="345">de</TOKEN>
<TOKEN id="token-2-32" pos="word" morph="none" start_char="347" end_char="351">China</TOKEN>
<TOKEN id="token-2-33" pos="word" morph="none" start_char="353" end_char="355">que</TOKEN>
<TOKEN id="token-2-34" pos="word" morph="none" start_char="357" end_char="361">quedó</TOKEN>
<TOKEN id="token-2-35" pos="word" morph="none" start_char="363" end_char="364">en</TOKEN>
<TOKEN id="token-2-36" pos="word" morph="none" start_char="366" end_char="375">cuarentena</TOKEN>
<TOKEN id="token-2-37" pos="word" morph="none" start_char="377" end_char="378">en</TOKEN>
<TOKEN id="token-2-38" pos="word" morph="none" start_char="380" end_char="384">enero</TOKEN>
<TOKEN id="token-2-39" pos="punct" morph="none" start_char="385" end_char="385">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="388" end_char="538">
<ORIGINAL_TEXT>Este estudio se basa en las imágenes de satélite que muestran una afluencia inhabitual en los estacionamientos de los hospitales en Wuhan desde agosto.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="388" end_char="391">Este</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="393" end_char="399">estudio</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="401" end_char="402">se</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="404" end_char="407">basa</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="409" end_char="410">en</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="412" end_char="414">las</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="416" end_char="423">imágenes</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="425" end_char="426">de</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="428" end_char="435">satélite</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="437" end_char="439">que</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="441" end_char="448">muestran</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="450" end_char="452">una</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="454" end_char="462">afluencia</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="464" end_char="473">inhabitual</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="475" end_char="476">en</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="478" end_char="480">los</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="482" end_char="497">estacionamientos</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="499" end_char="500">de</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="502" end_char="504">los</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="506" end_char="515">hospitales</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="517" end_char="518">en</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="520" end_char="524">Wuhan</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="526" end_char="530">desde</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="532" end_char="537">agosto</TOKEN>
<TOKEN id="token-3-24" pos="punct" morph="none" start_char="538" end_char="538">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="541" end_char="642">
<ORIGINAL_TEXT>Asimismo, muestra el aumento de las consultas de la palabra "tos" en el motor de búsqueda chino Baidu.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="541" end_char="548">Asimismo</TOKEN>
<TOKEN id="token-4-1" pos="punct" morph="none" start_char="549" end_char="549">,</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="551" end_char="557">muestra</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="559" end_char="560">el</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="562" end_char="568">aumento</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="570" end_char="571">de</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="573" end_char="575">las</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="577" end_char="585">consultas</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="587" end_char="588">de</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="590" end_char="591">la</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="593" end_char="599">palabra</TOKEN>
<TOKEN id="token-4-11" pos="punct" morph="none" start_char="601" end_char="601">"</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="602" end_char="604">tos</TOKEN>
<TOKEN id="token-4-13" pos="punct" morph="none" start_char="605" end_char="605">"</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="607" end_char="608">en</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="610" end_char="611">el</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="613" end_char="617">motor</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="619" end_char="620">de</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="622" end_char="629">búsqueda</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="631" end_char="635">chino</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="637" end_char="641">Baidu</TOKEN>
<TOKEN id="token-4-21" pos="punct" morph="none" start_char="642" end_char="642">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="645" end_char="839">
<ORIGINAL_TEXT>Este estudio, que todavía no ha sido publicado en ninguna revista científica, podría ser la prueba de que China habría escondido al mundo durante varios meses la existencia del nuevo coronavirus.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="645" end_char="648">Este</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="650" end_char="656">estudio</TOKEN>
<TOKEN id="token-5-2" pos="punct" morph="none" start_char="657" end_char="657">,</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="659" end_char="661">que</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="663" end_char="669">todavía</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="671" end_char="672">no</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="674" end_char="675">ha</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="677" end_char="680">sido</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="682" end_char="690">publicado</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="692" end_char="693">en</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="695" end_char="701">ninguna</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="703" end_char="709">revista</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="711" end_char="720">científica</TOKEN>
<TOKEN id="token-5-13" pos="punct" morph="none" start_char="721" end_char="721">,</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="723" end_char="728">podría</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="730" end_char="732">ser</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="734" end_char="735">la</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="737" end_char="742">prueba</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="744" end_char="745">de</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="747" end_char="749">que</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="751" end_char="755">China</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="757" end_char="762">habría</TOKEN>
<TOKEN id="token-5-22" pos="word" morph="none" start_char="764" end_char="772">escondido</TOKEN>
<TOKEN id="token-5-23" pos="word" morph="none" start_char="774" end_char="775">al</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="777" end_char="781">mundo</TOKEN>
<TOKEN id="token-5-25" pos="word" morph="none" start_char="783" end_char="789">durante</TOKEN>
<TOKEN id="token-5-26" pos="word" morph="none" start_char="791" end_char="796">varios</TOKEN>
<TOKEN id="token-5-27" pos="word" morph="none" start_char="798" end_char="802">meses</TOKEN>
<TOKEN id="token-5-28" pos="word" morph="none" start_char="804" end_char="805">la</TOKEN>
<TOKEN id="token-5-29" pos="word" morph="none" start_char="807" end_char="816">existencia</TOKEN>
<TOKEN id="token-5-30" pos="word" morph="none" start_char="818" end_char="820">del</TOKEN>
<TOKEN id="token-5-31" pos="word" morph="none" start_char="822" end_char="826">nuevo</TOKEN>
<TOKEN id="token-5-32" pos="word" morph="none" start_char="828" end_char="838">coronavirus</TOKEN>
<TOKEN id="token-5-33" pos="punct" morph="none" start_char="839" end_char="839">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="842" end_char="980">
<ORIGINAL_TEXT>Pero el ministerio chino de Relaciones Exteriores consideró el jueves que el estudio está "lleno de deficiencias" y "burdamente fabricado".</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="842" end_char="845">Pero</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="847" end_char="848">el</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="850" end_char="859">ministerio</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="861" end_char="865">chino</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="867" end_char="868">de</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="870" end_char="879">Relaciones</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="881" end_char="890">Exteriores</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="892" end_char="900">consideró</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="902" end_char="903">el</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="905" end_char="910">jueves</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="912" end_char="914">que</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="916" end_char="917">el</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="919" end_char="925">estudio</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="927" end_char="930">está</TOKEN>
<TOKEN id="token-6-14" pos="punct" morph="none" start_char="932" end_char="932">"</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="933" end_char="937">lleno</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="939" end_char="940">de</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="942" end_char="953">deficiencias</TOKEN>
<TOKEN id="token-6-18" pos="punct" morph="none" start_char="954" end_char="954">"</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="956" end_char="956">y</TOKEN>
<TOKEN id="token-6-20" pos="punct" morph="none" start_char="958" end_char="958">"</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="959" end_char="968">burdamente</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="970" end_char="978">fabricado</TOKEN>
<TOKEN id="token-6-23" pos="punct" morph="none" start_char="979" end_char="980">".</TOKEN>
</SEG>
<SEG id="segment-7" start_char="983" end_char="1168">
<ORIGINAL_TEXT>La portavoz del ministerio, Hua Chunying, ve en el mismo la prueba de la existencia de una campaña en Estados Unidos para "crear y diseminar deliberadamente desinformación contra China".</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="983" end_char="984">La</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="986" end_char="993">portavoz</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="995" end_char="997">del</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="999" end_char="1008">ministerio</TOKEN>
<TOKEN id="token-7-4" pos="punct" morph="none" start_char="1009" end_char="1009">,</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="1011" end_char="1013">Hua</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="1015" end_char="1022">Chunying</TOKEN>
<TOKEN id="token-7-7" pos="punct" morph="none" start_char="1023" end_char="1023">,</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="1025" end_char="1026">ve</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="1028" end_char="1029">en</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="1031" end_char="1032">el</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="1034" end_char="1038">mismo</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="1040" end_char="1041">la</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="1043" end_char="1048">prueba</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="1050" end_char="1051">de</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="1053" end_char="1054">la</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="1056" end_char="1065">existencia</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="1067" end_char="1068">de</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="1070" end_char="1072">una</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="1074" end_char="1080">campaña</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="1082" end_char="1083">en</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="1085" end_char="1091">Estados</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="1093" end_char="1098">Unidos</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="1100" end_char="1103">para</TOKEN>
<TOKEN id="token-7-24" pos="punct" morph="none" start_char="1105" end_char="1105">"</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="1106" end_char="1110">crear</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="1112" end_char="1112">y</TOKEN>
<TOKEN id="token-7-27" pos="word" morph="none" start_char="1114" end_char="1122">diseminar</TOKEN>
<TOKEN id="token-7-28" pos="word" morph="none" start_char="1124" end_char="1138">deliberadamente</TOKEN>
<TOKEN id="token-7-29" pos="word" morph="none" start_char="1140" end_char="1153">desinformación</TOKEN>
<TOKEN id="token-7-30" pos="word" morph="none" start_char="1155" end_char="1160">contra</TOKEN>
<TOKEN id="token-7-31" pos="word" morph="none" start_char="1162" end_char="1166">China</TOKEN>
<TOKEN id="token-7-32" pos="punct" morph="none" start_char="1167" end_char="1168">".</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1170" end_char="1336">
<ORIGINAL_TEXT>"Responsables políticos y medios estadounidenses actúan como si hubieran descubierto un tesoro, como si tuvieran la prueba de que China escondió la epidemia", lamentó.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="punct" morph="none" start_char="1170" end_char="1170">"</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="1171" end_char="1182">Responsables</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="1184" end_char="1192">políticos</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1194" end_char="1194">y</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1196" end_char="1201">medios</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1203" end_char="1217">estadounidenses</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="1219" end_char="1224">actúan</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="1226" end_char="1229">como</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1231" end_char="1232">si</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="1234" end_char="1241">hubieran</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="1243" end_char="1253">descubierto</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="1255" end_char="1256">un</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="1258" end_char="1263">tesoro</TOKEN>
<TOKEN id="token-8-13" pos="punct" morph="none" start_char="1264" end_char="1264">,</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="1266" end_char="1269">como</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="1271" end_char="1272">si</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="1274" end_char="1281">tuvieran</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="1283" end_char="1284">la</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="1286" end_char="1291">prueba</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="1293" end_char="1294">de</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="1296" end_char="1298">que</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="1300" end_char="1304">China</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="1306" end_char="1313">escondió</TOKEN>
<TOKEN id="token-8-23" pos="word" morph="none" start_char="1315" end_char="1316">la</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="1318" end_char="1325">epidemia</TOKEN>
<TOKEN id="token-8-25" pos="punct" morph="none" start_char="1326" end_char="1327">",</TOKEN>
<TOKEN id="token-8-26" pos="word" morph="none" start_char="1329" end_char="1335">lamentó</TOKEN>
<TOKEN id="token-8-27" pos="punct" morph="none" start_char="1336" end_char="1336">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1339" end_char="1521">
<ORIGINAL_TEXT>Según las autoridades chinas, el nuevo coronavirus fue detectado en diciembre y China compartió a principios de enero su código genético con la Organización Mundial de la Salud (OMS).</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1339" end_char="1343">Según</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1345" end_char="1347">las</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="1349" end_char="1359">autoridades</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1361" end_char="1366">chinas</TOKEN>
<TOKEN id="token-9-4" pos="punct" morph="none" start_char="1367" end_char="1367">,</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1369" end_char="1370">el</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1372" end_char="1376">nuevo</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1378" end_char="1388">coronavirus</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1390" end_char="1392">fue</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="1394" end_char="1402">detectado</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1404" end_char="1405">en</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1407" end_char="1415">diciembre</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="1417" end_char="1417">y</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1419" end_char="1423">China</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1425" end_char="1433">compartió</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1435" end_char="1435">a</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="1437" end_char="1446">principios</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1448" end_char="1449">de</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1451" end_char="1455">enero</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1457" end_char="1458">su</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1460" end_char="1465">código</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1467" end_char="1474">genético</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1476" end_char="1478">con</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="1480" end_char="1481">la</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1483" end_char="1494">Organización</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="1496" end_char="1502">Mundial</TOKEN>
<TOKEN id="token-9-26" pos="word" morph="none" start_char="1504" end_char="1505">de</TOKEN>
<TOKEN id="token-9-27" pos="word" morph="none" start_char="1507" end_char="1508">la</TOKEN>
<TOKEN id="token-9-28" pos="word" morph="none" start_char="1510" end_char="1514">Salud</TOKEN>
<TOKEN id="token-9-29" pos="punct" morph="none" start_char="1516" end_char="1516">(</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1517" end_char="1519">OMS</TOKEN>
<TOKEN id="token-9-31" pos="punct" morph="none" start_char="1520" end_char="1521">).</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1523" end_char="1625">
<ORIGINAL_TEXT>Estados Unidos y China se han enfrascado en los últimos meses en una batalla sobre el origen del virus.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1523" end_char="1529">Estados</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1531" end_char="1536">Unidos</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1538" end_char="1538">y</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1540" end_char="1544">China</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1546" end_char="1547">se</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1549" end_char="1551">han</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1553" end_char="1562">enfrascado</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1564" end_char="1565">en</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1567" end_char="1569">los</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1571" end_char="1577">últimos</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1579" end_char="1583">meses</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1585" end_char="1586">en</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1588" end_char="1590">una</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1592" end_char="1598">batalla</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1600" end_char="1604">sobre</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1606" end_char="1607">el</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1609" end_char="1614">origen</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1616" end_char="1618">del</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1620" end_char="1624">virus</TOKEN>
<TOKEN id="token-10-19" pos="punct" morph="none" start_char="1625" end_char="1625">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1627" end_char="1801">
<ORIGINAL_TEXT>Las autoridades estadounidense sugieren que pudo escapar de un laboratorio en Wuhan, mientras que Pekín dio a entender que pudo haber sido traído por soldados estadounidenses.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1627" end_char="1629">Las</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1631" end_char="1641">autoridades</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1643" end_char="1656">estadounidense</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1658" end_char="1665">sugieren</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1667" end_char="1669">que</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1671" end_char="1674">pudo</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1676" end_char="1682">escapar</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1684" end_char="1685">de</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1687" end_char="1688">un</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1690" end_char="1700">laboratorio</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1702" end_char="1703">en</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1705" end_char="1709">Wuhan</TOKEN>
<TOKEN id="token-11-12" pos="punct" morph="none" start_char="1710" end_char="1710">,</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1712" end_char="1719">mientras</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1721" end_char="1723">que</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1725" end_char="1729">Pekín</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1731" end_char="1733">dio</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1735" end_char="1735">a</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1737" end_char="1744">entender</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1746" end_char="1748">que</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1750" end_char="1753">pudo</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1755" end_char="1759">haber</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1761" end_char="1764">sido</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1766" end_char="1771">traído</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1773" end_char="1775">por</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1777" end_char="1784">soldados</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="1786" end_char="1800">estadounidenses</TOKEN>
<TOKEN id="token-11-27" pos="punct" morph="none" start_char="1801" end_char="1801">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1804" end_char="2021">
<ORIGINAL_TEXT>Pingüino Multimedia entrega este espacio a su público para la expresión personal de opiniones y comentarios, apelando al respeto entre los usuarios y desligándose por completo del contenido de los comentarios emitidos.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1804" end_char="1811">Pingüino</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1813" end_char="1822">Multimedia</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1824" end_char="1830">entrega</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1832" end_char="1835">este</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1837" end_char="1843">espacio</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1845" end_char="1845">a</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1847" end_char="1848">su</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1850" end_char="1856">público</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1858" end_char="1861">para</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1863" end_char="1864">la</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1866" end_char="1874">expresión</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1876" end_char="1883">personal</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1885" end_char="1886">de</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1888" end_char="1896">opiniones</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1898" end_char="1898">y</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1900" end_char="1910">comentarios</TOKEN>
<TOKEN id="token-12-16" pos="punct" morph="none" start_char="1911" end_char="1911">,</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1913" end_char="1920">apelando</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="1922" end_char="1923">al</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="1925" end_char="1931">respeto</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1933" end_char="1937">entre</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1939" end_char="1941">los</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1943" end_char="1950">usuarios</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1952" end_char="1952">y</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1954" end_char="1965">desligándose</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1967" end_char="1969">por</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="1971" end_char="1978">completo</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="1980" end_char="1982">del</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="1984" end_char="1992">contenido</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="1994" end_char="1995">de</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="1997" end_char="1999">los</TOKEN>
<TOKEN id="token-12-31" pos="word" morph="none" start_char="2001" end_char="2011">comentarios</TOKEN>
<TOKEN id="token-12-32" pos="word" morph="none" start_char="2013" end_char="2020">emitidos</TOKEN>
<TOKEN id="token-12-33" pos="punct" morph="none" start_char="2021" end_char="2021">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
