<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CAAW" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="15750" raw_text_md5="1c467e1f142e91812cbf4abc2563508b">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="29">
<ORIGINAL_TEXT>Where did covid 19 originate?</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="5">Where</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="7" end_char="9">did</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="11" end_char="15">covid</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="17" end_char="18">19</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="20" end_char="28">originate</TOKEN>
<TOKEN id="token-0-5" pos="punct" morph="none" start_char="29" end_char="29">?</TOKEN>
</SEG>
<SEG id="segment-1" start_char="33" end_char="150">
<ORIGINAL_TEXT>There have been numerous reports related to cases earlier then December of 2019 when China first identified the virus.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="33" end_char="37">There</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="39" end_char="42">have</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="44" end_char="47">been</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="49" end_char="56">numerous</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="58" end_char="64">reports</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="66" end_char="72">related</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="74" end_char="75">to</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="77" end_char="81">cases</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="83" end_char="89">earlier</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="91" end_char="94">then</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="96" end_char="103">December</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="105" end_char="106">of</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="108" end_char="111">2019</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="113" end_char="116">when</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="118" end_char="122">China</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="124" end_char="128">first</TOKEN>
<TOKEN id="token-1-16" pos="word" morph="none" start_char="130" end_char="139">identified</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="141" end_char="143">the</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="145" end_char="149">virus</TOKEN>
<TOKEN id="token-1-19" pos="punct" morph="none" start_char="150" end_char="150">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="153" end_char="235">
<ORIGINAL_TEXT>This cambridge study suggested Covid had been in China atleast since September 2019</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="153" end_char="156">This</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="158" end_char="166">cambridge</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="168" end_char="172">study</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="174" end_char="182">suggested</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="184" end_char="188">Covid</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="190" end_char="192">had</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="194" end_char="197">been</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="199" end_char="200">in</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="202" end_char="206">China</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="208" end_char="214">atleast</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="216" end_char="220">since</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="222" end_char="230">September</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="232" end_char="235">2019</TOKEN>
</SEG>
<SEG id="segment-3" start_char="238" end_char="305">
<ORIGINAL_TEXT>Yet there is also evidence that covid was in Brazil in November 2019</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="238" end_char="240">Yet</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="242" end_char="246">there</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="248" end_char="249">is</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="251" end_char="254">also</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="256" end_char="263">evidence</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="265" end_char="268">that</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="270" end_char="274">covid</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="276" end_char="278">was</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="280" end_char="281">in</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="283" end_char="288">Brazil</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="290" end_char="291">in</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="293" end_char="300">November</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="302" end_char="305">2019</TOKEN>
</SEG>
<SEG id="segment-4" start_char="308" end_char="330">
<ORIGINAL_TEXT>France in November 2019</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="308" end_char="313">France</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="315" end_char="316">in</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="318" end_char="325">November</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="327" end_char="330">2019</TOKEN>
</SEG>
<SEG id="segment-5" start_char="333" end_char="364">
<ORIGINAL_TEXT>And spain as early as March 2019</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="333" end_char="335">And</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="337" end_char="341">spain</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="343" end_char="344">as</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="346" end_char="350">early</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="352" end_char="353">as</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="355" end_char="359">March</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="361" end_char="364">2019</TOKEN>
</SEG>
<SEG id="segment-6" start_char="367" end_char="436">
<ORIGINAL_TEXT>So is Spain the most likely source as it is now the earliest sighting?</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="367" end_char="368">So</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="370" end_char="371">is</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="373" end_char="377">Spain</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="379" end_char="381">the</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="383" end_char="386">most</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="388" end_char="393">likely</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="395" end_char="400">source</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="402" end_char="403">as</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="405" end_char="406">it</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="408" end_char="409">is</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="411" end_char="413">now</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="415" end_char="417">the</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="419" end_char="426">earliest</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="428" end_char="435">sighting</TOKEN>
<TOKEN id="token-6-14" pos="punct" morph="none" start_char="436" end_char="436">?</TOKEN>
</SEG>
<SEG id="segment-7" start_char="440" end_char="531">
<ORIGINAL_TEXT>Fort Detrick was closed in July 2019 because of 6 cases of mishandling biological materials.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="440" end_char="443">Fort</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="445" end_char="451">Detrick</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="453" end_char="455">was</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="457" end_char="462">closed</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="464" end_char="465">in</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="467" end_char="470">July</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="472" end_char="475">2019</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="477" end_char="483">because</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="485" end_char="486">of</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="488" end_char="488">6</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="490" end_char="494">cases</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="496" end_char="497">of</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="499" end_char="509">mishandling</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="511" end_char="520">biological</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="522" end_char="530">materials</TOKEN>
<TOKEN id="token-7-15" pos="punct" morph="none" start_char="531" end_char="531">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="533" end_char="563">
<ORIGINAL_TEXT>There were also leaks of agents</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="533" end_char="537">There</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="539" end_char="542">were</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="544" end_char="547">also</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="549" end_char="553">leaks</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="555" end_char="556">of</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="558" end_char="563">agents</TOKEN>
</SEG>
<SEG id="segment-9" start_char="566" end_char="659">
<ORIGINAL_TEXT>Considering it was closed after the leaks happened, could fort Detrick be the original source?</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="566" end_char="576">Considering</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="578" end_char="579">it</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="581" end_char="583">was</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="585" end_char="590">closed</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="592" end_char="596">after</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="598" end_char="600">the</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="602" end_char="606">leaks</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="608" end_char="615">happened</TOKEN>
<TOKEN id="token-9-8" pos="punct" morph="none" start_char="616" end_char="616">,</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="618" end_char="622">could</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="624" end_char="627">fort</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="629" end_char="635">Detrick</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="637" end_char="638">be</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="640" end_char="642">the</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="644" end_char="651">original</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="653" end_char="658">source</TOKEN>
<TOKEN id="token-9-16" pos="punct" morph="none" start_char="659" end_char="659">?</TOKEN>
</SEG>
<SEG id="segment-10" start_char="662" end_char="736">
<ORIGINAL_TEXT>https://www.nytimes.com/2019/08/05/health/germs-fort-detrick-biohazard.html</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="url" morph="none" start_char="662" end_char="736">https://www.nytimes.com/2019/08/05/health/germs-fort-detrick-biohazard.html</TOKEN>
</SEG>
<SEG id="segment-11" start_char="739" end_char="939">
<ORIGINAL_TEXT>Most shockingly a residential home near fort Detrick suffered a respiratory outbreak https://abcnews.go.com/US/respiratory-outbreak-investigated-retirement-community-54-residents-fall/story?id=64275865</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="739" end_char="742">Most</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="744" end_char="753">shockingly</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="755" end_char="755">a</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="757" end_char="767">residential</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="769" end_char="772">home</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="774" end_char="777">near</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="779" end_char="782">fort</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="784" end_char="790">Detrick</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="792" end_char="799">suffered</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="801" end_char="801">a</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="803" end_char="813">respiratory</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="815" end_char="822">outbreak</TOKEN>
<TOKEN id="token-11-12" pos="url" morph="none" start_char="824" end_char="939">https://abcnews.go.com/US/respiratory-outbreak-investigated-retirement-community-54-residents-fall/story?id=64275865</TOKEN>
</SEG>
<SEG id="segment-12" start_char="942" end_char="1001">
<ORIGINAL_TEXT>"He said the outbreak began with the first case on June 30."</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="punct" morph="none" start_char="942" end_char="942">"</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="943" end_char="944">He</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="946" end_char="949">said</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="951" end_char="953">the</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="955" end_char="962">outbreak</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="964" end_char="968">began</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="970" end_char="973">with</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="975" end_char="977">the</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="979" end_char="983">first</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="985" end_char="988">case</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="990" end_char="991">on</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="993" end_char="996">June</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="998" end_char="999">30</TOKEN>
<TOKEN id="token-12-13" pos="punct" morph="none" start_char="1000" end_char="1001">."</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1004" end_char="1100">
<ORIGINAL_TEXT>This is a few weeks before fort Detrick was closed due to mis management of biological materials.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1004" end_char="1007">This</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1009" end_char="1010">is</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1012" end_char="1012">a</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1014" end_char="1016">few</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1018" end_char="1022">weeks</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1024" end_char="1029">before</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1031" end_char="1034">fort</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1036" end_char="1042">Detrick</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1044" end_char="1046">was</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1048" end_char="1053">closed</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1055" end_char="1057">due</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1059" end_char="1060">to</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1062" end_char="1064">mis</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="1066" end_char="1075">management</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="1077" end_char="1078">of</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="1080" end_char="1089">biological</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="1091" end_char="1099">materials</TOKEN>
<TOKEN id="token-13-17" pos="punct" morph="none" start_char="1100" end_char="1100">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1103" end_char="1158">
<ORIGINAL_TEXT>The retirement home is a 4 hour drive from the facility.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1103" end_char="1105">The</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1107" end_char="1116">retirement</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1118" end_char="1121">home</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1123" end_char="1124">is</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1126" end_char="1126">a</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1128" end_char="1128">4</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1130" end_char="1133">hour</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1135" end_char="1139">drive</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1141" end_char="1144">from</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1146" end_char="1148">the</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1150" end_char="1157">facility</TOKEN>
<TOKEN id="token-14-11" pos="punct" morph="none" start_char="1158" end_char="1158">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1161" end_char="1298">
<ORIGINAL_TEXT>We know they closed fort Detrick in July due to.mis management of materials, we dont know how long that miss management was happening for.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1161" end_char="1162">We</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1164" end_char="1167">know</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1169" end_char="1172">they</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="1174" end_char="1179">closed</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1181" end_char="1184">fort</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1186" end_char="1192">Detrick</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="1194" end_char="1195">in</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1197" end_char="1200">July</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1202" end_char="1204">due</TOKEN>
<TOKEN id="token-15-9" pos="unknown" morph="none" start_char="1206" end_char="1211">to.mis</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1213" end_char="1222">management</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="1224" end_char="1225">of</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="1227" end_char="1235">materials</TOKEN>
<TOKEN id="token-15-13" pos="punct" morph="none" start_char="1236" end_char="1236">,</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="1238" end_char="1239">we</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="1241" end_char="1244">dont</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="1246" end_char="1249">know</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="1251" end_char="1253">how</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="1255" end_char="1258">long</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="1260" end_char="1263">that</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="1265" end_char="1268">miss</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="1270" end_char="1279">management</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="1281" end_char="1283">was</TOKEN>
<TOKEN id="token-15-23" pos="word" morph="none" start_char="1285" end_char="1293">happening</TOKEN>
<TOKEN id="token-15-24" pos="word" morph="none" start_char="1295" end_char="1297">for</TOKEN>
<TOKEN id="token-15-25" pos="punct" morph="none" start_char="1298" end_char="1298">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1300" end_char="1474">
<ORIGINAL_TEXT>Details as to what actually was released and mishandled is also sketchy as under national security the America government has refused to comment or allow a open investigation.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1300" end_char="1306">Details</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1308" end_char="1309">as</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1311" end_char="1312">to</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1314" end_char="1317">what</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1319" end_char="1326">actually</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1328" end_char="1330">was</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="1332" end_char="1339">released</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="1341" end_char="1343">and</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="1345" end_char="1354">mishandled</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="1356" end_char="1357">is</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="1359" end_char="1362">also</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="1364" end_char="1370">sketchy</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="1372" end_char="1373">as</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="1375" end_char="1379">under</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="1381" end_char="1388">national</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="1390" end_char="1397">security</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="1399" end_char="1401">the</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="1403" end_char="1409">America</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="1411" end_char="1420">government</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="1422" end_char="1424">has</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="1426" end_char="1432">refused</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="1434" end_char="1435">to</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="1437" end_char="1443">comment</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="1445" end_char="1446">or</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="1448" end_char="1452">allow</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="1454" end_char="1454">a</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="1456" end_char="1459">open</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="1461" end_char="1473">investigation</TOKEN>
<TOKEN id="token-16-28" pos="punct" morph="none" start_char="1474" end_char="1474">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="1477" end_char="1587">
<ORIGINAL_TEXT>Another question:- We know Donald Trump claimed to have seen evidence that covid 19 had come from a laboratory.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="1477" end_char="1483">Another</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="1485" end_char="1492">question</TOKEN>
<TOKEN id="token-17-2" pos="punct" morph="none" start_char="1493" end_char="1494">:-</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="1496" end_char="1497">We</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="1499" end_char="1502">know</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="1504" end_char="1509">Donald</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="1511" end_char="1515">Trump</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="1517" end_char="1523">claimed</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="1525" end_char="1526">to</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="1528" end_char="1531">have</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="1533" end_char="1536">seen</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="1538" end_char="1545">evidence</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="1547" end_char="1550">that</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="1552" end_char="1556">covid</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="1558" end_char="1559">19</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="1561" end_char="1563">had</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="1565" end_char="1568">come</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="1570" end_char="1573">from</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="1575" end_char="1575">a</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="1577" end_char="1586">laboratory</TOKEN>
<TOKEN id="token-17-20" pos="punct" morph="none" start_char="1587" end_char="1587">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="1589" end_char="1647">
<ORIGINAL_TEXT>Could it be that the evidence he saw was from fort Detrick?</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="1589" end_char="1593">Could</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="1595" end_char="1596">it</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="1598" end_char="1599">be</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="1601" end_char="1604">that</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="1606" end_char="1608">the</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="1610" end_char="1617">evidence</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="1619" end_char="1620">he</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="1622" end_char="1624">saw</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="1626" end_char="1628">was</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="1630" end_char="1633">from</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="1635" end_char="1638">fort</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="1640" end_char="1646">Detrick</TOKEN>
<TOKEN id="token-18-12" pos="punct" morph="none" start_char="1647" end_char="1647">?</TOKEN>
</SEG>
<SEG id="segment-19" start_char="1649" end_char="1732">
<ORIGINAL_TEXT>But rather then accept responsibility he sort to take an opportunity to blame China?</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="1649" end_char="1651">But</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="1653" end_char="1658">rather</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="1660" end_char="1663">then</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="1665" end_char="1670">accept</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="1672" end_char="1685">responsibility</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="1687" end_char="1688">he</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="1690" end_char="1693">sort</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="1695" end_char="1696">to</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="1698" end_char="1701">take</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="1703" end_char="1704">an</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="1706" end_char="1716">opportunity</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="1718" end_char="1719">to</TOKEN>
<TOKEN id="token-19-12" pos="word" morph="none" start_char="1721" end_char="1725">blame</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="1727" end_char="1731">China</TOKEN>
<TOKEN id="token-19-14" pos="punct" morph="none" start_char="1732" end_char="1732">?</TOKEN>
</SEG>
<SEG id="segment-20" start_char="1736" end_char="1766">
<ORIGINAL_TEXT>"Where did covid 19 originate?"</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="punct" morph="none" start_char="1736" end_char="1736">"</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="1737" end_char="1741">Where</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="1743" end_char="1745">did</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="1747" end_char="1751">covid</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="1753" end_char="1754">19</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="1756" end_char="1764">originate</TOKEN>
<TOKEN id="token-20-6" pos="punct" morph="none" start_char="1765" end_char="1766">?"</TOKEN>
</SEG>
<SEG id="segment-21" start_char="1768" end_char="1773">
<ORIGINAL_TEXT>Wuhan.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="1768" end_char="1772">Wuhan</TOKEN>
<TOKEN id="token-21-1" pos="punct" morph="none" start_char="1773" end_char="1773">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="1777" end_char="1824">
<ORIGINAL_TEXT>Quote from: Bored chemist on 05/02/2021 18:56:50</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="1777" end_char="1781">Quote</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="1783" end_char="1786">from</TOKEN>
<TOKEN id="token-22-2" pos="punct" morph="none" start_char="1787" end_char="1787">:</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="1789" end_char="1793">Bored</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="1795" end_char="1801">chemist</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="1803" end_char="1804">on</TOKEN>
<TOKEN id="token-22-6" pos="unknown" morph="none" start_char="1806" end_char="1815">05/02/2021</TOKEN>
<TOKEN id="token-22-7" pos="unknown" morph="none" start_char="1817" end_char="1824">18:56:50</TOKEN>
</SEG>
<SEG id="segment-23" start_char="1827" end_char="1856">
<ORIGINAL_TEXT>"Where did covid 19 originate?</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="punct" morph="none" start_char="1827" end_char="1827">"</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="1828" end_char="1832">Where</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="1834" end_char="1836">did</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="1838" end_char="1842">covid</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="1844" end_char="1845">19</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="1847" end_char="1855">originate</TOKEN>
<TOKEN id="token-23-6" pos="punct" morph="none" start_char="1856" end_char="1856">?</TOKEN>
</SEG>
<SEG id="segment-24" start_char="1858" end_char="1864">
<ORIGINAL_TEXT>"Wuhan.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="punct" morph="none" start_char="1858" end_char="1858">"</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="1859" end_char="1863">Wuhan</TOKEN>
<TOKEN id="token-24-2" pos="punct" morph="none" start_char="1864" end_char="1864">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="1867" end_char="1916">
<ORIGINAL_TEXT>A theory not a fact you should be more scientific.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="1867" end_char="1867">A</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="1869" end_char="1874">theory</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="1876" end_char="1878">not</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="1880" end_char="1880">a</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="1882" end_char="1885">fact</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="1887" end_char="1889">you</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="1891" end_char="1896">should</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="1898" end_char="1899">be</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="1901" end_char="1904">more</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="1906" end_char="1915">scientific</TOKEN>
<TOKEN id="token-25-10" pos="punct" morph="none" start_char="1916" end_char="1916">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="1920" end_char="1960">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 19:56:55</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="1920" end_char="1924">Quote</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="1926" end_char="1929">from</TOKEN>
<TOKEN id="token-26-2" pos="punct" morph="none" start_char="1930" end_char="1930">:</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="1932" end_char="1937">Jolly2</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="1939" end_char="1940">on</TOKEN>
<TOKEN id="token-26-5" pos="unknown" morph="none" start_char="1942" end_char="1951">06/02/2021</TOKEN>
<TOKEN id="token-26-6" pos="unknown" morph="none" start_char="1953" end_char="1960">19:56:55</TOKEN>
</SEG>
<SEG id="segment-27" start_char="1963" end_char="1981">
<ORIGINAL_TEXT>A theory not a fact</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="1963" end_char="1963">A</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="1965" end_char="1970">theory</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="1972" end_char="1974">not</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="1976" end_char="1976">a</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="1978" end_char="1981">fact</TOKEN>
</SEG>
<SEG id="segment-28" start_char="1985" end_char="1994">
<ORIGINAL_TEXT>A bit like</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="1985" end_char="1985">A</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="1987" end_char="1989">bit</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="1991" end_char="1994">like</TOKEN>
</SEG>
<SEG id="segment-29" start_char="1997" end_char="2037">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="1997" end_char="2001">Quote</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="2003" end_char="2006">from</TOKEN>
<TOKEN id="token-29-2" pos="punct" morph="none" start_char="2007" end_char="2007">:</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="2009" end_char="2014">Jolly2</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="2016" end_char="2017">on</TOKEN>
<TOKEN id="token-29-5" pos="unknown" morph="none" start_char="2019" end_char="2028">05/02/2021</TOKEN>
<TOKEN id="token-29-6" pos="unknown" morph="none" start_char="2030" end_char="2037">18:12:36</TOKEN>
</SEG>
<SEG id="segment-30" start_char="2040" end_char="2075">
<ORIGINAL_TEXT>covid was in Brazil in November 2019</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="2040" end_char="2044">covid</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="2046" end_char="2048">was</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="2050" end_char="2051">in</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="2053" end_char="2058">Brazil</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="2060" end_char="2061">in</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="2063" end_char="2070">November</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="2072" end_char="2075">2019</TOKEN>
</SEG>
<SEG id="segment-31" start_char="2079" end_char="2119">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="word" morph="none" start_char="2079" end_char="2083">Quote</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="2085" end_char="2088">from</TOKEN>
<TOKEN id="token-31-2" pos="punct" morph="none" start_char="2089" end_char="2089">:</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="2091" end_char="2096">Jolly2</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="2098" end_char="2099">on</TOKEN>
<TOKEN id="token-31-5" pos="unknown" morph="none" start_char="2101" end_char="2110">05/02/2021</TOKEN>
<TOKEN id="token-31-6" pos="unknown" morph="none" start_char="2112" end_char="2119">18:12:36</TOKEN>
</SEG>
<SEG id="segment-32" start_char="2122" end_char="2144">
<ORIGINAL_TEXT>France in November 2019</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="2122" end_char="2127">France</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="2129" end_char="2130">in</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="2132" end_char="2139">November</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="2141" end_char="2144">2019</TOKEN>
</SEG>
<SEG id="segment-33" start_char="2148" end_char="2188">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="word" morph="none" start_char="2148" end_char="2152">Quote</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="2154" end_char="2157">from</TOKEN>
<TOKEN id="token-33-2" pos="punct" morph="none" start_char="2158" end_char="2158">:</TOKEN>
<TOKEN id="token-33-3" pos="word" morph="none" start_char="2160" end_char="2165">Jolly2</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="2167" end_char="2168">on</TOKEN>
<TOKEN id="token-33-5" pos="unknown" morph="none" start_char="2170" end_char="2179">05/02/2021</TOKEN>
<TOKEN id="token-33-6" pos="unknown" morph="none" start_char="2181" end_char="2188">18:12:36</TOKEN>
</SEG>
<SEG id="segment-34" start_char="2191" end_char="2222">
<ORIGINAL_TEXT>And spain as early as March 2019</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="2191" end_char="2193">And</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="2195" end_char="2199">spain</TOKEN>
<TOKEN id="token-34-2" pos="word" morph="none" start_char="2201" end_char="2202">as</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="2204" end_char="2208">early</TOKEN>
<TOKEN id="token-34-4" pos="word" morph="none" start_char="2210" end_char="2211">as</TOKEN>
<TOKEN id="token-34-5" pos="word" morph="none" start_char="2213" end_char="2217">March</TOKEN>
<TOKEN id="token-34-6" pos="word" morph="none" start_char="2219" end_char="2222">2019</TOKEN>
</SEG>
<SEG id="segment-35" start_char="2226" end_char="2266">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:15:55</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="2226" end_char="2230">Quote</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="2232" end_char="2235">from</TOKEN>
<TOKEN id="token-35-2" pos="punct" morph="none" start_char="2236" end_char="2236">:</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="2238" end_char="2243">Jolly2</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="2245" end_char="2246">on</TOKEN>
<TOKEN id="token-35-5" pos="unknown" morph="none" start_char="2248" end_char="2257">05/02/2021</TOKEN>
<TOKEN id="token-35-6" pos="unknown" morph="none" start_char="2259" end_char="2266">18:15:55</TOKEN>
</SEG>
<SEG id="segment-36" start_char="2269" end_char="2360">
<ORIGINAL_TEXT>We know Donald Trump claimed to have seen evidence that covid 19 had come from a laboratory.</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="2269" end_char="2270">We</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="2272" end_char="2275">know</TOKEN>
<TOKEN id="token-36-2" pos="word" morph="none" start_char="2277" end_char="2282">Donald</TOKEN>
<TOKEN id="token-36-3" pos="word" morph="none" start_char="2284" end_char="2288">Trump</TOKEN>
<TOKEN id="token-36-4" pos="word" morph="none" start_char="2290" end_char="2296">claimed</TOKEN>
<TOKEN id="token-36-5" pos="word" morph="none" start_char="2298" end_char="2299">to</TOKEN>
<TOKEN id="token-36-6" pos="word" morph="none" start_char="2301" end_char="2304">have</TOKEN>
<TOKEN id="token-36-7" pos="word" morph="none" start_char="2306" end_char="2309">seen</TOKEN>
<TOKEN id="token-36-8" pos="word" morph="none" start_char="2311" end_char="2318">evidence</TOKEN>
<TOKEN id="token-36-9" pos="word" morph="none" start_char="2320" end_char="2323">that</TOKEN>
<TOKEN id="token-36-10" pos="word" morph="none" start_char="2325" end_char="2329">covid</TOKEN>
<TOKEN id="token-36-11" pos="word" morph="none" start_char="2331" end_char="2332">19</TOKEN>
<TOKEN id="token-36-12" pos="word" morph="none" start_char="2334" end_char="2336">had</TOKEN>
<TOKEN id="token-36-13" pos="word" morph="none" start_char="2338" end_char="2341">come</TOKEN>
<TOKEN id="token-36-14" pos="word" morph="none" start_char="2343" end_char="2346">from</TOKEN>
<TOKEN id="token-36-15" pos="word" morph="none" start_char="2348" end_char="2348">a</TOKEN>
<TOKEN id="token-36-16" pos="word" morph="none" start_char="2350" end_char="2359">laboratory</TOKEN>
<TOKEN id="token-36-17" pos="punct" morph="none" start_char="2360" end_char="2360">.</TOKEN>
</SEG>
<SEG id="segment-37" start_char="2362" end_char="2420">
<ORIGINAL_TEXT>Could it be that the evidence he saw was from fort Detrick?</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="2362" end_char="2366">Could</TOKEN>
<TOKEN id="token-37-1" pos="word" morph="none" start_char="2368" end_char="2369">it</TOKEN>
<TOKEN id="token-37-2" pos="word" morph="none" start_char="2371" end_char="2372">be</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="2374" end_char="2377">that</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="2379" end_char="2381">the</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="2383" end_char="2390">evidence</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="2392" end_char="2393">he</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="2395" end_char="2397">saw</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="2399" end_char="2401">was</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="2403" end_char="2406">from</TOKEN>
<TOKEN id="token-37-10" pos="word" morph="none" start_char="2408" end_char="2411">fort</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="2413" end_char="2419">Detrick</TOKEN>
<TOKEN id="token-37-12" pos="punct" morph="none" start_char="2420" end_char="2420">?</TOKEN>
</SEG>
<SEG id="segment-38" start_char="2424" end_char="2453">
<ORIGINAL_TEXT>you should be more scientific.</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="2424" end_char="2426">you</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="2428" end_char="2433">should</TOKEN>
<TOKEN id="token-38-2" pos="word" morph="none" start_char="2435" end_char="2436">be</TOKEN>
<TOKEN id="token-38-3" pos="word" morph="none" start_char="2438" end_char="2441">more</TOKEN>
<TOKEN id="token-38-4" pos="word" morph="none" start_char="2443" end_char="2452">scientific</TOKEN>
<TOKEN id="token-38-5" pos="punct" morph="none" start_char="2453" end_char="2453">.</TOKEN>
</SEG>
<SEG id="segment-39" start_char="2457" end_char="2504">
<ORIGINAL_TEXT>Quote from: Bored chemist on 06/02/2021 20:13:01</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="word" morph="none" start_char="2457" end_char="2461">Quote</TOKEN>
<TOKEN id="token-39-1" pos="word" morph="none" start_char="2463" end_char="2466">from</TOKEN>
<TOKEN id="token-39-2" pos="punct" morph="none" start_char="2467" end_char="2467">:</TOKEN>
<TOKEN id="token-39-3" pos="word" morph="none" start_char="2469" end_char="2473">Bored</TOKEN>
<TOKEN id="token-39-4" pos="word" morph="none" start_char="2475" end_char="2481">chemist</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="2483" end_char="2484">on</TOKEN>
<TOKEN id="token-39-6" pos="unknown" morph="none" start_char="2486" end_char="2495">06/02/2021</TOKEN>
<TOKEN id="token-39-7" pos="unknown" morph="none" start_char="2497" end_char="2504">20:13:01</TOKEN>
</SEG>
<SEG id="segment-40" start_char="2507" end_char="2547">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 19:56:55</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="word" morph="none" start_char="2507" end_char="2511">Quote</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="2513" end_char="2516">from</TOKEN>
<TOKEN id="token-40-2" pos="punct" morph="none" start_char="2517" end_char="2517">:</TOKEN>
<TOKEN id="token-40-3" pos="word" morph="none" start_char="2519" end_char="2524">Jolly2</TOKEN>
<TOKEN id="token-40-4" pos="word" morph="none" start_char="2526" end_char="2527">on</TOKEN>
<TOKEN id="token-40-5" pos="unknown" morph="none" start_char="2529" end_char="2538">06/02/2021</TOKEN>
<TOKEN id="token-40-6" pos="unknown" morph="none" start_char="2540" end_char="2547">19:56:55</TOKEN>
</SEG>
<SEG id="segment-41" start_char="2550" end_char="2568">
<ORIGINAL_TEXT>A theory not a fact</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="2550" end_char="2550">A</TOKEN>
<TOKEN id="token-41-1" pos="word" morph="none" start_char="2552" end_char="2557">theory</TOKEN>
<TOKEN id="token-41-2" pos="word" morph="none" start_char="2559" end_char="2561">not</TOKEN>
<TOKEN id="token-41-3" pos="word" morph="none" start_char="2563" end_char="2563">a</TOKEN>
<TOKEN id="token-41-4" pos="word" morph="none" start_char="2565" end_char="2568">fact</TOKEN>
</SEG>
<SEG id="segment-42" start_char="2571" end_char="2621">
<ORIGINAL_TEXT>A bit likeQuote from: Jolly2 on 05/02/2021 18:12:36</ORIGINAL_TEXT>
<TOKEN id="token-42-0" pos="word" morph="none" start_char="2571" end_char="2571">A</TOKEN>
<TOKEN id="token-42-1" pos="word" morph="none" start_char="2573" end_char="2575">bit</TOKEN>
<TOKEN id="token-42-2" pos="word" morph="none" start_char="2577" end_char="2585">likeQuote</TOKEN>
<TOKEN id="token-42-3" pos="word" morph="none" start_char="2587" end_char="2590">from</TOKEN>
<TOKEN id="token-42-4" pos="punct" morph="none" start_char="2591" end_char="2591">:</TOKEN>
<TOKEN id="token-42-5" pos="word" morph="none" start_char="2593" end_char="2598">Jolly2</TOKEN>
<TOKEN id="token-42-6" pos="word" morph="none" start_char="2600" end_char="2601">on</TOKEN>
<TOKEN id="token-42-7" pos="unknown" morph="none" start_char="2603" end_char="2612">05/02/2021</TOKEN>
<TOKEN id="token-42-8" pos="unknown" morph="none" start_char="2614" end_char="2621">18:12:36</TOKEN>
</SEG>
<SEG id="segment-43" start_char="2624" end_char="2659">
<ORIGINAL_TEXT>covid was in Brazil in November 2019</ORIGINAL_TEXT>
<TOKEN id="token-43-0" pos="word" morph="none" start_char="2624" end_char="2628">covid</TOKEN>
<TOKEN id="token-43-1" pos="word" morph="none" start_char="2630" end_char="2632">was</TOKEN>
<TOKEN id="token-43-2" pos="word" morph="none" start_char="2634" end_char="2635">in</TOKEN>
<TOKEN id="token-43-3" pos="word" morph="none" start_char="2637" end_char="2642">Brazil</TOKEN>
<TOKEN id="token-43-4" pos="word" morph="none" start_char="2644" end_char="2645">in</TOKEN>
<TOKEN id="token-43-5" pos="word" morph="none" start_char="2647" end_char="2654">November</TOKEN>
<TOKEN id="token-43-6" pos="word" morph="none" start_char="2656" end_char="2659">2019</TOKEN>
</SEG>
<SEG id="segment-44" start_char="2662" end_char="2702">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36</ORIGINAL_TEXT>
<TOKEN id="token-44-0" pos="word" morph="none" start_char="2662" end_char="2666">Quote</TOKEN>
<TOKEN id="token-44-1" pos="word" morph="none" start_char="2668" end_char="2671">from</TOKEN>
<TOKEN id="token-44-2" pos="punct" morph="none" start_char="2672" end_char="2672">:</TOKEN>
<TOKEN id="token-44-3" pos="word" morph="none" start_char="2674" end_char="2679">Jolly2</TOKEN>
<TOKEN id="token-44-4" pos="word" morph="none" start_char="2681" end_char="2682">on</TOKEN>
<TOKEN id="token-44-5" pos="unknown" morph="none" start_char="2684" end_char="2693">05/02/2021</TOKEN>
<TOKEN id="token-44-6" pos="unknown" morph="none" start_char="2695" end_char="2702">18:12:36</TOKEN>
</SEG>
<SEG id="segment-45" start_char="2705" end_char="2727">
<ORIGINAL_TEXT>France in November 2019</ORIGINAL_TEXT>
<TOKEN id="token-45-0" pos="word" morph="none" start_char="2705" end_char="2710">France</TOKEN>
<TOKEN id="token-45-1" pos="word" morph="none" start_char="2712" end_char="2713">in</TOKEN>
<TOKEN id="token-45-2" pos="word" morph="none" start_char="2715" end_char="2722">November</TOKEN>
<TOKEN id="token-45-3" pos="word" morph="none" start_char="2724" end_char="2727">2019</TOKEN>
</SEG>
<SEG id="segment-46" start_char="2730" end_char="2770">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36</ORIGINAL_TEXT>
<TOKEN id="token-46-0" pos="word" morph="none" start_char="2730" end_char="2734">Quote</TOKEN>
<TOKEN id="token-46-1" pos="word" morph="none" start_char="2736" end_char="2739">from</TOKEN>
<TOKEN id="token-46-2" pos="punct" morph="none" start_char="2740" end_char="2740">:</TOKEN>
<TOKEN id="token-46-3" pos="word" morph="none" start_char="2742" end_char="2747">Jolly2</TOKEN>
<TOKEN id="token-46-4" pos="word" morph="none" start_char="2749" end_char="2750">on</TOKEN>
<TOKEN id="token-46-5" pos="unknown" morph="none" start_char="2752" end_char="2761">05/02/2021</TOKEN>
<TOKEN id="token-46-6" pos="unknown" morph="none" start_char="2763" end_char="2770">18:12:36</TOKEN>
</SEG>
<SEG id="segment-47" start_char="2773" end_char="2804">
<ORIGINAL_TEXT>And spain as early as March 2019</ORIGINAL_TEXT>
<TOKEN id="token-47-0" pos="word" morph="none" start_char="2773" end_char="2775">And</TOKEN>
<TOKEN id="token-47-1" pos="word" morph="none" start_char="2777" end_char="2781">spain</TOKEN>
<TOKEN id="token-47-2" pos="word" morph="none" start_char="2783" end_char="2784">as</TOKEN>
<TOKEN id="token-47-3" pos="word" morph="none" start_char="2786" end_char="2790">early</TOKEN>
<TOKEN id="token-47-4" pos="word" morph="none" start_char="2792" end_char="2793">as</TOKEN>
<TOKEN id="token-47-5" pos="word" morph="none" start_char="2795" end_char="2799">March</TOKEN>
<TOKEN id="token-47-6" pos="word" morph="none" start_char="2801" end_char="2804">2019</TOKEN>
</SEG>
<SEG id="segment-48" start_char="2807" end_char="2847">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:15:55</ORIGINAL_TEXT>
<TOKEN id="token-48-0" pos="word" morph="none" start_char="2807" end_char="2811">Quote</TOKEN>
<TOKEN id="token-48-1" pos="word" morph="none" start_char="2813" end_char="2816">from</TOKEN>
<TOKEN id="token-48-2" pos="punct" morph="none" start_char="2817" end_char="2817">:</TOKEN>
<TOKEN id="token-48-3" pos="word" morph="none" start_char="2819" end_char="2824">Jolly2</TOKEN>
<TOKEN id="token-48-4" pos="word" morph="none" start_char="2826" end_char="2827">on</TOKEN>
<TOKEN id="token-48-5" pos="unknown" morph="none" start_char="2829" end_char="2838">05/02/2021</TOKEN>
<TOKEN id="token-48-6" pos="unknown" morph="none" start_char="2840" end_char="2847">18:15:55</TOKEN>
</SEG>
<SEG id="segment-49" start_char="2850" end_char="2941">
<ORIGINAL_TEXT>We know Donald Trump claimed to have seen evidence that covid 19 had come from a laboratory.</ORIGINAL_TEXT>
<TOKEN id="token-49-0" pos="word" morph="none" start_char="2850" end_char="2851">We</TOKEN>
<TOKEN id="token-49-1" pos="word" morph="none" start_char="2853" end_char="2856">know</TOKEN>
<TOKEN id="token-49-2" pos="word" morph="none" start_char="2858" end_char="2863">Donald</TOKEN>
<TOKEN id="token-49-3" pos="word" morph="none" start_char="2865" end_char="2869">Trump</TOKEN>
<TOKEN id="token-49-4" pos="word" morph="none" start_char="2871" end_char="2877">claimed</TOKEN>
<TOKEN id="token-49-5" pos="word" morph="none" start_char="2879" end_char="2880">to</TOKEN>
<TOKEN id="token-49-6" pos="word" morph="none" start_char="2882" end_char="2885">have</TOKEN>
<TOKEN id="token-49-7" pos="word" morph="none" start_char="2887" end_char="2890">seen</TOKEN>
<TOKEN id="token-49-8" pos="word" morph="none" start_char="2892" end_char="2899">evidence</TOKEN>
<TOKEN id="token-49-9" pos="word" morph="none" start_char="2901" end_char="2904">that</TOKEN>
<TOKEN id="token-49-10" pos="word" morph="none" start_char="2906" end_char="2910">covid</TOKEN>
<TOKEN id="token-49-11" pos="word" morph="none" start_char="2912" end_char="2913">19</TOKEN>
<TOKEN id="token-49-12" pos="word" morph="none" start_char="2915" end_char="2917">had</TOKEN>
<TOKEN id="token-49-13" pos="word" morph="none" start_char="2919" end_char="2922">come</TOKEN>
<TOKEN id="token-49-14" pos="word" morph="none" start_char="2924" end_char="2927">from</TOKEN>
<TOKEN id="token-49-15" pos="word" morph="none" start_char="2929" end_char="2929">a</TOKEN>
<TOKEN id="token-49-16" pos="word" morph="none" start_char="2931" end_char="2940">laboratory</TOKEN>
<TOKEN id="token-49-17" pos="punct" morph="none" start_char="2941" end_char="2941">.</TOKEN>
</SEG>
<SEG id="segment-50" start_char="2943" end_char="3001">
<ORIGINAL_TEXT>Could it be that the evidence he saw was from fort Detrick?</ORIGINAL_TEXT>
<TOKEN id="token-50-0" pos="word" morph="none" start_char="2943" end_char="2947">Could</TOKEN>
<TOKEN id="token-50-1" pos="word" morph="none" start_char="2949" end_char="2950">it</TOKEN>
<TOKEN id="token-50-2" pos="word" morph="none" start_char="2952" end_char="2953">be</TOKEN>
<TOKEN id="token-50-3" pos="word" morph="none" start_char="2955" end_char="2958">that</TOKEN>
<TOKEN id="token-50-4" pos="word" morph="none" start_char="2960" end_char="2962">the</TOKEN>
<TOKEN id="token-50-5" pos="word" morph="none" start_char="2964" end_char="2971">evidence</TOKEN>
<TOKEN id="token-50-6" pos="word" morph="none" start_char="2973" end_char="2974">he</TOKEN>
<TOKEN id="token-50-7" pos="word" morph="none" start_char="2976" end_char="2978">saw</TOKEN>
<TOKEN id="token-50-8" pos="word" morph="none" start_char="2980" end_char="2982">was</TOKEN>
<TOKEN id="token-50-9" pos="word" morph="none" start_char="2984" end_char="2987">from</TOKEN>
<TOKEN id="token-50-10" pos="word" morph="none" start_char="2989" end_char="2992">fort</TOKEN>
<TOKEN id="token-50-11" pos="word" morph="none" start_char="2994" end_char="3000">Detrick</TOKEN>
<TOKEN id="token-50-12" pos="punct" morph="none" start_char="3001" end_char="3001">?</TOKEN>
</SEG>
<SEG id="segment-51" start_char="3004" end_char="3033">
<ORIGINAL_TEXT>you should be more scientific.</ORIGINAL_TEXT>
<TOKEN id="token-51-0" pos="word" morph="none" start_char="3004" end_char="3006">you</TOKEN>
<TOKEN id="token-51-1" pos="word" morph="none" start_char="3008" end_char="3013">should</TOKEN>
<TOKEN id="token-51-2" pos="word" morph="none" start_char="3015" end_char="3016">be</TOKEN>
<TOKEN id="token-51-3" pos="word" morph="none" start_char="3018" end_char="3021">more</TOKEN>
<TOKEN id="token-51-4" pos="word" morph="none" start_char="3023" end_char="3032">scientific</TOKEN>
<TOKEN id="token-51-5" pos="punct" morph="none" start_char="3033" end_char="3033">.</TOKEN>
</SEG>
<SEG id="segment-52" start_char="3036" end_char="3071">
<ORIGINAL_TEXT>Hardly they found samples in sewage.</ORIGINAL_TEXT>
<TOKEN id="token-52-0" pos="word" morph="none" start_char="3036" end_char="3041">Hardly</TOKEN>
<TOKEN id="token-52-1" pos="word" morph="none" start_char="3043" end_char="3046">they</TOKEN>
<TOKEN id="token-52-2" pos="word" morph="none" start_char="3048" end_char="3052">found</TOKEN>
<TOKEN id="token-52-3" pos="word" morph="none" start_char="3054" end_char="3060">samples</TOKEN>
<TOKEN id="token-52-4" pos="word" morph="none" start_char="3062" end_char="3063">in</TOKEN>
<TOKEN id="token-52-5" pos="word" morph="none" start_char="3065" end_char="3070">sewage</TOKEN>
<TOKEN id="token-52-6" pos="punct" morph="none" start_char="3071" end_char="3071">.</TOKEN>
</SEG>
<SEG id="segment-53" start_char="3073" end_char="3122">
<ORIGINAL_TEXT>I'm not claiming any facts for the origin You are.</ORIGINAL_TEXT>
<TOKEN id="token-53-0" pos="word" morph="none" start_char="3073" end_char="3075">I'm</TOKEN>
<TOKEN id="token-53-1" pos="word" morph="none" start_char="3077" end_char="3079">not</TOKEN>
<TOKEN id="token-53-2" pos="word" morph="none" start_char="3081" end_char="3088">claiming</TOKEN>
<TOKEN id="token-53-3" pos="word" morph="none" start_char="3090" end_char="3092">any</TOKEN>
<TOKEN id="token-53-4" pos="word" morph="none" start_char="3094" end_char="3098">facts</TOKEN>
<TOKEN id="token-53-5" pos="word" morph="none" start_char="3100" end_char="3102">for</TOKEN>
<TOKEN id="token-53-6" pos="word" morph="none" start_char="3104" end_char="3106">the</TOKEN>
<TOKEN id="token-53-7" pos="word" morph="none" start_char="3108" end_char="3113">origin</TOKEN>
<TOKEN id="token-53-8" pos="word" morph="none" start_char="3115" end_char="3117">You</TOKEN>
<TOKEN id="token-53-9" pos="word" morph="none" start_char="3119" end_char="3121">are</TOKEN>
<TOKEN id="token-53-10" pos="punct" morph="none" start_char="3122" end_char="3122">.</TOKEN>
</SEG>
<SEG id="segment-54" start_char="3126" end_char="3166">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 20:16:16</ORIGINAL_TEXT>
<TOKEN id="token-54-0" pos="word" morph="none" start_char="3126" end_char="3130">Quote</TOKEN>
<TOKEN id="token-54-1" pos="word" morph="none" start_char="3132" end_char="3135">from</TOKEN>
<TOKEN id="token-54-2" pos="punct" morph="none" start_char="3136" end_char="3136">:</TOKEN>
<TOKEN id="token-54-3" pos="word" morph="none" start_char="3138" end_char="3143">Jolly2</TOKEN>
<TOKEN id="token-54-4" pos="word" morph="none" start_char="3145" end_char="3146">on</TOKEN>
<TOKEN id="token-54-5" pos="unknown" morph="none" start_char="3148" end_char="3157">06/02/2021</TOKEN>
<TOKEN id="token-54-6" pos="unknown" morph="none" start_char="3159" end_char="3166">20:16:16</TOKEN>
</SEG>
<SEG id="segment-55" start_char="3169" end_char="3197">
<ORIGINAL_TEXT>they found samples in sewage.</ORIGINAL_TEXT>
<TOKEN id="token-55-0" pos="word" morph="none" start_char="3169" end_char="3172">they</TOKEN>
<TOKEN id="token-55-1" pos="word" morph="none" start_char="3174" end_char="3178">found</TOKEN>
<TOKEN id="token-55-2" pos="word" morph="none" start_char="3180" end_char="3186">samples</TOKEN>
<TOKEN id="token-55-3" pos="word" morph="none" start_char="3188" end_char="3189">in</TOKEN>
<TOKEN id="token-55-4" pos="word" morph="none" start_char="3191" end_char="3196">sewage</TOKEN>
<TOKEN id="token-55-5" pos="punct" morph="none" start_char="3197" end_char="3197">.</TOKEN>
</SEG>
<SEG id="segment-56" start_char="3201" end_char="3232">
<ORIGINAL_TEXT>How sure are you of that "fact"?</ORIGINAL_TEXT>
<TOKEN id="token-56-0" pos="word" morph="none" start_char="3201" end_char="3203">How</TOKEN>
<TOKEN id="token-56-1" pos="word" morph="none" start_char="3205" end_char="3208">sure</TOKEN>
<TOKEN id="token-56-2" pos="word" morph="none" start_char="3210" end_char="3212">are</TOKEN>
<TOKEN id="token-56-3" pos="word" morph="none" start_char="3214" end_char="3216">you</TOKEN>
<TOKEN id="token-56-4" pos="word" morph="none" start_char="3218" end_char="3219">of</TOKEN>
<TOKEN id="token-56-5" pos="word" morph="none" start_char="3221" end_char="3224">that</TOKEN>
<TOKEN id="token-56-6" pos="punct" morph="none" start_char="3226" end_char="3226">"</TOKEN>
<TOKEN id="token-56-7" pos="word" morph="none" start_char="3227" end_char="3230">fact</TOKEN>
<TOKEN id="token-56-8" pos="punct" morph="none" start_char="3231" end_char="3232">"?</TOKEN>
</SEG>
<SEG id="segment-57" start_char="3236" end_char="3283">
<ORIGINAL_TEXT>Quote from: Bored chemist on 06/02/2021 20:19:01</ORIGINAL_TEXT>
<TOKEN id="token-57-0" pos="word" morph="none" start_char="3236" end_char="3240">Quote</TOKEN>
<TOKEN id="token-57-1" pos="word" morph="none" start_char="3242" end_char="3245">from</TOKEN>
<TOKEN id="token-57-2" pos="punct" morph="none" start_char="3246" end_char="3246">:</TOKEN>
<TOKEN id="token-57-3" pos="word" morph="none" start_char="3248" end_char="3252">Bored</TOKEN>
<TOKEN id="token-57-4" pos="word" morph="none" start_char="3254" end_char="3260">chemist</TOKEN>
<TOKEN id="token-57-5" pos="word" morph="none" start_char="3262" end_char="3263">on</TOKEN>
<TOKEN id="token-57-6" pos="unknown" morph="none" start_char="3265" end_char="3274">06/02/2021</TOKEN>
<TOKEN id="token-57-7" pos="unknown" morph="none" start_char="3276" end_char="3283">20:19:01</TOKEN>
</SEG>
<SEG id="segment-58" start_char="3286" end_char="3326">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 20:16:16</ORIGINAL_TEXT>
<TOKEN id="token-58-0" pos="word" morph="none" start_char="3286" end_char="3290">Quote</TOKEN>
<TOKEN id="token-58-1" pos="word" morph="none" start_char="3292" end_char="3295">from</TOKEN>
<TOKEN id="token-58-2" pos="punct" morph="none" start_char="3296" end_char="3296">:</TOKEN>
<TOKEN id="token-58-3" pos="word" morph="none" start_char="3298" end_char="3303">Jolly2</TOKEN>
<TOKEN id="token-58-4" pos="word" morph="none" start_char="3305" end_char="3306">on</TOKEN>
<TOKEN id="token-58-5" pos="unknown" morph="none" start_char="3308" end_char="3317">06/02/2021</TOKEN>
<TOKEN id="token-58-6" pos="unknown" morph="none" start_char="3319" end_char="3326">20:16:16</TOKEN>
</SEG>
<SEG id="segment-59" start_char="3329" end_char="3357">
<ORIGINAL_TEXT>they found samples in sewage.</ORIGINAL_TEXT>
<TOKEN id="token-59-0" pos="word" morph="none" start_char="3329" end_char="3332">they</TOKEN>
<TOKEN id="token-59-1" pos="word" morph="none" start_char="3334" end_char="3338">found</TOKEN>
<TOKEN id="token-59-2" pos="word" morph="none" start_char="3340" end_char="3346">samples</TOKEN>
<TOKEN id="token-59-3" pos="word" morph="none" start_char="3348" end_char="3349">in</TOKEN>
<TOKEN id="token-59-4" pos="word" morph="none" start_char="3351" end_char="3356">sewage</TOKEN>
<TOKEN id="token-59-5" pos="punct" morph="none" start_char="3357" end_char="3357">.</TOKEN>
</SEG>
<SEG id="segment-60" start_char="3360" end_char="3391">
<ORIGINAL_TEXT>How sure are you of that "fact"?</ORIGINAL_TEXT>
<TOKEN id="token-60-0" pos="word" morph="none" start_char="3360" end_char="3362">How</TOKEN>
<TOKEN id="token-60-1" pos="word" morph="none" start_char="3364" end_char="3367">sure</TOKEN>
<TOKEN id="token-60-2" pos="word" morph="none" start_char="3369" end_char="3371">are</TOKEN>
<TOKEN id="token-60-3" pos="word" morph="none" start_char="3373" end_char="3375">you</TOKEN>
<TOKEN id="token-60-4" pos="word" morph="none" start_char="3377" end_char="3378">of</TOKEN>
<TOKEN id="token-60-5" pos="word" morph="none" start_char="3380" end_char="3383">that</TOKEN>
<TOKEN id="token-60-6" pos="punct" morph="none" start_char="3385" end_char="3385">"</TOKEN>
<TOKEN id="token-60-7" pos="word" morph="none" start_char="3386" end_char="3389">fact</TOKEN>
<TOKEN id="token-60-8" pos="punct" morph="none" start_char="3390" end_char="3391">"?</TOKEN>
</SEG>
<SEG id="segment-61" start_char="3394" end_char="3435">
<ORIGINAL_TEXT>Sure enough to post it here for discussion</ORIGINAL_TEXT>
<TOKEN id="token-61-0" pos="word" morph="none" start_char="3394" end_char="3397">Sure</TOKEN>
<TOKEN id="token-61-1" pos="word" morph="none" start_char="3399" end_char="3404">enough</TOKEN>
<TOKEN id="token-61-2" pos="word" morph="none" start_char="3406" end_char="3407">to</TOKEN>
<TOKEN id="token-61-3" pos="word" morph="none" start_char="3409" end_char="3412">post</TOKEN>
<TOKEN id="token-61-4" pos="word" morph="none" start_char="3414" end_char="3415">it</TOKEN>
<TOKEN id="token-61-5" pos="word" morph="none" start_char="3417" end_char="3420">here</TOKEN>
<TOKEN id="token-61-6" pos="word" morph="none" start_char="3422" end_char="3424">for</TOKEN>
<TOKEN id="token-61-7" pos="word" morph="none" start_char="3426" end_char="3435">discussion</TOKEN>
</SEG>
<SEG id="segment-62" start_char="3439" end_char="3479">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 20:27:47</ORIGINAL_TEXT>
<TOKEN id="token-62-0" pos="word" morph="none" start_char="3439" end_char="3443">Quote</TOKEN>
<TOKEN id="token-62-1" pos="word" morph="none" start_char="3445" end_char="3448">from</TOKEN>
<TOKEN id="token-62-2" pos="punct" morph="none" start_char="3449" end_char="3449">:</TOKEN>
<TOKEN id="token-62-3" pos="word" morph="none" start_char="3451" end_char="3456">Jolly2</TOKEN>
<TOKEN id="token-62-4" pos="word" morph="none" start_char="3458" end_char="3459">on</TOKEN>
<TOKEN id="token-62-5" pos="unknown" morph="none" start_char="3461" end_char="3470">06/02/2021</TOKEN>
<TOKEN id="token-62-6" pos="unknown" morph="none" start_char="3472" end_char="3479">20:27:47</TOKEN>
</SEG>
<SEG id="segment-63" start_char="3482" end_char="3529">
<ORIGINAL_TEXT>Quote from: Bored chemist on 06/02/2021 20:19:01</ORIGINAL_TEXT>
<TOKEN id="token-63-0" pos="word" morph="none" start_char="3482" end_char="3486">Quote</TOKEN>
<TOKEN id="token-63-1" pos="word" morph="none" start_char="3488" end_char="3491">from</TOKEN>
<TOKEN id="token-63-2" pos="punct" morph="none" start_char="3492" end_char="3492">:</TOKEN>
<TOKEN id="token-63-3" pos="word" morph="none" start_char="3494" end_char="3498">Bored</TOKEN>
<TOKEN id="token-63-4" pos="word" morph="none" start_char="3500" end_char="3506">chemist</TOKEN>
<TOKEN id="token-63-5" pos="word" morph="none" start_char="3508" end_char="3509">on</TOKEN>
<TOKEN id="token-63-6" pos="unknown" morph="none" start_char="3511" end_char="3520">06/02/2021</TOKEN>
<TOKEN id="token-63-7" pos="unknown" morph="none" start_char="3522" end_char="3529">20:19:01</TOKEN>
</SEG>
<SEG id="segment-64" start_char="3532" end_char="3572">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 20:16:16</ORIGINAL_TEXT>
<TOKEN id="token-64-0" pos="word" morph="none" start_char="3532" end_char="3536">Quote</TOKEN>
<TOKEN id="token-64-1" pos="word" morph="none" start_char="3538" end_char="3541">from</TOKEN>
<TOKEN id="token-64-2" pos="punct" morph="none" start_char="3542" end_char="3542">:</TOKEN>
<TOKEN id="token-64-3" pos="word" morph="none" start_char="3544" end_char="3549">Jolly2</TOKEN>
<TOKEN id="token-64-4" pos="word" morph="none" start_char="3551" end_char="3552">on</TOKEN>
<TOKEN id="token-64-5" pos="unknown" morph="none" start_char="3554" end_char="3563">06/02/2021</TOKEN>
<TOKEN id="token-64-6" pos="unknown" morph="none" start_char="3565" end_char="3572">20:16:16</TOKEN>
</SEG>
<SEG id="segment-65" start_char="3575" end_char="3603">
<ORIGINAL_TEXT>they found samples in sewage.</ORIGINAL_TEXT>
<TOKEN id="token-65-0" pos="word" morph="none" start_char="3575" end_char="3578">they</TOKEN>
<TOKEN id="token-65-1" pos="word" morph="none" start_char="3580" end_char="3584">found</TOKEN>
<TOKEN id="token-65-2" pos="word" morph="none" start_char="3586" end_char="3592">samples</TOKEN>
<TOKEN id="token-65-3" pos="word" morph="none" start_char="3594" end_char="3595">in</TOKEN>
<TOKEN id="token-65-4" pos="word" morph="none" start_char="3597" end_char="3602">sewage</TOKEN>
<TOKEN id="token-65-5" pos="punct" morph="none" start_char="3603" end_char="3603">.</TOKEN>
</SEG>
<SEG id="segment-66" start_char="3606" end_char="3679">
<ORIGINAL_TEXT>How sure are you of that "fact"?Sure enough to post it here for discussion</ORIGINAL_TEXT>
<TOKEN id="token-66-0" pos="word" morph="none" start_char="3606" end_char="3608">How</TOKEN>
<TOKEN id="token-66-1" pos="word" morph="none" start_char="3610" end_char="3613">sure</TOKEN>
<TOKEN id="token-66-2" pos="word" morph="none" start_char="3615" end_char="3617">are</TOKEN>
<TOKEN id="token-66-3" pos="word" morph="none" start_char="3619" end_char="3621">you</TOKEN>
<TOKEN id="token-66-4" pos="word" morph="none" start_char="3623" end_char="3624">of</TOKEN>
<TOKEN id="token-66-5" pos="word" morph="none" start_char="3626" end_char="3629">that</TOKEN>
<TOKEN id="token-66-6" pos="punct" morph="none" start_char="3631" end_char="3631">"</TOKEN>
<TOKEN id="token-66-7" pos="unknown" morph="none" start_char="3632" end_char="3641">fact"?Sure</TOKEN>
<TOKEN id="token-66-8" pos="word" morph="none" start_char="3643" end_char="3648">enough</TOKEN>
<TOKEN id="token-66-9" pos="word" morph="none" start_char="3650" end_char="3651">to</TOKEN>
<TOKEN id="token-66-10" pos="word" morph="none" start_char="3653" end_char="3656">post</TOKEN>
<TOKEN id="token-66-11" pos="word" morph="none" start_char="3658" end_char="3659">it</TOKEN>
<TOKEN id="token-66-12" pos="word" morph="none" start_char="3661" end_char="3664">here</TOKEN>
<TOKEN id="token-66-13" pos="word" morph="none" start_char="3666" end_char="3668">for</TOKEN>
<TOKEN id="token-66-14" pos="word" morph="none" start_char="3670" end_char="3679">discussion</TOKEN>
</SEG>
<SEG id="segment-67" start_char="3682" end_char="3712">
<ORIGINAL_TEXT>Then I guess I can do the same.</ORIGINAL_TEXT>
<TOKEN id="token-67-0" pos="word" morph="none" start_char="3682" end_char="3685">Then</TOKEN>
<TOKEN id="token-67-1" pos="word" morph="none" start_char="3687" end_char="3687">I</TOKEN>
<TOKEN id="token-67-2" pos="word" morph="none" start_char="3689" end_char="3693">guess</TOKEN>
<TOKEN id="token-67-3" pos="word" morph="none" start_char="3695" end_char="3695">I</TOKEN>
<TOKEN id="token-67-4" pos="word" morph="none" start_char="3697" end_char="3699">can</TOKEN>
<TOKEN id="token-67-5" pos="word" morph="none" start_char="3701" end_char="3702">do</TOKEN>
<TOKEN id="token-67-6" pos="word" morph="none" start_char="3704" end_char="3706">the</TOKEN>
<TOKEN id="token-67-7" pos="word" morph="none" start_char="3708" end_char="3711">same</TOKEN>
<TOKEN id="token-67-8" pos="punct" morph="none" start_char="3712" end_char="3712">.</TOKEN>
</SEG>
<SEG id="segment-68" start_char="3715" end_char="3762">
<ORIGINAL_TEXT>Quote from: Bored chemist on 05/02/2021 18:56:50</ORIGINAL_TEXT>
<TOKEN id="token-68-0" pos="word" morph="none" start_char="3715" end_char="3719">Quote</TOKEN>
<TOKEN id="token-68-1" pos="word" morph="none" start_char="3721" end_char="3724">from</TOKEN>
<TOKEN id="token-68-2" pos="punct" morph="none" start_char="3725" end_char="3725">:</TOKEN>
<TOKEN id="token-68-3" pos="word" morph="none" start_char="3727" end_char="3731">Bored</TOKEN>
<TOKEN id="token-68-4" pos="word" morph="none" start_char="3733" end_char="3739">chemist</TOKEN>
<TOKEN id="token-68-5" pos="word" morph="none" start_char="3741" end_char="3742">on</TOKEN>
<TOKEN id="token-68-6" pos="unknown" morph="none" start_char="3744" end_char="3753">05/02/2021</TOKEN>
<TOKEN id="token-68-7" pos="unknown" morph="none" start_char="3755" end_char="3762">18:56:50</TOKEN>
</SEG>
<SEG id="segment-69" start_char="3765" end_char="3794">
<ORIGINAL_TEXT>"Where did covid 19 originate?</ORIGINAL_TEXT>
<TOKEN id="token-69-0" pos="punct" morph="none" start_char="3765" end_char="3765">"</TOKEN>
<TOKEN id="token-69-1" pos="word" morph="none" start_char="3766" end_char="3770">Where</TOKEN>
<TOKEN id="token-69-2" pos="word" morph="none" start_char="3772" end_char="3774">did</TOKEN>
<TOKEN id="token-69-3" pos="word" morph="none" start_char="3776" end_char="3780">covid</TOKEN>
<TOKEN id="token-69-4" pos="word" morph="none" start_char="3782" end_char="3783">19</TOKEN>
<TOKEN id="token-69-5" pos="word" morph="none" start_char="3785" end_char="3793">originate</TOKEN>
<TOKEN id="token-69-6" pos="punct" morph="none" start_char="3794" end_char="3794">?</TOKEN>
</SEG>
<SEG id="segment-70" start_char="3796" end_char="3802">
<ORIGINAL_TEXT>"Wuhan.</ORIGINAL_TEXT>
<TOKEN id="token-70-0" pos="punct" morph="none" start_char="3796" end_char="3796">"</TOKEN>
<TOKEN id="token-70-1" pos="word" morph="none" start_char="3797" end_char="3801">Wuhan</TOKEN>
<TOKEN id="token-70-2" pos="punct" morph="none" start_char="3802" end_char="3802">.</TOKEN>
</SEG>
<SEG id="segment-71" start_char="3805" end_char="3851">
<ORIGINAL_TEXT>Do you have any actual evidence that I'm wrong?</ORIGINAL_TEXT>
<TOKEN id="token-71-0" pos="word" morph="none" start_char="3805" end_char="3806">Do</TOKEN>
<TOKEN id="token-71-1" pos="word" morph="none" start_char="3808" end_char="3810">you</TOKEN>
<TOKEN id="token-71-2" pos="word" morph="none" start_char="3812" end_char="3815">have</TOKEN>
<TOKEN id="token-71-3" pos="word" morph="none" start_char="3817" end_char="3819">any</TOKEN>
<TOKEN id="token-71-4" pos="word" morph="none" start_char="3821" end_char="3826">actual</TOKEN>
<TOKEN id="token-71-5" pos="word" morph="none" start_char="3828" end_char="3835">evidence</TOKEN>
<TOKEN id="token-71-6" pos="word" morph="none" start_char="3837" end_char="3840">that</TOKEN>
<TOKEN id="token-71-7" pos="word" morph="none" start_char="3842" end_char="3844">I'm</TOKEN>
<TOKEN id="token-71-8" pos="word" morph="none" start_char="3846" end_char="3850">wrong</TOKEN>
<TOKEN id="token-71-9" pos="punct" morph="none" start_char="3851" end_char="3851">?</TOKEN>
</SEG>
<SEG id="segment-72" start_char="3853" end_char="4000">
<ORIGINAL_TEXT>(To do that you would have to prove that it was- for example- possible for the virus to transfer from Spain to china, but not the other way round.).</ORIGINAL_TEXT>
<TOKEN id="token-72-0" pos="punct" morph="none" start_char="3853" end_char="3853">(</TOKEN>
<TOKEN id="token-72-1" pos="word" morph="none" start_char="3854" end_char="3855">To</TOKEN>
<TOKEN id="token-72-2" pos="word" morph="none" start_char="3857" end_char="3858">do</TOKEN>
<TOKEN id="token-72-3" pos="word" morph="none" start_char="3860" end_char="3863">that</TOKEN>
<TOKEN id="token-72-4" pos="word" morph="none" start_char="3865" end_char="3867">you</TOKEN>
<TOKEN id="token-72-5" pos="word" morph="none" start_char="3869" end_char="3873">would</TOKEN>
<TOKEN id="token-72-6" pos="word" morph="none" start_char="3875" end_char="3878">have</TOKEN>
<TOKEN id="token-72-7" pos="word" morph="none" start_char="3880" end_char="3881">to</TOKEN>
<TOKEN id="token-72-8" pos="word" morph="none" start_char="3883" end_char="3887">prove</TOKEN>
<TOKEN id="token-72-9" pos="word" morph="none" start_char="3889" end_char="3892">that</TOKEN>
<TOKEN id="token-72-10" pos="word" morph="none" start_char="3894" end_char="3895">it</TOKEN>
<TOKEN id="token-72-11" pos="word" morph="none" start_char="3897" end_char="3899">was</TOKEN>
<TOKEN id="token-72-12" pos="punct" morph="none" start_char="3900" end_char="3900">-</TOKEN>
<TOKEN id="token-72-13" pos="word" morph="none" start_char="3902" end_char="3904">for</TOKEN>
<TOKEN id="token-72-14" pos="word" morph="none" start_char="3906" end_char="3912">example</TOKEN>
<TOKEN id="token-72-15" pos="punct" morph="none" start_char="3913" end_char="3913">-</TOKEN>
<TOKEN id="token-72-16" pos="word" morph="none" start_char="3915" end_char="3922">possible</TOKEN>
<TOKEN id="token-72-17" pos="word" morph="none" start_char="3924" end_char="3926">for</TOKEN>
<TOKEN id="token-72-18" pos="word" morph="none" start_char="3928" end_char="3930">the</TOKEN>
<TOKEN id="token-72-19" pos="word" morph="none" start_char="3932" end_char="3936">virus</TOKEN>
<TOKEN id="token-72-20" pos="word" morph="none" start_char="3938" end_char="3939">to</TOKEN>
<TOKEN id="token-72-21" pos="word" morph="none" start_char="3941" end_char="3948">transfer</TOKEN>
<TOKEN id="token-72-22" pos="word" morph="none" start_char="3950" end_char="3953">from</TOKEN>
<TOKEN id="token-72-23" pos="word" morph="none" start_char="3955" end_char="3959">Spain</TOKEN>
<TOKEN id="token-72-24" pos="word" morph="none" start_char="3961" end_char="3962">to</TOKEN>
<TOKEN id="token-72-25" pos="word" morph="none" start_char="3964" end_char="3968">china</TOKEN>
<TOKEN id="token-72-26" pos="punct" morph="none" start_char="3969" end_char="3969">,</TOKEN>
<TOKEN id="token-72-27" pos="word" morph="none" start_char="3971" end_char="3973">but</TOKEN>
<TOKEN id="token-72-28" pos="word" morph="none" start_char="3975" end_char="3977">not</TOKEN>
<TOKEN id="token-72-29" pos="word" morph="none" start_char="3979" end_char="3981">the</TOKEN>
<TOKEN id="token-72-30" pos="word" morph="none" start_char="3983" end_char="3987">other</TOKEN>
<TOKEN id="token-72-31" pos="word" morph="none" start_char="3989" end_char="3991">way</TOKEN>
<TOKEN id="token-72-32" pos="word" morph="none" start_char="3993" end_char="3997">round</TOKEN>
<TOKEN id="token-72-33" pos="punct" morph="none" start_char="3998" end_char="4000">.).</TOKEN>
</SEG>
<SEG id="segment-73" start_char="4003" end_char="4179">
<ORIGINAL_TEXT>I also look forward to your explanation of how the virus didn't spread much while it was in Spain, France, Brazil or wherever, but suddenly became massively infections in China.</ORIGINAL_TEXT>
<TOKEN id="token-73-0" pos="word" morph="none" start_char="4003" end_char="4003">I</TOKEN>
<TOKEN id="token-73-1" pos="word" morph="none" start_char="4005" end_char="4008">also</TOKEN>
<TOKEN id="token-73-2" pos="word" morph="none" start_char="4010" end_char="4013">look</TOKEN>
<TOKEN id="token-73-3" pos="word" morph="none" start_char="4015" end_char="4021">forward</TOKEN>
<TOKEN id="token-73-4" pos="word" morph="none" start_char="4023" end_char="4024">to</TOKEN>
<TOKEN id="token-73-5" pos="word" morph="none" start_char="4026" end_char="4029">your</TOKEN>
<TOKEN id="token-73-6" pos="word" morph="none" start_char="4031" end_char="4041">explanation</TOKEN>
<TOKEN id="token-73-7" pos="word" morph="none" start_char="4043" end_char="4044">of</TOKEN>
<TOKEN id="token-73-8" pos="word" morph="none" start_char="4046" end_char="4048">how</TOKEN>
<TOKEN id="token-73-9" pos="word" morph="none" start_char="4050" end_char="4052">the</TOKEN>
<TOKEN id="token-73-10" pos="word" morph="none" start_char="4054" end_char="4058">virus</TOKEN>
<TOKEN id="token-73-11" pos="word" morph="none" start_char="4060" end_char="4065">didn't</TOKEN>
<TOKEN id="token-73-12" pos="word" morph="none" start_char="4067" end_char="4072">spread</TOKEN>
<TOKEN id="token-73-13" pos="word" morph="none" start_char="4074" end_char="4077">much</TOKEN>
<TOKEN id="token-73-14" pos="word" morph="none" start_char="4079" end_char="4083">while</TOKEN>
<TOKEN id="token-73-15" pos="word" morph="none" start_char="4085" end_char="4086">it</TOKEN>
<TOKEN id="token-73-16" pos="word" morph="none" start_char="4088" end_char="4090">was</TOKEN>
<TOKEN id="token-73-17" pos="word" morph="none" start_char="4092" end_char="4093">in</TOKEN>
<TOKEN id="token-73-18" pos="word" morph="none" start_char="4095" end_char="4099">Spain</TOKEN>
<TOKEN id="token-73-19" pos="punct" morph="none" start_char="4100" end_char="4100">,</TOKEN>
<TOKEN id="token-73-20" pos="word" morph="none" start_char="4102" end_char="4107">France</TOKEN>
<TOKEN id="token-73-21" pos="punct" morph="none" start_char="4108" end_char="4108">,</TOKEN>
<TOKEN id="token-73-22" pos="word" morph="none" start_char="4110" end_char="4115">Brazil</TOKEN>
<TOKEN id="token-73-23" pos="word" morph="none" start_char="4117" end_char="4118">or</TOKEN>
<TOKEN id="token-73-24" pos="word" morph="none" start_char="4120" end_char="4127">wherever</TOKEN>
<TOKEN id="token-73-25" pos="punct" morph="none" start_char="4128" end_char="4128">,</TOKEN>
<TOKEN id="token-73-26" pos="word" morph="none" start_char="4130" end_char="4132">but</TOKEN>
<TOKEN id="token-73-27" pos="word" morph="none" start_char="4134" end_char="4141">suddenly</TOKEN>
<TOKEN id="token-73-28" pos="word" morph="none" start_char="4143" end_char="4148">became</TOKEN>
<TOKEN id="token-73-29" pos="word" morph="none" start_char="4150" end_char="4158">massively</TOKEN>
<TOKEN id="token-73-30" pos="word" morph="none" start_char="4160" end_char="4169">infections</TOKEN>
<TOKEN id="token-73-31" pos="word" morph="none" start_char="4171" end_char="4172">in</TOKEN>
<TOKEN id="token-73-32" pos="word" morph="none" start_char="4174" end_char="4178">China</TOKEN>
<TOKEN id="token-73-33" pos="punct" morph="none" start_char="4179" end_char="4179">.</TOKEN>
</SEG>
<SEG id="segment-74" start_char="4182" end_char="4240">
<ORIGINAL_TEXT>But, those observations and deductions are merely evidence.</ORIGINAL_TEXT>
<TOKEN id="token-74-0" pos="word" morph="none" start_char="4182" end_char="4184">But</TOKEN>
<TOKEN id="token-74-1" pos="punct" morph="none" start_char="4185" end_char="4185">,</TOKEN>
<TOKEN id="token-74-2" pos="word" morph="none" start_char="4187" end_char="4191">those</TOKEN>
<TOKEN id="token-74-3" pos="word" morph="none" start_char="4193" end_char="4204">observations</TOKEN>
<TOKEN id="token-74-4" pos="word" morph="none" start_char="4206" end_char="4208">and</TOKEN>
<TOKEN id="token-74-5" pos="word" morph="none" start_char="4210" end_char="4219">deductions</TOKEN>
<TOKEN id="token-74-6" pos="word" morph="none" start_char="4221" end_char="4223">are</TOKEN>
<TOKEN id="token-74-7" pos="word" morph="none" start_char="4225" end_char="4230">merely</TOKEN>
<TOKEN id="token-74-8" pos="word" morph="none" start_char="4232" end_char="4239">evidence</TOKEN>
<TOKEN id="token-74-9" pos="punct" morph="none" start_char="4240" end_char="4240">.</TOKEN>
</SEG>
<SEG id="segment-75" start_char="4242" end_char="4286">
<ORIGINAL_TEXT>I don't expect you to pay them any attention.</ORIGINAL_TEXT>
<TOKEN id="token-75-0" pos="word" morph="none" start_char="4242" end_char="4242">I</TOKEN>
<TOKEN id="token-75-1" pos="word" morph="none" start_char="4244" end_char="4248">don't</TOKEN>
<TOKEN id="token-75-2" pos="word" morph="none" start_char="4250" end_char="4255">expect</TOKEN>
<TOKEN id="token-75-3" pos="word" morph="none" start_char="4257" end_char="4259">you</TOKEN>
<TOKEN id="token-75-4" pos="word" morph="none" start_char="4261" end_char="4262">to</TOKEN>
<TOKEN id="token-75-5" pos="word" morph="none" start_char="4264" end_char="4266">pay</TOKEN>
<TOKEN id="token-75-6" pos="word" morph="none" start_char="4268" end_char="4271">them</TOKEN>
<TOKEN id="token-75-7" pos="word" morph="none" start_char="4273" end_char="4275">any</TOKEN>
<TOKEN id="token-75-8" pos="word" morph="none" start_char="4277" end_char="4285">attention</TOKEN>
<TOKEN id="token-75-9" pos="punct" morph="none" start_char="4286" end_char="4286">.</TOKEN>
</SEG>
<SEG id="segment-76" start_char="4291" end_char="4346">
<ORIGINAL_TEXT>The retirement home is a 4 hour drive from the facility.</ORIGINAL_TEXT>
<TOKEN id="token-76-0" pos="word" morph="none" start_char="4291" end_char="4293">The</TOKEN>
<TOKEN id="token-76-1" pos="word" morph="none" start_char="4295" end_char="4304">retirement</TOKEN>
<TOKEN id="token-76-2" pos="word" morph="none" start_char="4306" end_char="4309">home</TOKEN>
<TOKEN id="token-76-3" pos="word" morph="none" start_char="4311" end_char="4312">is</TOKEN>
<TOKEN id="token-76-4" pos="word" morph="none" start_char="4314" end_char="4314">a</TOKEN>
<TOKEN id="token-76-5" pos="word" morph="none" start_char="4316" end_char="4316">4</TOKEN>
<TOKEN id="token-76-6" pos="word" morph="none" start_char="4318" end_char="4321">hour</TOKEN>
<TOKEN id="token-76-7" pos="word" morph="none" start_char="4323" end_char="4327">drive</TOKEN>
<TOKEN id="token-76-8" pos="word" morph="none" start_char="4329" end_char="4332">from</TOKEN>
<TOKEN id="token-76-9" pos="word" morph="none" start_char="4334" end_char="4336">the</TOKEN>
<TOKEN id="token-76-10" pos="word" morph="none" start_char="4338" end_char="4345">facility</TOKEN>
<TOKEN id="token-76-11" pos="punct" morph="none" start_char="4346" end_char="4346">.</TOKEN>
</SEG>
<SEG id="segment-77" start_char="4351" end_char="4431">
<ORIGINAL_TEXT>https://theconversation.com/was-coronavirus-really-in-europe-in-march-2019-141582</ORIGINAL_TEXT>
<TOKEN id="token-77-0" pos="url" morph="none" start_char="4351" end_char="4431">https://theconversation.com/was-coronavirus-really-in-europe-in-march-2019-141582</TOKEN>
</SEG>
<SEG id="segment-78" start_char="4435" end_char="4482">
<ORIGINAL_TEXT>Quote from: Bored chemist on 06/02/2021 20:57:05</ORIGINAL_TEXT>
<TOKEN id="token-78-0" pos="word" morph="none" start_char="4435" end_char="4439">Quote</TOKEN>
<TOKEN id="token-78-1" pos="word" morph="none" start_char="4441" end_char="4444">from</TOKEN>
<TOKEN id="token-78-2" pos="punct" morph="none" start_char="4445" end_char="4445">:</TOKEN>
<TOKEN id="token-78-3" pos="word" morph="none" start_char="4447" end_char="4451">Bored</TOKEN>
<TOKEN id="token-78-4" pos="word" morph="none" start_char="4453" end_char="4459">chemist</TOKEN>
<TOKEN id="token-78-5" pos="word" morph="none" start_char="4461" end_char="4462">on</TOKEN>
<TOKEN id="token-78-6" pos="unknown" morph="none" start_char="4464" end_char="4473">06/02/2021</TOKEN>
<TOKEN id="token-78-7" pos="unknown" morph="none" start_char="4475" end_char="4482">20:57:05</TOKEN>
</SEG>
<SEG id="segment-79" start_char="4485" end_char="4525">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 20:27:47</ORIGINAL_TEXT>
<TOKEN id="token-79-0" pos="word" morph="none" start_char="4485" end_char="4489">Quote</TOKEN>
<TOKEN id="token-79-1" pos="word" morph="none" start_char="4491" end_char="4494">from</TOKEN>
<TOKEN id="token-79-2" pos="punct" morph="none" start_char="4495" end_char="4495">:</TOKEN>
<TOKEN id="token-79-3" pos="word" morph="none" start_char="4497" end_char="4502">Jolly2</TOKEN>
<TOKEN id="token-79-4" pos="word" morph="none" start_char="4504" end_char="4505">on</TOKEN>
<TOKEN id="token-79-5" pos="unknown" morph="none" start_char="4507" end_char="4516">06/02/2021</TOKEN>
<TOKEN id="token-79-6" pos="unknown" morph="none" start_char="4518" end_char="4525">20:27:47</TOKEN>
</SEG>
<SEG id="segment-80" start_char="4528" end_char="4575">
<ORIGINAL_TEXT>Quote from: Bored chemist on 06/02/2021 20:19:01</ORIGINAL_TEXT>
<TOKEN id="token-80-0" pos="word" morph="none" start_char="4528" end_char="4532">Quote</TOKEN>
<TOKEN id="token-80-1" pos="word" morph="none" start_char="4534" end_char="4537">from</TOKEN>
<TOKEN id="token-80-2" pos="punct" morph="none" start_char="4538" end_char="4538">:</TOKEN>
<TOKEN id="token-80-3" pos="word" morph="none" start_char="4540" end_char="4544">Bored</TOKEN>
<TOKEN id="token-80-4" pos="word" morph="none" start_char="4546" end_char="4552">chemist</TOKEN>
<TOKEN id="token-80-5" pos="word" morph="none" start_char="4554" end_char="4555">on</TOKEN>
<TOKEN id="token-80-6" pos="unknown" morph="none" start_char="4557" end_char="4566">06/02/2021</TOKEN>
<TOKEN id="token-80-7" pos="unknown" morph="none" start_char="4568" end_char="4575">20:19:01</TOKEN>
</SEG>
<SEG id="segment-81" start_char="4578" end_char="4618">
<ORIGINAL_TEXT>Quote from: Jolly2 on 06/02/2021 20:16:16</ORIGINAL_TEXT>
<TOKEN id="token-81-0" pos="word" morph="none" start_char="4578" end_char="4582">Quote</TOKEN>
<TOKEN id="token-81-1" pos="word" morph="none" start_char="4584" end_char="4587">from</TOKEN>
<TOKEN id="token-81-2" pos="punct" morph="none" start_char="4588" end_char="4588">:</TOKEN>
<TOKEN id="token-81-3" pos="word" morph="none" start_char="4590" end_char="4595">Jolly2</TOKEN>
<TOKEN id="token-81-4" pos="word" morph="none" start_char="4597" end_char="4598">on</TOKEN>
<TOKEN id="token-81-5" pos="unknown" morph="none" start_char="4600" end_char="4609">06/02/2021</TOKEN>
<TOKEN id="token-81-6" pos="unknown" morph="none" start_char="4611" end_char="4618">20:16:16</TOKEN>
</SEG>
<SEG id="segment-82" start_char="4621" end_char="4649">
<ORIGINAL_TEXT>they found samples in sewage.</ORIGINAL_TEXT>
<TOKEN id="token-82-0" pos="word" morph="none" start_char="4621" end_char="4624">they</TOKEN>
<TOKEN id="token-82-1" pos="word" morph="none" start_char="4626" end_char="4630">found</TOKEN>
<TOKEN id="token-82-2" pos="word" morph="none" start_char="4632" end_char="4638">samples</TOKEN>
<TOKEN id="token-82-3" pos="word" morph="none" start_char="4640" end_char="4641">in</TOKEN>
<TOKEN id="token-82-4" pos="word" morph="none" start_char="4643" end_char="4648">sewage</TOKEN>
<TOKEN id="token-82-5" pos="punct" morph="none" start_char="4649" end_char="4649">.</TOKEN>
</SEG>
<SEG id="segment-83" start_char="4652" end_char="4805">
<ORIGINAL_TEXT>How sure are you of that "fact"?Sure enough to post it here for discussion Then I guess I can do the same.Quote from: Bored chemist on 05/02/2021 18:56:50</ORIGINAL_TEXT>
<TOKEN id="token-83-0" pos="word" morph="none" start_char="4652" end_char="4654">How</TOKEN>
<TOKEN id="token-83-1" pos="word" morph="none" start_char="4656" end_char="4659">sure</TOKEN>
<TOKEN id="token-83-2" pos="word" morph="none" start_char="4661" end_char="4663">are</TOKEN>
<TOKEN id="token-83-3" pos="word" morph="none" start_char="4665" end_char="4667">you</TOKEN>
<TOKEN id="token-83-4" pos="word" morph="none" start_char="4669" end_char="4670">of</TOKEN>
<TOKEN id="token-83-5" pos="word" morph="none" start_char="4672" end_char="4675">that</TOKEN>
<TOKEN id="token-83-6" pos="punct" morph="none" start_char="4677" end_char="4677">"</TOKEN>
<TOKEN id="token-83-7" pos="unknown" morph="none" start_char="4678" end_char="4687">fact"?Sure</TOKEN>
<TOKEN id="token-83-8" pos="word" morph="none" start_char="4689" end_char="4694">enough</TOKEN>
<TOKEN id="token-83-9" pos="word" morph="none" start_char="4696" end_char="4697">to</TOKEN>
<TOKEN id="token-83-10" pos="word" morph="none" start_char="4699" end_char="4702">post</TOKEN>
<TOKEN id="token-83-11" pos="word" morph="none" start_char="4704" end_char="4705">it</TOKEN>
<TOKEN id="token-83-12" pos="word" morph="none" start_char="4707" end_char="4710">here</TOKEN>
<TOKEN id="token-83-13" pos="word" morph="none" start_char="4712" end_char="4714">for</TOKEN>
<TOKEN id="token-83-14" pos="word" morph="none" start_char="4716" end_char="4725">discussion</TOKEN>
<TOKEN id="token-83-15" pos="word" morph="none" start_char="4727" end_char="4730">Then</TOKEN>
<TOKEN id="token-83-16" pos="word" morph="none" start_char="4732" end_char="4732">I</TOKEN>
<TOKEN id="token-83-17" pos="word" morph="none" start_char="4734" end_char="4738">guess</TOKEN>
<TOKEN id="token-83-18" pos="word" morph="none" start_char="4740" end_char="4740">I</TOKEN>
<TOKEN id="token-83-19" pos="word" morph="none" start_char="4742" end_char="4744">can</TOKEN>
<TOKEN id="token-83-20" pos="word" morph="none" start_char="4746" end_char="4747">do</TOKEN>
<TOKEN id="token-83-21" pos="word" morph="none" start_char="4749" end_char="4751">the</TOKEN>
<TOKEN id="token-83-22" pos="unknown" morph="none" start_char="4753" end_char="4762">same.Quote</TOKEN>
<TOKEN id="token-83-23" pos="word" morph="none" start_char="4764" end_char="4767">from</TOKEN>
<TOKEN id="token-83-24" pos="punct" morph="none" start_char="4768" end_char="4768">:</TOKEN>
<TOKEN id="token-83-25" pos="word" morph="none" start_char="4770" end_char="4774">Bored</TOKEN>
<TOKEN id="token-83-26" pos="word" morph="none" start_char="4776" end_char="4782">chemist</TOKEN>
<TOKEN id="token-83-27" pos="word" morph="none" start_char="4784" end_char="4785">on</TOKEN>
<TOKEN id="token-83-28" pos="unknown" morph="none" start_char="4787" end_char="4796">05/02/2021</TOKEN>
<TOKEN id="token-83-29" pos="unknown" morph="none" start_char="4798" end_char="4805">18:56:50</TOKEN>
</SEG>
<SEG id="segment-84" start_char="4808" end_char="4837">
<ORIGINAL_TEXT>"Where did covid 19 originate?</ORIGINAL_TEXT>
<TOKEN id="token-84-0" pos="punct" morph="none" start_char="4808" end_char="4808">"</TOKEN>
<TOKEN id="token-84-1" pos="word" morph="none" start_char="4809" end_char="4813">Where</TOKEN>
<TOKEN id="token-84-2" pos="word" morph="none" start_char="4815" end_char="4817">did</TOKEN>
<TOKEN id="token-84-3" pos="word" morph="none" start_char="4819" end_char="4823">covid</TOKEN>
<TOKEN id="token-84-4" pos="word" morph="none" start_char="4825" end_char="4826">19</TOKEN>
<TOKEN id="token-84-5" pos="word" morph="none" start_char="4828" end_char="4836">originate</TOKEN>
<TOKEN id="token-84-6" pos="punct" morph="none" start_char="4837" end_char="4837">?</TOKEN>
</SEG>
<SEG id="segment-85" start_char="4839" end_char="4892">
<ORIGINAL_TEXT>"Wuhan.Do you have any actual evidence that I'm wrong?</ORIGINAL_TEXT>
<TOKEN id="token-85-0" pos="punct" morph="none" start_char="4839" end_char="4839">"</TOKEN>
<TOKEN id="token-85-1" pos="unknown" morph="none" start_char="4840" end_char="4847">Wuhan.Do</TOKEN>
<TOKEN id="token-85-2" pos="word" morph="none" start_char="4849" end_char="4851">you</TOKEN>
<TOKEN id="token-85-3" pos="word" morph="none" start_char="4853" end_char="4856">have</TOKEN>
<TOKEN id="token-85-4" pos="word" morph="none" start_char="4858" end_char="4860">any</TOKEN>
<TOKEN id="token-85-5" pos="word" morph="none" start_char="4862" end_char="4867">actual</TOKEN>
<TOKEN id="token-85-6" pos="word" morph="none" start_char="4869" end_char="4876">evidence</TOKEN>
<TOKEN id="token-85-7" pos="word" morph="none" start_char="4878" end_char="4881">that</TOKEN>
<TOKEN id="token-85-8" pos="word" morph="none" start_char="4883" end_char="4885">I'm</TOKEN>
<TOKEN id="token-85-9" pos="word" morph="none" start_char="4887" end_char="4891">wrong</TOKEN>
<TOKEN id="token-85-10" pos="punct" morph="none" start_char="4892" end_char="4892">?</TOKEN>
</SEG>
<SEG id="segment-86" start_char="4894" end_char="5039">
<ORIGINAL_TEXT>(To do that you would have to prove that it was- for example- possible for the virus to transfer from Spain to china, but not the other way round.</ORIGINAL_TEXT>
<TOKEN id="token-86-0" pos="punct" morph="none" start_char="4894" end_char="4894">(</TOKEN>
<TOKEN id="token-86-1" pos="word" morph="none" start_char="4895" end_char="4896">To</TOKEN>
<TOKEN id="token-86-2" pos="word" morph="none" start_char="4898" end_char="4899">do</TOKEN>
<TOKEN id="token-86-3" pos="word" morph="none" start_char="4901" end_char="4904">that</TOKEN>
<TOKEN id="token-86-4" pos="word" morph="none" start_char="4906" end_char="4908">you</TOKEN>
<TOKEN id="token-86-5" pos="word" morph="none" start_char="4910" end_char="4914">would</TOKEN>
<TOKEN id="token-86-6" pos="word" morph="none" start_char="4916" end_char="4919">have</TOKEN>
<TOKEN id="token-86-7" pos="word" morph="none" start_char="4921" end_char="4922">to</TOKEN>
<TOKEN id="token-86-8" pos="word" morph="none" start_char="4924" end_char="4928">prove</TOKEN>
<TOKEN id="token-86-9" pos="word" morph="none" start_char="4930" end_char="4933">that</TOKEN>
<TOKEN id="token-86-10" pos="word" morph="none" start_char="4935" end_char="4936">it</TOKEN>
<TOKEN id="token-86-11" pos="word" morph="none" start_char="4938" end_char="4940">was</TOKEN>
<TOKEN id="token-86-12" pos="punct" morph="none" start_char="4941" end_char="4941">-</TOKEN>
<TOKEN id="token-86-13" pos="word" morph="none" start_char="4943" end_char="4945">for</TOKEN>
<TOKEN id="token-86-14" pos="word" morph="none" start_char="4947" end_char="4953">example</TOKEN>
<TOKEN id="token-86-15" pos="punct" morph="none" start_char="4954" end_char="4954">-</TOKEN>
<TOKEN id="token-86-16" pos="word" morph="none" start_char="4956" end_char="4963">possible</TOKEN>
<TOKEN id="token-86-17" pos="word" morph="none" start_char="4965" end_char="4967">for</TOKEN>
<TOKEN id="token-86-18" pos="word" morph="none" start_char="4969" end_char="4971">the</TOKEN>
<TOKEN id="token-86-19" pos="word" morph="none" start_char="4973" end_char="4977">virus</TOKEN>
<TOKEN id="token-86-20" pos="word" morph="none" start_char="4979" end_char="4980">to</TOKEN>
<TOKEN id="token-86-21" pos="word" morph="none" start_char="4982" end_char="4989">transfer</TOKEN>
<TOKEN id="token-86-22" pos="word" morph="none" start_char="4991" end_char="4994">from</TOKEN>
<TOKEN id="token-86-23" pos="word" morph="none" start_char="4996" end_char="5000">Spain</TOKEN>
<TOKEN id="token-86-24" pos="word" morph="none" start_char="5002" end_char="5003">to</TOKEN>
<TOKEN id="token-86-25" pos="word" morph="none" start_char="5005" end_char="5009">china</TOKEN>
<TOKEN id="token-86-26" pos="punct" morph="none" start_char="5010" end_char="5010">,</TOKEN>
<TOKEN id="token-86-27" pos="word" morph="none" start_char="5012" end_char="5014">but</TOKEN>
<TOKEN id="token-86-28" pos="word" morph="none" start_char="5016" end_char="5018">not</TOKEN>
<TOKEN id="token-86-29" pos="word" morph="none" start_char="5020" end_char="5022">the</TOKEN>
<TOKEN id="token-86-30" pos="word" morph="none" start_char="5024" end_char="5028">other</TOKEN>
<TOKEN id="token-86-31" pos="word" morph="none" start_char="5030" end_char="5032">way</TOKEN>
<TOKEN id="token-86-32" pos="word" morph="none" start_char="5034" end_char="5038">round</TOKEN>
<TOKEN id="token-86-33" pos="punct" morph="none" start_char="5039" end_char="5039">.</TOKEN>
</SEG>
<SEG id="segment-87" start_char="5041" end_char="5323">
<ORIGINAL_TEXT>).I also look forward to your explanation of how the virus didn't spread much while it was in Spain, France, Brazil or wherever, but suddenly became massively infections in China.But, those observations and deductions are merely evidence.I don't expect you to pay them any attention.</ORIGINAL_TEXT>
<TOKEN id="token-87-0" pos="punct" morph="none" start_char="5041" end_char="5042">).</TOKEN>
<TOKEN id="token-87-1" pos="word" morph="none" start_char="5043" end_char="5043">I</TOKEN>
<TOKEN id="token-87-2" pos="word" morph="none" start_char="5045" end_char="5048">also</TOKEN>
<TOKEN id="token-87-3" pos="word" morph="none" start_char="5050" end_char="5053">look</TOKEN>
<TOKEN id="token-87-4" pos="word" morph="none" start_char="5055" end_char="5061">forward</TOKEN>
<TOKEN id="token-87-5" pos="word" morph="none" start_char="5063" end_char="5064">to</TOKEN>
<TOKEN id="token-87-6" pos="word" morph="none" start_char="5066" end_char="5069">your</TOKEN>
<TOKEN id="token-87-7" pos="word" morph="none" start_char="5071" end_char="5081">explanation</TOKEN>
<TOKEN id="token-87-8" pos="word" morph="none" start_char="5083" end_char="5084">of</TOKEN>
<TOKEN id="token-87-9" pos="word" morph="none" start_char="5086" end_char="5088">how</TOKEN>
<TOKEN id="token-87-10" pos="word" morph="none" start_char="5090" end_char="5092">the</TOKEN>
<TOKEN id="token-87-11" pos="word" morph="none" start_char="5094" end_char="5098">virus</TOKEN>
<TOKEN id="token-87-12" pos="word" morph="none" start_char="5100" end_char="5105">didn't</TOKEN>
<TOKEN id="token-87-13" pos="word" morph="none" start_char="5107" end_char="5112">spread</TOKEN>
<TOKEN id="token-87-14" pos="word" morph="none" start_char="5114" end_char="5117">much</TOKEN>
<TOKEN id="token-87-15" pos="word" morph="none" start_char="5119" end_char="5123">while</TOKEN>
<TOKEN id="token-87-16" pos="word" morph="none" start_char="5125" end_char="5126">it</TOKEN>
<TOKEN id="token-87-17" pos="word" morph="none" start_char="5128" end_char="5130">was</TOKEN>
<TOKEN id="token-87-18" pos="word" morph="none" start_char="5132" end_char="5133">in</TOKEN>
<TOKEN id="token-87-19" pos="word" morph="none" start_char="5135" end_char="5139">Spain</TOKEN>
<TOKEN id="token-87-20" pos="punct" morph="none" start_char="5140" end_char="5140">,</TOKEN>
<TOKEN id="token-87-21" pos="word" morph="none" start_char="5142" end_char="5147">France</TOKEN>
<TOKEN id="token-87-22" pos="punct" morph="none" start_char="5148" end_char="5148">,</TOKEN>
<TOKEN id="token-87-23" pos="word" morph="none" start_char="5150" end_char="5155">Brazil</TOKEN>
<TOKEN id="token-87-24" pos="word" morph="none" start_char="5157" end_char="5158">or</TOKEN>
<TOKEN id="token-87-25" pos="word" morph="none" start_char="5160" end_char="5167">wherever</TOKEN>
<TOKEN id="token-87-26" pos="punct" morph="none" start_char="5168" end_char="5168">,</TOKEN>
<TOKEN id="token-87-27" pos="word" morph="none" start_char="5170" end_char="5172">but</TOKEN>
<TOKEN id="token-87-28" pos="word" morph="none" start_char="5174" end_char="5181">suddenly</TOKEN>
<TOKEN id="token-87-29" pos="word" morph="none" start_char="5183" end_char="5188">became</TOKEN>
<TOKEN id="token-87-30" pos="word" morph="none" start_char="5190" end_char="5198">massively</TOKEN>
<TOKEN id="token-87-31" pos="word" morph="none" start_char="5200" end_char="5209">infections</TOKEN>
<TOKEN id="token-87-32" pos="word" morph="none" start_char="5211" end_char="5212">in</TOKEN>
<TOKEN id="token-87-33" pos="unknown" morph="none" start_char="5214" end_char="5222">China.But</TOKEN>
<TOKEN id="token-87-34" pos="punct" morph="none" start_char="5223" end_char="5223">,</TOKEN>
<TOKEN id="token-87-35" pos="word" morph="none" start_char="5225" end_char="5229">those</TOKEN>
<TOKEN id="token-87-36" pos="word" morph="none" start_char="5231" end_char="5242">observations</TOKEN>
<TOKEN id="token-87-37" pos="word" morph="none" start_char="5244" end_char="5246">and</TOKEN>
<TOKEN id="token-87-38" pos="word" morph="none" start_char="5248" end_char="5257">deductions</TOKEN>
<TOKEN id="token-87-39" pos="word" morph="none" start_char="5259" end_char="5261">are</TOKEN>
<TOKEN id="token-87-40" pos="word" morph="none" start_char="5263" end_char="5268">merely</TOKEN>
<TOKEN id="token-87-41" pos="unknown" morph="none" start_char="5270" end_char="5279">evidence.I</TOKEN>
<TOKEN id="token-87-42" pos="word" morph="none" start_char="5281" end_char="5285">don't</TOKEN>
<TOKEN id="token-87-43" pos="word" morph="none" start_char="5287" end_char="5292">expect</TOKEN>
<TOKEN id="token-87-44" pos="word" morph="none" start_char="5294" end_char="5296">you</TOKEN>
<TOKEN id="token-87-45" pos="word" morph="none" start_char="5298" end_char="5299">to</TOKEN>
<TOKEN id="token-87-46" pos="word" morph="none" start_char="5301" end_char="5303">pay</TOKEN>
<TOKEN id="token-87-47" pos="word" morph="none" start_char="5305" end_char="5308">them</TOKEN>
<TOKEN id="token-87-48" pos="word" morph="none" start_char="5310" end_char="5312">any</TOKEN>
<TOKEN id="token-87-49" pos="word" morph="none" start_char="5314" end_char="5322">attention</TOKEN>
<TOKEN id="token-87-50" pos="punct" morph="none" start_char="5323" end_char="5323">.</TOKEN>
</SEG>
<SEG id="segment-88" start_char="5326" end_char="5360">
<ORIGINAL_TEXT>China was the first to identify it.</ORIGINAL_TEXT>
<TOKEN id="token-88-0" pos="word" morph="none" start_char="5326" end_char="5330">China</TOKEN>
<TOKEN id="token-88-1" pos="word" morph="none" start_char="5332" end_char="5334">was</TOKEN>
<TOKEN id="token-88-2" pos="word" morph="none" start_char="5336" end_char="5338">the</TOKEN>
<TOKEN id="token-88-3" pos="word" morph="none" start_char="5340" end_char="5344">first</TOKEN>
<TOKEN id="token-88-4" pos="word" morph="none" start_char="5346" end_char="5347">to</TOKEN>
<TOKEN id="token-88-5" pos="word" morph="none" start_char="5349" end_char="5356">identify</TOKEN>
<TOKEN id="token-88-6" pos="word" morph="none" start_char="5358" end_char="5359">it</TOKEN>
<TOKEN id="token-88-7" pos="punct" morph="none" start_char="5360" end_char="5360">.</TOKEN>
</SEG>
<SEG id="segment-89" start_char="5362" end_char="5487">
<ORIGINAL_TEXT>Means it spread undetected, as the cambridge study suggested covid had been in China for around 4 months before they found it.</ORIGINAL_TEXT>
<TOKEN id="token-89-0" pos="word" morph="none" start_char="5362" end_char="5366">Means</TOKEN>
<TOKEN id="token-89-1" pos="word" morph="none" start_char="5368" end_char="5369">it</TOKEN>
<TOKEN id="token-89-2" pos="word" morph="none" start_char="5371" end_char="5376">spread</TOKEN>
<TOKEN id="token-89-3" pos="word" morph="none" start_char="5378" end_char="5387">undetected</TOKEN>
<TOKEN id="token-89-4" pos="punct" morph="none" start_char="5388" end_char="5388">,</TOKEN>
<TOKEN id="token-89-5" pos="word" morph="none" start_char="5390" end_char="5391">as</TOKEN>
<TOKEN id="token-89-6" pos="word" morph="none" start_char="5393" end_char="5395">the</TOKEN>
<TOKEN id="token-89-7" pos="word" morph="none" start_char="5397" end_char="5405">cambridge</TOKEN>
<TOKEN id="token-89-8" pos="word" morph="none" start_char="5407" end_char="5411">study</TOKEN>
<TOKEN id="token-89-9" pos="word" morph="none" start_char="5413" end_char="5421">suggested</TOKEN>
<TOKEN id="token-89-10" pos="word" morph="none" start_char="5423" end_char="5427">covid</TOKEN>
<TOKEN id="token-89-11" pos="word" morph="none" start_char="5429" end_char="5431">had</TOKEN>
<TOKEN id="token-89-12" pos="word" morph="none" start_char="5433" end_char="5436">been</TOKEN>
<TOKEN id="token-89-13" pos="word" morph="none" start_char="5438" end_char="5439">in</TOKEN>
<TOKEN id="token-89-14" pos="word" morph="none" start_char="5441" end_char="5445">China</TOKEN>
<TOKEN id="token-89-15" pos="word" morph="none" start_char="5447" end_char="5449">for</TOKEN>
<TOKEN id="token-89-16" pos="word" morph="none" start_char="5451" end_char="5456">around</TOKEN>
<TOKEN id="token-89-17" pos="word" morph="none" start_char="5458" end_char="5458">4</TOKEN>
<TOKEN id="token-89-18" pos="word" morph="none" start_char="5460" end_char="5465">months</TOKEN>
<TOKEN id="token-89-19" pos="word" morph="none" start_char="5467" end_char="5472">before</TOKEN>
<TOKEN id="token-89-20" pos="word" morph="none" start_char="5474" end_char="5477">they</TOKEN>
<TOKEN id="token-89-21" pos="word" morph="none" start_char="5479" end_char="5483">found</TOKEN>
<TOKEN id="token-89-22" pos="word" morph="none" start_char="5485" end_char="5486">it</TOKEN>
<TOKEN id="token-89-23" pos="punct" morph="none" start_char="5487" end_char="5487">.</TOKEN>
</SEG>
<SEG id="segment-90" start_char="5491" end_char="5508">
<ORIGINAL_TEXT>Quote from: Jolly2</ORIGINAL_TEXT>
<TOKEN id="token-90-0" pos="word" morph="none" start_char="5491" end_char="5495">Quote</TOKEN>
<TOKEN id="token-90-1" pos="word" morph="none" start_char="5497" end_char="5500">from</TOKEN>
<TOKEN id="token-90-2" pos="punct" morph="none" start_char="5501" end_char="5501">:</TOKEN>
<TOKEN id="token-90-3" pos="word" morph="none" start_char="5503" end_char="5508">Jolly2</TOKEN>
</SEG>
<SEG id="segment-91" start_char="5511" end_char="5569">
<ORIGINAL_TEXT>spain as early as March 2019...they found samples in sewage</ORIGINAL_TEXT>
<TOKEN id="token-91-0" pos="word" morph="none" start_char="5511" end_char="5515">spain</TOKEN>
<TOKEN id="token-91-1" pos="word" morph="none" start_char="5517" end_char="5518">as</TOKEN>
<TOKEN id="token-91-2" pos="word" morph="none" start_char="5520" end_char="5524">early</TOKEN>
<TOKEN id="token-91-3" pos="word" morph="none" start_char="5526" end_char="5527">as</TOKEN>
<TOKEN id="token-91-4" pos="word" morph="none" start_char="5529" end_char="5533">March</TOKEN>
<TOKEN id="token-91-5" pos="unknown" morph="none" start_char="5535" end_char="5545">2019...they</TOKEN>
<TOKEN id="token-91-6" pos="word" morph="none" start_char="5547" end_char="5551">found</TOKEN>
<TOKEN id="token-91-7" pos="word" morph="none" start_char="5553" end_char="5559">samples</TOKEN>
<TOKEN id="token-91-8" pos="word" morph="none" start_char="5561" end_char="5562">in</TOKEN>
<TOKEN id="token-91-9" pos="word" morph="none" start_char="5564" end_char="5569">sewage</TOKEN>
</SEG>
<SEG id="segment-92" start_char="5573" end_char="5618">
<ORIGINAL_TEXT>The quality of RNA in sewage is very variable.</ORIGINAL_TEXT>
<TOKEN id="token-92-0" pos="word" morph="none" start_char="5573" end_char="5575">The</TOKEN>
<TOKEN id="token-92-1" pos="word" morph="none" start_char="5577" end_char="5583">quality</TOKEN>
<TOKEN id="token-92-2" pos="word" morph="none" start_char="5585" end_char="5586">of</TOKEN>
<TOKEN id="token-92-3" pos="word" morph="none" start_char="5588" end_char="5590">RNA</TOKEN>
<TOKEN id="token-92-4" pos="word" morph="none" start_char="5592" end_char="5593">in</TOKEN>
<TOKEN id="token-92-5" pos="word" morph="none" start_char="5595" end_char="5600">sewage</TOKEN>
<TOKEN id="token-92-6" pos="word" morph="none" start_char="5602" end_char="5603">is</TOKEN>
<TOKEN id="token-92-7" pos="word" morph="none" start_char="5605" end_char="5608">very</TOKEN>
<TOKEN id="token-92-8" pos="word" morph="none" start_char="5610" end_char="5617">variable</TOKEN>
<TOKEN id="token-92-9" pos="punct" morph="none" start_char="5618" end_char="5618">.</TOKEN>
</SEG>
<SEG id="segment-93" start_char="5620" end_char="6150">
<ORIGINAL_TEXT>- We know that the fatty coat of the SARS-COV2 virus is broken down by soaps and detergents - Most people flush soap down the sewer when they take a bath or shower - Most people flush detergents down the sewer when they wash the dishes (and use even more destructive chemicals in the dishwasher) - Industrial processes can also flush destructive chemicals into sewers - So RNA is badly degraded when it is collected (within 1 week) - And, depending on how it is stored, may continue degrading if it is stored for months afterwards.</ORIGINAL_TEXT>
<TOKEN id="token-93-0" pos="punct" morph="none" start_char="5620" end_char="5620">-</TOKEN>
<TOKEN id="token-93-1" pos="word" morph="none" start_char="5622" end_char="5623">We</TOKEN>
<TOKEN id="token-93-2" pos="word" morph="none" start_char="5625" end_char="5628">know</TOKEN>
<TOKEN id="token-93-3" pos="word" morph="none" start_char="5630" end_char="5633">that</TOKEN>
<TOKEN id="token-93-4" pos="word" morph="none" start_char="5635" end_char="5637">the</TOKEN>
<TOKEN id="token-93-5" pos="word" morph="none" start_char="5639" end_char="5643">fatty</TOKEN>
<TOKEN id="token-93-6" pos="word" morph="none" start_char="5645" end_char="5648">coat</TOKEN>
<TOKEN id="token-93-7" pos="word" morph="none" start_char="5650" end_char="5651">of</TOKEN>
<TOKEN id="token-93-8" pos="word" morph="none" start_char="5653" end_char="5655">the</TOKEN>
<TOKEN id="token-93-9" pos="unknown" morph="none" start_char="5657" end_char="5665">SARS-COV2</TOKEN>
<TOKEN id="token-93-10" pos="word" morph="none" start_char="5667" end_char="5671">virus</TOKEN>
<TOKEN id="token-93-11" pos="word" morph="none" start_char="5673" end_char="5674">is</TOKEN>
<TOKEN id="token-93-12" pos="word" morph="none" start_char="5676" end_char="5681">broken</TOKEN>
<TOKEN id="token-93-13" pos="word" morph="none" start_char="5683" end_char="5686">down</TOKEN>
<TOKEN id="token-93-14" pos="word" morph="none" start_char="5688" end_char="5689">by</TOKEN>
<TOKEN id="token-93-15" pos="word" morph="none" start_char="5691" end_char="5695">soaps</TOKEN>
<TOKEN id="token-93-16" pos="word" morph="none" start_char="5697" end_char="5699">and</TOKEN>
<TOKEN id="token-93-17" pos="word" morph="none" start_char="5701" end_char="5710">detergents</TOKEN>
<TOKEN id="token-93-18" pos="punct" morph="none" start_char="5712" end_char="5712">-</TOKEN>
<TOKEN id="token-93-19" pos="word" morph="none" start_char="5714" end_char="5717">Most</TOKEN>
<TOKEN id="token-93-20" pos="word" morph="none" start_char="5719" end_char="5724">people</TOKEN>
<TOKEN id="token-93-21" pos="word" morph="none" start_char="5726" end_char="5730">flush</TOKEN>
<TOKEN id="token-93-22" pos="word" morph="none" start_char="5732" end_char="5735">soap</TOKEN>
<TOKEN id="token-93-23" pos="word" morph="none" start_char="5737" end_char="5740">down</TOKEN>
<TOKEN id="token-93-24" pos="word" morph="none" start_char="5742" end_char="5744">the</TOKEN>
<TOKEN id="token-93-25" pos="word" morph="none" start_char="5746" end_char="5750">sewer</TOKEN>
<TOKEN id="token-93-26" pos="word" morph="none" start_char="5752" end_char="5755">when</TOKEN>
<TOKEN id="token-93-27" pos="word" morph="none" start_char="5757" end_char="5760">they</TOKEN>
<TOKEN id="token-93-28" pos="word" morph="none" start_char="5762" end_char="5765">take</TOKEN>
<TOKEN id="token-93-29" pos="word" morph="none" start_char="5767" end_char="5767">a</TOKEN>
<TOKEN id="token-93-30" pos="word" morph="none" start_char="5769" end_char="5772">bath</TOKEN>
<TOKEN id="token-93-31" pos="word" morph="none" start_char="5774" end_char="5775">or</TOKEN>
<TOKEN id="token-93-32" pos="word" morph="none" start_char="5777" end_char="5782">shower</TOKEN>
<TOKEN id="token-93-33" pos="punct" morph="none" start_char="5784" end_char="5784">-</TOKEN>
<TOKEN id="token-93-34" pos="word" morph="none" start_char="5786" end_char="5789">Most</TOKEN>
<TOKEN id="token-93-35" pos="word" morph="none" start_char="5791" end_char="5796">people</TOKEN>
<TOKEN id="token-93-36" pos="word" morph="none" start_char="5798" end_char="5802">flush</TOKEN>
<TOKEN id="token-93-37" pos="word" morph="none" start_char="5804" end_char="5813">detergents</TOKEN>
<TOKEN id="token-93-38" pos="word" morph="none" start_char="5815" end_char="5818">down</TOKEN>
<TOKEN id="token-93-39" pos="word" morph="none" start_char="5820" end_char="5822">the</TOKEN>
<TOKEN id="token-93-40" pos="word" morph="none" start_char="5824" end_char="5828">sewer</TOKEN>
<TOKEN id="token-93-41" pos="word" morph="none" start_char="5830" end_char="5833">when</TOKEN>
<TOKEN id="token-93-42" pos="word" morph="none" start_char="5835" end_char="5838">they</TOKEN>
<TOKEN id="token-93-43" pos="word" morph="none" start_char="5840" end_char="5843">wash</TOKEN>
<TOKEN id="token-93-44" pos="word" morph="none" start_char="5845" end_char="5847">the</TOKEN>
<TOKEN id="token-93-45" pos="word" morph="none" start_char="5849" end_char="5854">dishes</TOKEN>
<TOKEN id="token-93-46" pos="punct" morph="none" start_char="5856" end_char="5856">(</TOKEN>
<TOKEN id="token-93-47" pos="word" morph="none" start_char="5857" end_char="5859">and</TOKEN>
<TOKEN id="token-93-48" pos="word" morph="none" start_char="5861" end_char="5863">use</TOKEN>
<TOKEN id="token-93-49" pos="word" morph="none" start_char="5865" end_char="5868">even</TOKEN>
<TOKEN id="token-93-50" pos="word" morph="none" start_char="5870" end_char="5873">more</TOKEN>
<TOKEN id="token-93-51" pos="word" morph="none" start_char="5875" end_char="5885">destructive</TOKEN>
<TOKEN id="token-93-52" pos="word" morph="none" start_char="5887" end_char="5895">chemicals</TOKEN>
<TOKEN id="token-93-53" pos="word" morph="none" start_char="5897" end_char="5898">in</TOKEN>
<TOKEN id="token-93-54" pos="word" morph="none" start_char="5900" end_char="5902">the</TOKEN>
<TOKEN id="token-93-55" pos="word" morph="none" start_char="5904" end_char="5913">dishwasher</TOKEN>
<TOKEN id="token-93-56" pos="punct" morph="none" start_char="5914" end_char="5914">)</TOKEN>
<TOKEN id="token-93-57" pos="punct" morph="none" start_char="5916" end_char="5916">-</TOKEN>
<TOKEN id="token-93-58" pos="word" morph="none" start_char="5918" end_char="5927">Industrial</TOKEN>
<TOKEN id="token-93-59" pos="word" morph="none" start_char="5929" end_char="5937">processes</TOKEN>
<TOKEN id="token-93-60" pos="word" morph="none" start_char="5939" end_char="5941">can</TOKEN>
<TOKEN id="token-93-61" pos="word" morph="none" start_char="5943" end_char="5946">also</TOKEN>
<TOKEN id="token-93-62" pos="word" morph="none" start_char="5948" end_char="5952">flush</TOKEN>
<TOKEN id="token-93-63" pos="word" morph="none" start_char="5954" end_char="5964">destructive</TOKEN>
<TOKEN id="token-93-64" pos="word" morph="none" start_char="5966" end_char="5974">chemicals</TOKEN>
<TOKEN id="token-93-65" pos="word" morph="none" start_char="5976" end_char="5979">into</TOKEN>
<TOKEN id="token-93-66" pos="word" morph="none" start_char="5981" end_char="5986">sewers</TOKEN>
<TOKEN id="token-93-67" pos="punct" morph="none" start_char="5988" end_char="5988">-</TOKEN>
<TOKEN id="token-93-68" pos="word" morph="none" start_char="5990" end_char="5991">So</TOKEN>
<TOKEN id="token-93-69" pos="word" morph="none" start_char="5993" end_char="5995">RNA</TOKEN>
<TOKEN id="token-93-70" pos="word" morph="none" start_char="5997" end_char="5998">is</TOKEN>
<TOKEN id="token-93-71" pos="word" morph="none" start_char="6000" end_char="6004">badly</TOKEN>
<TOKEN id="token-93-72" pos="word" morph="none" start_char="6006" end_char="6013">degraded</TOKEN>
<TOKEN id="token-93-73" pos="word" morph="none" start_char="6015" end_char="6018">when</TOKEN>
<TOKEN id="token-93-74" pos="word" morph="none" start_char="6020" end_char="6021">it</TOKEN>
<TOKEN id="token-93-75" pos="word" morph="none" start_char="6023" end_char="6024">is</TOKEN>
<TOKEN id="token-93-76" pos="word" morph="none" start_char="6026" end_char="6034">collected</TOKEN>
<TOKEN id="token-93-77" pos="punct" morph="none" start_char="6036" end_char="6036">(</TOKEN>
<TOKEN id="token-93-78" pos="word" morph="none" start_char="6037" end_char="6042">within</TOKEN>
<TOKEN id="token-93-79" pos="word" morph="none" start_char="6044" end_char="6044">1</TOKEN>
<TOKEN id="token-93-80" pos="word" morph="none" start_char="6046" end_char="6049">week</TOKEN>
<TOKEN id="token-93-81" pos="punct" morph="none" start_char="6050" end_char="6050">)</TOKEN>
<TOKEN id="token-93-82" pos="punct" morph="none" start_char="6052" end_char="6052">-</TOKEN>
<TOKEN id="token-93-83" pos="word" morph="none" start_char="6054" end_char="6056">And</TOKEN>
<TOKEN id="token-93-84" pos="punct" morph="none" start_char="6057" end_char="6057">,</TOKEN>
<TOKEN id="token-93-85" pos="word" morph="none" start_char="6059" end_char="6067">depending</TOKEN>
<TOKEN id="token-93-86" pos="word" morph="none" start_char="6069" end_char="6070">on</TOKEN>
<TOKEN id="token-93-87" pos="word" morph="none" start_char="6072" end_char="6074">how</TOKEN>
<TOKEN id="token-93-88" pos="word" morph="none" start_char="6076" end_char="6077">it</TOKEN>
<TOKEN id="token-93-89" pos="word" morph="none" start_char="6079" end_char="6080">is</TOKEN>
<TOKEN id="token-93-90" pos="word" morph="none" start_char="6082" end_char="6087">stored</TOKEN>
<TOKEN id="token-93-91" pos="punct" morph="none" start_char="6088" end_char="6088">,</TOKEN>
<TOKEN id="token-93-92" pos="word" morph="none" start_char="6090" end_char="6092">may</TOKEN>
<TOKEN id="token-93-93" pos="word" morph="none" start_char="6094" end_char="6101">continue</TOKEN>
<TOKEN id="token-93-94" pos="word" morph="none" start_char="6103" end_char="6111">degrading</TOKEN>
<TOKEN id="token-93-95" pos="word" morph="none" start_char="6113" end_char="6114">if</TOKEN>
<TOKEN id="token-93-96" pos="word" morph="none" start_char="6116" end_char="6117">it</TOKEN>
<TOKEN id="token-93-97" pos="word" morph="none" start_char="6119" end_char="6120">is</TOKEN>
<TOKEN id="token-93-98" pos="word" morph="none" start_char="6122" end_char="6127">stored</TOKEN>
<TOKEN id="token-93-99" pos="word" morph="none" start_char="6129" end_char="6131">for</TOKEN>
<TOKEN id="token-93-100" pos="word" morph="none" start_char="6133" end_char="6138">months</TOKEN>
<TOKEN id="token-93-101" pos="word" morph="none" start_char="6140" end_char="6149">afterwards</TOKEN>
<TOKEN id="token-93-102" pos="punct" morph="none" start_char="6150" end_char="6150">.</TOKEN>
</SEG>
<SEG id="segment-94" start_char="6153" end_char="6258">
<ORIGINAL_TEXT>So we have degraded RNA from stored sewage samples, which is compared to a new viral sequence (SARS-COV2).</ORIGINAL_TEXT>
<TOKEN id="token-94-0" pos="word" morph="none" start_char="6153" end_char="6154">So</TOKEN>
<TOKEN id="token-94-1" pos="word" morph="none" start_char="6156" end_char="6157">we</TOKEN>
<TOKEN id="token-94-2" pos="word" morph="none" start_char="6159" end_char="6162">have</TOKEN>
<TOKEN id="token-94-3" pos="word" morph="none" start_char="6164" end_char="6171">degraded</TOKEN>
<TOKEN id="token-94-4" pos="word" morph="none" start_char="6173" end_char="6175">RNA</TOKEN>
<TOKEN id="token-94-5" pos="word" morph="none" start_char="6177" end_char="6180">from</TOKEN>
<TOKEN id="token-94-6" pos="word" morph="none" start_char="6182" end_char="6187">stored</TOKEN>
<TOKEN id="token-94-7" pos="word" morph="none" start_char="6189" end_char="6194">sewage</TOKEN>
<TOKEN id="token-94-8" pos="word" morph="none" start_char="6196" end_char="6202">samples</TOKEN>
<TOKEN id="token-94-9" pos="punct" morph="none" start_char="6203" end_char="6203">,</TOKEN>
<TOKEN id="token-94-10" pos="word" morph="none" start_char="6205" end_char="6209">which</TOKEN>
<TOKEN id="token-94-11" pos="word" morph="none" start_char="6211" end_char="6212">is</TOKEN>
<TOKEN id="token-94-12" pos="word" morph="none" start_char="6214" end_char="6221">compared</TOKEN>
<TOKEN id="token-94-13" pos="word" morph="none" start_char="6223" end_char="6224">to</TOKEN>
<TOKEN id="token-94-14" pos="word" morph="none" start_char="6226" end_char="6226">a</TOKEN>
<TOKEN id="token-94-15" pos="word" morph="none" start_char="6228" end_char="6230">new</TOKEN>
<TOKEN id="token-94-16" pos="word" morph="none" start_char="6232" end_char="6236">viral</TOKEN>
<TOKEN id="token-94-17" pos="word" morph="none" start_char="6238" end_char="6245">sequence</TOKEN>
<TOKEN id="token-94-18" pos="punct" morph="none" start_char="6247" end_char="6247">(</TOKEN>
<TOKEN id="token-94-19" pos="unknown" morph="none" start_char="6248" end_char="6256">SARS-COV2</TOKEN>
<TOKEN id="token-94-20" pos="punct" morph="none" start_char="6257" end_char="6258">).</TOKEN>
</SEG>
<SEG id="segment-95" start_char="6260" end_char="6507">
<ORIGINAL_TEXT>- This comparison must exclude the 4 common coronaviruses that are responsible for something like 30% of the "common cold" infections - And residual infections of MERS and SARS, which have 50% (MERS) or 80% (SARS) genetic similarities to SARS-COV2.</ORIGINAL_TEXT>
<TOKEN id="token-95-0" pos="punct" morph="none" start_char="6260" end_char="6260">-</TOKEN>
<TOKEN id="token-95-1" pos="word" morph="none" start_char="6262" end_char="6265">This</TOKEN>
<TOKEN id="token-95-2" pos="word" morph="none" start_char="6267" end_char="6276">comparison</TOKEN>
<TOKEN id="token-95-3" pos="word" morph="none" start_char="6278" end_char="6281">must</TOKEN>
<TOKEN id="token-95-4" pos="word" morph="none" start_char="6283" end_char="6289">exclude</TOKEN>
<TOKEN id="token-95-5" pos="word" morph="none" start_char="6291" end_char="6293">the</TOKEN>
<TOKEN id="token-95-6" pos="word" morph="none" start_char="6295" end_char="6295">4</TOKEN>
<TOKEN id="token-95-7" pos="word" morph="none" start_char="6297" end_char="6302">common</TOKEN>
<TOKEN id="token-95-8" pos="word" morph="none" start_char="6304" end_char="6316">coronaviruses</TOKEN>
<TOKEN id="token-95-9" pos="word" morph="none" start_char="6318" end_char="6321">that</TOKEN>
<TOKEN id="token-95-10" pos="word" morph="none" start_char="6323" end_char="6325">are</TOKEN>
<TOKEN id="token-95-11" pos="word" morph="none" start_char="6327" end_char="6337">responsible</TOKEN>
<TOKEN id="token-95-12" pos="word" morph="none" start_char="6339" end_char="6341">for</TOKEN>
<TOKEN id="token-95-13" pos="word" morph="none" start_char="6343" end_char="6351">something</TOKEN>
<TOKEN id="token-95-14" pos="word" morph="none" start_char="6353" end_char="6356">like</TOKEN>
<TOKEN id="token-95-15" pos="word" morph="none" start_char="6358" end_char="6359">30</TOKEN>
<TOKEN id="token-95-16" pos="punct" morph="none" start_char="6360" end_char="6360">%</TOKEN>
<TOKEN id="token-95-17" pos="word" morph="none" start_char="6362" end_char="6363">of</TOKEN>
<TOKEN id="token-95-18" pos="word" morph="none" start_char="6365" end_char="6367">the</TOKEN>
<TOKEN id="token-95-19" pos="punct" morph="none" start_char="6369" end_char="6369">"</TOKEN>
<TOKEN id="token-95-20" pos="word" morph="none" start_char="6370" end_char="6375">common</TOKEN>
<TOKEN id="token-95-21" pos="word" morph="none" start_char="6377" end_char="6380">cold</TOKEN>
<TOKEN id="token-95-22" pos="punct" morph="none" start_char="6381" end_char="6381">"</TOKEN>
<TOKEN id="token-95-23" pos="word" morph="none" start_char="6383" end_char="6392">infections</TOKEN>
<TOKEN id="token-95-24" pos="punct" morph="none" start_char="6394" end_char="6394">-</TOKEN>
<TOKEN id="token-95-25" pos="word" morph="none" start_char="6396" end_char="6398">And</TOKEN>
<TOKEN id="token-95-26" pos="word" morph="none" start_char="6400" end_char="6407">residual</TOKEN>
<TOKEN id="token-95-27" pos="word" morph="none" start_char="6409" end_char="6418">infections</TOKEN>
<TOKEN id="token-95-28" pos="word" morph="none" start_char="6420" end_char="6421">of</TOKEN>
<TOKEN id="token-95-29" pos="word" morph="none" start_char="6423" end_char="6426">MERS</TOKEN>
<TOKEN id="token-95-30" pos="word" morph="none" start_char="6428" end_char="6430">and</TOKEN>
<TOKEN id="token-95-31" pos="word" morph="none" start_char="6432" end_char="6435">SARS</TOKEN>
<TOKEN id="token-95-32" pos="punct" morph="none" start_char="6436" end_char="6436">,</TOKEN>
<TOKEN id="token-95-33" pos="word" morph="none" start_char="6438" end_char="6442">which</TOKEN>
<TOKEN id="token-95-34" pos="word" morph="none" start_char="6444" end_char="6447">have</TOKEN>
<TOKEN id="token-95-35" pos="word" morph="none" start_char="6449" end_char="6450">50</TOKEN>
<TOKEN id="token-95-36" pos="punct" morph="none" start_char="6451" end_char="6451">%</TOKEN>
<TOKEN id="token-95-37" pos="punct" morph="none" start_char="6453" end_char="6453">(</TOKEN>
<TOKEN id="token-95-38" pos="word" morph="none" start_char="6454" end_char="6457">MERS</TOKEN>
<TOKEN id="token-95-39" pos="punct" morph="none" start_char="6458" end_char="6458">)</TOKEN>
<TOKEN id="token-95-40" pos="word" morph="none" start_char="6460" end_char="6461">or</TOKEN>
<TOKEN id="token-95-41" pos="word" morph="none" start_char="6463" end_char="6464">80</TOKEN>
<TOKEN id="token-95-42" pos="punct" morph="none" start_char="6465" end_char="6465">%</TOKEN>
<TOKEN id="token-95-43" pos="punct" morph="none" start_char="6467" end_char="6467">(</TOKEN>
<TOKEN id="token-95-44" pos="word" morph="none" start_char="6468" end_char="6471">SARS</TOKEN>
<TOKEN id="token-95-45" pos="punct" morph="none" start_char="6472" end_char="6472">)</TOKEN>
<TOKEN id="token-95-46" pos="word" morph="none" start_char="6474" end_char="6480">genetic</TOKEN>
<TOKEN id="token-95-47" pos="word" morph="none" start_char="6482" end_char="6493">similarities</TOKEN>
<TOKEN id="token-95-48" pos="word" morph="none" start_char="6495" end_char="6496">to</TOKEN>
<TOKEN id="token-95-49" pos="unknown" morph="none" start_char="6498" end_char="6506">SARS-COV2</TOKEN>
<TOKEN id="token-95-50" pos="punct" morph="none" start_char="6507" end_char="6507">.</TOKEN>
</SEG>
<SEG id="segment-96" start_char="6509" end_char="6619">
<ORIGINAL_TEXT>These viruses were never eliminated, but since R 1, the occasional local outbreak will occur, and then die out.</ORIGINAL_TEXT>
<TOKEN id="token-96-0" pos="word" morph="none" start_char="6509" end_char="6513">These</TOKEN>
<TOKEN id="token-96-1" pos="word" morph="none" start_char="6515" end_char="6521">viruses</TOKEN>
<TOKEN id="token-96-2" pos="word" morph="none" start_char="6523" end_char="6526">were</TOKEN>
<TOKEN id="token-96-3" pos="word" morph="none" start_char="6528" end_char="6532">never</TOKEN>
<TOKEN id="token-96-4" pos="word" morph="none" start_char="6534" end_char="6543">eliminated</TOKEN>
<TOKEN id="token-96-5" pos="punct" morph="none" start_char="6544" end_char="6544">,</TOKEN>
<TOKEN id="token-96-6" pos="word" morph="none" start_char="6546" end_char="6548">but</TOKEN>
<TOKEN id="token-96-7" pos="word" morph="none" start_char="6550" end_char="6554">since</TOKEN>
<TOKEN id="token-96-8" pos="word" morph="none" start_char="6556" end_char="6556">R</TOKEN>
<TOKEN id="token-96-9" pos="word" morph="none" start_char="6558" end_char="6558">1</TOKEN>
<TOKEN id="token-96-10" pos="punct" morph="none" start_char="6559" end_char="6559">,</TOKEN>
<TOKEN id="token-96-11" pos="word" morph="none" start_char="6561" end_char="6563">the</TOKEN>
<TOKEN id="token-96-12" pos="word" morph="none" start_char="6565" end_char="6574">occasional</TOKEN>
<TOKEN id="token-96-13" pos="word" morph="none" start_char="6576" end_char="6580">local</TOKEN>
<TOKEN id="token-96-14" pos="word" morph="none" start_char="6582" end_char="6589">outbreak</TOKEN>
<TOKEN id="token-96-15" pos="word" morph="none" start_char="6591" end_char="6594">will</TOKEN>
<TOKEN id="token-96-16" pos="word" morph="none" start_char="6596" end_char="6600">occur</TOKEN>
<TOKEN id="token-96-17" pos="punct" morph="none" start_char="6601" end_char="6601">,</TOKEN>
<TOKEN id="token-96-18" pos="word" morph="none" start_char="6603" end_char="6605">and</TOKEN>
<TOKEN id="token-96-19" pos="word" morph="none" start_char="6607" end_char="6610">then</TOKEN>
<TOKEN id="token-96-20" pos="word" morph="none" start_char="6612" end_char="6614">die</TOKEN>
<TOKEN id="token-96-21" pos="word" morph="none" start_char="6616" end_char="6618">out</TOKEN>
<TOKEN id="token-96-22" pos="punct" morph="none" start_char="6619" end_char="6619">.</TOKEN>
</SEG>
<SEG id="segment-97" start_char="6621" end_char="6717">
<ORIGINAL_TEXT>- There are other coronaviruses circulating in bats that may occasionally spill over into humans.</ORIGINAL_TEXT>
<TOKEN id="token-97-0" pos="punct" morph="none" start_char="6621" end_char="6621">-</TOKEN>
<TOKEN id="token-97-1" pos="word" morph="none" start_char="6623" end_char="6627">There</TOKEN>
<TOKEN id="token-97-2" pos="word" morph="none" start_char="6629" end_char="6631">are</TOKEN>
<TOKEN id="token-97-3" pos="word" morph="none" start_char="6633" end_char="6637">other</TOKEN>
<TOKEN id="token-97-4" pos="word" morph="none" start_char="6639" end_char="6651">coronaviruses</TOKEN>
<TOKEN id="token-97-5" pos="word" morph="none" start_char="6653" end_char="6663">circulating</TOKEN>
<TOKEN id="token-97-6" pos="word" morph="none" start_char="6665" end_char="6666">in</TOKEN>
<TOKEN id="token-97-7" pos="word" morph="none" start_char="6668" end_char="6671">bats</TOKEN>
<TOKEN id="token-97-8" pos="word" morph="none" start_char="6673" end_char="6676">that</TOKEN>
<TOKEN id="token-97-9" pos="word" morph="none" start_char="6678" end_char="6680">may</TOKEN>
<TOKEN id="token-97-10" pos="word" morph="none" start_char="6682" end_char="6693">occasionally</TOKEN>
<TOKEN id="token-97-11" pos="word" morph="none" start_char="6695" end_char="6699">spill</TOKEN>
<TOKEN id="token-97-12" pos="word" morph="none" start_char="6701" end_char="6704">over</TOKEN>
<TOKEN id="token-97-13" pos="word" morph="none" start_char="6706" end_char="6709">into</TOKEN>
<TOKEN id="token-97-14" pos="word" morph="none" start_char="6711" end_char="6716">humans</TOKEN>
<TOKEN id="token-97-15" pos="punct" morph="none" start_char="6717" end_char="6717">.</TOKEN>
</SEG>
<SEG id="segment-98" start_char="6719" end_char="6919">
<ORIGINAL_TEXT>But provided they aren't spread by human-to-human contact, these will show up as sporadic positive samples for coronavirus, without going on to create a pandemic, an epidemic, or even a local outbreak.</ORIGINAL_TEXT>
<TOKEN id="token-98-0" pos="word" morph="none" start_char="6719" end_char="6721">But</TOKEN>
<TOKEN id="token-98-1" pos="word" morph="none" start_char="6723" end_char="6730">provided</TOKEN>
<TOKEN id="token-98-2" pos="word" morph="none" start_char="6732" end_char="6735">they</TOKEN>
<TOKEN id="token-98-3" pos="word" morph="none" start_char="6737" end_char="6742">aren't</TOKEN>
<TOKEN id="token-98-4" pos="word" morph="none" start_char="6744" end_char="6749">spread</TOKEN>
<TOKEN id="token-98-5" pos="word" morph="none" start_char="6751" end_char="6752">by</TOKEN>
<TOKEN id="token-98-6" pos="unknown" morph="none" start_char="6754" end_char="6767">human-to-human</TOKEN>
<TOKEN id="token-98-7" pos="word" morph="none" start_char="6769" end_char="6775">contact</TOKEN>
<TOKEN id="token-98-8" pos="punct" morph="none" start_char="6776" end_char="6776">,</TOKEN>
<TOKEN id="token-98-9" pos="word" morph="none" start_char="6778" end_char="6782">these</TOKEN>
<TOKEN id="token-98-10" pos="word" morph="none" start_char="6784" end_char="6787">will</TOKEN>
<TOKEN id="token-98-11" pos="word" morph="none" start_char="6789" end_char="6792">show</TOKEN>
<TOKEN id="token-98-12" pos="word" morph="none" start_char="6794" end_char="6795">up</TOKEN>
<TOKEN id="token-98-13" pos="word" morph="none" start_char="6797" end_char="6798">as</TOKEN>
<TOKEN id="token-98-14" pos="word" morph="none" start_char="6800" end_char="6807">sporadic</TOKEN>
<TOKEN id="token-98-15" pos="word" morph="none" start_char="6809" end_char="6816">positive</TOKEN>
<TOKEN id="token-98-16" pos="word" morph="none" start_char="6818" end_char="6824">samples</TOKEN>
<TOKEN id="token-98-17" pos="word" morph="none" start_char="6826" end_char="6828">for</TOKEN>
<TOKEN id="token-98-18" pos="word" morph="none" start_char="6830" end_char="6840">coronavirus</TOKEN>
<TOKEN id="token-98-19" pos="punct" morph="none" start_char="6841" end_char="6841">,</TOKEN>
<TOKEN id="token-98-20" pos="word" morph="none" start_char="6843" end_char="6849">without</TOKEN>
<TOKEN id="token-98-21" pos="word" morph="none" start_char="6851" end_char="6855">going</TOKEN>
<TOKEN id="token-98-22" pos="word" morph="none" start_char="6857" end_char="6858">on</TOKEN>
<TOKEN id="token-98-23" pos="word" morph="none" start_char="6860" end_char="6861">to</TOKEN>
<TOKEN id="token-98-24" pos="word" morph="none" start_char="6863" end_char="6868">create</TOKEN>
<TOKEN id="token-98-25" pos="word" morph="none" start_char="6870" end_char="6870">a</TOKEN>
<TOKEN id="token-98-26" pos="word" morph="none" start_char="6872" end_char="6879">pandemic</TOKEN>
<TOKEN id="token-98-27" pos="punct" morph="none" start_char="6880" end_char="6880">,</TOKEN>
<TOKEN id="token-98-28" pos="word" morph="none" start_char="6882" end_char="6883">an</TOKEN>
<TOKEN id="token-98-29" pos="word" morph="none" start_char="6885" end_char="6892">epidemic</TOKEN>
<TOKEN id="token-98-30" pos="punct" morph="none" start_char="6893" end_char="6893">,</TOKEN>
<TOKEN id="token-98-31" pos="word" morph="none" start_char="6895" end_char="6896">or</TOKEN>
<TOKEN id="token-98-32" pos="word" morph="none" start_char="6898" end_char="6901">even</TOKEN>
<TOKEN id="token-98-33" pos="word" morph="none" start_char="6903" end_char="6903">a</TOKEN>
<TOKEN id="token-98-34" pos="word" morph="none" start_char="6905" end_char="6909">local</TOKEN>
<TOKEN id="token-98-35" pos="word" morph="none" start_char="6911" end_char="6918">outbreak</TOKEN>
<TOKEN id="token-98-36" pos="punct" morph="none" start_char="6919" end_char="6919">.</TOKEN>
</SEG>
<SEG id="segment-99" start_char="6922" end_char="7141">
<ORIGINAL_TEXT>It is pretty clear that the pandemic form of the coronavirus spread out from Wuhan (although previous coronavirus samples suggest it may have been carried to Wuhan from bats in a warmer province of China, further south).</ORIGINAL_TEXT>
<TOKEN id="token-99-0" pos="word" morph="none" start_char="6922" end_char="6923">It</TOKEN>
<TOKEN id="token-99-1" pos="word" morph="none" start_char="6925" end_char="6926">is</TOKEN>
<TOKEN id="token-99-2" pos="word" morph="none" start_char="6928" end_char="6933">pretty</TOKEN>
<TOKEN id="token-99-3" pos="word" morph="none" start_char="6935" end_char="6939">clear</TOKEN>
<TOKEN id="token-99-4" pos="word" morph="none" start_char="6941" end_char="6944">that</TOKEN>
<TOKEN id="token-99-5" pos="word" morph="none" start_char="6946" end_char="6948">the</TOKEN>
<TOKEN id="token-99-6" pos="word" morph="none" start_char="6950" end_char="6957">pandemic</TOKEN>
<TOKEN id="token-99-7" pos="word" morph="none" start_char="6959" end_char="6962">form</TOKEN>
<TOKEN id="token-99-8" pos="word" morph="none" start_char="6964" end_char="6965">of</TOKEN>
<TOKEN id="token-99-9" pos="word" morph="none" start_char="6967" end_char="6969">the</TOKEN>
<TOKEN id="token-99-10" pos="word" morph="none" start_char="6971" end_char="6981">coronavirus</TOKEN>
<TOKEN id="token-99-11" pos="word" morph="none" start_char="6983" end_char="6988">spread</TOKEN>
<TOKEN id="token-99-12" pos="word" morph="none" start_char="6990" end_char="6992">out</TOKEN>
<TOKEN id="token-99-13" pos="word" morph="none" start_char="6994" end_char="6997">from</TOKEN>
<TOKEN id="token-99-14" pos="word" morph="none" start_char="6999" end_char="7003">Wuhan</TOKEN>
<TOKEN id="token-99-15" pos="punct" morph="none" start_char="7005" end_char="7005">(</TOKEN>
<TOKEN id="token-99-16" pos="word" morph="none" start_char="7006" end_char="7013">although</TOKEN>
<TOKEN id="token-99-17" pos="word" morph="none" start_char="7015" end_char="7022">previous</TOKEN>
<TOKEN id="token-99-18" pos="word" morph="none" start_char="7024" end_char="7034">coronavirus</TOKEN>
<TOKEN id="token-99-19" pos="word" morph="none" start_char="7036" end_char="7042">samples</TOKEN>
<TOKEN id="token-99-20" pos="word" morph="none" start_char="7044" end_char="7050">suggest</TOKEN>
<TOKEN id="token-99-21" pos="word" morph="none" start_char="7052" end_char="7053">it</TOKEN>
<TOKEN id="token-99-22" pos="word" morph="none" start_char="7055" end_char="7057">may</TOKEN>
<TOKEN id="token-99-23" pos="word" morph="none" start_char="7059" end_char="7062">have</TOKEN>
<TOKEN id="token-99-24" pos="word" morph="none" start_char="7064" end_char="7067">been</TOKEN>
<TOKEN id="token-99-25" pos="word" morph="none" start_char="7069" end_char="7075">carried</TOKEN>
<TOKEN id="token-99-26" pos="word" morph="none" start_char="7077" end_char="7078">to</TOKEN>
<TOKEN id="token-99-27" pos="word" morph="none" start_char="7080" end_char="7084">Wuhan</TOKEN>
<TOKEN id="token-99-28" pos="word" morph="none" start_char="7086" end_char="7089">from</TOKEN>
<TOKEN id="token-99-29" pos="word" morph="none" start_char="7091" end_char="7094">bats</TOKEN>
<TOKEN id="token-99-30" pos="word" morph="none" start_char="7096" end_char="7097">in</TOKEN>
<TOKEN id="token-99-31" pos="word" morph="none" start_char="7099" end_char="7099">a</TOKEN>
<TOKEN id="token-99-32" pos="word" morph="none" start_char="7101" end_char="7106">warmer</TOKEN>
<TOKEN id="token-99-33" pos="word" morph="none" start_char="7108" end_char="7115">province</TOKEN>
<TOKEN id="token-99-34" pos="word" morph="none" start_char="7117" end_char="7118">of</TOKEN>
<TOKEN id="token-99-35" pos="word" morph="none" start_char="7120" end_char="7124">China</TOKEN>
<TOKEN id="token-99-36" pos="punct" morph="none" start_char="7125" end_char="7125">,</TOKEN>
<TOKEN id="token-99-37" pos="word" morph="none" start_char="7127" end_char="7133">further</TOKEN>
<TOKEN id="token-99-38" pos="word" morph="none" start_char="7135" end_char="7139">south</TOKEN>
<TOKEN id="token-99-39" pos="punct" morph="none" start_char="7140" end_char="7141">).</TOKEN>
</SEG>
<SEG id="segment-100" start_char="7144" end_char="7258">
<ORIGINAL_TEXT>The very first COVID-19 case detected in Australia was a man who flew from Wuhan to visit his parents in Melbourne.</ORIGINAL_TEXT>
<TOKEN id="token-100-0" pos="word" morph="none" start_char="7144" end_char="7146">The</TOKEN>
<TOKEN id="token-100-1" pos="word" morph="none" start_char="7148" end_char="7151">very</TOKEN>
<TOKEN id="token-100-2" pos="word" morph="none" start_char="7153" end_char="7157">first</TOKEN>
<TOKEN id="token-100-3" pos="unknown" morph="none" start_char="7159" end_char="7166">COVID-19</TOKEN>
<TOKEN id="token-100-4" pos="word" morph="none" start_char="7168" end_char="7171">case</TOKEN>
<TOKEN id="token-100-5" pos="word" morph="none" start_char="7173" end_char="7180">detected</TOKEN>
<TOKEN id="token-100-6" pos="word" morph="none" start_char="7182" end_char="7183">in</TOKEN>
<TOKEN id="token-100-7" pos="word" morph="none" start_char="7185" end_char="7193">Australia</TOKEN>
<TOKEN id="token-100-8" pos="word" morph="none" start_char="7195" end_char="7197">was</TOKEN>
<TOKEN id="token-100-9" pos="word" morph="none" start_char="7199" end_char="7199">a</TOKEN>
<TOKEN id="token-100-10" pos="word" morph="none" start_char="7201" end_char="7203">man</TOKEN>
<TOKEN id="token-100-11" pos="word" morph="none" start_char="7205" end_char="7207">who</TOKEN>
<TOKEN id="token-100-12" pos="word" morph="none" start_char="7209" end_char="7212">flew</TOKEN>
<TOKEN id="token-100-13" pos="word" morph="none" start_char="7214" end_char="7217">from</TOKEN>
<TOKEN id="token-100-14" pos="word" morph="none" start_char="7219" end_char="7223">Wuhan</TOKEN>
<TOKEN id="token-100-15" pos="word" morph="none" start_char="7225" end_char="7226">to</TOKEN>
<TOKEN id="token-100-16" pos="word" morph="none" start_char="7228" end_char="7232">visit</TOKEN>
<TOKEN id="token-100-17" pos="word" morph="none" start_char="7234" end_char="7236">his</TOKEN>
<TOKEN id="token-100-18" pos="word" morph="none" start_char="7238" end_char="7244">parents</TOKEN>
<TOKEN id="token-100-19" pos="word" morph="none" start_char="7246" end_char="7247">in</TOKEN>
<TOKEN id="token-100-20" pos="word" morph="none" start_char="7249" end_char="7257">Melbourne</TOKEN>
<TOKEN id="token-100-21" pos="punct" morph="none" start_char="7258" end_char="7258">.</TOKEN>
</SEG>
<SEG id="segment-101" start_char="7260" end_char="7428">
<ORIGINAL_TEXT>- Fortunately, he knew of the pandemic in Wuhan (he had just come from there), self-isolated, wore a mask, and was hospitalized, all without spreading it to anyone else.</ORIGINAL_TEXT>
<TOKEN id="token-101-0" pos="punct" morph="none" start_char="7260" end_char="7260">-</TOKEN>
<TOKEN id="token-101-1" pos="word" morph="none" start_char="7262" end_char="7272">Fortunately</TOKEN>
<TOKEN id="token-101-2" pos="punct" morph="none" start_char="7273" end_char="7273">,</TOKEN>
<TOKEN id="token-101-3" pos="word" morph="none" start_char="7275" end_char="7276">he</TOKEN>
<TOKEN id="token-101-4" pos="word" morph="none" start_char="7278" end_char="7281">knew</TOKEN>
<TOKEN id="token-101-5" pos="word" morph="none" start_char="7283" end_char="7284">of</TOKEN>
<TOKEN id="token-101-6" pos="word" morph="none" start_char="7286" end_char="7288">the</TOKEN>
<TOKEN id="token-101-7" pos="word" morph="none" start_char="7290" end_char="7297">pandemic</TOKEN>
<TOKEN id="token-101-8" pos="word" morph="none" start_char="7299" end_char="7300">in</TOKEN>
<TOKEN id="token-101-9" pos="word" morph="none" start_char="7302" end_char="7306">Wuhan</TOKEN>
<TOKEN id="token-101-10" pos="punct" morph="none" start_char="7308" end_char="7308">(</TOKEN>
<TOKEN id="token-101-11" pos="word" morph="none" start_char="7309" end_char="7310">he</TOKEN>
<TOKEN id="token-101-12" pos="word" morph="none" start_char="7312" end_char="7314">had</TOKEN>
<TOKEN id="token-101-13" pos="word" morph="none" start_char="7316" end_char="7319">just</TOKEN>
<TOKEN id="token-101-14" pos="word" morph="none" start_char="7321" end_char="7324">come</TOKEN>
<TOKEN id="token-101-15" pos="word" morph="none" start_char="7326" end_char="7329">from</TOKEN>
<TOKEN id="token-101-16" pos="word" morph="none" start_char="7331" end_char="7335">there</TOKEN>
<TOKEN id="token-101-17" pos="punct" morph="none" start_char="7336" end_char="7337">),</TOKEN>
<TOKEN id="token-101-18" pos="unknown" morph="none" start_char="7339" end_char="7351">self-isolated</TOKEN>
<TOKEN id="token-101-19" pos="punct" morph="none" start_char="7352" end_char="7352">,</TOKEN>
<TOKEN id="token-101-20" pos="word" morph="none" start_char="7354" end_char="7357">wore</TOKEN>
<TOKEN id="token-101-21" pos="word" morph="none" start_char="7359" end_char="7359">a</TOKEN>
<TOKEN id="token-101-22" pos="word" morph="none" start_char="7361" end_char="7364">mask</TOKEN>
<TOKEN id="token-101-23" pos="punct" morph="none" start_char="7365" end_char="7365">,</TOKEN>
<TOKEN id="token-101-24" pos="word" morph="none" start_char="7367" end_char="7369">and</TOKEN>
<TOKEN id="token-101-25" pos="word" morph="none" start_char="7371" end_char="7373">was</TOKEN>
<TOKEN id="token-101-26" pos="word" morph="none" start_char="7375" end_char="7386">hospitalized</TOKEN>
<TOKEN id="token-101-27" pos="punct" morph="none" start_char="7387" end_char="7387">,</TOKEN>
<TOKEN id="token-101-28" pos="word" morph="none" start_char="7389" end_char="7391">all</TOKEN>
<TOKEN id="token-101-29" pos="word" morph="none" start_char="7393" end_char="7399">without</TOKEN>
<TOKEN id="token-101-30" pos="word" morph="none" start_char="7401" end_char="7409">spreading</TOKEN>
<TOKEN id="token-101-31" pos="word" morph="none" start_char="7411" end_char="7412">it</TOKEN>
<TOKEN id="token-101-32" pos="word" morph="none" start_char="7414" end_char="7415">to</TOKEN>
<TOKEN id="token-101-33" pos="word" morph="none" start_char="7417" end_char="7422">anyone</TOKEN>
<TOKEN id="token-101-34" pos="word" morph="none" start_char="7424" end_char="7427">else</TOKEN>
<TOKEN id="token-101-35" pos="punct" morph="none" start_char="7428" end_char="7428">.</TOKEN>
</SEG>
<SEG id="segment-102" start_char="7431" end_char="7546">
<ORIGINAL_TEXT>The very first COVID-19 case detected in Germany was a woman who flew from Wuhan to run a training course in Munich.</ORIGINAL_TEXT>
<TOKEN id="token-102-0" pos="word" morph="none" start_char="7431" end_char="7433">The</TOKEN>
<TOKEN id="token-102-1" pos="word" morph="none" start_char="7435" end_char="7438">very</TOKEN>
<TOKEN id="token-102-2" pos="word" morph="none" start_char="7440" end_char="7444">first</TOKEN>
<TOKEN id="token-102-3" pos="unknown" morph="none" start_char="7446" end_char="7453">COVID-19</TOKEN>
<TOKEN id="token-102-4" pos="word" morph="none" start_char="7455" end_char="7458">case</TOKEN>
<TOKEN id="token-102-5" pos="word" morph="none" start_char="7460" end_char="7467">detected</TOKEN>
<TOKEN id="token-102-6" pos="word" morph="none" start_char="7469" end_char="7470">in</TOKEN>
<TOKEN id="token-102-7" pos="word" morph="none" start_char="7472" end_char="7478">Germany</TOKEN>
<TOKEN id="token-102-8" pos="word" morph="none" start_char="7480" end_char="7482">was</TOKEN>
<TOKEN id="token-102-9" pos="word" morph="none" start_char="7484" end_char="7484">a</TOKEN>
<TOKEN id="token-102-10" pos="word" morph="none" start_char="7486" end_char="7490">woman</TOKEN>
<TOKEN id="token-102-11" pos="word" morph="none" start_char="7492" end_char="7494">who</TOKEN>
<TOKEN id="token-102-12" pos="word" morph="none" start_char="7496" end_char="7499">flew</TOKEN>
<TOKEN id="token-102-13" pos="word" morph="none" start_char="7501" end_char="7504">from</TOKEN>
<TOKEN id="token-102-14" pos="word" morph="none" start_char="7506" end_char="7510">Wuhan</TOKEN>
<TOKEN id="token-102-15" pos="word" morph="none" start_char="7512" end_char="7513">to</TOKEN>
<TOKEN id="token-102-16" pos="word" morph="none" start_char="7515" end_char="7517">run</TOKEN>
<TOKEN id="token-102-17" pos="word" morph="none" start_char="7519" end_char="7519">a</TOKEN>
<TOKEN id="token-102-18" pos="word" morph="none" start_char="7521" end_char="7528">training</TOKEN>
<TOKEN id="token-102-19" pos="word" morph="none" start_char="7530" end_char="7535">course</TOKEN>
<TOKEN id="token-102-20" pos="word" morph="none" start_char="7537" end_char="7538">in</TOKEN>
<TOKEN id="token-102-21" pos="word" morph="none" start_char="7540" end_char="7545">Munich</TOKEN>
<TOKEN id="token-102-22" pos="punct" morph="none" start_char="7546" end_char="7546">.</TOKEN>
</SEG>
<SEG id="segment-103" start_char="7548" end_char="7655">
<ORIGINAL_TEXT>- She only developed symptoms on the return flight to China, but several people on the course were infected.</ORIGINAL_TEXT>
<TOKEN id="token-103-0" pos="punct" morph="none" start_char="7548" end_char="7548">-</TOKEN>
<TOKEN id="token-103-1" pos="word" morph="none" start_char="7550" end_char="7552">She</TOKEN>
<TOKEN id="token-103-2" pos="word" morph="none" start_char="7554" end_char="7557">only</TOKEN>
<TOKEN id="token-103-3" pos="word" morph="none" start_char="7559" end_char="7567">developed</TOKEN>
<TOKEN id="token-103-4" pos="word" morph="none" start_char="7569" end_char="7576">symptoms</TOKEN>
<TOKEN id="token-103-5" pos="word" morph="none" start_char="7578" end_char="7579">on</TOKEN>
<TOKEN id="token-103-6" pos="word" morph="none" start_char="7581" end_char="7583">the</TOKEN>
<TOKEN id="token-103-7" pos="word" morph="none" start_char="7585" end_char="7590">return</TOKEN>
<TOKEN id="token-103-8" pos="word" morph="none" start_char="7592" end_char="7597">flight</TOKEN>
<TOKEN id="token-103-9" pos="word" morph="none" start_char="7599" end_char="7600">to</TOKEN>
<TOKEN id="token-103-10" pos="word" morph="none" start_char="7602" end_char="7606">China</TOKEN>
<TOKEN id="token-103-11" pos="punct" morph="none" start_char="7607" end_char="7607">,</TOKEN>
<TOKEN id="token-103-12" pos="word" morph="none" start_char="7609" end_char="7611">but</TOKEN>
<TOKEN id="token-103-13" pos="word" morph="none" start_char="7613" end_char="7619">several</TOKEN>
<TOKEN id="token-103-14" pos="word" morph="none" start_char="7621" end_char="7626">people</TOKEN>
<TOKEN id="token-103-15" pos="word" morph="none" start_char="7628" end_char="7629">on</TOKEN>
<TOKEN id="token-103-16" pos="word" morph="none" start_char="7631" end_char="7633">the</TOKEN>
<TOKEN id="token-103-17" pos="word" morph="none" start_char="7635" end_char="7640">course</TOKEN>
<TOKEN id="token-103-18" pos="word" morph="none" start_char="7642" end_char="7645">were</TOKEN>
<TOKEN id="token-103-19" pos="word" morph="none" start_char="7647" end_char="7654">infected</TOKEN>
<TOKEN id="token-103-20" pos="punct" morph="none" start_char="7655" end_char="7655">.</TOKEN>
</SEG>
<SEG id="segment-104" start_char="7658" end_char="7757">
<ORIGINAL_TEXT>Listen (43 minutes): https://www.abc.net.au/radionational/programs/rn-presents/patient-zero/12523222</ORIGINAL_TEXT>
<TOKEN id="token-104-0" pos="word" morph="none" start_char="7658" end_char="7663">Listen</TOKEN>
<TOKEN id="token-104-1" pos="punct" morph="none" start_char="7665" end_char="7665">(</TOKEN>
<TOKEN id="token-104-2" pos="word" morph="none" start_char="7666" end_char="7667">43</TOKEN>
<TOKEN id="token-104-3" pos="word" morph="none" start_char="7669" end_char="7675">minutes</TOKEN>
<TOKEN id="token-104-4" pos="punct" morph="none" start_char="7676" end_char="7677">):</TOKEN>
<TOKEN id="token-104-5" pos="url" morph="none" start_char="7679" end_char="7757">https://www.abc.net.au/radionational/programs/rn-presents/patient-zero/12523222</TOKEN>
</SEG>
<SEG id="segment-105" start_char="7761" end_char="7801">
<ORIGINAL_TEXT>Quote from: Jolly2 on 07/02/2021 04:17:23</ORIGINAL_TEXT>
<TOKEN id="token-105-0" pos="word" morph="none" start_char="7761" end_char="7765">Quote</TOKEN>
<TOKEN id="token-105-1" pos="word" morph="none" start_char="7767" end_char="7770">from</TOKEN>
<TOKEN id="token-105-2" pos="punct" morph="none" start_char="7771" end_char="7771">:</TOKEN>
<TOKEN id="token-105-3" pos="word" morph="none" start_char="7773" end_char="7778">Jolly2</TOKEN>
<TOKEN id="token-105-4" pos="word" morph="none" start_char="7780" end_char="7781">on</TOKEN>
<TOKEN id="token-105-5" pos="unknown" morph="none" start_char="7783" end_char="7792">07/02/2021</TOKEN>
<TOKEN id="token-105-6" pos="unknown" morph="none" start_char="7794" end_char="7801">04:17:23</TOKEN>
</SEG>
<SEG id="segment-106" start_char="7804" end_char="7804">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN id="token-106-0" pos="punct" morph="none" start_char="7804" end_char="7804">.</TOKEN>
</SEG>
<SEG id="segment-107" start_char="7806" end_char="7832">
<ORIGINAL_TEXT>Means it spread undetected,</ORIGINAL_TEXT>
<TOKEN id="token-107-0" pos="word" morph="none" start_char="7806" end_char="7810">Means</TOKEN>
<TOKEN id="token-107-1" pos="word" morph="none" start_char="7812" end_char="7813">it</TOKEN>
<TOKEN id="token-107-2" pos="word" morph="none" start_char="7815" end_char="7820">spread</TOKEN>
<TOKEN id="token-107-3" pos="word" morph="none" start_char="7822" end_char="7831">undetected</TOKEN>
<TOKEN id="token-107-4" pos="punct" morph="none" start_char="7832" end_char="7832">,</TOKEN>
</SEG>
<SEG id="segment-108" start_char="7836" end_char="7940">
<ORIGINAL_TEXT>You may be stunned to realise this, but Chines people notice if all the grannies and granddads are dying.</ORIGINAL_TEXT>
<TOKEN id="token-108-0" pos="word" morph="none" start_char="7836" end_char="7838">You</TOKEN>
<TOKEN id="token-108-1" pos="word" morph="none" start_char="7840" end_char="7842">may</TOKEN>
<TOKEN id="token-108-2" pos="word" morph="none" start_char="7844" end_char="7845">be</TOKEN>
<TOKEN id="token-108-3" pos="word" morph="none" start_char="7847" end_char="7853">stunned</TOKEN>
<TOKEN id="token-108-4" pos="word" morph="none" start_char="7855" end_char="7856">to</TOKEN>
<TOKEN id="token-108-5" pos="word" morph="none" start_char="7858" end_char="7864">realise</TOKEN>
<TOKEN id="token-108-6" pos="word" morph="none" start_char="7866" end_char="7869">this</TOKEN>
<TOKEN id="token-108-7" pos="punct" morph="none" start_char="7870" end_char="7870">,</TOKEN>
<TOKEN id="token-108-8" pos="word" morph="none" start_char="7872" end_char="7874">but</TOKEN>
<TOKEN id="token-108-9" pos="word" morph="none" start_char="7876" end_char="7881">Chines</TOKEN>
<TOKEN id="token-108-10" pos="word" morph="none" start_char="7883" end_char="7888">people</TOKEN>
<TOKEN id="token-108-11" pos="word" morph="none" start_char="7890" end_char="7895">notice</TOKEN>
<TOKEN id="token-108-12" pos="word" morph="none" start_char="7897" end_char="7898">if</TOKEN>
<TOKEN id="token-108-13" pos="word" morph="none" start_char="7900" end_char="7902">all</TOKEN>
<TOKEN id="token-108-14" pos="word" morph="none" start_char="7904" end_char="7906">the</TOKEN>
<TOKEN id="token-108-15" pos="word" morph="none" start_char="7908" end_char="7915">grannies</TOKEN>
<TOKEN id="token-108-16" pos="word" morph="none" start_char="7917" end_char="7919">and</TOKEN>
<TOKEN id="token-108-17" pos="word" morph="none" start_char="7921" end_char="7929">granddads</TOKEN>
<TOKEN id="token-108-18" pos="word" morph="none" start_char="7931" end_char="7933">are</TOKEN>
<TOKEN id="token-108-19" pos="word" morph="none" start_char="7935" end_char="7939">dying</TOKEN>
<TOKEN id="token-108-20" pos="punct" morph="none" start_char="7940" end_char="7940">.</TOKEN>
</SEG>
<SEG id="segment-109" start_char="7942" end_char="8140">
<ORIGINAL_TEXT>So it's essentially impossible that it spread undetected through China but somehow magically got there from Spain (or wherever) - where it was also magically showing restraint and not killing anyone.</ORIGINAL_TEXT>
<TOKEN id="token-109-0" pos="word" morph="none" start_char="7942" end_char="7943">So</TOKEN>
<TOKEN id="token-109-1" pos="word" morph="none" start_char="7945" end_char="7948">it's</TOKEN>
<TOKEN id="token-109-2" pos="word" morph="none" start_char="7950" end_char="7960">essentially</TOKEN>
<TOKEN id="token-109-3" pos="word" morph="none" start_char="7962" end_char="7971">impossible</TOKEN>
<TOKEN id="token-109-4" pos="word" morph="none" start_char="7973" end_char="7976">that</TOKEN>
<TOKEN id="token-109-5" pos="word" morph="none" start_char="7978" end_char="7979">it</TOKEN>
<TOKEN id="token-109-6" pos="word" morph="none" start_char="7981" end_char="7986">spread</TOKEN>
<TOKEN id="token-109-7" pos="word" morph="none" start_char="7988" end_char="7997">undetected</TOKEN>
<TOKEN id="token-109-8" pos="word" morph="none" start_char="7999" end_char="8005">through</TOKEN>
<TOKEN id="token-109-9" pos="word" morph="none" start_char="8007" end_char="8011">China</TOKEN>
<TOKEN id="token-109-10" pos="word" morph="none" start_char="8013" end_char="8015">but</TOKEN>
<TOKEN id="token-109-11" pos="word" morph="none" start_char="8017" end_char="8023">somehow</TOKEN>
<TOKEN id="token-109-12" pos="word" morph="none" start_char="8025" end_char="8033">magically</TOKEN>
<TOKEN id="token-109-13" pos="word" morph="none" start_char="8035" end_char="8037">got</TOKEN>
<TOKEN id="token-109-14" pos="word" morph="none" start_char="8039" end_char="8043">there</TOKEN>
<TOKEN id="token-109-15" pos="word" morph="none" start_char="8045" end_char="8048">from</TOKEN>
<TOKEN id="token-109-16" pos="word" morph="none" start_char="8050" end_char="8054">Spain</TOKEN>
<TOKEN id="token-109-17" pos="punct" morph="none" start_char="8056" end_char="8056">(</TOKEN>
<TOKEN id="token-109-18" pos="word" morph="none" start_char="8057" end_char="8058">or</TOKEN>
<TOKEN id="token-109-19" pos="word" morph="none" start_char="8060" end_char="8067">wherever</TOKEN>
<TOKEN id="token-109-20" pos="punct" morph="none" start_char="8068" end_char="8068">)</TOKEN>
<TOKEN id="token-109-21" pos="punct" morph="none" start_char="8070" end_char="8070">-</TOKEN>
<TOKEN id="token-109-22" pos="word" morph="none" start_char="8072" end_char="8076">where</TOKEN>
<TOKEN id="token-109-23" pos="word" morph="none" start_char="8078" end_char="8079">it</TOKEN>
<TOKEN id="token-109-24" pos="word" morph="none" start_char="8081" end_char="8083">was</TOKEN>
<TOKEN id="token-109-25" pos="word" morph="none" start_char="8085" end_char="8088">also</TOKEN>
<TOKEN id="token-109-26" pos="word" morph="none" start_char="8090" end_char="8098">magically</TOKEN>
<TOKEN id="token-109-27" pos="word" morph="none" start_char="8100" end_char="8106">showing</TOKEN>
<TOKEN id="token-109-28" pos="word" morph="none" start_char="8108" end_char="8116">restraint</TOKEN>
<TOKEN id="token-109-29" pos="word" morph="none" start_char="8118" end_char="8120">and</TOKEN>
<TOKEN id="token-109-30" pos="word" morph="none" start_char="8122" end_char="8124">not</TOKEN>
<TOKEN id="token-109-31" pos="word" morph="none" start_char="8126" end_char="8132">killing</TOKEN>
<TOKEN id="token-109-32" pos="word" morph="none" start_char="8134" end_char="8139">anyone</TOKEN>
<TOKEN id="token-109-33" pos="punct" morph="none" start_char="8140" end_char="8140">.</TOKEN>
</SEG>
<SEG id="segment-110" start_char="8143" end_char="8168">
<ORIGINAL_TEXT>Your suggestion is absurd.</ORIGINAL_TEXT>
<TOKEN id="token-110-0" pos="word" morph="none" start_char="8143" end_char="8146">Your</TOKEN>
<TOKEN id="token-110-1" pos="word" morph="none" start_char="8148" end_char="8157">suggestion</TOKEN>
<TOKEN id="token-110-2" pos="word" morph="none" start_char="8159" end_char="8160">is</TOKEN>
<TOKEN id="token-110-3" pos="word" morph="none" start_char="8162" end_char="8167">absurd</TOKEN>
<TOKEN id="token-110-4" pos="punct" morph="none" start_char="8168" end_char="8168">.</TOKEN>
</SEG>
<SEG id="segment-111" start_char="8170" end_char="8202">
<ORIGINAL_TEXT>Why are you still clinging to it?</ORIGINAL_TEXT>
<TOKEN id="token-111-0" pos="word" morph="none" start_char="8170" end_char="8172">Why</TOKEN>
<TOKEN id="token-111-1" pos="word" morph="none" start_char="8174" end_char="8176">are</TOKEN>
<TOKEN id="token-111-2" pos="word" morph="none" start_char="8178" end_char="8180">you</TOKEN>
<TOKEN id="token-111-3" pos="word" morph="none" start_char="8182" end_char="8186">still</TOKEN>
<TOKEN id="token-111-4" pos="word" morph="none" start_char="8188" end_char="8195">clinging</TOKEN>
<TOKEN id="token-111-5" pos="word" morph="none" start_char="8197" end_char="8198">to</TOKEN>
<TOKEN id="token-111-6" pos="word" morph="none" start_char="8200" end_char="8201">it</TOKEN>
<TOKEN id="token-111-7" pos="punct" morph="none" start_char="8202" end_char="8202">?</TOKEN>
</SEG>
<SEG id="segment-112" start_char="8206" end_char="8253">
<ORIGINAL_TEXT>Quote from: Bored chemist on 07/02/2021 10:24:14</ORIGINAL_TEXT>
<TOKEN id="token-112-0" pos="word" morph="none" start_char="8206" end_char="8210">Quote</TOKEN>
<TOKEN id="token-112-1" pos="word" morph="none" start_char="8212" end_char="8215">from</TOKEN>
<TOKEN id="token-112-2" pos="punct" morph="none" start_char="8216" end_char="8216">:</TOKEN>
<TOKEN id="token-112-3" pos="word" morph="none" start_char="8218" end_char="8222">Bored</TOKEN>
<TOKEN id="token-112-4" pos="word" morph="none" start_char="8224" end_char="8230">chemist</TOKEN>
<TOKEN id="token-112-5" pos="word" morph="none" start_char="8232" end_char="8233">on</TOKEN>
<TOKEN id="token-112-6" pos="unknown" morph="none" start_char="8235" end_char="8244">07/02/2021</TOKEN>
<TOKEN id="token-112-7" pos="unknown" morph="none" start_char="8246" end_char="8253">10:24:14</TOKEN>
</SEG>
<SEG id="segment-113" start_char="8256" end_char="8296">
<ORIGINAL_TEXT>Quote from: Jolly2 on 07/02/2021 04:17:23</ORIGINAL_TEXT>
<TOKEN id="token-113-0" pos="word" morph="none" start_char="8256" end_char="8260">Quote</TOKEN>
<TOKEN id="token-113-1" pos="word" morph="none" start_char="8262" end_char="8265">from</TOKEN>
<TOKEN id="token-113-2" pos="punct" morph="none" start_char="8266" end_char="8266">:</TOKEN>
<TOKEN id="token-113-3" pos="word" morph="none" start_char="8268" end_char="8273">Jolly2</TOKEN>
<TOKEN id="token-113-4" pos="word" morph="none" start_char="8275" end_char="8276">on</TOKEN>
<TOKEN id="token-113-5" pos="unknown" morph="none" start_char="8278" end_char="8287">07/02/2021</TOKEN>
<TOKEN id="token-113-6" pos="unknown" morph="none" start_char="8289" end_char="8296">04:17:23</TOKEN>
</SEG>
<SEG id="segment-114" start_char="8299" end_char="8299">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN id="token-114-0" pos="punct" morph="none" start_char="8299" end_char="8299">.</TOKEN>
</SEG>
<SEG id="segment-115" start_char="8301" end_char="8327">
<ORIGINAL_TEXT>Means it spread undetected,</ORIGINAL_TEXT>
<TOKEN id="token-115-0" pos="word" morph="none" start_char="8301" end_char="8305">Means</TOKEN>
<TOKEN id="token-115-1" pos="word" morph="none" start_char="8307" end_char="8308">it</TOKEN>
<TOKEN id="token-115-2" pos="word" morph="none" start_char="8310" end_char="8315">spread</TOKEN>
<TOKEN id="token-115-3" pos="word" morph="none" start_char="8317" end_char="8326">undetected</TOKEN>
<TOKEN id="token-115-4" pos="punct" morph="none" start_char="8327" end_char="8327">,</TOKEN>
</SEG>
<SEG id="segment-116" start_char="8330" end_char="8692">
<ORIGINAL_TEXT>You may be stunned to realise this, but Chines people notice if all the grannies and granddads are dying.So it's essentially impossible that it spread undetected through China but somehow magically got there from Spain (or wherever) - where it was also magically showing restraint and not killing anyone.Your suggestion is absurd.Why are you still clinging to it?</ORIGINAL_TEXT>
<TOKEN id="token-116-0" pos="word" morph="none" start_char="8330" end_char="8332">You</TOKEN>
<TOKEN id="token-116-1" pos="word" morph="none" start_char="8334" end_char="8336">may</TOKEN>
<TOKEN id="token-116-2" pos="word" morph="none" start_char="8338" end_char="8339">be</TOKEN>
<TOKEN id="token-116-3" pos="word" morph="none" start_char="8341" end_char="8347">stunned</TOKEN>
<TOKEN id="token-116-4" pos="word" morph="none" start_char="8349" end_char="8350">to</TOKEN>
<TOKEN id="token-116-5" pos="word" morph="none" start_char="8352" end_char="8358">realise</TOKEN>
<TOKEN id="token-116-6" pos="word" morph="none" start_char="8360" end_char="8363">this</TOKEN>
<TOKEN id="token-116-7" pos="punct" morph="none" start_char="8364" end_char="8364">,</TOKEN>
<TOKEN id="token-116-8" pos="word" morph="none" start_char="8366" end_char="8368">but</TOKEN>
<TOKEN id="token-116-9" pos="word" morph="none" start_char="8370" end_char="8375">Chines</TOKEN>
<TOKEN id="token-116-10" pos="word" morph="none" start_char="8377" end_char="8382">people</TOKEN>
<TOKEN id="token-116-11" pos="word" morph="none" start_char="8384" end_char="8389">notice</TOKEN>
<TOKEN id="token-116-12" pos="word" morph="none" start_char="8391" end_char="8392">if</TOKEN>
<TOKEN id="token-116-13" pos="word" morph="none" start_char="8394" end_char="8396">all</TOKEN>
<TOKEN id="token-116-14" pos="word" morph="none" start_char="8398" end_char="8400">the</TOKEN>
<TOKEN id="token-116-15" pos="word" morph="none" start_char="8402" end_char="8409">grannies</TOKEN>
<TOKEN id="token-116-16" pos="word" morph="none" start_char="8411" end_char="8413">and</TOKEN>
<TOKEN id="token-116-17" pos="word" morph="none" start_char="8415" end_char="8423">granddads</TOKEN>
<TOKEN id="token-116-18" pos="word" morph="none" start_char="8425" end_char="8427">are</TOKEN>
<TOKEN id="token-116-19" pos="unknown" morph="none" start_char="8429" end_char="8436">dying.So</TOKEN>
<TOKEN id="token-116-20" pos="word" morph="none" start_char="8438" end_char="8441">it's</TOKEN>
<TOKEN id="token-116-21" pos="word" morph="none" start_char="8443" end_char="8453">essentially</TOKEN>
<TOKEN id="token-116-22" pos="word" morph="none" start_char="8455" end_char="8464">impossible</TOKEN>
<TOKEN id="token-116-23" pos="word" morph="none" start_char="8466" end_char="8469">that</TOKEN>
<TOKEN id="token-116-24" pos="word" morph="none" start_char="8471" end_char="8472">it</TOKEN>
<TOKEN id="token-116-25" pos="word" morph="none" start_char="8474" end_char="8479">spread</TOKEN>
<TOKEN id="token-116-26" pos="word" morph="none" start_char="8481" end_char="8490">undetected</TOKEN>
<TOKEN id="token-116-27" pos="word" morph="none" start_char="8492" end_char="8498">through</TOKEN>
<TOKEN id="token-116-28" pos="word" morph="none" start_char="8500" end_char="8504">China</TOKEN>
<TOKEN id="token-116-29" pos="word" morph="none" start_char="8506" end_char="8508">but</TOKEN>
<TOKEN id="token-116-30" pos="word" morph="none" start_char="8510" end_char="8516">somehow</TOKEN>
<TOKEN id="token-116-31" pos="word" morph="none" start_char="8518" end_char="8526">magically</TOKEN>
<TOKEN id="token-116-32" pos="word" morph="none" start_char="8528" end_char="8530">got</TOKEN>
<TOKEN id="token-116-33" pos="word" morph="none" start_char="8532" end_char="8536">there</TOKEN>
<TOKEN id="token-116-34" pos="word" morph="none" start_char="8538" end_char="8541">from</TOKEN>
<TOKEN id="token-116-35" pos="word" morph="none" start_char="8543" end_char="8547">Spain</TOKEN>
<TOKEN id="token-116-36" pos="punct" morph="none" start_char="8549" end_char="8549">(</TOKEN>
<TOKEN id="token-116-37" pos="word" morph="none" start_char="8550" end_char="8551">or</TOKEN>
<TOKEN id="token-116-38" pos="word" morph="none" start_char="8553" end_char="8560">wherever</TOKEN>
<TOKEN id="token-116-39" pos="punct" morph="none" start_char="8561" end_char="8561">)</TOKEN>
<TOKEN id="token-116-40" pos="punct" morph="none" start_char="8563" end_char="8563">-</TOKEN>
<TOKEN id="token-116-41" pos="word" morph="none" start_char="8565" end_char="8569">where</TOKEN>
<TOKEN id="token-116-42" pos="word" morph="none" start_char="8571" end_char="8572">it</TOKEN>
<TOKEN id="token-116-43" pos="word" morph="none" start_char="8574" end_char="8576">was</TOKEN>
<TOKEN id="token-116-44" pos="word" morph="none" start_char="8578" end_char="8581">also</TOKEN>
<TOKEN id="token-116-45" pos="word" morph="none" start_char="8583" end_char="8591">magically</TOKEN>
<TOKEN id="token-116-46" pos="word" morph="none" start_char="8593" end_char="8599">showing</TOKEN>
<TOKEN id="token-116-47" pos="word" morph="none" start_char="8601" end_char="8609">restraint</TOKEN>
<TOKEN id="token-116-48" pos="word" morph="none" start_char="8611" end_char="8613">and</TOKEN>
<TOKEN id="token-116-49" pos="word" morph="none" start_char="8615" end_char="8617">not</TOKEN>
<TOKEN id="token-116-50" pos="word" morph="none" start_char="8619" end_char="8625">killing</TOKEN>
<TOKEN id="token-116-51" pos="unknown" morph="none" start_char="8627" end_char="8637">anyone.Your</TOKEN>
<TOKEN id="token-116-52" pos="word" morph="none" start_char="8639" end_char="8648">suggestion</TOKEN>
<TOKEN id="token-116-53" pos="word" morph="none" start_char="8650" end_char="8651">is</TOKEN>
<TOKEN id="token-116-54" pos="unknown" morph="none" start_char="8653" end_char="8662">absurd.Why</TOKEN>
<TOKEN id="token-116-55" pos="word" morph="none" start_char="8664" end_char="8666">are</TOKEN>
<TOKEN id="token-116-56" pos="word" morph="none" start_char="8668" end_char="8670">you</TOKEN>
<TOKEN id="token-116-57" pos="word" morph="none" start_char="8672" end_char="8676">still</TOKEN>
<TOKEN id="token-116-58" pos="word" morph="none" start_char="8678" end_char="8685">clinging</TOKEN>
<TOKEN id="token-116-59" pos="word" morph="none" start_char="8687" end_char="8688">to</TOKEN>
<TOKEN id="token-116-60" pos="word" morph="none" start_char="8690" end_char="8691">it</TOKEN>
<TOKEN id="token-116-61" pos="punct" morph="none" start_char="8692" end_char="8692">?</TOKEN>
</SEG>
<SEG id="segment-117" start_char="8695" end_char="8847">
<ORIGINAL_TEXT>The death rate is low for covid, the symptoms Express themselves like flu, something like 80% of people infected have mild symptoms or are A symptomatic.</ORIGINAL_TEXT>
<TOKEN id="token-117-0" pos="word" morph="none" start_char="8695" end_char="8697">The</TOKEN>
<TOKEN id="token-117-1" pos="word" morph="none" start_char="8699" end_char="8703">death</TOKEN>
<TOKEN id="token-117-2" pos="word" morph="none" start_char="8705" end_char="8708">rate</TOKEN>
<TOKEN id="token-117-3" pos="word" morph="none" start_char="8710" end_char="8711">is</TOKEN>
<TOKEN id="token-117-4" pos="word" morph="none" start_char="8713" end_char="8715">low</TOKEN>
<TOKEN id="token-117-5" pos="word" morph="none" start_char="8717" end_char="8719">for</TOKEN>
<TOKEN id="token-117-6" pos="word" morph="none" start_char="8721" end_char="8725">covid</TOKEN>
<TOKEN id="token-117-7" pos="punct" morph="none" start_char="8726" end_char="8726">,</TOKEN>
<TOKEN id="token-117-8" pos="word" morph="none" start_char="8728" end_char="8730">the</TOKEN>
<TOKEN id="token-117-9" pos="word" morph="none" start_char="8732" end_char="8739">symptoms</TOKEN>
<TOKEN id="token-117-10" pos="word" morph="none" start_char="8741" end_char="8747">Express</TOKEN>
<TOKEN id="token-117-11" pos="word" morph="none" start_char="8749" end_char="8758">themselves</TOKEN>
<TOKEN id="token-117-12" pos="word" morph="none" start_char="8760" end_char="8763">like</TOKEN>
<TOKEN id="token-117-13" pos="word" morph="none" start_char="8765" end_char="8767">flu</TOKEN>
<TOKEN id="token-117-14" pos="punct" morph="none" start_char="8768" end_char="8768">,</TOKEN>
<TOKEN id="token-117-15" pos="word" morph="none" start_char="8770" end_char="8778">something</TOKEN>
<TOKEN id="token-117-16" pos="word" morph="none" start_char="8780" end_char="8783">like</TOKEN>
<TOKEN id="token-117-17" pos="word" morph="none" start_char="8785" end_char="8786">80</TOKEN>
<TOKEN id="token-117-18" pos="punct" morph="none" start_char="8787" end_char="8787">%</TOKEN>
<TOKEN id="token-117-19" pos="word" morph="none" start_char="8789" end_char="8790">of</TOKEN>
<TOKEN id="token-117-20" pos="word" morph="none" start_char="8792" end_char="8797">people</TOKEN>
<TOKEN id="token-117-21" pos="word" morph="none" start_char="8799" end_char="8806">infected</TOKEN>
<TOKEN id="token-117-22" pos="word" morph="none" start_char="8808" end_char="8811">have</TOKEN>
<TOKEN id="token-117-23" pos="word" morph="none" start_char="8813" end_char="8816">mild</TOKEN>
<TOKEN id="token-117-24" pos="word" morph="none" start_char="8818" end_char="8825">symptoms</TOKEN>
<TOKEN id="token-117-25" pos="word" morph="none" start_char="8827" end_char="8828">or</TOKEN>
<TOKEN id="token-117-26" pos="word" morph="none" start_char="8830" end_char="8832">are</TOKEN>
<TOKEN id="token-117-27" pos="word" morph="none" start_char="8834" end_char="8834">A</TOKEN>
<TOKEN id="token-117-28" pos="word" morph="none" start_char="8836" end_char="8846">symptomatic</TOKEN>
<TOKEN id="token-117-29" pos="punct" morph="none" start_char="8847" end_char="8847">.</TOKEN>
</SEG>
<SEG id="segment-118" start_char="8849" end_char="8920">
<ORIGINAL_TEXT>Deaths only occur with the elderly or people with underlying conditions.</ORIGINAL_TEXT>
<TOKEN id="token-118-0" pos="word" morph="none" start_char="8849" end_char="8854">Deaths</TOKEN>
<TOKEN id="token-118-1" pos="word" morph="none" start_char="8856" end_char="8859">only</TOKEN>
<TOKEN id="token-118-2" pos="word" morph="none" start_char="8861" end_char="8865">occur</TOKEN>
<TOKEN id="token-118-3" pos="word" morph="none" start_char="8867" end_char="8870">with</TOKEN>
<TOKEN id="token-118-4" pos="word" morph="none" start_char="8872" end_char="8874">the</TOKEN>
<TOKEN id="token-118-5" pos="word" morph="none" start_char="8876" end_char="8882">elderly</TOKEN>
<TOKEN id="token-118-6" pos="word" morph="none" start_char="8884" end_char="8885">or</TOKEN>
<TOKEN id="token-118-7" pos="word" morph="none" start_char="8887" end_char="8892">people</TOKEN>
<TOKEN id="token-118-8" pos="word" morph="none" start_char="8894" end_char="8897">with</TOKEN>
<TOKEN id="token-118-9" pos="word" morph="none" start_char="8899" end_char="8908">underlying</TOKEN>
<TOKEN id="token-118-10" pos="word" morph="none" start_char="8910" end_char="8919">conditions</TOKEN>
<TOKEN id="token-118-11" pos="punct" morph="none" start_char="8920" end_char="8920">.</TOKEN>
</SEG>
<SEG id="segment-119" start_char="8923" end_char="9059">
<ORIGINAL_TEXT>With up to a month incubation time, inherently means the virus will spread, as it has, arround the population atleast a month undetected.</ORIGINAL_TEXT>
<TOKEN id="token-119-0" pos="word" morph="none" start_char="8923" end_char="8926">With</TOKEN>
<TOKEN id="token-119-1" pos="word" morph="none" start_char="8928" end_char="8929">up</TOKEN>
<TOKEN id="token-119-2" pos="word" morph="none" start_char="8931" end_char="8932">to</TOKEN>
<TOKEN id="token-119-3" pos="word" morph="none" start_char="8934" end_char="8934">a</TOKEN>
<TOKEN id="token-119-4" pos="word" morph="none" start_char="8936" end_char="8940">month</TOKEN>
<TOKEN id="token-119-5" pos="word" morph="none" start_char="8942" end_char="8951">incubation</TOKEN>
<TOKEN id="token-119-6" pos="word" morph="none" start_char="8953" end_char="8956">time</TOKEN>
<TOKEN id="token-119-7" pos="punct" morph="none" start_char="8957" end_char="8957">,</TOKEN>
<TOKEN id="token-119-8" pos="word" morph="none" start_char="8959" end_char="8968">inherently</TOKEN>
<TOKEN id="token-119-9" pos="word" morph="none" start_char="8970" end_char="8974">means</TOKEN>
<TOKEN id="token-119-10" pos="word" morph="none" start_char="8976" end_char="8978">the</TOKEN>
<TOKEN id="token-119-11" pos="word" morph="none" start_char="8980" end_char="8984">virus</TOKEN>
<TOKEN id="token-119-12" pos="word" morph="none" start_char="8986" end_char="8989">will</TOKEN>
<TOKEN id="token-119-13" pos="word" morph="none" start_char="8991" end_char="8996">spread</TOKEN>
<TOKEN id="token-119-14" pos="punct" morph="none" start_char="8997" end_char="8997">,</TOKEN>
<TOKEN id="token-119-15" pos="word" morph="none" start_char="8999" end_char="9000">as</TOKEN>
<TOKEN id="token-119-16" pos="word" morph="none" start_char="9002" end_char="9003">it</TOKEN>
<TOKEN id="token-119-17" pos="word" morph="none" start_char="9005" end_char="9007">has</TOKEN>
<TOKEN id="token-119-18" pos="punct" morph="none" start_char="9008" end_char="9008">,</TOKEN>
<TOKEN id="token-119-19" pos="word" morph="none" start_char="9010" end_char="9016">arround</TOKEN>
<TOKEN id="token-119-20" pos="word" morph="none" start_char="9018" end_char="9020">the</TOKEN>
<TOKEN id="token-119-21" pos="word" morph="none" start_char="9022" end_char="9031">population</TOKEN>
<TOKEN id="token-119-22" pos="word" morph="none" start_char="9033" end_char="9039">atleast</TOKEN>
<TOKEN id="token-119-23" pos="word" morph="none" start_char="9041" end_char="9041">a</TOKEN>
<TOKEN id="token-119-24" pos="word" morph="none" start_char="9043" end_char="9047">month</TOKEN>
<TOKEN id="token-119-25" pos="word" morph="none" start_char="9049" end_char="9058">undetected</TOKEN>
<TOKEN id="token-119-26" pos="punct" morph="none" start_char="9059" end_char="9059">.</TOKEN>
</SEG>
<SEG id="segment-120" start_char="9061" end_char="9112">
<ORIGINAL_TEXT>With 80% presenting flu like symptoms if any at all.</ORIGINAL_TEXT>
<TOKEN id="token-120-0" pos="word" morph="none" start_char="9061" end_char="9064">With</TOKEN>
<TOKEN id="token-120-1" pos="word" morph="none" start_char="9066" end_char="9067">80</TOKEN>
<TOKEN id="token-120-2" pos="punct" morph="none" start_char="9068" end_char="9068">%</TOKEN>
<TOKEN id="token-120-3" pos="word" morph="none" start_char="9070" end_char="9079">presenting</TOKEN>
<TOKEN id="token-120-4" pos="word" morph="none" start_char="9081" end_char="9083">flu</TOKEN>
<TOKEN id="token-120-5" pos="word" morph="none" start_char="9085" end_char="9088">like</TOKEN>
<TOKEN id="token-120-6" pos="word" morph="none" start_char="9090" end_char="9097">symptoms</TOKEN>
<TOKEN id="token-120-7" pos="word" morph="none" start_char="9099" end_char="9100">if</TOKEN>
<TOKEN id="token-120-8" pos="word" morph="none" start_char="9102" end_char="9104">any</TOKEN>
<TOKEN id="token-120-9" pos="word" morph="none" start_char="9106" end_char="9107">at</TOKEN>
<TOKEN id="token-120-10" pos="word" morph="none" start_char="9109" end_char="9111">all</TOKEN>
<TOKEN id="token-120-11" pos="punct" morph="none" start_char="9112" end_char="9112">.</TOKEN>
</SEG>
<SEG id="segment-121" start_char="9115" end_char="9155">
<ORIGINAL_TEXT>So your position doesn't match the stats.</ORIGINAL_TEXT>
<TOKEN id="token-121-0" pos="word" morph="none" start_char="9115" end_char="9116">So</TOKEN>
<TOKEN id="token-121-1" pos="word" morph="none" start_char="9118" end_char="9121">your</TOKEN>
<TOKEN id="token-121-2" pos="word" morph="none" start_char="9123" end_char="9130">position</TOKEN>
<TOKEN id="token-121-3" pos="word" morph="none" start_char="9132" end_char="9138">doesn't</TOKEN>
<TOKEN id="token-121-4" pos="word" morph="none" start_char="9140" end_char="9144">match</TOKEN>
<TOKEN id="token-121-5" pos="word" morph="none" start_char="9146" end_char="9148">the</TOKEN>
<TOKEN id="token-121-6" pos="word" morph="none" start_char="9150" end_char="9154">stats</TOKEN>
<TOKEN id="token-121-7" pos="punct" morph="none" start_char="9155" end_char="9155">.</TOKEN>
</SEG>
<SEG id="segment-122" start_char="9159" end_char="9204">
<ORIGINAL_TEXT>Quote from: alancalverd on 07/02/2021 01:11:23</ORIGINAL_TEXT>
<TOKEN id="token-122-0" pos="word" morph="none" start_char="9159" end_char="9163">Quote</TOKEN>
<TOKEN id="token-122-1" pos="word" morph="none" start_char="9165" end_char="9168">from</TOKEN>
<TOKEN id="token-122-2" pos="punct" morph="none" start_char="9169" end_char="9169">:</TOKEN>
<TOKEN id="token-122-3" pos="word" morph="none" start_char="9171" end_char="9181">alancalverd</TOKEN>
<TOKEN id="token-122-4" pos="word" morph="none" start_char="9183" end_char="9184">on</TOKEN>
<TOKEN id="token-122-5" pos="unknown" morph="none" start_char="9186" end_char="9195">07/02/2021</TOKEN>
<TOKEN id="token-122-6" pos="unknown" morph="none" start_char="9197" end_char="9204">01:11:23</TOKEN>
</SEG>
<SEG id="segment-123" start_char="9207" end_char="9309">
<ORIGINAL_TEXT>Quote from: Bored chemist on Yesterday at 20:19:01Quote from: Jolly2 on Yesterday at 20:16:16they found</ORIGINAL_TEXT>
<TOKEN id="token-123-0" pos="word" morph="none" start_char="9207" end_char="9211">Quote</TOKEN>
<TOKEN id="token-123-1" pos="word" morph="none" start_char="9213" end_char="9216">from</TOKEN>
<TOKEN id="token-123-2" pos="punct" morph="none" start_char="9217" end_char="9217">:</TOKEN>
<TOKEN id="token-123-3" pos="word" morph="none" start_char="9219" end_char="9223">Bored</TOKEN>
<TOKEN id="token-123-4" pos="word" morph="none" start_char="9225" end_char="9231">chemist</TOKEN>
<TOKEN id="token-123-5" pos="word" morph="none" start_char="9233" end_char="9234">on</TOKEN>
<TOKEN id="token-123-6" pos="word" morph="none" start_char="9236" end_char="9244">Yesterday</TOKEN>
<TOKEN id="token-123-7" pos="word" morph="none" start_char="9246" end_char="9247">at</TOKEN>
<TOKEN id="token-123-8" pos="unknown" morph="none" start_char="9249" end_char="9261">20:19:01Quote</TOKEN>
<TOKEN id="token-123-9" pos="word" morph="none" start_char="9263" end_char="9266">from</TOKEN>
<TOKEN id="token-123-10" pos="punct" morph="none" start_char="9267" end_char="9267">:</TOKEN>
<TOKEN id="token-123-11" pos="word" morph="none" start_char="9269" end_char="9274">Jolly2</TOKEN>
<TOKEN id="token-123-12" pos="word" morph="none" start_char="9276" end_char="9277">on</TOKEN>
<TOKEN id="token-123-13" pos="word" morph="none" start_char="9279" end_char="9287">Yesterday</TOKEN>
<TOKEN id="token-123-14" pos="word" morph="none" start_char="9289" end_char="9290">at</TOKEN>
<TOKEN id="token-123-15" pos="unknown" morph="none" start_char="9292" end_char="9303">20:16:16they</TOKEN>
<TOKEN id="token-123-16" pos="word" morph="none" start_char="9305" end_char="9309">found</TOKEN>
</SEG>
<SEG id="segment-124" start_char="9312" end_char="9425">
<ORIGINAL_TEXT>The issue here is that we don't know how long fort Detrick was releasing materials or even what materials escaped.</ORIGINAL_TEXT>
<TOKEN id="token-124-0" pos="word" morph="none" start_char="9312" end_char="9314">The</TOKEN>
<TOKEN id="token-124-1" pos="word" morph="none" start_char="9316" end_char="9320">issue</TOKEN>
<TOKEN id="token-124-2" pos="word" morph="none" start_char="9322" end_char="9325">here</TOKEN>
<TOKEN id="token-124-3" pos="word" morph="none" start_char="9327" end_char="9328">is</TOKEN>
<TOKEN id="token-124-4" pos="word" morph="none" start_char="9330" end_char="9333">that</TOKEN>
<TOKEN id="token-124-5" pos="word" morph="none" start_char="9335" end_char="9336">we</TOKEN>
<TOKEN id="token-124-6" pos="word" morph="none" start_char="9338" end_char="9342">don't</TOKEN>
<TOKEN id="token-124-7" pos="word" morph="none" start_char="9344" end_char="9347">know</TOKEN>
<TOKEN id="token-124-8" pos="word" morph="none" start_char="9349" end_char="9351">how</TOKEN>
<TOKEN id="token-124-9" pos="word" morph="none" start_char="9353" end_char="9356">long</TOKEN>
<TOKEN id="token-124-10" pos="word" morph="none" start_char="9358" end_char="9361">fort</TOKEN>
<TOKEN id="token-124-11" pos="word" morph="none" start_char="9363" end_char="9369">Detrick</TOKEN>
<TOKEN id="token-124-12" pos="word" morph="none" start_char="9371" end_char="9373">was</TOKEN>
<TOKEN id="token-124-13" pos="word" morph="none" start_char="9375" end_char="9383">releasing</TOKEN>
<TOKEN id="token-124-14" pos="word" morph="none" start_char="9385" end_char="9393">materials</TOKEN>
<TOKEN id="token-124-15" pos="word" morph="none" start_char="9395" end_char="9396">or</TOKEN>
<TOKEN id="token-124-16" pos="word" morph="none" start_char="9398" end_char="9401">even</TOKEN>
<TOKEN id="token-124-17" pos="word" morph="none" start_char="9403" end_char="9406">what</TOKEN>
<TOKEN id="token-124-18" pos="word" morph="none" start_char="9408" end_char="9416">materials</TOKEN>
<TOKEN id="token-124-19" pos="word" morph="none" start_char="9418" end_char="9424">escaped</TOKEN>
<TOKEN id="token-124-20" pos="punct" morph="none" start_char="9425" end_char="9425">.</TOKEN>
</SEG>
<SEG id="segment-125" start_char="9427" end_char="9486">
<ORIGINAL_TEXT>How long would it take for a virus to travel a 4 hour drive?</ORIGINAL_TEXT>
<TOKEN id="token-125-0" pos="word" morph="none" start_char="9427" end_char="9429">How</TOKEN>
<TOKEN id="token-125-1" pos="word" morph="none" start_char="9431" end_char="9434">long</TOKEN>
<TOKEN id="token-125-2" pos="word" morph="none" start_char="9436" end_char="9440">would</TOKEN>
<TOKEN id="token-125-3" pos="word" morph="none" start_char="9442" end_char="9443">it</TOKEN>
<TOKEN id="token-125-4" pos="word" morph="none" start_char="9445" end_char="9448">take</TOKEN>
<TOKEN id="token-125-5" pos="word" morph="none" start_char="9450" end_char="9452">for</TOKEN>
<TOKEN id="token-125-6" pos="word" morph="none" start_char="9454" end_char="9454">a</TOKEN>
<TOKEN id="token-125-7" pos="word" morph="none" start_char="9456" end_char="9460">virus</TOKEN>
<TOKEN id="token-125-8" pos="word" morph="none" start_char="9462" end_char="9463">to</TOKEN>
<TOKEN id="token-125-9" pos="word" morph="none" start_char="9465" end_char="9470">travel</TOKEN>
<TOKEN id="token-125-10" pos="word" morph="none" start_char="9472" end_char="9472">a</TOKEN>
<TOKEN id="token-125-11" pos="word" morph="none" start_char="9474" end_char="9474">4</TOKEN>
<TOKEN id="token-125-12" pos="word" morph="none" start_char="9476" end_char="9479">hour</TOKEN>
<TOKEN id="token-125-13" pos="word" morph="none" start_char="9481" end_char="9485">drive</TOKEN>
<TOKEN id="token-125-14" pos="punct" morph="none" start_char="9486" end_char="9486">?</TOKEN>
</SEG>
<SEG id="segment-126" start_char="9488" end_char="9633">
<ORIGINAL_TEXT>Hard to say, but considering covid spread rather rapidly across borders, of 1000s of miles in weeks, a four hour drive is a rather short distance.</ORIGINAL_TEXT>
<TOKEN id="token-126-0" pos="word" morph="none" start_char="9488" end_char="9491">Hard</TOKEN>
<TOKEN id="token-126-1" pos="word" morph="none" start_char="9493" end_char="9494">to</TOKEN>
<TOKEN id="token-126-2" pos="word" morph="none" start_char="9496" end_char="9498">say</TOKEN>
<TOKEN id="token-126-3" pos="punct" morph="none" start_char="9499" end_char="9499">,</TOKEN>
<TOKEN id="token-126-4" pos="word" morph="none" start_char="9501" end_char="9503">but</TOKEN>
<TOKEN id="token-126-5" pos="word" morph="none" start_char="9505" end_char="9515">considering</TOKEN>
<TOKEN id="token-126-6" pos="word" morph="none" start_char="9517" end_char="9521">covid</TOKEN>
<TOKEN id="token-126-7" pos="word" morph="none" start_char="9523" end_char="9528">spread</TOKEN>
<TOKEN id="token-126-8" pos="word" morph="none" start_char="9530" end_char="9535">rather</TOKEN>
<TOKEN id="token-126-9" pos="word" morph="none" start_char="9537" end_char="9543">rapidly</TOKEN>
<TOKEN id="token-126-10" pos="word" morph="none" start_char="9545" end_char="9550">across</TOKEN>
<TOKEN id="token-126-11" pos="word" morph="none" start_char="9552" end_char="9558">borders</TOKEN>
<TOKEN id="token-126-12" pos="punct" morph="none" start_char="9559" end_char="9559">,</TOKEN>
<TOKEN id="token-126-13" pos="word" morph="none" start_char="9561" end_char="9562">of</TOKEN>
<TOKEN id="token-126-14" pos="word" morph="none" start_char="9564" end_char="9568">1000s</TOKEN>
<TOKEN id="token-126-15" pos="word" morph="none" start_char="9570" end_char="9571">of</TOKEN>
<TOKEN id="token-126-16" pos="word" morph="none" start_char="9573" end_char="9577">miles</TOKEN>
<TOKEN id="token-126-17" pos="word" morph="none" start_char="9579" end_char="9580">in</TOKEN>
<TOKEN id="token-126-18" pos="word" morph="none" start_char="9582" end_char="9586">weeks</TOKEN>
<TOKEN id="token-126-19" pos="punct" morph="none" start_char="9587" end_char="9587">,</TOKEN>
<TOKEN id="token-126-20" pos="word" morph="none" start_char="9589" end_char="9589">a</TOKEN>
<TOKEN id="token-126-21" pos="word" morph="none" start_char="9591" end_char="9594">four</TOKEN>
<TOKEN id="token-126-22" pos="word" morph="none" start_char="9596" end_char="9599">hour</TOKEN>
<TOKEN id="token-126-23" pos="word" morph="none" start_char="9601" end_char="9605">drive</TOKEN>
<TOKEN id="token-126-24" pos="word" morph="none" start_char="9607" end_char="9608">is</TOKEN>
<TOKEN id="token-126-25" pos="word" morph="none" start_char="9610" end_char="9610">a</TOKEN>
<TOKEN id="token-126-26" pos="word" morph="none" start_char="9612" end_char="9617">rather</TOKEN>
<TOKEN id="token-126-27" pos="word" morph="none" start_char="9619" end_char="9623">short</TOKEN>
<TOKEN id="token-126-28" pos="word" morph="none" start_char="9625" end_char="9632">distance</TOKEN>
<TOKEN id="token-126-29" pos="punct" morph="none" start_char="9633" end_char="9633">.</TOKEN>
</SEG>
<SEG id="segment-127" start_char="9636" end_char="9733">
<ORIGINAL_TEXT>We know the out break happened 2 weeks before fort Detrick was closed for miss managing materials.</ORIGINAL_TEXT>
<TOKEN id="token-127-0" pos="word" morph="none" start_char="9636" end_char="9637">We</TOKEN>
<TOKEN id="token-127-1" pos="word" morph="none" start_char="9639" end_char="9642">know</TOKEN>
<TOKEN id="token-127-2" pos="word" morph="none" start_char="9644" end_char="9646">the</TOKEN>
<TOKEN id="token-127-3" pos="word" morph="none" start_char="9648" end_char="9650">out</TOKEN>
<TOKEN id="token-127-4" pos="word" morph="none" start_char="9652" end_char="9656">break</TOKEN>
<TOKEN id="token-127-5" pos="word" morph="none" start_char="9658" end_char="9665">happened</TOKEN>
<TOKEN id="token-127-6" pos="word" morph="none" start_char="9667" end_char="9667">2</TOKEN>
<TOKEN id="token-127-7" pos="word" morph="none" start_char="9669" end_char="9673">weeks</TOKEN>
<TOKEN id="token-127-8" pos="word" morph="none" start_char="9675" end_char="9680">before</TOKEN>
<TOKEN id="token-127-9" pos="word" morph="none" start_char="9682" end_char="9685">fort</TOKEN>
<TOKEN id="token-127-10" pos="word" morph="none" start_char="9687" end_char="9693">Detrick</TOKEN>
<TOKEN id="token-127-11" pos="word" morph="none" start_char="9695" end_char="9697">was</TOKEN>
<TOKEN id="token-127-12" pos="word" morph="none" start_char="9699" end_char="9704">closed</TOKEN>
<TOKEN id="token-127-13" pos="word" morph="none" start_char="9706" end_char="9708">for</TOKEN>
<TOKEN id="token-127-14" pos="word" morph="none" start_char="9710" end_char="9713">miss</TOKEN>
<TOKEN id="token-127-15" pos="word" morph="none" start_char="9715" end_char="9722">managing</TOKEN>
<TOKEN id="token-127-16" pos="word" morph="none" start_char="9724" end_char="9732">materials</TOKEN>
<TOKEN id="token-127-17" pos="punct" morph="none" start_char="9733" end_char="9733">.</TOKEN>
</SEG>
<SEG id="segment-128" start_char="9735" end_char="9875">
<ORIGINAL_TEXT>The out break could have been the sign that lead to fort Detricks closing, but under national security, America isn't revealing what happened</ORIGINAL_TEXT>
<TOKEN id="token-128-0" pos="word" morph="none" start_char="9735" end_char="9737">The</TOKEN>
<TOKEN id="token-128-1" pos="word" morph="none" start_char="9739" end_char="9741">out</TOKEN>
<TOKEN id="token-128-2" pos="word" morph="none" start_char="9743" end_char="9747">break</TOKEN>
<TOKEN id="token-128-3" pos="word" morph="none" start_char="9749" end_char="9753">could</TOKEN>
<TOKEN id="token-128-4" pos="word" morph="none" start_char="9755" end_char="9758">have</TOKEN>
<TOKEN id="token-128-5" pos="word" morph="none" start_char="9760" end_char="9763">been</TOKEN>
<TOKEN id="token-128-6" pos="word" morph="none" start_char="9765" end_char="9767">the</TOKEN>
<TOKEN id="token-128-7" pos="word" morph="none" start_char="9769" end_char="9772">sign</TOKEN>
<TOKEN id="token-128-8" pos="word" morph="none" start_char="9774" end_char="9777">that</TOKEN>
<TOKEN id="token-128-9" pos="word" morph="none" start_char="9779" end_char="9782">lead</TOKEN>
<TOKEN id="token-128-10" pos="word" morph="none" start_char="9784" end_char="9785">to</TOKEN>
<TOKEN id="token-128-11" pos="word" morph="none" start_char="9787" end_char="9790">fort</TOKEN>
<TOKEN id="token-128-12" pos="word" morph="none" start_char="9792" end_char="9799">Detricks</TOKEN>
<TOKEN id="token-128-13" pos="word" morph="none" start_char="9801" end_char="9807">closing</TOKEN>
<TOKEN id="token-128-14" pos="punct" morph="none" start_char="9808" end_char="9808">,</TOKEN>
<TOKEN id="token-128-15" pos="word" morph="none" start_char="9810" end_char="9812">but</TOKEN>
<TOKEN id="token-128-16" pos="word" morph="none" start_char="9814" end_char="9818">under</TOKEN>
<TOKEN id="token-128-17" pos="word" morph="none" start_char="9820" end_char="9827">national</TOKEN>
<TOKEN id="token-128-18" pos="word" morph="none" start_char="9829" end_char="9836">security</TOKEN>
<TOKEN id="token-128-19" pos="punct" morph="none" start_char="9837" end_char="9837">,</TOKEN>
<TOKEN id="token-128-20" pos="word" morph="none" start_char="9839" end_char="9845">America</TOKEN>
<TOKEN id="token-128-21" pos="word" morph="none" start_char="9847" end_char="9851">isn't</TOKEN>
<TOKEN id="token-128-22" pos="word" morph="none" start_char="9853" end_char="9861">revealing</TOKEN>
<TOKEN id="token-128-23" pos="word" morph="none" start_char="9863" end_char="9866">what</TOKEN>
<TOKEN id="token-128-24" pos="word" morph="none" start_char="9868" end_char="9875">happened</TOKEN>
</SEG>
<SEG id="segment-129" start_char="9879" end_char="9920">
<ORIGINAL_TEXT>Quote from: evan_au on 07/02/2021 04:19:18</ORIGINAL_TEXT>
<TOKEN id="token-129-0" pos="word" morph="none" start_char="9879" end_char="9883">Quote</TOKEN>
<TOKEN id="token-129-1" pos="word" morph="none" start_char="9885" end_char="9888">from</TOKEN>
<TOKEN id="token-129-2" pos="punct" morph="none" start_char="9889" end_char="9889">:</TOKEN>
<TOKEN id="token-129-3" pos="word" morph="none" start_char="9891" end_char="9897">evan_au</TOKEN>
<TOKEN id="token-129-4" pos="word" morph="none" start_char="9899" end_char="9900">on</TOKEN>
<TOKEN id="token-129-5" pos="unknown" morph="none" start_char="9902" end_char="9911">07/02/2021</TOKEN>
<TOKEN id="token-129-6" pos="unknown" morph="none" start_char="9913" end_char="9920">04:19:18</TOKEN>
</SEG>
<SEG id="segment-130" start_char="9923" end_char="9940">
<ORIGINAL_TEXT>Quote from: Jolly2</ORIGINAL_TEXT>
<TOKEN id="token-130-0" pos="word" morph="none" start_char="9923" end_char="9927">Quote</TOKEN>
<TOKEN id="token-130-1" pos="word" morph="none" start_char="9929" end_char="9932">from</TOKEN>
<TOKEN id="token-130-2" pos="punct" morph="none" start_char="9933" end_char="9933">:</TOKEN>
<TOKEN id="token-130-3" pos="word" morph="none" start_char="9935" end_char="9940">Jolly2</TOKEN>
</SEG>
<SEG id="segment-131" start_char="9943" end_char="10001">
<ORIGINAL_TEXT>spain as early as March 2019...they found samples in sewage</ORIGINAL_TEXT>
<TOKEN id="token-131-0" pos="word" morph="none" start_char="9943" end_char="9947">spain</TOKEN>
<TOKEN id="token-131-1" pos="word" morph="none" start_char="9949" end_char="9950">as</TOKEN>
<TOKEN id="token-131-2" pos="word" morph="none" start_char="9952" end_char="9956">early</TOKEN>
<TOKEN id="token-131-3" pos="word" morph="none" start_char="9958" end_char="9959">as</TOKEN>
<TOKEN id="token-131-4" pos="word" morph="none" start_char="9961" end_char="9965">March</TOKEN>
<TOKEN id="token-131-5" pos="unknown" morph="none" start_char="9967" end_char="9977">2019...they</TOKEN>
<TOKEN id="token-131-6" pos="word" morph="none" start_char="9979" end_char="9983">found</TOKEN>
<TOKEN id="token-131-7" pos="word" morph="none" start_char="9985" end_char="9991">samples</TOKEN>
<TOKEN id="token-131-8" pos="word" morph="none" start_char="9993" end_char="9994">in</TOKEN>
<TOKEN id="token-131-9" pos="word" morph="none" start_char="9996" end_char="10001">sewage</TOKEN>
</SEG>
<SEG id="segment-132" start_char="10004" end_char="10928">
<ORIGINAL_TEXT>The quality of RNA in sewage is very variable.- We know that the fatty coat of the SARS-COV2 virus is broken down by soaps and detergents- Most people flush soap down the sewer when they take a bath or shower- Most people flush detergents down the sewer when they wash the dishes (and use even more destructive chemicals in the dishwasher)- Industrial processes can also flush destructive chemicals into sewers- So RNA is badly degraded when it is collected (within 1 week)- And, depending on how it is stored, may continue degrading if it is stored for months afterwards.So we have degraded RNA from stored sewage samples, which is compared to a new viral sequence (SARS-COV2).- This comparison must exclude the 4 common coronaviruses that are responsible for something like 30% of the "common cold" infections- And residual infections of MERS and SARS, which have 50% (MERS) or 80% (SARS) genetic similarities to SARS-COV2.</ORIGINAL_TEXT>
<TOKEN id="token-132-0" pos="word" morph="none" start_char="10004" end_char="10006">The</TOKEN>
<TOKEN id="token-132-1" pos="word" morph="none" start_char="10008" end_char="10014">quality</TOKEN>
<TOKEN id="token-132-2" pos="word" morph="none" start_char="10016" end_char="10017">of</TOKEN>
<TOKEN id="token-132-3" pos="word" morph="none" start_char="10019" end_char="10021">RNA</TOKEN>
<TOKEN id="token-132-4" pos="word" morph="none" start_char="10023" end_char="10024">in</TOKEN>
<TOKEN id="token-132-5" pos="word" morph="none" start_char="10026" end_char="10031">sewage</TOKEN>
<TOKEN id="token-132-6" pos="word" morph="none" start_char="10033" end_char="10034">is</TOKEN>
<TOKEN id="token-132-7" pos="word" morph="none" start_char="10036" end_char="10039">very</TOKEN>
<TOKEN id="token-132-8" pos="word" morph="none" start_char="10041" end_char="10048">variable</TOKEN>
<TOKEN id="token-132-9" pos="punct" morph="none" start_char="10049" end_char="10050">.-</TOKEN>
<TOKEN id="token-132-10" pos="word" morph="none" start_char="10052" end_char="10053">We</TOKEN>
<TOKEN id="token-132-11" pos="word" morph="none" start_char="10055" end_char="10058">know</TOKEN>
<TOKEN id="token-132-12" pos="word" morph="none" start_char="10060" end_char="10063">that</TOKEN>
<TOKEN id="token-132-13" pos="word" morph="none" start_char="10065" end_char="10067">the</TOKEN>
<TOKEN id="token-132-14" pos="word" morph="none" start_char="10069" end_char="10073">fatty</TOKEN>
<TOKEN id="token-132-15" pos="word" morph="none" start_char="10075" end_char="10078">coat</TOKEN>
<TOKEN id="token-132-16" pos="word" morph="none" start_char="10080" end_char="10081">of</TOKEN>
<TOKEN id="token-132-17" pos="word" morph="none" start_char="10083" end_char="10085">the</TOKEN>
<TOKEN id="token-132-18" pos="unknown" morph="none" start_char="10087" end_char="10095">SARS-COV2</TOKEN>
<TOKEN id="token-132-19" pos="word" morph="none" start_char="10097" end_char="10101">virus</TOKEN>
<TOKEN id="token-132-20" pos="word" morph="none" start_char="10103" end_char="10104">is</TOKEN>
<TOKEN id="token-132-21" pos="word" morph="none" start_char="10106" end_char="10111">broken</TOKEN>
<TOKEN id="token-132-22" pos="word" morph="none" start_char="10113" end_char="10116">down</TOKEN>
<TOKEN id="token-132-23" pos="word" morph="none" start_char="10118" end_char="10119">by</TOKEN>
<TOKEN id="token-132-24" pos="word" morph="none" start_char="10121" end_char="10125">soaps</TOKEN>
<TOKEN id="token-132-25" pos="word" morph="none" start_char="10127" end_char="10129">and</TOKEN>
<TOKEN id="token-132-26" pos="word" morph="none" start_char="10131" end_char="10140">detergents</TOKEN>
<TOKEN id="token-132-27" pos="punct" morph="none" start_char="10141" end_char="10141">-</TOKEN>
<TOKEN id="token-132-28" pos="word" morph="none" start_char="10143" end_char="10146">Most</TOKEN>
<TOKEN id="token-132-29" pos="word" morph="none" start_char="10148" end_char="10153">people</TOKEN>
<TOKEN id="token-132-30" pos="word" morph="none" start_char="10155" end_char="10159">flush</TOKEN>
<TOKEN id="token-132-31" pos="word" morph="none" start_char="10161" end_char="10164">soap</TOKEN>
<TOKEN id="token-132-32" pos="word" morph="none" start_char="10166" end_char="10169">down</TOKEN>
<TOKEN id="token-132-33" pos="word" morph="none" start_char="10171" end_char="10173">the</TOKEN>
<TOKEN id="token-132-34" pos="word" morph="none" start_char="10175" end_char="10179">sewer</TOKEN>
<TOKEN id="token-132-35" pos="word" morph="none" start_char="10181" end_char="10184">when</TOKEN>
<TOKEN id="token-132-36" pos="word" morph="none" start_char="10186" end_char="10189">they</TOKEN>
<TOKEN id="token-132-37" pos="word" morph="none" start_char="10191" end_char="10194">take</TOKEN>
<TOKEN id="token-132-38" pos="word" morph="none" start_char="10196" end_char="10196">a</TOKEN>
<TOKEN id="token-132-39" pos="word" morph="none" start_char="10198" end_char="10201">bath</TOKEN>
<TOKEN id="token-132-40" pos="word" morph="none" start_char="10203" end_char="10204">or</TOKEN>
<TOKEN id="token-132-41" pos="word" morph="none" start_char="10206" end_char="10211">shower</TOKEN>
<TOKEN id="token-132-42" pos="punct" morph="none" start_char="10212" end_char="10212">-</TOKEN>
<TOKEN id="token-132-43" pos="word" morph="none" start_char="10214" end_char="10217">Most</TOKEN>
<TOKEN id="token-132-44" pos="word" morph="none" start_char="10219" end_char="10224">people</TOKEN>
<TOKEN id="token-132-45" pos="word" morph="none" start_char="10226" end_char="10230">flush</TOKEN>
<TOKEN id="token-132-46" pos="word" morph="none" start_char="10232" end_char="10241">detergents</TOKEN>
<TOKEN id="token-132-47" pos="word" morph="none" start_char="10243" end_char="10246">down</TOKEN>
<TOKEN id="token-132-48" pos="word" morph="none" start_char="10248" end_char="10250">the</TOKEN>
<TOKEN id="token-132-49" pos="word" morph="none" start_char="10252" end_char="10256">sewer</TOKEN>
<TOKEN id="token-132-50" pos="word" morph="none" start_char="10258" end_char="10261">when</TOKEN>
<TOKEN id="token-132-51" pos="word" morph="none" start_char="10263" end_char="10266">they</TOKEN>
<TOKEN id="token-132-52" pos="word" morph="none" start_char="10268" end_char="10271">wash</TOKEN>
<TOKEN id="token-132-53" pos="word" morph="none" start_char="10273" end_char="10275">the</TOKEN>
<TOKEN id="token-132-54" pos="word" morph="none" start_char="10277" end_char="10282">dishes</TOKEN>
<TOKEN id="token-132-55" pos="punct" morph="none" start_char="10284" end_char="10284">(</TOKEN>
<TOKEN id="token-132-56" pos="word" morph="none" start_char="10285" end_char="10287">and</TOKEN>
<TOKEN id="token-132-57" pos="word" morph="none" start_char="10289" end_char="10291">use</TOKEN>
<TOKEN id="token-132-58" pos="word" morph="none" start_char="10293" end_char="10296">even</TOKEN>
<TOKEN id="token-132-59" pos="word" morph="none" start_char="10298" end_char="10301">more</TOKEN>
<TOKEN id="token-132-60" pos="word" morph="none" start_char="10303" end_char="10313">destructive</TOKEN>
<TOKEN id="token-132-61" pos="word" morph="none" start_char="10315" end_char="10323">chemicals</TOKEN>
<TOKEN id="token-132-62" pos="word" morph="none" start_char="10325" end_char="10326">in</TOKEN>
<TOKEN id="token-132-63" pos="word" morph="none" start_char="10328" end_char="10330">the</TOKEN>
<TOKEN id="token-132-64" pos="word" morph="none" start_char="10332" end_char="10341">dishwasher</TOKEN>
<TOKEN id="token-132-65" pos="punct" morph="none" start_char="10342" end_char="10343">)-</TOKEN>
<TOKEN id="token-132-66" pos="word" morph="none" start_char="10345" end_char="10354">Industrial</TOKEN>
<TOKEN id="token-132-67" pos="word" morph="none" start_char="10356" end_char="10364">processes</TOKEN>
<TOKEN id="token-132-68" pos="word" morph="none" start_char="10366" end_char="10368">can</TOKEN>
<TOKEN id="token-132-69" pos="word" morph="none" start_char="10370" end_char="10373">also</TOKEN>
<TOKEN id="token-132-70" pos="word" morph="none" start_char="10375" end_char="10379">flush</TOKEN>
<TOKEN id="token-132-71" pos="word" morph="none" start_char="10381" end_char="10391">destructive</TOKEN>
<TOKEN id="token-132-72" pos="word" morph="none" start_char="10393" end_char="10401">chemicals</TOKEN>
<TOKEN id="token-132-73" pos="word" morph="none" start_char="10403" end_char="10406">into</TOKEN>
<TOKEN id="token-132-74" pos="word" morph="none" start_char="10408" end_char="10413">sewers</TOKEN>
<TOKEN id="token-132-75" pos="punct" morph="none" start_char="10414" end_char="10414">-</TOKEN>
<TOKEN id="token-132-76" pos="word" morph="none" start_char="10416" end_char="10417">So</TOKEN>
<TOKEN id="token-132-77" pos="word" morph="none" start_char="10419" end_char="10421">RNA</TOKEN>
<TOKEN id="token-132-78" pos="word" morph="none" start_char="10423" end_char="10424">is</TOKEN>
<TOKEN id="token-132-79" pos="word" morph="none" start_char="10426" end_char="10430">badly</TOKEN>
<TOKEN id="token-132-80" pos="word" morph="none" start_char="10432" end_char="10439">degraded</TOKEN>
<TOKEN id="token-132-81" pos="word" morph="none" start_char="10441" end_char="10444">when</TOKEN>
<TOKEN id="token-132-82" pos="word" morph="none" start_char="10446" end_char="10447">it</TOKEN>
<TOKEN id="token-132-83" pos="word" morph="none" start_char="10449" end_char="10450">is</TOKEN>
<TOKEN id="token-132-84" pos="word" morph="none" start_char="10452" end_char="10460">collected</TOKEN>
<TOKEN id="token-132-85" pos="punct" morph="none" start_char="10462" end_char="10462">(</TOKEN>
<TOKEN id="token-132-86" pos="word" morph="none" start_char="10463" end_char="10468">within</TOKEN>
<TOKEN id="token-132-87" pos="word" morph="none" start_char="10470" end_char="10470">1</TOKEN>
<TOKEN id="token-132-88" pos="word" morph="none" start_char="10472" end_char="10475">week</TOKEN>
<TOKEN id="token-132-89" pos="punct" morph="none" start_char="10476" end_char="10477">)-</TOKEN>
<TOKEN id="token-132-90" pos="word" morph="none" start_char="10479" end_char="10481">And</TOKEN>
<TOKEN id="token-132-91" pos="punct" morph="none" start_char="10482" end_char="10482">,</TOKEN>
<TOKEN id="token-132-92" pos="word" morph="none" start_char="10484" end_char="10492">depending</TOKEN>
<TOKEN id="token-132-93" pos="word" morph="none" start_char="10494" end_char="10495">on</TOKEN>
<TOKEN id="token-132-94" pos="word" morph="none" start_char="10497" end_char="10499">how</TOKEN>
<TOKEN id="token-132-95" pos="word" morph="none" start_char="10501" end_char="10502">it</TOKEN>
<TOKEN id="token-132-96" pos="word" morph="none" start_char="10504" end_char="10505">is</TOKEN>
<TOKEN id="token-132-97" pos="word" morph="none" start_char="10507" end_char="10512">stored</TOKEN>
<TOKEN id="token-132-98" pos="punct" morph="none" start_char="10513" end_char="10513">,</TOKEN>
<TOKEN id="token-132-99" pos="word" morph="none" start_char="10515" end_char="10517">may</TOKEN>
<TOKEN id="token-132-100" pos="word" morph="none" start_char="10519" end_char="10526">continue</TOKEN>
<TOKEN id="token-132-101" pos="word" morph="none" start_char="10528" end_char="10536">degrading</TOKEN>
<TOKEN id="token-132-102" pos="word" morph="none" start_char="10538" end_char="10539">if</TOKEN>
<TOKEN id="token-132-103" pos="word" morph="none" start_char="10541" end_char="10542">it</TOKEN>
<TOKEN id="token-132-104" pos="word" morph="none" start_char="10544" end_char="10545">is</TOKEN>
<TOKEN id="token-132-105" pos="word" morph="none" start_char="10547" end_char="10552">stored</TOKEN>
<TOKEN id="token-132-106" pos="word" morph="none" start_char="10554" end_char="10556">for</TOKEN>
<TOKEN id="token-132-107" pos="word" morph="none" start_char="10558" end_char="10563">months</TOKEN>
<TOKEN id="token-132-108" pos="unknown" morph="none" start_char="10565" end_char="10577">afterwards.So</TOKEN>
<TOKEN id="token-132-109" pos="word" morph="none" start_char="10579" end_char="10580">we</TOKEN>
<TOKEN id="token-132-110" pos="word" morph="none" start_char="10582" end_char="10585">have</TOKEN>
<TOKEN id="token-132-111" pos="word" morph="none" start_char="10587" end_char="10594">degraded</TOKEN>
<TOKEN id="token-132-112" pos="word" morph="none" start_char="10596" end_char="10598">RNA</TOKEN>
<TOKEN id="token-132-113" pos="word" morph="none" start_char="10600" end_char="10603">from</TOKEN>
<TOKEN id="token-132-114" pos="word" morph="none" start_char="10605" end_char="10610">stored</TOKEN>
<TOKEN id="token-132-115" pos="word" morph="none" start_char="10612" end_char="10617">sewage</TOKEN>
<TOKEN id="token-132-116" pos="word" morph="none" start_char="10619" end_char="10625">samples</TOKEN>
<TOKEN id="token-132-117" pos="punct" morph="none" start_char="10626" end_char="10626">,</TOKEN>
<TOKEN id="token-132-118" pos="word" morph="none" start_char="10628" end_char="10632">which</TOKEN>
<TOKEN id="token-132-119" pos="word" morph="none" start_char="10634" end_char="10635">is</TOKEN>
<TOKEN id="token-132-120" pos="word" morph="none" start_char="10637" end_char="10644">compared</TOKEN>
<TOKEN id="token-132-121" pos="word" morph="none" start_char="10646" end_char="10647">to</TOKEN>
<TOKEN id="token-132-122" pos="word" morph="none" start_char="10649" end_char="10649">a</TOKEN>
<TOKEN id="token-132-123" pos="word" morph="none" start_char="10651" end_char="10653">new</TOKEN>
<TOKEN id="token-132-124" pos="word" morph="none" start_char="10655" end_char="10659">viral</TOKEN>
<TOKEN id="token-132-125" pos="word" morph="none" start_char="10661" end_char="10668">sequence</TOKEN>
<TOKEN id="token-132-126" pos="punct" morph="none" start_char="10670" end_char="10670">(</TOKEN>
<TOKEN id="token-132-127" pos="unknown" morph="none" start_char="10671" end_char="10679">SARS-COV2</TOKEN>
<TOKEN id="token-132-128" pos="punct" morph="none" start_char="10680" end_char="10682">).-</TOKEN>
<TOKEN id="token-132-129" pos="word" morph="none" start_char="10684" end_char="10687">This</TOKEN>
<TOKEN id="token-132-130" pos="word" morph="none" start_char="10689" end_char="10698">comparison</TOKEN>
<TOKEN id="token-132-131" pos="word" morph="none" start_char="10700" end_char="10703">must</TOKEN>
<TOKEN id="token-132-132" pos="word" morph="none" start_char="10705" end_char="10711">exclude</TOKEN>
<TOKEN id="token-132-133" pos="word" morph="none" start_char="10713" end_char="10715">the</TOKEN>
<TOKEN id="token-132-134" pos="word" morph="none" start_char="10717" end_char="10717">4</TOKEN>
<TOKEN id="token-132-135" pos="word" morph="none" start_char="10719" end_char="10724">common</TOKEN>
<TOKEN id="token-132-136" pos="word" morph="none" start_char="10726" end_char="10738">coronaviruses</TOKEN>
<TOKEN id="token-132-137" pos="word" morph="none" start_char="10740" end_char="10743">that</TOKEN>
<TOKEN id="token-132-138" pos="word" morph="none" start_char="10745" end_char="10747">are</TOKEN>
<TOKEN id="token-132-139" pos="word" morph="none" start_char="10749" end_char="10759">responsible</TOKEN>
<TOKEN id="token-132-140" pos="word" morph="none" start_char="10761" end_char="10763">for</TOKEN>
<TOKEN id="token-132-141" pos="word" morph="none" start_char="10765" end_char="10773">something</TOKEN>
<TOKEN id="token-132-142" pos="word" morph="none" start_char="10775" end_char="10778">like</TOKEN>
<TOKEN id="token-132-143" pos="word" morph="none" start_char="10780" end_char="10781">30</TOKEN>
<TOKEN id="token-132-144" pos="punct" morph="none" start_char="10782" end_char="10782">%</TOKEN>
<TOKEN id="token-132-145" pos="word" morph="none" start_char="10784" end_char="10785">of</TOKEN>
<TOKEN id="token-132-146" pos="word" morph="none" start_char="10787" end_char="10789">the</TOKEN>
<TOKEN id="token-132-147" pos="punct" morph="none" start_char="10791" end_char="10791">"</TOKEN>
<TOKEN id="token-132-148" pos="word" morph="none" start_char="10792" end_char="10797">common</TOKEN>
<TOKEN id="token-132-149" pos="word" morph="none" start_char="10799" end_char="10802">cold</TOKEN>
<TOKEN id="token-132-150" pos="punct" morph="none" start_char="10803" end_char="10803">"</TOKEN>
<TOKEN id="token-132-151" pos="word" morph="none" start_char="10805" end_char="10814">infections</TOKEN>
<TOKEN id="token-132-152" pos="punct" morph="none" start_char="10815" end_char="10815">-</TOKEN>
<TOKEN id="token-132-153" pos="word" morph="none" start_char="10817" end_char="10819">And</TOKEN>
<TOKEN id="token-132-154" pos="word" morph="none" start_char="10821" end_char="10828">residual</TOKEN>
<TOKEN id="token-132-155" pos="word" morph="none" start_char="10830" end_char="10839">infections</TOKEN>
<TOKEN id="token-132-156" pos="word" morph="none" start_char="10841" end_char="10842">of</TOKEN>
<TOKEN id="token-132-157" pos="word" morph="none" start_char="10844" end_char="10847">MERS</TOKEN>
<TOKEN id="token-132-158" pos="word" morph="none" start_char="10849" end_char="10851">and</TOKEN>
<TOKEN id="token-132-159" pos="word" morph="none" start_char="10853" end_char="10856">SARS</TOKEN>
<TOKEN id="token-132-160" pos="punct" morph="none" start_char="10857" end_char="10857">,</TOKEN>
<TOKEN id="token-132-161" pos="word" morph="none" start_char="10859" end_char="10863">which</TOKEN>
<TOKEN id="token-132-162" pos="word" morph="none" start_char="10865" end_char="10868">have</TOKEN>
<TOKEN id="token-132-163" pos="word" morph="none" start_char="10870" end_char="10871">50</TOKEN>
<TOKEN id="token-132-164" pos="punct" morph="none" start_char="10872" end_char="10872">%</TOKEN>
<TOKEN id="token-132-165" pos="punct" morph="none" start_char="10874" end_char="10874">(</TOKEN>
<TOKEN id="token-132-166" pos="word" morph="none" start_char="10875" end_char="10878">MERS</TOKEN>
<TOKEN id="token-132-167" pos="punct" morph="none" start_char="10879" end_char="10879">)</TOKEN>
<TOKEN id="token-132-168" pos="word" morph="none" start_char="10881" end_char="10882">or</TOKEN>
<TOKEN id="token-132-169" pos="word" morph="none" start_char="10884" end_char="10885">80</TOKEN>
<TOKEN id="token-132-170" pos="punct" morph="none" start_char="10886" end_char="10886">%</TOKEN>
<TOKEN id="token-132-171" pos="punct" morph="none" start_char="10888" end_char="10888">(</TOKEN>
<TOKEN id="token-132-172" pos="word" morph="none" start_char="10889" end_char="10892">SARS</TOKEN>
<TOKEN id="token-132-173" pos="punct" morph="none" start_char="10893" end_char="10893">)</TOKEN>
<TOKEN id="token-132-174" pos="word" morph="none" start_char="10895" end_char="10901">genetic</TOKEN>
<TOKEN id="token-132-175" pos="word" morph="none" start_char="10903" end_char="10914">similarities</TOKEN>
<TOKEN id="token-132-176" pos="word" morph="none" start_char="10916" end_char="10917">to</TOKEN>
<TOKEN id="token-132-177" pos="unknown" morph="none" start_char="10919" end_char="10927">SARS-COV2</TOKEN>
<TOKEN id="token-132-178" pos="punct" morph="none" start_char="10928" end_char="10928">.</TOKEN>
</SEG>
<SEG id="segment-133" start_char="10930" end_char="11137">
<ORIGINAL_TEXT>These viruses were never eliminated, but since R 1, the occasional local outbreak will occur, and then die out.- There are other coronaviruses circulating in bats that may occasionally spill over into humans.</ORIGINAL_TEXT>
<TOKEN id="token-133-0" pos="word" morph="none" start_char="10930" end_char="10934">These</TOKEN>
<TOKEN id="token-133-1" pos="word" morph="none" start_char="10936" end_char="10942">viruses</TOKEN>
<TOKEN id="token-133-2" pos="word" morph="none" start_char="10944" end_char="10947">were</TOKEN>
<TOKEN id="token-133-3" pos="word" morph="none" start_char="10949" end_char="10953">never</TOKEN>
<TOKEN id="token-133-4" pos="word" morph="none" start_char="10955" end_char="10964">eliminated</TOKEN>
<TOKEN id="token-133-5" pos="punct" morph="none" start_char="10965" end_char="10965">,</TOKEN>
<TOKEN id="token-133-6" pos="word" morph="none" start_char="10967" end_char="10969">but</TOKEN>
<TOKEN id="token-133-7" pos="word" morph="none" start_char="10971" end_char="10975">since</TOKEN>
<TOKEN id="token-133-8" pos="word" morph="none" start_char="10977" end_char="10977">R</TOKEN>
<TOKEN id="token-133-9" pos="word" morph="none" start_char="10979" end_char="10979">1</TOKEN>
<TOKEN id="token-133-10" pos="punct" morph="none" start_char="10980" end_char="10980">,</TOKEN>
<TOKEN id="token-133-11" pos="word" morph="none" start_char="10982" end_char="10984">the</TOKEN>
<TOKEN id="token-133-12" pos="word" morph="none" start_char="10986" end_char="10995">occasional</TOKEN>
<TOKEN id="token-133-13" pos="word" morph="none" start_char="10997" end_char="11001">local</TOKEN>
<TOKEN id="token-133-14" pos="word" morph="none" start_char="11003" end_char="11010">outbreak</TOKEN>
<TOKEN id="token-133-15" pos="word" morph="none" start_char="11012" end_char="11015">will</TOKEN>
<TOKEN id="token-133-16" pos="word" morph="none" start_char="11017" end_char="11021">occur</TOKEN>
<TOKEN id="token-133-17" pos="punct" morph="none" start_char="11022" end_char="11022">,</TOKEN>
<TOKEN id="token-133-18" pos="word" morph="none" start_char="11024" end_char="11026">and</TOKEN>
<TOKEN id="token-133-19" pos="word" morph="none" start_char="11028" end_char="11031">then</TOKEN>
<TOKEN id="token-133-20" pos="word" morph="none" start_char="11033" end_char="11035">die</TOKEN>
<TOKEN id="token-133-21" pos="word" morph="none" start_char="11037" end_char="11039">out</TOKEN>
<TOKEN id="token-133-22" pos="punct" morph="none" start_char="11040" end_char="11041">.-</TOKEN>
<TOKEN id="token-133-23" pos="word" morph="none" start_char="11043" end_char="11047">There</TOKEN>
<TOKEN id="token-133-24" pos="word" morph="none" start_char="11049" end_char="11051">are</TOKEN>
<TOKEN id="token-133-25" pos="word" morph="none" start_char="11053" end_char="11057">other</TOKEN>
<TOKEN id="token-133-26" pos="word" morph="none" start_char="11059" end_char="11071">coronaviruses</TOKEN>
<TOKEN id="token-133-27" pos="word" morph="none" start_char="11073" end_char="11083">circulating</TOKEN>
<TOKEN id="token-133-28" pos="word" morph="none" start_char="11085" end_char="11086">in</TOKEN>
<TOKEN id="token-133-29" pos="word" morph="none" start_char="11088" end_char="11091">bats</TOKEN>
<TOKEN id="token-133-30" pos="word" morph="none" start_char="11093" end_char="11096">that</TOKEN>
<TOKEN id="token-133-31" pos="word" morph="none" start_char="11098" end_char="11100">may</TOKEN>
<TOKEN id="token-133-32" pos="word" morph="none" start_char="11102" end_char="11113">occasionally</TOKEN>
<TOKEN id="token-133-33" pos="word" morph="none" start_char="11115" end_char="11119">spill</TOKEN>
<TOKEN id="token-133-34" pos="word" morph="none" start_char="11121" end_char="11124">over</TOKEN>
<TOKEN id="token-133-35" pos="word" morph="none" start_char="11126" end_char="11129">into</TOKEN>
<TOKEN id="token-133-36" pos="word" morph="none" start_char="11131" end_char="11136">humans</TOKEN>
<TOKEN id="token-133-37" pos="punct" morph="none" start_char="11137" end_char="11137">.</TOKEN>
</SEG>
<SEG id="segment-134" start_char="11139" end_char="11674">
<ORIGINAL_TEXT>But provided they aren't spread by human-to-human contact, these will show up as sporadic positive samples for coronavirus, without going on to create a pandemic, an epidemic, or even a local outbreak.It is pretty clear that the pandemic form of the coronavirus spread out from Wuhan (although previous coronavirus samples suggest it may have been carried to Wuhan from bats in a warmer province of China, further south).The very first COVID-19 case detected in Australia was a man who flew from Wuhan to visit his parents in Melbourne.</ORIGINAL_TEXT>
<TOKEN id="token-134-0" pos="word" morph="none" start_char="11139" end_char="11141">But</TOKEN>
<TOKEN id="token-134-1" pos="word" morph="none" start_char="11143" end_char="11150">provided</TOKEN>
<TOKEN id="token-134-2" pos="word" morph="none" start_char="11152" end_char="11155">they</TOKEN>
<TOKEN id="token-134-3" pos="word" morph="none" start_char="11157" end_char="11162">aren't</TOKEN>
<TOKEN id="token-134-4" pos="word" morph="none" start_char="11164" end_char="11169">spread</TOKEN>
<TOKEN id="token-134-5" pos="word" morph="none" start_char="11171" end_char="11172">by</TOKEN>
<TOKEN id="token-134-6" pos="unknown" morph="none" start_char="11174" end_char="11187">human-to-human</TOKEN>
<TOKEN id="token-134-7" pos="word" morph="none" start_char="11189" end_char="11195">contact</TOKEN>
<TOKEN id="token-134-8" pos="punct" morph="none" start_char="11196" end_char="11196">,</TOKEN>
<TOKEN id="token-134-9" pos="word" morph="none" start_char="11198" end_char="11202">these</TOKEN>
<TOKEN id="token-134-10" pos="word" morph="none" start_char="11204" end_char="11207">will</TOKEN>
<TOKEN id="token-134-11" pos="word" morph="none" start_char="11209" end_char="11212">show</TOKEN>
<TOKEN id="token-134-12" pos="word" morph="none" start_char="11214" end_char="11215">up</TOKEN>
<TOKEN id="token-134-13" pos="word" morph="none" start_char="11217" end_char="11218">as</TOKEN>
<TOKEN id="token-134-14" pos="word" morph="none" start_char="11220" end_char="11227">sporadic</TOKEN>
<TOKEN id="token-134-15" pos="word" morph="none" start_char="11229" end_char="11236">positive</TOKEN>
<TOKEN id="token-134-16" pos="word" morph="none" start_char="11238" end_char="11244">samples</TOKEN>
<TOKEN id="token-134-17" pos="word" morph="none" start_char="11246" end_char="11248">for</TOKEN>
<TOKEN id="token-134-18" pos="word" morph="none" start_char="11250" end_char="11260">coronavirus</TOKEN>
<TOKEN id="token-134-19" pos="punct" morph="none" start_char="11261" end_char="11261">,</TOKEN>
<TOKEN id="token-134-20" pos="word" morph="none" start_char="11263" end_char="11269">without</TOKEN>
<TOKEN id="token-134-21" pos="word" morph="none" start_char="11271" end_char="11275">going</TOKEN>
<TOKEN id="token-134-22" pos="word" morph="none" start_char="11277" end_char="11278">on</TOKEN>
<TOKEN id="token-134-23" pos="word" morph="none" start_char="11280" end_char="11281">to</TOKEN>
<TOKEN id="token-134-24" pos="word" morph="none" start_char="11283" end_char="11288">create</TOKEN>
<TOKEN id="token-134-25" pos="word" morph="none" start_char="11290" end_char="11290">a</TOKEN>
<TOKEN id="token-134-26" pos="word" morph="none" start_char="11292" end_char="11299">pandemic</TOKEN>
<TOKEN id="token-134-27" pos="punct" morph="none" start_char="11300" end_char="11300">,</TOKEN>
<TOKEN id="token-134-28" pos="word" morph="none" start_char="11302" end_char="11303">an</TOKEN>
<TOKEN id="token-134-29" pos="word" morph="none" start_char="11305" end_char="11312">epidemic</TOKEN>
<TOKEN id="token-134-30" pos="punct" morph="none" start_char="11313" end_char="11313">,</TOKEN>
<TOKEN id="token-134-31" pos="word" morph="none" start_char="11315" end_char="11316">or</TOKEN>
<TOKEN id="token-134-32" pos="word" morph="none" start_char="11318" end_char="11321">even</TOKEN>
<TOKEN id="token-134-33" pos="word" morph="none" start_char="11323" end_char="11323">a</TOKEN>
<TOKEN id="token-134-34" pos="word" morph="none" start_char="11325" end_char="11329">local</TOKEN>
<TOKEN id="token-134-35" pos="unknown" morph="none" start_char="11331" end_char="11341">outbreak.It</TOKEN>
<TOKEN id="token-134-36" pos="word" morph="none" start_char="11343" end_char="11344">is</TOKEN>
<TOKEN id="token-134-37" pos="word" morph="none" start_char="11346" end_char="11351">pretty</TOKEN>
<TOKEN id="token-134-38" pos="word" morph="none" start_char="11353" end_char="11357">clear</TOKEN>
<TOKEN id="token-134-39" pos="word" morph="none" start_char="11359" end_char="11362">that</TOKEN>
<TOKEN id="token-134-40" pos="word" morph="none" start_char="11364" end_char="11366">the</TOKEN>
<TOKEN id="token-134-41" pos="word" morph="none" start_char="11368" end_char="11375">pandemic</TOKEN>
<TOKEN id="token-134-42" pos="word" morph="none" start_char="11377" end_char="11380">form</TOKEN>
<TOKEN id="token-134-43" pos="word" morph="none" start_char="11382" end_char="11383">of</TOKEN>
<TOKEN id="token-134-44" pos="word" morph="none" start_char="11385" end_char="11387">the</TOKEN>
<TOKEN id="token-134-45" pos="word" morph="none" start_char="11389" end_char="11399">coronavirus</TOKEN>
<TOKEN id="token-134-46" pos="word" morph="none" start_char="11401" end_char="11406">spread</TOKEN>
<TOKEN id="token-134-47" pos="word" morph="none" start_char="11408" end_char="11410">out</TOKEN>
<TOKEN id="token-134-48" pos="word" morph="none" start_char="11412" end_char="11415">from</TOKEN>
<TOKEN id="token-134-49" pos="word" morph="none" start_char="11417" end_char="11421">Wuhan</TOKEN>
<TOKEN id="token-134-50" pos="punct" morph="none" start_char="11423" end_char="11423">(</TOKEN>
<TOKEN id="token-134-51" pos="word" morph="none" start_char="11424" end_char="11431">although</TOKEN>
<TOKEN id="token-134-52" pos="word" morph="none" start_char="11433" end_char="11440">previous</TOKEN>
<TOKEN id="token-134-53" pos="word" morph="none" start_char="11442" end_char="11452">coronavirus</TOKEN>
<TOKEN id="token-134-54" pos="word" morph="none" start_char="11454" end_char="11460">samples</TOKEN>
<TOKEN id="token-134-55" pos="word" morph="none" start_char="11462" end_char="11468">suggest</TOKEN>
<TOKEN id="token-134-56" pos="word" morph="none" start_char="11470" end_char="11471">it</TOKEN>
<TOKEN id="token-134-57" pos="word" morph="none" start_char="11473" end_char="11475">may</TOKEN>
<TOKEN id="token-134-58" pos="word" morph="none" start_char="11477" end_char="11480">have</TOKEN>
<TOKEN id="token-134-59" pos="word" morph="none" start_char="11482" end_char="11485">been</TOKEN>
<TOKEN id="token-134-60" pos="word" morph="none" start_char="11487" end_char="11493">carried</TOKEN>
<TOKEN id="token-134-61" pos="word" morph="none" start_char="11495" end_char="11496">to</TOKEN>
<TOKEN id="token-134-62" pos="word" morph="none" start_char="11498" end_char="11502">Wuhan</TOKEN>
<TOKEN id="token-134-63" pos="word" morph="none" start_char="11504" end_char="11507">from</TOKEN>
<TOKEN id="token-134-64" pos="word" morph="none" start_char="11509" end_char="11512">bats</TOKEN>
<TOKEN id="token-134-65" pos="word" morph="none" start_char="11514" end_char="11515">in</TOKEN>
<TOKEN id="token-134-66" pos="word" morph="none" start_char="11517" end_char="11517">a</TOKEN>
<TOKEN id="token-134-67" pos="word" morph="none" start_char="11519" end_char="11524">warmer</TOKEN>
<TOKEN id="token-134-68" pos="word" morph="none" start_char="11526" end_char="11533">province</TOKEN>
<TOKEN id="token-134-69" pos="word" morph="none" start_char="11535" end_char="11536">of</TOKEN>
<TOKEN id="token-134-70" pos="word" morph="none" start_char="11538" end_char="11542">China</TOKEN>
<TOKEN id="token-134-71" pos="punct" morph="none" start_char="11543" end_char="11543">,</TOKEN>
<TOKEN id="token-134-72" pos="word" morph="none" start_char="11545" end_char="11551">further</TOKEN>
<TOKEN id="token-134-73" pos="unknown" morph="none" start_char="11553" end_char="11562">south).The</TOKEN>
<TOKEN id="token-134-74" pos="word" morph="none" start_char="11564" end_char="11567">very</TOKEN>
<TOKEN id="token-134-75" pos="word" morph="none" start_char="11569" end_char="11573">first</TOKEN>
<TOKEN id="token-134-76" pos="unknown" morph="none" start_char="11575" end_char="11582">COVID-19</TOKEN>
<TOKEN id="token-134-77" pos="word" morph="none" start_char="11584" end_char="11587">case</TOKEN>
<TOKEN id="token-134-78" pos="word" morph="none" start_char="11589" end_char="11596">detected</TOKEN>
<TOKEN id="token-134-79" pos="word" morph="none" start_char="11598" end_char="11599">in</TOKEN>
<TOKEN id="token-134-80" pos="word" morph="none" start_char="11601" end_char="11609">Australia</TOKEN>
<TOKEN id="token-134-81" pos="word" morph="none" start_char="11611" end_char="11613">was</TOKEN>
<TOKEN id="token-134-82" pos="word" morph="none" start_char="11615" end_char="11615">a</TOKEN>
<TOKEN id="token-134-83" pos="word" morph="none" start_char="11617" end_char="11619">man</TOKEN>
<TOKEN id="token-134-84" pos="word" morph="none" start_char="11621" end_char="11623">who</TOKEN>
<TOKEN id="token-134-85" pos="word" morph="none" start_char="11625" end_char="11628">flew</TOKEN>
<TOKEN id="token-134-86" pos="word" morph="none" start_char="11630" end_char="11633">from</TOKEN>
<TOKEN id="token-134-87" pos="word" morph="none" start_char="11635" end_char="11639">Wuhan</TOKEN>
<TOKEN id="token-134-88" pos="word" morph="none" start_char="11641" end_char="11642">to</TOKEN>
<TOKEN id="token-134-89" pos="word" morph="none" start_char="11644" end_char="11648">visit</TOKEN>
<TOKEN id="token-134-90" pos="word" morph="none" start_char="11650" end_char="11652">his</TOKEN>
<TOKEN id="token-134-91" pos="word" morph="none" start_char="11654" end_char="11660">parents</TOKEN>
<TOKEN id="token-134-92" pos="word" morph="none" start_char="11662" end_char="11663">in</TOKEN>
<TOKEN id="token-134-93" pos="word" morph="none" start_char="11665" end_char="11673">Melbourne</TOKEN>
<TOKEN id="token-134-94" pos="punct" morph="none" start_char="11674" end_char="11674">.</TOKEN>
</SEG>
<SEG id="segment-135" start_char="11676" end_char="11844">
<ORIGINAL_TEXT>- Fortunately, he knew of the pandemic in Wuhan (he had just come from there), self-isolated, wore a mask, and was hospitalized, all without spreading it to anyone else.</ORIGINAL_TEXT>
<TOKEN id="token-135-0" pos="punct" morph="none" start_char="11676" end_char="11676">-</TOKEN>
<TOKEN id="token-135-1" pos="word" morph="none" start_char="11678" end_char="11688">Fortunately</TOKEN>
<TOKEN id="token-135-2" pos="punct" morph="none" start_char="11689" end_char="11689">,</TOKEN>
<TOKEN id="token-135-3" pos="word" morph="none" start_char="11691" end_char="11692">he</TOKEN>
<TOKEN id="token-135-4" pos="word" morph="none" start_char="11694" end_char="11697">knew</TOKEN>
<TOKEN id="token-135-5" pos="word" morph="none" start_char="11699" end_char="11700">of</TOKEN>
<TOKEN id="token-135-6" pos="word" morph="none" start_char="11702" end_char="11704">the</TOKEN>
<TOKEN id="token-135-7" pos="word" morph="none" start_char="11706" end_char="11713">pandemic</TOKEN>
<TOKEN id="token-135-8" pos="word" morph="none" start_char="11715" end_char="11716">in</TOKEN>
<TOKEN id="token-135-9" pos="word" morph="none" start_char="11718" end_char="11722">Wuhan</TOKEN>
<TOKEN id="token-135-10" pos="punct" morph="none" start_char="11724" end_char="11724">(</TOKEN>
<TOKEN id="token-135-11" pos="word" morph="none" start_char="11725" end_char="11726">he</TOKEN>
<TOKEN id="token-135-12" pos="word" morph="none" start_char="11728" end_char="11730">had</TOKEN>
<TOKEN id="token-135-13" pos="word" morph="none" start_char="11732" end_char="11735">just</TOKEN>
<TOKEN id="token-135-14" pos="word" morph="none" start_char="11737" end_char="11740">come</TOKEN>
<TOKEN id="token-135-15" pos="word" morph="none" start_char="11742" end_char="11745">from</TOKEN>
<TOKEN id="token-135-16" pos="word" morph="none" start_char="11747" end_char="11751">there</TOKEN>
<TOKEN id="token-135-17" pos="punct" morph="none" start_char="11752" end_char="11753">),</TOKEN>
<TOKEN id="token-135-18" pos="unknown" morph="none" start_char="11755" end_char="11767">self-isolated</TOKEN>
<TOKEN id="token-135-19" pos="punct" morph="none" start_char="11768" end_char="11768">,</TOKEN>
<TOKEN id="token-135-20" pos="word" morph="none" start_char="11770" end_char="11773">wore</TOKEN>
<TOKEN id="token-135-21" pos="word" morph="none" start_char="11775" end_char="11775">a</TOKEN>
<TOKEN id="token-135-22" pos="word" morph="none" start_char="11777" end_char="11780">mask</TOKEN>
<TOKEN id="token-135-23" pos="punct" morph="none" start_char="11781" end_char="11781">,</TOKEN>
<TOKEN id="token-135-24" pos="word" morph="none" start_char="11783" end_char="11785">and</TOKEN>
<TOKEN id="token-135-25" pos="word" morph="none" start_char="11787" end_char="11789">was</TOKEN>
<TOKEN id="token-135-26" pos="word" morph="none" start_char="11791" end_char="11802">hospitalized</TOKEN>
<TOKEN id="token-135-27" pos="punct" morph="none" start_char="11803" end_char="11803">,</TOKEN>
<TOKEN id="token-135-28" pos="word" morph="none" start_char="11805" end_char="11807">all</TOKEN>
<TOKEN id="token-135-29" pos="word" morph="none" start_char="11809" end_char="11815">without</TOKEN>
<TOKEN id="token-135-30" pos="word" morph="none" start_char="11817" end_char="11825">spreading</TOKEN>
<TOKEN id="token-135-31" pos="word" morph="none" start_char="11827" end_char="11828">it</TOKEN>
<TOKEN id="token-135-32" pos="word" morph="none" start_char="11830" end_char="11831">to</TOKEN>
<TOKEN id="token-135-33" pos="word" morph="none" start_char="11833" end_char="11838">anyone</TOKEN>
<TOKEN id="token-135-34" pos="word" morph="none" start_char="11840" end_char="11843">else</TOKEN>
<TOKEN id="token-135-35" pos="punct" morph="none" start_char="11844" end_char="11844">.</TOKEN>
</SEG>
<SEG id="segment-136" start_char="11846" end_char="11961">
<ORIGINAL_TEXT>The very first COVID-19 case detected in Germany was a woman who flew from Wuhan to run a training course in Munich.</ORIGINAL_TEXT>
<TOKEN id="token-136-0" pos="word" morph="none" start_char="11846" end_char="11848">The</TOKEN>
<TOKEN id="token-136-1" pos="word" morph="none" start_char="11850" end_char="11853">very</TOKEN>
<TOKEN id="token-136-2" pos="word" morph="none" start_char="11855" end_char="11859">first</TOKEN>
<TOKEN id="token-136-3" pos="unknown" morph="none" start_char="11861" end_char="11868">COVID-19</TOKEN>
<TOKEN id="token-136-4" pos="word" morph="none" start_char="11870" end_char="11873">case</TOKEN>
<TOKEN id="token-136-5" pos="word" morph="none" start_char="11875" end_char="11882">detected</TOKEN>
<TOKEN id="token-136-6" pos="word" morph="none" start_char="11884" end_char="11885">in</TOKEN>
<TOKEN id="token-136-7" pos="word" morph="none" start_char="11887" end_char="11893">Germany</TOKEN>
<TOKEN id="token-136-8" pos="word" morph="none" start_char="11895" end_char="11897">was</TOKEN>
<TOKEN id="token-136-9" pos="word" morph="none" start_char="11899" end_char="11899">a</TOKEN>
<TOKEN id="token-136-10" pos="word" morph="none" start_char="11901" end_char="11905">woman</TOKEN>
<TOKEN id="token-136-11" pos="word" morph="none" start_char="11907" end_char="11909">who</TOKEN>
<TOKEN id="token-136-12" pos="word" morph="none" start_char="11911" end_char="11914">flew</TOKEN>
<TOKEN id="token-136-13" pos="word" morph="none" start_char="11916" end_char="11919">from</TOKEN>
<TOKEN id="token-136-14" pos="word" morph="none" start_char="11921" end_char="11925">Wuhan</TOKEN>
<TOKEN id="token-136-15" pos="word" morph="none" start_char="11927" end_char="11928">to</TOKEN>
<TOKEN id="token-136-16" pos="word" morph="none" start_char="11930" end_char="11932">run</TOKEN>
<TOKEN id="token-136-17" pos="word" morph="none" start_char="11934" end_char="11934">a</TOKEN>
<TOKEN id="token-136-18" pos="word" morph="none" start_char="11936" end_char="11943">training</TOKEN>
<TOKEN id="token-136-19" pos="word" morph="none" start_char="11945" end_char="11950">course</TOKEN>
<TOKEN id="token-136-20" pos="word" morph="none" start_char="11952" end_char="11953">in</TOKEN>
<TOKEN id="token-136-21" pos="word" morph="none" start_char="11955" end_char="11960">Munich</TOKEN>
<TOKEN id="token-136-22" pos="punct" morph="none" start_char="11961" end_char="11961">.</TOKEN>
</SEG>
<SEG id="segment-137" start_char="11963" end_char="12170">
<ORIGINAL_TEXT>- She only developed symptoms on the return flight to China, but several people on the course were infected.Listen (43 minutes): https://www.abc.net.au/radionational/programs/rn-presents/patient-zero/12523222</ORIGINAL_TEXT>
<TOKEN id="token-137-0" pos="punct" morph="none" start_char="11963" end_char="11963">-</TOKEN>
<TOKEN id="token-137-1" pos="word" morph="none" start_char="11965" end_char="11967">She</TOKEN>
<TOKEN id="token-137-2" pos="word" morph="none" start_char="11969" end_char="11972">only</TOKEN>
<TOKEN id="token-137-3" pos="word" morph="none" start_char="11974" end_char="11982">developed</TOKEN>
<TOKEN id="token-137-4" pos="word" morph="none" start_char="11984" end_char="11991">symptoms</TOKEN>
<TOKEN id="token-137-5" pos="word" morph="none" start_char="11993" end_char="11994">on</TOKEN>
<TOKEN id="token-137-6" pos="word" morph="none" start_char="11996" end_char="11998">the</TOKEN>
<TOKEN id="token-137-7" pos="word" morph="none" start_char="12000" end_char="12005">return</TOKEN>
<TOKEN id="token-137-8" pos="word" morph="none" start_char="12007" end_char="12012">flight</TOKEN>
<TOKEN id="token-137-9" pos="word" morph="none" start_char="12014" end_char="12015">to</TOKEN>
<TOKEN id="token-137-10" pos="word" morph="none" start_char="12017" end_char="12021">China</TOKEN>
<TOKEN id="token-137-11" pos="punct" morph="none" start_char="12022" end_char="12022">,</TOKEN>
<TOKEN id="token-137-12" pos="word" morph="none" start_char="12024" end_char="12026">but</TOKEN>
<TOKEN id="token-137-13" pos="word" morph="none" start_char="12028" end_char="12034">several</TOKEN>
<TOKEN id="token-137-14" pos="word" morph="none" start_char="12036" end_char="12041">people</TOKEN>
<TOKEN id="token-137-15" pos="word" morph="none" start_char="12043" end_char="12044">on</TOKEN>
<TOKEN id="token-137-16" pos="word" morph="none" start_char="12046" end_char="12048">the</TOKEN>
<TOKEN id="token-137-17" pos="word" morph="none" start_char="12050" end_char="12055">course</TOKEN>
<TOKEN id="token-137-18" pos="word" morph="none" start_char="12057" end_char="12060">were</TOKEN>
<TOKEN id="token-137-19" pos="unknown" morph="none" start_char="12062" end_char="12076">infected.Listen</TOKEN>
<TOKEN id="token-137-20" pos="punct" morph="none" start_char="12078" end_char="12078">(</TOKEN>
<TOKEN id="token-137-21" pos="word" morph="none" start_char="12079" end_char="12080">43</TOKEN>
<TOKEN id="token-137-22" pos="word" morph="none" start_char="12082" end_char="12088">minutes</TOKEN>
<TOKEN id="token-137-23" pos="punct" morph="none" start_char="12089" end_char="12090">):</TOKEN>
<TOKEN id="token-137-24" pos="url" morph="none" start_char="12092" end_char="12170">https://www.abc.net.au/radionational/programs/rn-presents/patient-zero/12523222</TOKEN>
</SEG>
<SEG id="segment-138" start_char="12173" end_char="12442">
<ORIGINAL_TEXT>This is all speculation, if the virus is embeded in the excrement then the pooh package could very well offer a protective barrier to soaps and detergents, virus particles on the outside of a pooh may well be destroyed but those embedded inside could have been protected</ORIGINAL_TEXT>
<TOKEN id="token-138-0" pos="word" morph="none" start_char="12173" end_char="12176">This</TOKEN>
<TOKEN id="token-138-1" pos="word" morph="none" start_char="12178" end_char="12179">is</TOKEN>
<TOKEN id="token-138-2" pos="word" morph="none" start_char="12181" end_char="12183">all</TOKEN>
<TOKEN id="token-138-3" pos="word" morph="none" start_char="12185" end_char="12195">speculation</TOKEN>
<TOKEN id="token-138-4" pos="punct" morph="none" start_char="12196" end_char="12196">,</TOKEN>
<TOKEN id="token-138-5" pos="word" morph="none" start_char="12198" end_char="12199">if</TOKEN>
<TOKEN id="token-138-6" pos="word" morph="none" start_char="12201" end_char="12203">the</TOKEN>
<TOKEN id="token-138-7" pos="word" morph="none" start_char="12205" end_char="12209">virus</TOKEN>
<TOKEN id="token-138-8" pos="word" morph="none" start_char="12211" end_char="12212">is</TOKEN>
<TOKEN id="token-138-9" pos="word" morph="none" start_char="12214" end_char="12220">embeded</TOKEN>
<TOKEN id="token-138-10" pos="word" morph="none" start_char="12222" end_char="12223">in</TOKEN>
<TOKEN id="token-138-11" pos="word" morph="none" start_char="12225" end_char="12227">the</TOKEN>
<TOKEN id="token-138-12" pos="word" morph="none" start_char="12229" end_char="12237">excrement</TOKEN>
<TOKEN id="token-138-13" pos="word" morph="none" start_char="12239" end_char="12242">then</TOKEN>
<TOKEN id="token-138-14" pos="word" morph="none" start_char="12244" end_char="12246">the</TOKEN>
<TOKEN id="token-138-15" pos="word" morph="none" start_char="12248" end_char="12251">pooh</TOKEN>
<TOKEN id="token-138-16" pos="word" morph="none" start_char="12253" end_char="12259">package</TOKEN>
<TOKEN id="token-138-17" pos="word" morph="none" start_char="12261" end_char="12265">could</TOKEN>
<TOKEN id="token-138-18" pos="word" morph="none" start_char="12267" end_char="12270">very</TOKEN>
<TOKEN id="token-138-19" pos="word" morph="none" start_char="12272" end_char="12275">well</TOKEN>
<TOKEN id="token-138-20" pos="word" morph="none" start_char="12277" end_char="12281">offer</TOKEN>
<TOKEN id="token-138-21" pos="word" morph="none" start_char="12283" end_char="12283">a</TOKEN>
<TOKEN id="token-138-22" pos="word" morph="none" start_char="12285" end_char="12294">protective</TOKEN>
<TOKEN id="token-138-23" pos="word" morph="none" start_char="12296" end_char="12302">barrier</TOKEN>
<TOKEN id="token-138-24" pos="word" morph="none" start_char="12304" end_char="12305">to</TOKEN>
<TOKEN id="token-138-25" pos="word" morph="none" start_char="12307" end_char="12311">soaps</TOKEN>
<TOKEN id="token-138-26" pos="word" morph="none" start_char="12313" end_char="12315">and</TOKEN>
<TOKEN id="token-138-27" pos="word" morph="none" start_char="12317" end_char="12326">detergents</TOKEN>
<TOKEN id="token-138-28" pos="punct" morph="none" start_char="12327" end_char="12327">,</TOKEN>
<TOKEN id="token-138-29" pos="word" morph="none" start_char="12329" end_char="12333">virus</TOKEN>
<TOKEN id="token-138-30" pos="word" morph="none" start_char="12335" end_char="12343">particles</TOKEN>
<TOKEN id="token-138-31" pos="word" morph="none" start_char="12345" end_char="12346">on</TOKEN>
<TOKEN id="token-138-32" pos="word" morph="none" start_char="12348" end_char="12350">the</TOKEN>
<TOKEN id="token-138-33" pos="word" morph="none" start_char="12352" end_char="12358">outside</TOKEN>
<TOKEN id="token-138-34" pos="word" morph="none" start_char="12360" end_char="12361">of</TOKEN>
<TOKEN id="token-138-35" pos="word" morph="none" start_char="12363" end_char="12363">a</TOKEN>
<TOKEN id="token-138-36" pos="word" morph="none" start_char="12365" end_char="12368">pooh</TOKEN>
<TOKEN id="token-138-37" pos="word" morph="none" start_char="12370" end_char="12372">may</TOKEN>
<TOKEN id="token-138-38" pos="word" morph="none" start_char="12374" end_char="12377">well</TOKEN>
<TOKEN id="token-138-39" pos="word" morph="none" start_char="12379" end_char="12380">be</TOKEN>
<TOKEN id="token-138-40" pos="word" morph="none" start_char="12382" end_char="12390">destroyed</TOKEN>
<TOKEN id="token-138-41" pos="word" morph="none" start_char="12392" end_char="12394">but</TOKEN>
<TOKEN id="token-138-42" pos="word" morph="none" start_char="12396" end_char="12400">those</TOKEN>
<TOKEN id="token-138-43" pos="word" morph="none" start_char="12402" end_char="12409">embedded</TOKEN>
<TOKEN id="token-138-44" pos="word" morph="none" start_char="12411" end_char="12416">inside</TOKEN>
<TOKEN id="token-138-45" pos="word" morph="none" start_char="12418" end_char="12422">could</TOKEN>
<TOKEN id="token-138-46" pos="word" morph="none" start_char="12424" end_char="12427">have</TOKEN>
<TOKEN id="token-138-47" pos="word" morph="none" start_char="12429" end_char="12432">been</TOKEN>
<TOKEN id="token-138-48" pos="word" morph="none" start_char="12434" end_char="12442">protected</TOKEN>
</SEG>
<SEG id="segment-139" start_char="12446" end_char="12486">
<ORIGINAL_TEXT>Quote from: Jolly2 on 07/02/2021 16:56:51</ORIGINAL_TEXT>
<TOKEN id="token-139-0" pos="word" morph="none" start_char="12446" end_char="12450">Quote</TOKEN>
<TOKEN id="token-139-1" pos="word" morph="none" start_char="12452" end_char="12455">from</TOKEN>
<TOKEN id="token-139-2" pos="punct" morph="none" start_char="12456" end_char="12456">:</TOKEN>
<TOKEN id="token-139-3" pos="word" morph="none" start_char="12458" end_char="12463">Jolly2</TOKEN>
<TOKEN id="token-139-4" pos="word" morph="none" start_char="12465" end_char="12466">on</TOKEN>
<TOKEN id="token-139-5" pos="unknown" morph="none" start_char="12468" end_char="12477">07/02/2021</TOKEN>
<TOKEN id="token-139-6" pos="unknown" morph="none" start_char="12479" end_char="12486">16:56:51</TOKEN>
</SEG>
<SEG id="segment-140" start_char="12489" end_char="12512">
<ORIGINAL_TEXT>This is all speculation,</ORIGINAL_TEXT>
<TOKEN id="token-140-0" pos="word" morph="none" start_char="12489" end_char="12492">This</TOKEN>
<TOKEN id="token-140-1" pos="word" morph="none" start_char="12494" end_char="12495">is</TOKEN>
<TOKEN id="token-140-2" pos="word" morph="none" start_char="12497" end_char="12499">all</TOKEN>
<TOKEN id="token-140-3" pos="word" morph="none" start_char="12501" end_char="12511">speculation</TOKEN>
<TOKEN id="token-140-4" pos="punct" morph="none" start_char="12512" end_char="12512">,</TOKEN>
</SEG>
<SEG id="segment-141" start_char="12516" end_char="12525">
<ORIGINAL_TEXT>A bit like</ORIGINAL_TEXT>
<TOKEN id="token-141-0" pos="word" morph="none" start_char="12516" end_char="12516">A</TOKEN>
<TOKEN id="token-141-1" pos="word" morph="none" start_char="12518" end_char="12520">bit</TOKEN>
<TOKEN id="token-141-2" pos="word" morph="none" start_char="12522" end_char="12525">like</TOKEN>
</SEG>
<SEG id="segment-142" start_char="12528" end_char="12575">
<ORIGINAL_TEXT>Quote from: Bored chemist on 06/02/2021 20:13:01</ORIGINAL_TEXT>
<TOKEN id="token-142-0" pos="word" morph="none" start_char="12528" end_char="12532">Quote</TOKEN>
<TOKEN id="token-142-1" pos="word" morph="none" start_char="12534" end_char="12537">from</TOKEN>
<TOKEN id="token-142-2" pos="punct" morph="none" start_char="12538" end_char="12538">:</TOKEN>
<TOKEN id="token-142-3" pos="word" morph="none" start_char="12540" end_char="12544">Bored</TOKEN>
<TOKEN id="token-142-4" pos="word" morph="none" start_char="12546" end_char="12552">chemist</TOKEN>
<TOKEN id="token-142-5" pos="word" morph="none" start_char="12554" end_char="12555">on</TOKEN>
<TOKEN id="token-142-6" pos="unknown" morph="none" start_char="12557" end_char="12566">06/02/2021</TOKEN>
<TOKEN id="token-142-7" pos="unknown" morph="none" start_char="12568" end_char="12575">20:13:01</TOKEN>
</SEG>
<SEG id="segment-143" start_char="12578" end_char="12924">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36covid was in Brazil in November 2019Quote from: Jolly2 on 05/02/2021 18:12:36France in November 2019Quote from: Jolly2 on 05/02/2021 18:12:36And spain as early as March 2019Quote from: Jolly2 on 05/02/2021 18:15:55We know Donald Trump claimed to have seen evidence that covid 19 had come from a laboratory.</ORIGINAL_TEXT>
<TOKEN id="token-143-0" pos="word" morph="none" start_char="12578" end_char="12582">Quote</TOKEN>
<TOKEN id="token-143-1" pos="word" morph="none" start_char="12584" end_char="12587">from</TOKEN>
<TOKEN id="token-143-2" pos="punct" morph="none" start_char="12588" end_char="12588">:</TOKEN>
<TOKEN id="token-143-3" pos="word" morph="none" start_char="12590" end_char="12595">Jolly2</TOKEN>
<TOKEN id="token-143-4" pos="word" morph="none" start_char="12597" end_char="12598">on</TOKEN>
<TOKEN id="token-143-5" pos="unknown" morph="none" start_char="12600" end_char="12609">05/02/2021</TOKEN>
<TOKEN id="token-143-6" pos="unknown" morph="none" start_char="12611" end_char="12623">18:12:36covid</TOKEN>
<TOKEN id="token-143-7" pos="word" morph="none" start_char="12625" end_char="12627">was</TOKEN>
<TOKEN id="token-143-8" pos="word" morph="none" start_char="12629" end_char="12630">in</TOKEN>
<TOKEN id="token-143-9" pos="word" morph="none" start_char="12632" end_char="12637">Brazil</TOKEN>
<TOKEN id="token-143-10" pos="word" morph="none" start_char="12639" end_char="12640">in</TOKEN>
<TOKEN id="token-143-11" pos="word" morph="none" start_char="12642" end_char="12649">November</TOKEN>
<TOKEN id="token-143-12" pos="word" morph="none" start_char="12651" end_char="12659">2019Quote</TOKEN>
<TOKEN id="token-143-13" pos="word" morph="none" start_char="12661" end_char="12664">from</TOKEN>
<TOKEN id="token-143-14" pos="punct" morph="none" start_char="12665" end_char="12665">:</TOKEN>
<TOKEN id="token-143-15" pos="word" morph="none" start_char="12667" end_char="12672">Jolly2</TOKEN>
<TOKEN id="token-143-16" pos="word" morph="none" start_char="12674" end_char="12675">on</TOKEN>
<TOKEN id="token-143-17" pos="unknown" morph="none" start_char="12677" end_char="12686">05/02/2021</TOKEN>
<TOKEN id="token-143-18" pos="unknown" morph="none" start_char="12688" end_char="12701">18:12:36France</TOKEN>
<TOKEN id="token-143-19" pos="word" morph="none" start_char="12703" end_char="12704">in</TOKEN>
<TOKEN id="token-143-20" pos="word" morph="none" start_char="12706" end_char="12713">November</TOKEN>
<TOKEN id="token-143-21" pos="word" morph="none" start_char="12715" end_char="12723">2019Quote</TOKEN>
<TOKEN id="token-143-22" pos="word" morph="none" start_char="12725" end_char="12728">from</TOKEN>
<TOKEN id="token-143-23" pos="punct" morph="none" start_char="12729" end_char="12729">:</TOKEN>
<TOKEN id="token-143-24" pos="word" morph="none" start_char="12731" end_char="12736">Jolly2</TOKEN>
<TOKEN id="token-143-25" pos="word" morph="none" start_char="12738" end_char="12739">on</TOKEN>
<TOKEN id="token-143-26" pos="unknown" morph="none" start_char="12741" end_char="12750">05/02/2021</TOKEN>
<TOKEN id="token-143-27" pos="unknown" morph="none" start_char="12752" end_char="12762">18:12:36And</TOKEN>
<TOKEN id="token-143-28" pos="word" morph="none" start_char="12764" end_char="12768">spain</TOKEN>
<TOKEN id="token-143-29" pos="word" morph="none" start_char="12770" end_char="12771">as</TOKEN>
<TOKEN id="token-143-30" pos="word" morph="none" start_char="12773" end_char="12777">early</TOKEN>
<TOKEN id="token-143-31" pos="word" morph="none" start_char="12779" end_char="12780">as</TOKEN>
<TOKEN id="token-143-32" pos="word" morph="none" start_char="12782" end_char="12786">March</TOKEN>
<TOKEN id="token-143-33" pos="word" morph="none" start_char="12788" end_char="12796">2019Quote</TOKEN>
<TOKEN id="token-143-34" pos="word" morph="none" start_char="12798" end_char="12801">from</TOKEN>
<TOKEN id="token-143-35" pos="punct" morph="none" start_char="12802" end_char="12802">:</TOKEN>
<TOKEN id="token-143-36" pos="word" morph="none" start_char="12804" end_char="12809">Jolly2</TOKEN>
<TOKEN id="token-143-37" pos="word" morph="none" start_char="12811" end_char="12812">on</TOKEN>
<TOKEN id="token-143-38" pos="unknown" morph="none" start_char="12814" end_char="12823">05/02/2021</TOKEN>
<TOKEN id="token-143-39" pos="unknown" morph="none" start_char="12825" end_char="12834">18:15:55We</TOKEN>
<TOKEN id="token-143-40" pos="word" morph="none" start_char="12836" end_char="12839">know</TOKEN>
<TOKEN id="token-143-41" pos="word" morph="none" start_char="12841" end_char="12846">Donald</TOKEN>
<TOKEN id="token-143-42" pos="word" morph="none" start_char="12848" end_char="12852">Trump</TOKEN>
<TOKEN id="token-143-43" pos="word" morph="none" start_char="12854" end_char="12860">claimed</TOKEN>
<TOKEN id="token-143-44" pos="word" morph="none" start_char="12862" end_char="12863">to</TOKEN>
<TOKEN id="token-143-45" pos="word" morph="none" start_char="12865" end_char="12868">have</TOKEN>
<TOKEN id="token-143-46" pos="word" morph="none" start_char="12870" end_char="12873">seen</TOKEN>
<TOKEN id="token-143-47" pos="word" morph="none" start_char="12875" end_char="12882">evidence</TOKEN>
<TOKEN id="token-143-48" pos="word" morph="none" start_char="12884" end_char="12887">that</TOKEN>
<TOKEN id="token-143-49" pos="word" morph="none" start_char="12889" end_char="12893">covid</TOKEN>
<TOKEN id="token-143-50" pos="word" morph="none" start_char="12895" end_char="12896">19</TOKEN>
<TOKEN id="token-143-51" pos="word" morph="none" start_char="12898" end_char="12900">had</TOKEN>
<TOKEN id="token-143-52" pos="word" morph="none" start_char="12902" end_char="12905">come</TOKEN>
<TOKEN id="token-143-53" pos="word" morph="none" start_char="12907" end_char="12910">from</TOKEN>
<TOKEN id="token-143-54" pos="word" morph="none" start_char="12912" end_char="12912">a</TOKEN>
<TOKEN id="token-143-55" pos="word" morph="none" start_char="12914" end_char="12923">laboratory</TOKEN>
<TOKEN id="token-143-56" pos="punct" morph="none" start_char="12924" end_char="12924">.</TOKEN>
</SEG>
<SEG id="segment-144" start_char="12926" end_char="12984">
<ORIGINAL_TEXT>Could it be that the evidence he saw was from fort Detrick?</ORIGINAL_TEXT>
<TOKEN id="token-144-0" pos="word" morph="none" start_char="12926" end_char="12930">Could</TOKEN>
<TOKEN id="token-144-1" pos="word" morph="none" start_char="12932" end_char="12933">it</TOKEN>
<TOKEN id="token-144-2" pos="word" morph="none" start_char="12935" end_char="12936">be</TOKEN>
<TOKEN id="token-144-3" pos="word" morph="none" start_char="12938" end_char="12941">that</TOKEN>
<TOKEN id="token-144-4" pos="word" morph="none" start_char="12943" end_char="12945">the</TOKEN>
<TOKEN id="token-144-5" pos="word" morph="none" start_char="12947" end_char="12954">evidence</TOKEN>
<TOKEN id="token-144-6" pos="word" morph="none" start_char="12956" end_char="12957">he</TOKEN>
<TOKEN id="token-144-7" pos="word" morph="none" start_char="12959" end_char="12961">saw</TOKEN>
<TOKEN id="token-144-8" pos="word" morph="none" start_char="12963" end_char="12965">was</TOKEN>
<TOKEN id="token-144-9" pos="word" morph="none" start_char="12967" end_char="12970">from</TOKEN>
<TOKEN id="token-144-10" pos="word" morph="none" start_char="12972" end_char="12975">fort</TOKEN>
<TOKEN id="token-144-11" pos="word" morph="none" start_char="12977" end_char="12983">Detrick</TOKEN>
<TOKEN id="token-144-12" pos="punct" morph="none" start_char="12984" end_char="12984">?</TOKEN>
</SEG>
<SEG id="segment-145" start_char="12987" end_char="13027">
<ORIGINAL_TEXT>Quote from: Jolly2 on 07/02/2021 16:56:51</ORIGINAL_TEXT>
<TOKEN id="token-145-0" pos="word" morph="none" start_char="12987" end_char="12991">Quote</TOKEN>
<TOKEN id="token-145-1" pos="word" morph="none" start_char="12993" end_char="12996">from</TOKEN>
<TOKEN id="token-145-2" pos="punct" morph="none" start_char="12997" end_char="12997">:</TOKEN>
<TOKEN id="token-145-3" pos="word" morph="none" start_char="12999" end_char="13004">Jolly2</TOKEN>
<TOKEN id="token-145-4" pos="word" morph="none" start_char="13006" end_char="13007">on</TOKEN>
<TOKEN id="token-145-5" pos="unknown" morph="none" start_char="13009" end_char="13018">07/02/2021</TOKEN>
<TOKEN id="token-145-6" pos="unknown" morph="none" start_char="13020" end_char="13027">16:56:51</TOKEN>
</SEG>
<SEG id="segment-146" start_char="13030" end_char="13143">
<ORIGINAL_TEXT>virus particles on the outside of a pooh may well be destroyed but those embedded inside could have been protected</ORIGINAL_TEXT>
<TOKEN id="token-146-0" pos="word" morph="none" start_char="13030" end_char="13034">virus</TOKEN>
<TOKEN id="token-146-1" pos="word" morph="none" start_char="13036" end_char="13044">particles</TOKEN>
<TOKEN id="token-146-2" pos="word" morph="none" start_char="13046" end_char="13047">on</TOKEN>
<TOKEN id="token-146-3" pos="word" morph="none" start_char="13049" end_char="13051">the</TOKEN>
<TOKEN id="token-146-4" pos="word" morph="none" start_char="13053" end_char="13059">outside</TOKEN>
<TOKEN id="token-146-5" pos="word" morph="none" start_char="13061" end_char="13062">of</TOKEN>
<TOKEN id="token-146-6" pos="word" morph="none" start_char="13064" end_char="13064">a</TOKEN>
<TOKEN id="token-146-7" pos="word" morph="none" start_char="13066" end_char="13069">pooh</TOKEN>
<TOKEN id="token-146-8" pos="word" morph="none" start_char="13071" end_char="13073">may</TOKEN>
<TOKEN id="token-146-9" pos="word" morph="none" start_char="13075" end_char="13078">well</TOKEN>
<TOKEN id="token-146-10" pos="word" morph="none" start_char="13080" end_char="13081">be</TOKEN>
<TOKEN id="token-146-11" pos="word" morph="none" start_char="13083" end_char="13091">destroyed</TOKEN>
<TOKEN id="token-146-12" pos="word" morph="none" start_char="13093" end_char="13095">but</TOKEN>
<TOKEN id="token-146-13" pos="word" morph="none" start_char="13097" end_char="13101">those</TOKEN>
<TOKEN id="token-146-14" pos="word" morph="none" start_char="13103" end_char="13110">embedded</TOKEN>
<TOKEN id="token-146-15" pos="word" morph="none" start_char="13112" end_char="13117">inside</TOKEN>
<TOKEN id="token-146-16" pos="word" morph="none" start_char="13119" end_char="13123">could</TOKEN>
<TOKEN id="token-146-17" pos="word" morph="none" start_char="13125" end_char="13128">have</TOKEN>
<TOKEN id="token-146-18" pos="word" morph="none" start_char="13130" end_char="13133">been</TOKEN>
<TOKEN id="token-146-19" pos="word" morph="none" start_char="13135" end_char="13143">protected</TOKEN>
</SEG>
<SEG id="segment-147" start_char="13147" end_char="13229">
<ORIGINAL_TEXT>Among the things they would have been protected from is getting into a sample vial.</ORIGINAL_TEXT>
<TOKEN id="token-147-0" pos="word" morph="none" start_char="13147" end_char="13151">Among</TOKEN>
<TOKEN id="token-147-1" pos="word" morph="none" start_char="13153" end_char="13155">the</TOKEN>
<TOKEN id="token-147-2" pos="word" morph="none" start_char="13157" end_char="13162">things</TOKEN>
<TOKEN id="token-147-3" pos="word" morph="none" start_char="13164" end_char="13167">they</TOKEN>
<TOKEN id="token-147-4" pos="word" morph="none" start_char="13169" end_char="13173">would</TOKEN>
<TOKEN id="token-147-5" pos="word" morph="none" start_char="13175" end_char="13178">have</TOKEN>
<TOKEN id="token-147-6" pos="word" morph="none" start_char="13180" end_char="13183">been</TOKEN>
<TOKEN id="token-147-7" pos="word" morph="none" start_char="13185" end_char="13193">protected</TOKEN>
<TOKEN id="token-147-8" pos="word" morph="none" start_char="13195" end_char="13198">from</TOKEN>
<TOKEN id="token-147-9" pos="word" morph="none" start_char="13200" end_char="13201">is</TOKEN>
<TOKEN id="token-147-10" pos="word" morph="none" start_char="13203" end_char="13209">getting</TOKEN>
<TOKEN id="token-147-11" pos="word" morph="none" start_char="13211" end_char="13214">into</TOKEN>
<TOKEN id="token-147-12" pos="word" morph="none" start_char="13216" end_char="13216">a</TOKEN>
<TOKEN id="token-147-13" pos="word" morph="none" start_char="13218" end_char="13223">sample</TOKEN>
<TOKEN id="token-147-14" pos="word" morph="none" start_char="13225" end_char="13228">vial</TOKEN>
<TOKEN id="token-147-15" pos="punct" morph="none" start_char="13229" end_char="13229">.</TOKEN>
</SEG>
<SEG id="segment-148" start_char="13231" end_char="13287">
<ORIGINAL_TEXT>So they would never have made it to a lab to be detected.</ORIGINAL_TEXT>
<TOKEN id="token-148-0" pos="word" morph="none" start_char="13231" end_char="13232">So</TOKEN>
<TOKEN id="token-148-1" pos="word" morph="none" start_char="13234" end_char="13237">they</TOKEN>
<TOKEN id="token-148-2" pos="word" morph="none" start_char="13239" end_char="13243">would</TOKEN>
<TOKEN id="token-148-3" pos="word" morph="none" start_char="13245" end_char="13249">never</TOKEN>
<TOKEN id="token-148-4" pos="word" morph="none" start_char="13251" end_char="13254">have</TOKEN>
<TOKEN id="token-148-5" pos="word" morph="none" start_char="13256" end_char="13259">made</TOKEN>
<TOKEN id="token-148-6" pos="word" morph="none" start_char="13261" end_char="13262">it</TOKEN>
<TOKEN id="token-148-7" pos="word" morph="none" start_char="13264" end_char="13265">to</TOKEN>
<TOKEN id="token-148-8" pos="word" morph="none" start_char="13267" end_char="13267">a</TOKEN>
<TOKEN id="token-148-9" pos="word" morph="none" start_char="13269" end_char="13271">lab</TOKEN>
<TOKEN id="token-148-10" pos="word" morph="none" start_char="13273" end_char="13274">to</TOKEN>
<TOKEN id="token-148-11" pos="word" morph="none" start_char="13276" end_char="13277">be</TOKEN>
<TOKEN id="token-148-12" pos="word" morph="none" start_char="13279" end_char="13286">detected</TOKEN>
<TOKEN id="token-148-13" pos="punct" morph="none" start_char="13287" end_char="13287">.</TOKEN>
</SEG>
<SEG id="segment-149" start_char="13291" end_char="13291">
<ORIGINAL_TEXT>Ο</ORIGINAL_TEXT>
<TOKEN id="token-149-0" pos="word" morph="none" start_char="13291" end_char="13291">Ο</TOKEN>
</SEG>
<SEG id="segment-150" start_char="13294" end_char="13341">
<ORIGINAL_TEXT>Quote from: Bored chemist on 07/02/2021 17:25:40</ORIGINAL_TEXT>
<TOKEN id="token-150-0" pos="word" morph="none" start_char="13294" end_char="13298">Quote</TOKEN>
<TOKEN id="token-150-1" pos="word" morph="none" start_char="13300" end_char="13303">from</TOKEN>
<TOKEN id="token-150-2" pos="punct" morph="none" start_char="13304" end_char="13304">:</TOKEN>
<TOKEN id="token-150-3" pos="word" morph="none" start_char="13306" end_char="13310">Bored</TOKEN>
<TOKEN id="token-150-4" pos="word" morph="none" start_char="13312" end_char="13318">chemist</TOKEN>
<TOKEN id="token-150-5" pos="word" morph="none" start_char="13320" end_char="13321">on</TOKEN>
<TOKEN id="token-150-6" pos="unknown" morph="none" start_char="13323" end_char="13332">07/02/2021</TOKEN>
<TOKEN id="token-150-7" pos="unknown" morph="none" start_char="13334" end_char="13341">17:25:40</TOKEN>
</SEG>
<SEG id="segment-151" start_char="13344" end_char="13384">
<ORIGINAL_TEXT>Quote from: Jolly2 on 07/02/2021 16:56:51</ORIGINAL_TEXT>
<TOKEN id="token-151-0" pos="word" morph="none" start_char="13344" end_char="13348">Quote</TOKEN>
<TOKEN id="token-151-1" pos="word" morph="none" start_char="13350" end_char="13353">from</TOKEN>
<TOKEN id="token-151-2" pos="punct" morph="none" start_char="13354" end_char="13354">:</TOKEN>
<TOKEN id="token-151-3" pos="word" morph="none" start_char="13356" end_char="13361">Jolly2</TOKEN>
<TOKEN id="token-151-4" pos="word" morph="none" start_char="13363" end_char="13364">on</TOKEN>
<TOKEN id="token-151-5" pos="unknown" morph="none" start_char="13366" end_char="13375">07/02/2021</TOKEN>
<TOKEN id="token-151-6" pos="unknown" morph="none" start_char="13377" end_char="13384">16:56:51</TOKEN>
</SEG>
<SEG id="segment-152" start_char="13387" end_char="13410">
<ORIGINAL_TEXT>This is all speculation,</ORIGINAL_TEXT>
<TOKEN id="token-152-0" pos="word" morph="none" start_char="13387" end_char="13390">This</TOKEN>
<TOKEN id="token-152-1" pos="word" morph="none" start_char="13392" end_char="13393">is</TOKEN>
<TOKEN id="token-152-2" pos="word" morph="none" start_char="13395" end_char="13397">all</TOKEN>
<TOKEN id="token-152-3" pos="word" morph="none" start_char="13399" end_char="13409">speculation</TOKEN>
<TOKEN id="token-152-4" pos="punct" morph="none" start_char="13410" end_char="13410">,</TOKEN>
</SEG>
<SEG id="segment-153" start_char="13413" end_char="13470">
<ORIGINAL_TEXT>A bit likeQuote from: Bored chemist on 06/02/2021 20:13:01</ORIGINAL_TEXT>
<TOKEN id="token-153-0" pos="word" morph="none" start_char="13413" end_char="13413">A</TOKEN>
<TOKEN id="token-153-1" pos="word" morph="none" start_char="13415" end_char="13417">bit</TOKEN>
<TOKEN id="token-153-2" pos="word" morph="none" start_char="13419" end_char="13427">likeQuote</TOKEN>
<TOKEN id="token-153-3" pos="word" morph="none" start_char="13429" end_char="13432">from</TOKEN>
<TOKEN id="token-153-4" pos="punct" morph="none" start_char="13433" end_char="13433">:</TOKEN>
<TOKEN id="token-153-5" pos="word" morph="none" start_char="13435" end_char="13439">Bored</TOKEN>
<TOKEN id="token-153-6" pos="word" morph="none" start_char="13441" end_char="13447">chemist</TOKEN>
<TOKEN id="token-153-7" pos="word" morph="none" start_char="13449" end_char="13450">on</TOKEN>
<TOKEN id="token-153-8" pos="unknown" morph="none" start_char="13452" end_char="13461">06/02/2021</TOKEN>
<TOKEN id="token-153-9" pos="unknown" morph="none" start_char="13463" end_char="13470">20:13:01</TOKEN>
</SEG>
<SEG id="segment-154" start_char="13473" end_char="13819">
<ORIGINAL_TEXT>Quote from: Jolly2 on 05/02/2021 18:12:36covid was in Brazil in November 2019Quote from: Jolly2 on 05/02/2021 18:12:36France in November 2019Quote from: Jolly2 on 05/02/2021 18:12:36And spain as early as March 2019Quote from: Jolly2 on 05/02/2021 18:15:55We know Donald Trump claimed to have seen evidence that covid 19 had come from a laboratory.</ORIGINAL_TEXT>
<TOKEN id="token-154-0" pos="word" morph="none" start_char="13473" end_char="13477">Quote</TOKEN>
<TOKEN id="token-154-1" pos="word" morph="none" start_char="13479" end_char="13482">from</TOKEN>
<TOKEN id="token-154-2" pos="punct" morph="none" start_char="13483" end_char="13483">:</TOKEN>
<TOKEN id="token-154-3" pos="word" morph="none" start_char="13485" end_char="13490">Jolly2</TOKEN>
<TOKEN id="token-154-4" pos="word" morph="none" start_char="13492" end_char="13493">on</TOKEN>
<TOKEN id="token-154-5" pos="unknown" morph="none" start_char="13495" end_char="13504">05/02/2021</TOKEN>
<TOKEN id="token-154-6" pos="unknown" morph="none" start_char="13506" end_char="13518">18:12:36covid</TOKEN>
<TOKEN id="token-154-7" pos="word" morph="none" start_char="13520" end_char="13522">was</TOKEN>
<TOKEN id="token-154-8" pos="word" morph="none" start_char="13524" end_char="13525">in</TOKEN>
<TOKEN id="token-154-9" pos="word" morph="none" start_char="13527" end_char="13532">Brazil</TOKEN>
<TOKEN id="token-154-10" pos="word" morph="none" start_char="13534" end_char="13535">in</TOKEN>
<TOKEN id="token-154-11" pos="word" morph="none" start_char="13537" end_char="13544">November</TOKEN>
<TOKEN id="token-154-12" pos="word" morph="none" start_char="13546" end_char="13554">2019Quote</TOKEN>
<TOKEN id="token-154-13" pos="word" morph="none" start_char="13556" end_char="13559">from</TOKEN>
<TOKEN id="token-154-14" pos="punct" morph="none" start_char="13560" end_char="13560">:</TOKEN>
<TOKEN id="token-154-15" pos="word" morph="none" start_char="13562" end_char="13567">Jolly2</TOKEN>
<TOKEN id="token-154-16" pos="word" morph="none" start_char="13569" end_char="13570">on</TOKEN>
<TOKEN id="token-154-17" pos="unknown" morph="none" start_char="13572" end_char="13581">05/02/2021</TOKEN>
<TOKEN id="token-154-18" pos="unknown" morph="none" start_char="13583" end_char="13596">18:12:36France</TOKEN>
<TOKEN id="token-154-19" pos="word" morph="none" start_char="13598" end_char="13599">in</TOKEN>
<TOKEN id="token-154-20" pos="word" morph="none" start_char="13601" end_char="13608">November</TOKEN>
<TOKEN id="token-154-21" pos="word" morph="none" start_char="13610" end_char="13618">2019Quote</TOKEN>
<TOKEN id="token-154-22" pos="word" morph="none" start_char="13620" end_char="13623">from</TOKEN>
<TOKEN id="token-154-23" pos="punct" morph="none" start_char="13624" end_char="13624">:</TOKEN>
<TOKEN id="token-154-24" pos="word" morph="none" start_char="13626" end_char="13631">Jolly2</TOKEN>
<TOKEN id="token-154-25" pos="word" morph="none" start_char="13633" end_char="13634">on</TOKEN>
<TOKEN id="token-154-26" pos="unknown" morph="none" start_char="13636" end_char="13645">05/02/2021</TOKEN>
<TOKEN id="token-154-27" pos="unknown" morph="none" start_char="13647" end_char="13657">18:12:36And</TOKEN>
<TOKEN id="token-154-28" pos="word" morph="none" start_char="13659" end_char="13663">spain</TOKEN>
<TOKEN id="token-154-29" pos="word" morph="none" start_char="13665" end_char="13666">as</TOKEN>
<TOKEN id="token-154-30" pos="word" morph="none" start_char="13668" end_char="13672">early</TOKEN>
<TOKEN id="token-154-31" pos="word" morph="none" start_char="13674" end_char="13675">as</TOKEN>
<TOKEN id="token-154-32" pos="word" morph="none" start_char="13677" end_char="13681">March</TOKEN>
<TOKEN id="token-154-33" pos="word" morph="none" start_char="13683" end_char="13691">2019Quote</TOKEN>
<TOKEN id="token-154-34" pos="word" morph="none" start_char="13693" end_char="13696">from</TOKEN>
<TOKEN id="token-154-35" pos="punct" morph="none" start_char="13697" end_char="13697">:</TOKEN>
<TOKEN id="token-154-36" pos="word" morph="none" start_char="13699" end_char="13704">Jolly2</TOKEN>
<TOKEN id="token-154-37" pos="word" morph="none" start_char="13706" end_char="13707">on</TOKEN>
<TOKEN id="token-154-38" pos="unknown" morph="none" start_char="13709" end_char="13718">05/02/2021</TOKEN>
<TOKEN id="token-154-39" pos="unknown" morph="none" start_char="13720" end_char="13729">18:15:55We</TOKEN>
<TOKEN id="token-154-40" pos="word" morph="none" start_char="13731" end_char="13734">know</TOKEN>
<TOKEN id="token-154-41" pos="word" morph="none" start_char="13736" end_char="13741">Donald</TOKEN>
<TOKEN id="token-154-42" pos="word" morph="none" start_char="13743" end_char="13747">Trump</TOKEN>
<TOKEN id="token-154-43" pos="word" morph="none" start_char="13749" end_char="13755">claimed</TOKEN>
<TOKEN id="token-154-44" pos="word" morph="none" start_char="13757" end_char="13758">to</TOKEN>
<TOKEN id="token-154-45" pos="word" morph="none" start_char="13760" end_char="13763">have</TOKEN>
<TOKEN id="token-154-46" pos="word" morph="none" start_char="13765" end_char="13768">seen</TOKEN>
<TOKEN id="token-154-47" pos="word" morph="none" start_char="13770" end_char="13777">evidence</TOKEN>
<TOKEN id="token-154-48" pos="word" morph="none" start_char="13779" end_char="13782">that</TOKEN>
<TOKEN id="token-154-49" pos="word" morph="none" start_char="13784" end_char="13788">covid</TOKEN>
<TOKEN id="token-154-50" pos="word" morph="none" start_char="13790" end_char="13791">19</TOKEN>
<TOKEN id="token-154-51" pos="word" morph="none" start_char="13793" end_char="13795">had</TOKEN>
<TOKEN id="token-154-52" pos="word" morph="none" start_char="13797" end_char="13800">come</TOKEN>
<TOKEN id="token-154-53" pos="word" morph="none" start_char="13802" end_char="13805">from</TOKEN>
<TOKEN id="token-154-54" pos="word" morph="none" start_char="13807" end_char="13807">a</TOKEN>
<TOKEN id="token-154-55" pos="word" morph="none" start_char="13809" end_char="13818">laboratory</TOKEN>
<TOKEN id="token-154-56" pos="punct" morph="none" start_char="13819" end_char="13819">.</TOKEN>
</SEG>
<SEG id="segment-155" start_char="13821" end_char="13920">
<ORIGINAL_TEXT>Could it be that the evidence he saw was from fort Detrick?Quote from: Jolly2 on 07/02/2021 16:56:51</ORIGINAL_TEXT>
<TOKEN id="token-155-0" pos="word" morph="none" start_char="13821" end_char="13825">Could</TOKEN>
<TOKEN id="token-155-1" pos="word" morph="none" start_char="13827" end_char="13828">it</TOKEN>
<TOKEN id="token-155-2" pos="word" morph="none" start_char="13830" end_char="13831">be</TOKEN>
<TOKEN id="token-155-3" pos="word" morph="none" start_char="13833" end_char="13836">that</TOKEN>
<TOKEN id="token-155-4" pos="word" morph="none" start_char="13838" end_char="13840">the</TOKEN>
<TOKEN id="token-155-5" pos="word" morph="none" start_char="13842" end_char="13849">evidence</TOKEN>
<TOKEN id="token-155-6" pos="word" morph="none" start_char="13851" end_char="13852">he</TOKEN>
<TOKEN id="token-155-7" pos="word" morph="none" start_char="13854" end_char="13856">saw</TOKEN>
<TOKEN id="token-155-8" pos="word" morph="none" start_char="13858" end_char="13860">was</TOKEN>
<TOKEN id="token-155-9" pos="word" morph="none" start_char="13862" end_char="13865">from</TOKEN>
<TOKEN id="token-155-10" pos="word" morph="none" start_char="13867" end_char="13870">fort</TOKEN>
<TOKEN id="token-155-11" pos="unknown" morph="none" start_char="13872" end_char="13884">Detrick?Quote</TOKEN>
<TOKEN id="token-155-12" pos="word" morph="none" start_char="13886" end_char="13889">from</TOKEN>
<TOKEN id="token-155-13" pos="punct" morph="none" start_char="13890" end_char="13890">:</TOKEN>
<TOKEN id="token-155-14" pos="word" morph="none" start_char="13892" end_char="13897">Jolly2</TOKEN>
<TOKEN id="token-155-15" pos="word" morph="none" start_char="13899" end_char="13900">on</TOKEN>
<TOKEN id="token-155-16" pos="unknown" morph="none" start_char="13902" end_char="13911">07/02/2021</TOKEN>
<TOKEN id="token-155-17" pos="unknown" morph="none" start_char="13913" end_char="13920">16:56:51</TOKEN>
</SEG>
<SEG id="segment-156" start_char="13923" end_char="14036">
<ORIGINAL_TEXT>virus particles on the outside of a pooh may well be destroyed but those embedded inside could have been protected</ORIGINAL_TEXT>
<TOKEN id="token-156-0" pos="word" morph="none" start_char="13923" end_char="13927">virus</TOKEN>
<TOKEN id="token-156-1" pos="word" morph="none" start_char="13929" end_char="13937">particles</TOKEN>
<TOKEN id="token-156-2" pos="word" morph="none" start_char="13939" end_char="13940">on</TOKEN>
<TOKEN id="token-156-3" pos="word" morph="none" start_char="13942" end_char="13944">the</TOKEN>
<TOKEN id="token-156-4" pos="word" morph="none" start_char="13946" end_char="13952">outside</TOKEN>
<TOKEN id="token-156-5" pos="word" morph="none" start_char="13954" end_char="13955">of</TOKEN>
<TOKEN id="token-156-6" pos="word" morph="none" start_char="13957" end_char="13957">a</TOKEN>
<TOKEN id="token-156-7" pos="word" morph="none" start_char="13959" end_char="13962">pooh</TOKEN>
<TOKEN id="token-156-8" pos="word" morph="none" start_char="13964" end_char="13966">may</TOKEN>
<TOKEN id="token-156-9" pos="word" morph="none" start_char="13968" end_char="13971">well</TOKEN>
<TOKEN id="token-156-10" pos="word" morph="none" start_char="13973" end_char="13974">be</TOKEN>
<TOKEN id="token-156-11" pos="word" morph="none" start_char="13976" end_char="13984">destroyed</TOKEN>
<TOKEN id="token-156-12" pos="word" morph="none" start_char="13986" end_char="13988">but</TOKEN>
<TOKEN id="token-156-13" pos="word" morph="none" start_char="13990" end_char="13994">those</TOKEN>
<TOKEN id="token-156-14" pos="word" morph="none" start_char="13996" end_char="14003">embedded</TOKEN>
<TOKEN id="token-156-15" pos="word" morph="none" start_char="14005" end_char="14010">inside</TOKEN>
<TOKEN id="token-156-16" pos="word" morph="none" start_char="14012" end_char="14016">could</TOKEN>
<TOKEN id="token-156-17" pos="word" morph="none" start_char="14018" end_char="14021">have</TOKEN>
<TOKEN id="token-156-18" pos="word" morph="none" start_char="14023" end_char="14026">been</TOKEN>
<TOKEN id="token-156-19" pos="word" morph="none" start_char="14028" end_char="14036">protected</TOKEN>
</SEG>
<SEG id="segment-157" start_char="14039" end_char="14178">
<ORIGINAL_TEXT>Among the things they would have been protected from is getting into a sample vial.So they would never have made it to a lab to be detected.</ORIGINAL_TEXT>
<TOKEN id="token-157-0" pos="word" morph="none" start_char="14039" end_char="14043">Among</TOKEN>
<TOKEN id="token-157-1" pos="word" morph="none" start_char="14045" end_char="14047">the</TOKEN>
<TOKEN id="token-157-2" pos="word" morph="none" start_char="14049" end_char="14054">things</TOKEN>
<TOKEN id="token-157-3" pos="word" morph="none" start_char="14056" end_char="14059">they</TOKEN>
<TOKEN id="token-157-4" pos="word" morph="none" start_char="14061" end_char="14065">would</TOKEN>
<TOKEN id="token-157-5" pos="word" morph="none" start_char="14067" end_char="14070">have</TOKEN>
<TOKEN id="token-157-6" pos="word" morph="none" start_char="14072" end_char="14075">been</TOKEN>
<TOKEN id="token-157-7" pos="word" morph="none" start_char="14077" end_char="14085">protected</TOKEN>
<TOKEN id="token-157-8" pos="word" morph="none" start_char="14087" end_char="14090">from</TOKEN>
<TOKEN id="token-157-9" pos="word" morph="none" start_char="14092" end_char="14093">is</TOKEN>
<TOKEN id="token-157-10" pos="word" morph="none" start_char="14095" end_char="14101">getting</TOKEN>
<TOKEN id="token-157-11" pos="word" morph="none" start_char="14103" end_char="14106">into</TOKEN>
<TOKEN id="token-157-12" pos="word" morph="none" start_char="14108" end_char="14108">a</TOKEN>
<TOKEN id="token-157-13" pos="word" morph="none" start_char="14110" end_char="14115">sample</TOKEN>
<TOKEN id="token-157-14" pos="unknown" morph="none" start_char="14117" end_char="14123">vial.So</TOKEN>
<TOKEN id="token-157-15" pos="word" morph="none" start_char="14125" end_char="14128">they</TOKEN>
<TOKEN id="token-157-16" pos="word" morph="none" start_char="14130" end_char="14134">would</TOKEN>
<TOKEN id="token-157-17" pos="word" morph="none" start_char="14136" end_char="14140">never</TOKEN>
<TOKEN id="token-157-18" pos="word" morph="none" start_char="14142" end_char="14145">have</TOKEN>
<TOKEN id="token-157-19" pos="word" morph="none" start_char="14147" end_char="14150">made</TOKEN>
<TOKEN id="token-157-20" pos="word" morph="none" start_char="14152" end_char="14153">it</TOKEN>
<TOKEN id="token-157-21" pos="word" morph="none" start_char="14155" end_char="14156">to</TOKEN>
<TOKEN id="token-157-22" pos="word" morph="none" start_char="14158" end_char="14158">a</TOKEN>
<TOKEN id="token-157-23" pos="word" morph="none" start_char="14160" end_char="14162">lab</TOKEN>
<TOKEN id="token-157-24" pos="word" morph="none" start_char="14164" end_char="14165">to</TOKEN>
<TOKEN id="token-157-25" pos="word" morph="none" start_char="14167" end_char="14168">be</TOKEN>
<TOKEN id="token-157-26" pos="word" morph="none" start_char="14170" end_char="14177">detected</TOKEN>
<TOKEN id="token-157-27" pos="punct" morph="none" start_char="14178" end_char="14178">.</TOKEN>
</SEG>
<SEG id="segment-158" start_char="14181" end_char="14197">
<ORIGINAL_TEXT>Speculative also.</ORIGINAL_TEXT>
<TOKEN id="token-158-0" pos="word" morph="none" start_char="14181" end_char="14191">Speculative</TOKEN>
<TOKEN id="token-158-1" pos="word" morph="none" start_char="14193" end_char="14196">also</TOKEN>
<TOKEN id="token-158-2" pos="punct" morph="none" start_char="14197" end_char="14197">.</TOKEN>
</SEG>
<SEG id="segment-159" start_char="14199" end_char="14341">
<ORIGINAL_TEXT>Whole point of scientific investigation you speculate, calculate implications of the speculation and then test against experiment and evidence.</ORIGINAL_TEXT>
<TOKEN id="token-159-0" pos="word" morph="none" start_char="14199" end_char="14203">Whole</TOKEN>
<TOKEN id="token-159-1" pos="word" morph="none" start_char="14205" end_char="14209">point</TOKEN>
<TOKEN id="token-159-2" pos="word" morph="none" start_char="14211" end_char="14212">of</TOKEN>
<TOKEN id="token-159-3" pos="word" morph="none" start_char="14214" end_char="14223">scientific</TOKEN>
<TOKEN id="token-159-4" pos="word" morph="none" start_char="14225" end_char="14237">investigation</TOKEN>
<TOKEN id="token-159-5" pos="word" morph="none" start_char="14239" end_char="14241">you</TOKEN>
<TOKEN id="token-159-6" pos="word" morph="none" start_char="14243" end_char="14251">speculate</TOKEN>
<TOKEN id="token-159-7" pos="punct" morph="none" start_char="14252" end_char="14252">,</TOKEN>
<TOKEN id="token-159-8" pos="word" morph="none" start_char="14254" end_char="14262">calculate</TOKEN>
<TOKEN id="token-159-9" pos="word" morph="none" start_char="14264" end_char="14275">implications</TOKEN>
<TOKEN id="token-159-10" pos="word" morph="none" start_char="14277" end_char="14278">of</TOKEN>
<TOKEN id="token-159-11" pos="word" morph="none" start_char="14280" end_char="14282">the</TOKEN>
<TOKEN id="token-159-12" pos="word" morph="none" start_char="14284" end_char="14294">speculation</TOKEN>
<TOKEN id="token-159-13" pos="word" morph="none" start_char="14296" end_char="14298">and</TOKEN>
<TOKEN id="token-159-14" pos="word" morph="none" start_char="14300" end_char="14303">then</TOKEN>
<TOKEN id="token-159-15" pos="word" morph="none" start_char="14305" end_char="14308">test</TOKEN>
<TOKEN id="token-159-16" pos="word" morph="none" start_char="14310" end_char="14316">against</TOKEN>
<TOKEN id="token-159-17" pos="word" morph="none" start_char="14318" end_char="14327">experiment</TOKEN>
<TOKEN id="token-159-18" pos="word" morph="none" start_char="14329" end_char="14331">and</TOKEN>
<TOKEN id="token-159-19" pos="word" morph="none" start_char="14333" end_char="14340">evidence</TOKEN>
<TOKEN id="token-159-20" pos="punct" morph="none" start_char="14341" end_char="14341">.</TOKEN>
</SEG>
<SEG id="segment-160" start_char="14345" end_char="14386">
<ORIGINAL_TEXT>Quote from: Kryptid on 07/02/2021 04:14:41</ORIGINAL_TEXT>
<TOKEN id="token-160-0" pos="word" morph="none" start_char="14345" end_char="14349">Quote</TOKEN>
<TOKEN id="token-160-1" pos="word" morph="none" start_char="14351" end_char="14354">from</TOKEN>
<TOKEN id="token-160-2" pos="punct" morph="none" start_char="14355" end_char="14355">:</TOKEN>
<TOKEN id="token-160-3" pos="word" morph="none" start_char="14357" end_char="14363">Kryptid</TOKEN>
<TOKEN id="token-160-4" pos="word" morph="none" start_char="14365" end_char="14366">on</TOKEN>
<TOKEN id="token-160-5" pos="unknown" morph="none" start_char="14368" end_char="14377">07/02/2021</TOKEN>
<TOKEN id="token-160-6" pos="unknown" morph="none" start_char="14379" end_char="14386">04:14:41</TOKEN>
</SEG>
<SEG id="segment-161" start_char="14389" end_char="14469">
<ORIGINAL_TEXT>https://theconversation.com/was-coronavirus-really-in-europe-in-march-2019-141582</ORIGINAL_TEXT>
<TOKEN id="token-161-0" pos="url" morph="none" start_char="14389" end_char="14469">https://theconversation.com/was-coronavirus-really-in-europe-in-march-2019-141582</TOKEN>
</SEG>
<SEG id="segment-162" start_char="14472" end_char="14500">
<ORIGINAL_TEXT>I ponder what the hold up is.</ORIGINAL_TEXT>
<TOKEN id="token-162-0" pos="word" morph="none" start_char="14472" end_char="14472">I</TOKEN>
<TOKEN id="token-162-1" pos="word" morph="none" start_char="14474" end_char="14479">ponder</TOKEN>
<TOKEN id="token-162-2" pos="word" morph="none" start_char="14481" end_char="14484">what</TOKEN>
<TOKEN id="token-162-3" pos="word" morph="none" start_char="14486" end_char="14488">the</TOKEN>
<TOKEN id="token-162-4" pos="word" morph="none" start_char="14490" end_char="14493">hold</TOKEN>
<TOKEN id="token-162-5" pos="word" morph="none" start_char="14495" end_char="14496">up</TOKEN>
<TOKEN id="token-162-6" pos="word" morph="none" start_char="14498" end_char="14499">is</TOKEN>
<TOKEN id="token-162-7" pos="punct" morph="none" start_char="14500" end_char="14500">.</TOKEN>
</SEG>
<SEG id="segment-163" start_char="14502" end_char="14543">
<ORIGINAL_TEXT>Shouldn't take a year to do a peer review.</ORIGINAL_TEXT>
<TOKEN id="token-163-0" pos="word" morph="none" start_char="14502" end_char="14510">Shouldn't</TOKEN>
<TOKEN id="token-163-1" pos="word" morph="none" start_char="14512" end_char="14515">take</TOKEN>
<TOKEN id="token-163-2" pos="word" morph="none" start_char="14517" end_char="14517">a</TOKEN>
<TOKEN id="token-163-3" pos="word" morph="none" start_char="14519" end_char="14522">year</TOKEN>
<TOKEN id="token-163-4" pos="word" morph="none" start_char="14524" end_char="14525">to</TOKEN>
<TOKEN id="token-163-5" pos="word" morph="none" start_char="14527" end_char="14528">do</TOKEN>
<TOKEN id="token-163-6" pos="word" morph="none" start_char="14530" end_char="14530">a</TOKEN>
<TOKEN id="token-163-7" pos="word" morph="none" start_char="14532" end_char="14535">peer</TOKEN>
<TOKEN id="token-163-8" pos="word" morph="none" start_char="14537" end_char="14542">review</TOKEN>
<TOKEN id="token-163-9" pos="punct" morph="none" start_char="14543" end_char="14543">.</TOKEN>
</SEG>
<SEG id="segment-164" start_char="14545" end_char="14583">
<ORIGINAL_TEXT>The hold up is in-of-itself suspicious.</ORIGINAL_TEXT>
<TOKEN id="token-164-0" pos="word" morph="none" start_char="14545" end_char="14547">The</TOKEN>
<TOKEN id="token-164-1" pos="word" morph="none" start_char="14549" end_char="14552">hold</TOKEN>
<TOKEN id="token-164-2" pos="word" morph="none" start_char="14554" end_char="14555">up</TOKEN>
<TOKEN id="token-164-3" pos="word" morph="none" start_char="14557" end_char="14558">is</TOKEN>
<TOKEN id="token-164-4" pos="unknown" morph="none" start_char="14560" end_char="14571">in-of-itself</TOKEN>
<TOKEN id="token-164-5" pos="word" morph="none" start_char="14573" end_char="14582">suspicious</TOKEN>
<TOKEN id="token-164-6" pos="punct" morph="none" start_char="14583" end_char="14583">.</TOKEN>
</SEG>
<SEG id="segment-165" start_char="14586" end_char="14734">
<ORIGINAL_TEXT>Still returning to Trump he clearly claimed repeatedly that he had seen evidence covid had come from a laboratory and repeatedly sort to blame China.</ORIGINAL_TEXT>
<TOKEN id="token-165-0" pos="word" morph="none" start_char="14586" end_char="14590">Still</TOKEN>
<TOKEN id="token-165-1" pos="word" morph="none" start_char="14592" end_char="14600">returning</TOKEN>
<TOKEN id="token-165-2" pos="word" morph="none" start_char="14602" end_char="14603">to</TOKEN>
<TOKEN id="token-165-3" pos="word" morph="none" start_char="14605" end_char="14609">Trump</TOKEN>
<TOKEN id="token-165-4" pos="word" morph="none" start_char="14611" end_char="14612">he</TOKEN>
<TOKEN id="token-165-5" pos="word" morph="none" start_char="14614" end_char="14620">clearly</TOKEN>
<TOKEN id="token-165-6" pos="word" morph="none" start_char="14622" end_char="14628">claimed</TOKEN>
<TOKEN id="token-165-7" pos="word" morph="none" start_char="14630" end_char="14639">repeatedly</TOKEN>
<TOKEN id="token-165-8" pos="word" morph="none" start_char="14641" end_char="14644">that</TOKEN>
<TOKEN id="token-165-9" pos="word" morph="none" start_char="14646" end_char="14647">he</TOKEN>
<TOKEN id="token-165-10" pos="word" morph="none" start_char="14649" end_char="14651">had</TOKEN>
<TOKEN id="token-165-11" pos="word" morph="none" start_char="14653" end_char="14656">seen</TOKEN>
<TOKEN id="token-165-12" pos="word" morph="none" start_char="14658" end_char="14665">evidence</TOKEN>
<TOKEN id="token-165-13" pos="word" morph="none" start_char="14667" end_char="14671">covid</TOKEN>
<TOKEN id="token-165-14" pos="word" morph="none" start_char="14673" end_char="14675">had</TOKEN>
<TOKEN id="token-165-15" pos="word" morph="none" start_char="14677" end_char="14680">come</TOKEN>
<TOKEN id="token-165-16" pos="word" morph="none" start_char="14682" end_char="14685">from</TOKEN>
<TOKEN id="token-165-17" pos="word" morph="none" start_char="14687" end_char="14687">a</TOKEN>
<TOKEN id="token-165-18" pos="word" morph="none" start_char="14689" end_char="14698">laboratory</TOKEN>
<TOKEN id="token-165-19" pos="word" morph="none" start_char="14700" end_char="14702">and</TOKEN>
<TOKEN id="token-165-20" pos="word" morph="none" start_char="14704" end_char="14713">repeatedly</TOKEN>
<TOKEN id="token-165-21" pos="word" morph="none" start_char="14715" end_char="14718">sort</TOKEN>
<TOKEN id="token-165-22" pos="word" morph="none" start_char="14720" end_char="14721">to</TOKEN>
<TOKEN id="token-165-23" pos="word" morph="none" start_char="14723" end_char="14727">blame</TOKEN>
<TOKEN id="token-165-24" pos="word" morph="none" start_char="14729" end_char="14733">China</TOKEN>
<TOKEN id="token-165-25" pos="punct" morph="none" start_char="14734" end_char="14734">.</TOKEN>
</SEG>
<SEG id="segment-166" start_char="14737" end_char="14873">
<ORIGINAL_TEXT>Hillary Clinton before the 2016 election had a group investigate her greatest weakness when it came to challenging Trump in the election.</ORIGINAL_TEXT>
<TOKEN id="token-166-0" pos="word" morph="none" start_char="14737" end_char="14743">Hillary</TOKEN>
<TOKEN id="token-166-1" pos="word" morph="none" start_char="14745" end_char="14751">Clinton</TOKEN>
<TOKEN id="token-166-2" pos="word" morph="none" start_char="14753" end_char="14758">before</TOKEN>
<TOKEN id="token-166-3" pos="word" morph="none" start_char="14760" end_char="14762">the</TOKEN>
<TOKEN id="token-166-4" pos="word" morph="none" start_char="14764" end_char="14767">2016</TOKEN>
<TOKEN id="token-166-5" pos="word" morph="none" start_char="14769" end_char="14776">election</TOKEN>
<TOKEN id="token-166-6" pos="word" morph="none" start_char="14778" end_char="14780">had</TOKEN>
<TOKEN id="token-166-7" pos="word" morph="none" start_char="14782" end_char="14782">a</TOKEN>
<TOKEN id="token-166-8" pos="word" morph="none" start_char="14784" end_char="14788">group</TOKEN>
<TOKEN id="token-166-9" pos="word" morph="none" start_char="14790" end_char="14800">investigate</TOKEN>
<TOKEN id="token-166-10" pos="word" morph="none" start_char="14802" end_char="14804">her</TOKEN>
<TOKEN id="token-166-11" pos="word" morph="none" start_char="14806" end_char="14813">greatest</TOKEN>
<TOKEN id="token-166-12" pos="word" morph="none" start_char="14815" end_char="14822">weakness</TOKEN>
<TOKEN id="token-166-13" pos="word" morph="none" start_char="14824" end_char="14827">when</TOKEN>
<TOKEN id="token-166-14" pos="word" morph="none" start_char="14829" end_char="14830">it</TOKEN>
<TOKEN id="token-166-15" pos="word" morph="none" start_char="14832" end_char="14835">came</TOKEN>
<TOKEN id="token-166-16" pos="word" morph="none" start_char="14837" end_char="14838">to</TOKEN>
<TOKEN id="token-166-17" pos="word" morph="none" start_char="14840" end_char="14850">challenging</TOKEN>
<TOKEN id="token-166-18" pos="word" morph="none" start_char="14852" end_char="14856">Trump</TOKEN>
<TOKEN id="token-166-19" pos="word" morph="none" start_char="14858" end_char="14859">in</TOKEN>
<TOKEN id="token-166-20" pos="word" morph="none" start_char="14861" end_char="14863">the</TOKEN>
<TOKEN id="token-166-21" pos="word" morph="none" start_char="14865" end_char="14872">election</TOKEN>
<TOKEN id="token-166-22" pos="punct" morph="none" start_char="14873" end_char="14873">.</TOKEN>
</SEG>
<SEG id="segment-167" start_char="14875" end_char="15135">
<ORIGINAL_TEXT>The report concluded that her tries to Russia and her assistance in allowing Russia to gain a position in the company 'uranium 1' and so a substantial part of Americas uranium production would be seen as her greatest weakness if known by the general population.</ORIGINAL_TEXT>
<TOKEN id="token-167-0" pos="word" morph="none" start_char="14875" end_char="14877">The</TOKEN>
<TOKEN id="token-167-1" pos="word" morph="none" start_char="14879" end_char="14884">report</TOKEN>
<TOKEN id="token-167-2" pos="word" morph="none" start_char="14886" end_char="14894">concluded</TOKEN>
<TOKEN id="token-167-3" pos="word" morph="none" start_char="14896" end_char="14899">that</TOKEN>
<TOKEN id="token-167-4" pos="word" morph="none" start_char="14901" end_char="14903">her</TOKEN>
<TOKEN id="token-167-5" pos="word" morph="none" start_char="14905" end_char="14909">tries</TOKEN>
<TOKEN id="token-167-6" pos="word" morph="none" start_char="14911" end_char="14912">to</TOKEN>
<TOKEN id="token-167-7" pos="word" morph="none" start_char="14914" end_char="14919">Russia</TOKEN>
<TOKEN id="token-167-8" pos="word" morph="none" start_char="14921" end_char="14923">and</TOKEN>
<TOKEN id="token-167-9" pos="word" morph="none" start_char="14925" end_char="14927">her</TOKEN>
<TOKEN id="token-167-10" pos="word" morph="none" start_char="14929" end_char="14938">assistance</TOKEN>
<TOKEN id="token-167-11" pos="word" morph="none" start_char="14940" end_char="14941">in</TOKEN>
<TOKEN id="token-167-12" pos="word" morph="none" start_char="14943" end_char="14950">allowing</TOKEN>
<TOKEN id="token-167-13" pos="word" morph="none" start_char="14952" end_char="14957">Russia</TOKEN>
<TOKEN id="token-167-14" pos="word" morph="none" start_char="14959" end_char="14960">to</TOKEN>
<TOKEN id="token-167-15" pos="word" morph="none" start_char="14962" end_char="14965">gain</TOKEN>
<TOKEN id="token-167-16" pos="word" morph="none" start_char="14967" end_char="14967">a</TOKEN>
<TOKEN id="token-167-17" pos="word" morph="none" start_char="14969" end_char="14976">position</TOKEN>
<TOKEN id="token-167-18" pos="word" morph="none" start_char="14978" end_char="14979">in</TOKEN>
<TOKEN id="token-167-19" pos="word" morph="none" start_char="14981" end_char="14983">the</TOKEN>
<TOKEN id="token-167-20" pos="word" morph="none" start_char="14985" end_char="14991">company</TOKEN>
<TOKEN id="token-167-21" pos="punct" morph="none" start_char="14993" end_char="14993">'</TOKEN>
<TOKEN id="token-167-22" pos="word" morph="none" start_char="14994" end_char="15000">uranium</TOKEN>
<TOKEN id="token-167-23" pos="word" morph="none" start_char="15002" end_char="15002">1</TOKEN>
<TOKEN id="token-167-24" pos="punct" morph="none" start_char="15003" end_char="15003">'</TOKEN>
<TOKEN id="token-167-25" pos="word" morph="none" start_char="15005" end_char="15007">and</TOKEN>
<TOKEN id="token-167-26" pos="word" morph="none" start_char="15009" end_char="15010">so</TOKEN>
<TOKEN id="token-167-27" pos="word" morph="none" start_char="15012" end_char="15012">a</TOKEN>
<TOKEN id="token-167-28" pos="word" morph="none" start_char="15014" end_char="15024">substantial</TOKEN>
<TOKEN id="token-167-29" pos="word" morph="none" start_char="15026" end_char="15029">part</TOKEN>
<TOKEN id="token-167-30" pos="word" morph="none" start_char="15031" end_char="15032">of</TOKEN>
<TOKEN id="token-167-31" pos="word" morph="none" start_char="15034" end_char="15041">Americas</TOKEN>
<TOKEN id="token-167-32" pos="word" morph="none" start_char="15043" end_char="15049">uranium</TOKEN>
<TOKEN id="token-167-33" pos="word" morph="none" start_char="15051" end_char="15060">production</TOKEN>
<TOKEN id="token-167-34" pos="word" morph="none" start_char="15062" end_char="15066">would</TOKEN>
<TOKEN id="token-167-35" pos="word" morph="none" start_char="15068" end_char="15069">be</TOKEN>
<TOKEN id="token-167-36" pos="word" morph="none" start_char="15071" end_char="15074">seen</TOKEN>
<TOKEN id="token-167-37" pos="word" morph="none" start_char="15076" end_char="15077">as</TOKEN>
<TOKEN id="token-167-38" pos="word" morph="none" start_char="15079" end_char="15081">her</TOKEN>
<TOKEN id="token-167-39" pos="word" morph="none" start_char="15083" end_char="15090">greatest</TOKEN>
<TOKEN id="token-167-40" pos="word" morph="none" start_char="15092" end_char="15099">weakness</TOKEN>
<TOKEN id="token-167-41" pos="word" morph="none" start_char="15101" end_char="15102">if</TOKEN>
<TOKEN id="token-167-42" pos="word" morph="none" start_char="15104" end_char="15108">known</TOKEN>
<TOKEN id="token-167-43" pos="word" morph="none" start_char="15110" end_char="15111">by</TOKEN>
<TOKEN id="token-167-44" pos="word" morph="none" start_char="15113" end_char="15115">the</TOKEN>
<TOKEN id="token-167-45" pos="word" morph="none" start_char="15117" end_char="15123">general</TOKEN>
<TOKEN id="token-167-46" pos="word" morph="none" start_char="15125" end_char="15134">population</TOKEN>
<TOKEN id="token-167-47" pos="punct" morph="none" start_char="15135" end_char="15135">.</TOKEN>
</SEG>
<SEG id="segment-168" start_char="15137" end_char="15316">
<ORIGINAL_TEXT>So Mrs Clinton took her greatest weakness and started accusing Trump of being a Russian asset as a means of deflecting attention from herself and her own ties to country of Russia.</ORIGINAL_TEXT>
<TOKEN id="token-168-0" pos="word" morph="none" start_char="15137" end_char="15138">So</TOKEN>
<TOKEN id="token-168-1" pos="word" morph="none" start_char="15140" end_char="15142">Mrs</TOKEN>
<TOKEN id="token-168-2" pos="word" morph="none" start_char="15144" end_char="15150">Clinton</TOKEN>
<TOKEN id="token-168-3" pos="word" morph="none" start_char="15152" end_char="15155">took</TOKEN>
<TOKEN id="token-168-4" pos="word" morph="none" start_char="15157" end_char="15159">her</TOKEN>
<TOKEN id="token-168-5" pos="word" morph="none" start_char="15161" end_char="15168">greatest</TOKEN>
<TOKEN id="token-168-6" pos="word" morph="none" start_char="15170" end_char="15177">weakness</TOKEN>
<TOKEN id="token-168-7" pos="word" morph="none" start_char="15179" end_char="15181">and</TOKEN>
<TOKEN id="token-168-8" pos="word" morph="none" start_char="15183" end_char="15189">started</TOKEN>
<TOKEN id="token-168-9" pos="word" morph="none" start_char="15191" end_char="15198">accusing</TOKEN>
<TOKEN id="token-168-10" pos="word" morph="none" start_char="15200" end_char="15204">Trump</TOKEN>
<TOKEN id="token-168-11" pos="word" morph="none" start_char="15206" end_char="15207">of</TOKEN>
<TOKEN id="token-168-12" pos="word" morph="none" start_char="15209" end_char="15213">being</TOKEN>
<TOKEN id="token-168-13" pos="word" morph="none" start_char="15215" end_char="15215">a</TOKEN>
<TOKEN id="token-168-14" pos="word" morph="none" start_char="15217" end_char="15223">Russian</TOKEN>
<TOKEN id="token-168-15" pos="word" morph="none" start_char="15225" end_char="15229">asset</TOKEN>
<TOKEN id="token-168-16" pos="word" morph="none" start_char="15231" end_char="15232">as</TOKEN>
<TOKEN id="token-168-17" pos="word" morph="none" start_char="15234" end_char="15234">a</TOKEN>
<TOKEN id="token-168-18" pos="word" morph="none" start_char="15236" end_char="15240">means</TOKEN>
<TOKEN id="token-168-19" pos="word" morph="none" start_char="15242" end_char="15243">of</TOKEN>
<TOKEN id="token-168-20" pos="word" morph="none" start_char="15245" end_char="15254">deflecting</TOKEN>
<TOKEN id="token-168-21" pos="word" morph="none" start_char="15256" end_char="15264">attention</TOKEN>
<TOKEN id="token-168-22" pos="word" morph="none" start_char="15266" end_char="15269">from</TOKEN>
<TOKEN id="token-168-23" pos="word" morph="none" start_char="15271" end_char="15277">herself</TOKEN>
<TOKEN id="token-168-24" pos="word" morph="none" start_char="15279" end_char="15281">and</TOKEN>
<TOKEN id="token-168-25" pos="word" morph="none" start_char="15283" end_char="15285">her</TOKEN>
<TOKEN id="token-168-26" pos="word" morph="none" start_char="15287" end_char="15289">own</TOKEN>
<TOKEN id="token-168-27" pos="word" morph="none" start_char="15291" end_char="15294">ties</TOKEN>
<TOKEN id="token-168-28" pos="word" morph="none" start_char="15296" end_char="15297">to</TOKEN>
<TOKEN id="token-168-29" pos="word" morph="none" start_char="15299" end_char="15305">country</TOKEN>
<TOKEN id="token-168-30" pos="word" morph="none" start_char="15307" end_char="15308">of</TOKEN>
<TOKEN id="token-168-31" pos="word" morph="none" start_char="15310" end_char="15315">Russia</TOKEN>
<TOKEN id="token-168-32" pos="punct" morph="none" start_char="15316" end_char="15316">.</TOKEN>
</SEG>
<SEG id="segment-169" start_char="15319" end_char="15505">
<ORIGINAL_TEXT>The reason I tell this story is because of Trumps continual allegation that China released the virus; To speak with such certianly implies a desire much like Clinton to deflect attention.</ORIGINAL_TEXT>
<TOKEN id="token-169-0" pos="word" morph="none" start_char="15319" end_char="15321">The</TOKEN>
<TOKEN id="token-169-1" pos="word" morph="none" start_char="15323" end_char="15328">reason</TOKEN>
<TOKEN id="token-169-2" pos="word" morph="none" start_char="15330" end_char="15330">I</TOKEN>
<TOKEN id="token-169-3" pos="word" morph="none" start_char="15332" end_char="15335">tell</TOKEN>
<TOKEN id="token-169-4" pos="word" morph="none" start_char="15337" end_char="15340">this</TOKEN>
<TOKEN id="token-169-5" pos="word" morph="none" start_char="15342" end_char="15346">story</TOKEN>
<TOKEN id="token-169-6" pos="word" morph="none" start_char="15348" end_char="15349">is</TOKEN>
<TOKEN id="token-169-7" pos="word" morph="none" start_char="15351" end_char="15357">because</TOKEN>
<TOKEN id="token-169-8" pos="word" morph="none" start_char="15359" end_char="15360">of</TOKEN>
<TOKEN id="token-169-9" pos="word" morph="none" start_char="15362" end_char="15367">Trumps</TOKEN>
<TOKEN id="token-169-10" pos="word" morph="none" start_char="15369" end_char="15377">continual</TOKEN>
<TOKEN id="token-169-11" pos="word" morph="none" start_char="15379" end_char="15388">allegation</TOKEN>
<TOKEN id="token-169-12" pos="word" morph="none" start_char="15390" end_char="15393">that</TOKEN>
<TOKEN id="token-169-13" pos="word" morph="none" start_char="15395" end_char="15399">China</TOKEN>
<TOKEN id="token-169-14" pos="word" morph="none" start_char="15401" end_char="15408">released</TOKEN>
<TOKEN id="token-169-15" pos="word" morph="none" start_char="15410" end_char="15412">the</TOKEN>
<TOKEN id="token-169-16" pos="word" morph="none" start_char="15414" end_char="15418">virus</TOKEN>
<TOKEN id="token-169-17" pos="punct" morph="none" start_char="15419" end_char="15419">;</TOKEN>
<TOKEN id="token-169-18" pos="word" morph="none" start_char="15421" end_char="15422">To</TOKEN>
<TOKEN id="token-169-19" pos="word" morph="none" start_char="15424" end_char="15428">speak</TOKEN>
<TOKEN id="token-169-20" pos="word" morph="none" start_char="15430" end_char="15433">with</TOKEN>
<TOKEN id="token-169-21" pos="word" morph="none" start_char="15435" end_char="15438">such</TOKEN>
<TOKEN id="token-169-22" pos="word" morph="none" start_char="15440" end_char="15448">certianly</TOKEN>
<TOKEN id="token-169-23" pos="word" morph="none" start_char="15450" end_char="15456">implies</TOKEN>
<TOKEN id="token-169-24" pos="word" morph="none" start_char="15458" end_char="15458">a</TOKEN>
<TOKEN id="token-169-25" pos="word" morph="none" start_char="15460" end_char="15465">desire</TOKEN>
<TOKEN id="token-169-26" pos="word" morph="none" start_char="15467" end_char="15470">much</TOKEN>
<TOKEN id="token-169-27" pos="word" morph="none" start_char="15472" end_char="15475">like</TOKEN>
<TOKEN id="token-169-28" pos="word" morph="none" start_char="15477" end_char="15483">Clinton</TOKEN>
<TOKEN id="token-169-29" pos="word" morph="none" start_char="15485" end_char="15486">to</TOKEN>
<TOKEN id="token-169-30" pos="word" morph="none" start_char="15488" end_char="15494">deflect</TOKEN>
<TOKEN id="token-169-31" pos="word" morph="none" start_char="15496" end_char="15504">attention</TOKEN>
<TOKEN id="token-169-32" pos="punct" morph="none" start_char="15505" end_char="15505">.</TOKEN>
</SEG>
<SEG id="segment-170" start_char="15508" end_char="15746">
<ORIGINAL_TEXT>Trumps cliams he saw evidence covid came from a laboratory, I feel looking at this, if he did actually see evidence the evidence was from fort Detrick and so as a means of deflection he blamed China, but also as a means to wage a cold war.</ORIGINAL_TEXT>
<TOKEN id="token-170-0" pos="word" morph="none" start_char="15508" end_char="15513">Trumps</TOKEN>
<TOKEN id="token-170-1" pos="word" morph="none" start_char="15515" end_char="15520">cliams</TOKEN>
<TOKEN id="token-170-2" pos="word" morph="none" start_char="15522" end_char="15523">he</TOKEN>
<TOKEN id="token-170-3" pos="word" morph="none" start_char="15525" end_char="15527">saw</TOKEN>
<TOKEN id="token-170-4" pos="word" morph="none" start_char="15529" end_char="15536">evidence</TOKEN>
<TOKEN id="token-170-5" pos="word" morph="none" start_char="15538" end_char="15542">covid</TOKEN>
<TOKEN id="token-170-6" pos="word" morph="none" start_char="15544" end_char="15547">came</TOKEN>
<TOKEN id="token-170-7" pos="word" morph="none" start_char="15549" end_char="15552">from</TOKEN>
<TOKEN id="token-170-8" pos="word" morph="none" start_char="15554" end_char="15554">a</TOKEN>
<TOKEN id="token-170-9" pos="word" morph="none" start_char="15556" end_char="15565">laboratory</TOKEN>
<TOKEN id="token-170-10" pos="punct" morph="none" start_char="15566" end_char="15566">,</TOKEN>
<TOKEN id="token-170-11" pos="word" morph="none" start_char="15568" end_char="15568">I</TOKEN>
<TOKEN id="token-170-12" pos="word" morph="none" start_char="15570" end_char="15573">feel</TOKEN>
<TOKEN id="token-170-13" pos="word" morph="none" start_char="15575" end_char="15581">looking</TOKEN>
<TOKEN id="token-170-14" pos="word" morph="none" start_char="15583" end_char="15584">at</TOKEN>
<TOKEN id="token-170-15" pos="word" morph="none" start_char="15586" end_char="15589">this</TOKEN>
<TOKEN id="token-170-16" pos="punct" morph="none" start_char="15590" end_char="15590">,</TOKEN>
<TOKEN id="token-170-17" pos="word" morph="none" start_char="15592" end_char="15593">if</TOKEN>
<TOKEN id="token-170-18" pos="word" morph="none" start_char="15595" end_char="15596">he</TOKEN>
<TOKEN id="token-170-19" pos="word" morph="none" start_char="15598" end_char="15600">did</TOKEN>
<TOKEN id="token-170-20" pos="word" morph="none" start_char="15602" end_char="15609">actually</TOKEN>
<TOKEN id="token-170-21" pos="word" morph="none" start_char="15611" end_char="15613">see</TOKEN>
<TOKEN id="token-170-22" pos="word" morph="none" start_char="15615" end_char="15622">evidence</TOKEN>
<TOKEN id="token-170-23" pos="word" morph="none" start_char="15624" end_char="15626">the</TOKEN>
<TOKEN id="token-170-24" pos="word" morph="none" start_char="15628" end_char="15635">evidence</TOKEN>
<TOKEN id="token-170-25" pos="word" morph="none" start_char="15637" end_char="15639">was</TOKEN>
<TOKEN id="token-170-26" pos="word" morph="none" start_char="15641" end_char="15644">from</TOKEN>
<TOKEN id="token-170-27" pos="word" morph="none" start_char="15646" end_char="15649">fort</TOKEN>
<TOKEN id="token-170-28" pos="word" morph="none" start_char="15651" end_char="15657">Detrick</TOKEN>
<TOKEN id="token-170-29" pos="word" morph="none" start_char="15659" end_char="15661">and</TOKEN>
<TOKEN id="token-170-30" pos="word" morph="none" start_char="15663" end_char="15664">so</TOKEN>
<TOKEN id="token-170-31" pos="word" morph="none" start_char="15666" end_char="15667">as</TOKEN>
<TOKEN id="token-170-32" pos="word" morph="none" start_char="15669" end_char="15669">a</TOKEN>
<TOKEN id="token-170-33" pos="word" morph="none" start_char="15671" end_char="15675">means</TOKEN>
<TOKEN id="token-170-34" pos="word" morph="none" start_char="15677" end_char="15678">of</TOKEN>
<TOKEN id="token-170-35" pos="word" morph="none" start_char="15680" end_char="15689">deflection</TOKEN>
<TOKEN id="token-170-36" pos="word" morph="none" start_char="15691" end_char="15692">he</TOKEN>
<TOKEN id="token-170-37" pos="word" morph="none" start_char="15694" end_char="15699">blamed</TOKEN>
<TOKEN id="token-170-38" pos="word" morph="none" start_char="15701" end_char="15705">China</TOKEN>
<TOKEN id="token-170-39" pos="punct" morph="none" start_char="15706" end_char="15706">,</TOKEN>
<TOKEN id="token-170-40" pos="word" morph="none" start_char="15708" end_char="15710">but</TOKEN>
<TOKEN id="token-170-41" pos="word" morph="none" start_char="15712" end_char="15715">also</TOKEN>
<TOKEN id="token-170-42" pos="word" morph="none" start_char="15717" end_char="15718">as</TOKEN>
<TOKEN id="token-170-43" pos="word" morph="none" start_char="15720" end_char="15720">a</TOKEN>
<TOKEN id="token-170-44" pos="word" morph="none" start_char="15722" end_char="15726">means</TOKEN>
<TOKEN id="token-170-45" pos="word" morph="none" start_char="15728" end_char="15729">to</TOKEN>
<TOKEN id="token-170-46" pos="word" morph="none" start_char="15731" end_char="15734">wage</TOKEN>
<TOKEN id="token-170-47" pos="word" morph="none" start_char="15736" end_char="15736">a</TOKEN>
<TOKEN id="token-170-48" pos="word" morph="none" start_char="15738" end_char="15741">cold</TOKEN>
<TOKEN id="token-170-49" pos="word" morph="none" start_char="15743" end_char="15745">war</TOKEN>
<TOKEN id="token-170-50" pos="punct" morph="none" start_char="15746" end_char="15746">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
