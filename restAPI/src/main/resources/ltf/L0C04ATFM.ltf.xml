<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATFM" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="761" raw_text_md5="85e099bc33445eb0fb2fa20032b551e8">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="148">
<ORIGINAL_TEXT>Originally Answered: Since the CDC announced COVID immunity only lasts 3 months, will the immunity from the vaccines in development last any longer?</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="10">Originally</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="12" end_char="19">Answered</TOKEN>
<TOKEN id="token-0-2" pos="punct" morph="none" start_char="20" end_char="20">:</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="22" end_char="26">Since</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="28" end_char="30">the</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="32" end_char="34">CDC</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="36" end_char="44">announced</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="46" end_char="50">COVID</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="52" end_char="59">immunity</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="61" end_char="64">only</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="66" end_char="70">lasts</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="72" end_char="72">3</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="74" end_char="79">months</TOKEN>
<TOKEN id="token-0-13" pos="punct" morph="none" start_char="80" end_char="80">,</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="82" end_char="85">will</TOKEN>
<TOKEN id="token-0-15" pos="word" morph="none" start_char="87" end_char="89">the</TOKEN>
<TOKEN id="token-0-16" pos="word" morph="none" start_char="91" end_char="98">immunity</TOKEN>
<TOKEN id="token-0-17" pos="word" morph="none" start_char="100" end_char="103">from</TOKEN>
<TOKEN id="token-0-18" pos="word" morph="none" start_char="105" end_char="107">the</TOKEN>
<TOKEN id="token-0-19" pos="word" morph="none" start_char="109" end_char="116">vaccines</TOKEN>
<TOKEN id="token-0-20" pos="word" morph="none" start_char="118" end_char="119">in</TOKEN>
<TOKEN id="token-0-21" pos="word" morph="none" start_char="121" end_char="131">development</TOKEN>
<TOKEN id="token-0-22" pos="word" morph="none" start_char="133" end_char="136">last</TOKEN>
<TOKEN id="token-0-23" pos="word" morph="none" start_char="138" end_char="140">any</TOKEN>
<TOKEN id="token-0-24" pos="word" morph="none" start_char="142" end_char="147">longer</TOKEN>
<TOKEN id="token-0-25" pos="punct" morph="none" start_char="148" end_char="148">?</TOKEN>
</SEG>
<SEG id="segment-1" start_char="152" end_char="194">
<ORIGINAL_TEXT>They didn’t say that, and it can’t be true.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="152" end_char="155">They</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="157" end_char="162">didn’t</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="164" end_char="166">say</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="168" end_char="171">that</TOKEN>
<TOKEN id="token-1-4" pos="punct" morph="none" start_char="172" end_char="172">,</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="174" end_char="176">and</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="178" end_char="179">it</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="181" end_char="185">can’t</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="187" end_char="188">be</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="190" end_char="193">true</TOKEN>
<TOKEN id="token-1-10" pos="punct" morph="none" start_char="194" end_char="194">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="197" end_char="458">
<ORIGINAL_TEXT>If immunity lasted only three months for most people we would be seeing a wave of thousands of second-time infections in Italy, Spain, the UK, France, and New York by now from the nearly million people who had tested positive in those places by the end of April.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="197" end_char="198">If</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="200" end_char="207">immunity</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="209" end_char="214">lasted</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="216" end_char="219">only</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="221" end_char="225">three</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="227" end_char="232">months</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="234" end_char="236">for</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="238" end_char="241">most</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="243" end_char="248">people</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="250" end_char="251">we</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="253" end_char="257">would</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="259" end_char="260">be</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="262" end_char="267">seeing</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="269" end_char="269">a</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="271" end_char="274">wave</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="276" end_char="277">of</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="279" end_char="287">thousands</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="289" end_char="290">of</TOKEN>
<TOKEN id="token-2-18" pos="unknown" morph="none" start_char="292" end_char="302">second-time</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="304" end_char="313">infections</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="315" end_char="316">in</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="318" end_char="322">Italy</TOKEN>
<TOKEN id="token-2-22" pos="punct" morph="none" start_char="323" end_char="323">,</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="325" end_char="329">Spain</TOKEN>
<TOKEN id="token-2-24" pos="punct" morph="none" start_char="330" end_char="330">,</TOKEN>
<TOKEN id="token-2-25" pos="word" morph="none" start_char="332" end_char="334">the</TOKEN>
<TOKEN id="token-2-26" pos="word" morph="none" start_char="336" end_char="337">UK</TOKEN>
<TOKEN id="token-2-27" pos="punct" morph="none" start_char="338" end_char="338">,</TOKEN>
<TOKEN id="token-2-28" pos="word" morph="none" start_char="340" end_char="345">France</TOKEN>
<TOKEN id="token-2-29" pos="punct" morph="none" start_char="346" end_char="346">,</TOKEN>
<TOKEN id="token-2-30" pos="word" morph="none" start_char="348" end_char="350">and</TOKEN>
<TOKEN id="token-2-31" pos="word" morph="none" start_char="352" end_char="354">New</TOKEN>
<TOKEN id="token-2-32" pos="word" morph="none" start_char="356" end_char="359">York</TOKEN>
<TOKEN id="token-2-33" pos="word" morph="none" start_char="361" end_char="362">by</TOKEN>
<TOKEN id="token-2-34" pos="word" morph="none" start_char="364" end_char="366">now</TOKEN>
<TOKEN id="token-2-35" pos="word" morph="none" start_char="368" end_char="371">from</TOKEN>
<TOKEN id="token-2-36" pos="word" morph="none" start_char="373" end_char="375">the</TOKEN>
<TOKEN id="token-2-37" pos="word" morph="none" start_char="377" end_char="382">nearly</TOKEN>
<TOKEN id="token-2-38" pos="word" morph="none" start_char="384" end_char="390">million</TOKEN>
<TOKEN id="token-2-39" pos="word" morph="none" start_char="392" end_char="397">people</TOKEN>
<TOKEN id="token-2-40" pos="word" morph="none" start_char="399" end_char="401">who</TOKEN>
<TOKEN id="token-2-41" pos="word" morph="none" start_char="403" end_char="405">had</TOKEN>
<TOKEN id="token-2-42" pos="word" morph="none" start_char="407" end_char="412">tested</TOKEN>
<TOKEN id="token-2-43" pos="word" morph="none" start_char="414" end_char="421">positive</TOKEN>
<TOKEN id="token-2-44" pos="word" morph="none" start_char="423" end_char="424">in</TOKEN>
<TOKEN id="token-2-45" pos="word" morph="none" start_char="426" end_char="430">those</TOKEN>
<TOKEN id="token-2-46" pos="word" morph="none" start_char="432" end_char="437">places</TOKEN>
<TOKEN id="token-2-47" pos="word" morph="none" start_char="439" end_char="440">by</TOKEN>
<TOKEN id="token-2-48" pos="word" morph="none" start_char="442" end_char="444">the</TOKEN>
<TOKEN id="token-2-49" pos="word" morph="none" start_char="446" end_char="448">end</TOKEN>
<TOKEN id="token-2-50" pos="word" morph="none" start_char="450" end_char="451">of</TOKEN>
<TOKEN id="token-2-51" pos="word" morph="none" start_char="453" end_char="457">April</TOKEN>
<TOKEN id="token-2-52" pos="punct" morph="none" start_char="458" end_char="458">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="461" end_char="470">
<ORIGINAL_TEXT>We aren’t.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="461" end_char="462">We</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="464" end_char="469">aren’t</TOKEN>
<TOKEN id="token-3-2" pos="punct" morph="none" start_char="470" end_char="470">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="472" end_char="572">
<ORIGINAL_TEXT>We are seeing a tiny handful of probable laboratory errors in Spain, and no symptomatic reinfections.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="472" end_char="473">We</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="475" end_char="477">are</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="479" end_char="484">seeing</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="486" end_char="486">a</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="488" end_char="491">tiny</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="493" end_char="499">handful</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="501" end_char="502">of</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="504" end_char="511">probable</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="513" end_char="522">laboratory</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="524" end_char="529">errors</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="531" end_char="532">in</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="534" end_char="538">Spain</TOKEN>
<TOKEN id="token-4-12" pos="punct" morph="none" start_char="539" end_char="539">,</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="541" end_char="543">and</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="545" end_char="546">no</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="548" end_char="558">symptomatic</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="560" end_char="571">reinfections</TOKEN>
<TOKEN id="token-4-17" pos="punct" morph="none" start_char="572" end_char="572">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="575" end_char="618">
<ORIGINAL_TEXT>We know that it lasts at least three months.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="575" end_char="576">We</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="578" end_char="581">know</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="583" end_char="586">that</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="588" end_char="589">it</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="591" end_char="595">lasts</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="597" end_char="598">at</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="600" end_char="604">least</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="606" end_char="610">three</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="612" end_char="617">months</TOKEN>
<TOKEN id="token-5-9" pos="punct" morph="none" start_char="618" end_char="618">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="620" end_char="749">
<ORIGINAL_TEXT>In fact, given essentially no reinfections after three months, it almost certainly lasts at least six months for almost everybody.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="620" end_char="621">In</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="623" end_char="626">fact</TOKEN>
<TOKEN id="token-6-2" pos="punct" morph="none" start_char="627" end_char="627">,</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="629" end_char="633">given</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="635" end_char="645">essentially</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="647" end_char="648">no</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="650" end_char="661">reinfections</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="663" end_char="667">after</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="669" end_char="673">three</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="675" end_char="680">months</TOKEN>
<TOKEN id="token-6-10" pos="punct" morph="none" start_char="681" end_char="681">,</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="683" end_char="684">it</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="686" end_char="691">almost</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="693" end_char="701">certainly</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="703" end_char="707">lasts</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="709" end_char="710">at</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="712" end_char="716">least</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="718" end_char="720">six</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="722" end_char="727">months</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="729" end_char="731">for</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="733" end_char="738">almost</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="740" end_char="748">everybody</TOKEN>
<TOKEN id="token-6-22" pos="punct" morph="none" start_char="749" end_char="749">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="751" end_char="757">
<ORIGINAL_TEXT>In thre</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="751" end_char="752">In</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="754" end_char="757">thre</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
