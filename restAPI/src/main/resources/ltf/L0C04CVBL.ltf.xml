<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CVBL" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="4383" raw_text_md5="9c27fa05cd2ccf42f9ad0ecbfd206876">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="79">
<ORIGINAL_TEXT>Nurse who made accusations about horrific NYC COVID patient abuses in hospitals</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="5">Nurse</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="7" end_char="9">who</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="11" end_char="14">made</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="16" end_char="26">accusations</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="28" end_char="32">about</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="34" end_char="41">horrific</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="43" end_char="45">NYC</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="47" end_char="51">COVID</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="53" end_char="59">patient</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="61" end_char="66">abuses</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="68" end_char="69">in</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="71" end_char="79">hospitals</TOKEN>
</SEG>
<SEG id="segment-1" start_char="83" end_char="203">
<ORIGINAL_TEXT>The nurse who made the above accusations is on the State of Nevada Board of Nursing, Nursing Practice Advisory Committee.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="83" end_char="85">The</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="87" end_char="91">nurse</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="93" end_char="95">who</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="97" end_char="100">made</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="102" end_char="104">the</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="106" end_char="110">above</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="112" end_char="122">accusations</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="124" end_char="125">is</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="127" end_char="128">on</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="130" end_char="132">the</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="134" end_char="138">State</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="140" end_char="141">of</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="143" end_char="148">Nevada</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="150" end_char="154">Board</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="156" end_char="157">of</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="159" end_char="165">Nursing</TOKEN>
<TOKEN id="token-1-16" pos="punct" morph="none" start_char="166" end_char="166">,</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="168" end_char="174">Nursing</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="176" end_char="183">Practice</TOKEN>
<TOKEN id="token-1-19" pos="word" morph="none" start_char="185" end_char="192">Advisory</TOKEN>
<TOKEN id="token-1-20" pos="word" morph="none" start_char="194" end_char="202">Committee</TOKEN>
<TOKEN id="token-1-21" pos="punct" morph="none" start_char="203" end_char="203">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="206" end_char="302">
<ORIGINAL_TEXT>Nicole Sirotek, RN, She made the trip to NYC to help when NYC was begging for more medical staff.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="206" end_char="211">Nicole</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="213" end_char="219">Sirotek</TOKEN>
<TOKEN id="token-2-2" pos="punct" morph="none" start_char="220" end_char="220">,</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="222" end_char="223">RN</TOKEN>
<TOKEN id="token-2-4" pos="punct" morph="none" start_char="224" end_char="224">,</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="226" end_char="228">She</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="230" end_char="233">made</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="235" end_char="237">the</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="239" end_char="242">trip</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="244" end_char="245">to</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="247" end_char="249">NYC</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="251" end_char="252">to</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="254" end_char="257">help</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="259" end_char="262">when</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="264" end_char="266">NYC</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="268" end_char="270">was</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="272" end_char="278">begging</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="280" end_char="282">for</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="284" end_char="287">more</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="289" end_char="295">medical</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="297" end_char="301">staff</TOKEN>
<TOKEN id="token-2-21" pos="punct" morph="none" start_char="302" end_char="302">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="304" end_char="360">
<ORIGINAL_TEXT>She deserves to have her warnings heard and investigated.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="304" end_char="306">She</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="308" end_char="315">deserves</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="317" end_char="318">to</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="320" end_char="323">have</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="325" end_char="327">her</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="329" end_char="336">warnings</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="338" end_char="342">heard</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="344" end_char="346">and</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="348" end_char="359">investigated</TOKEN>
<TOKEN id="token-3-9" pos="punct" morph="none" start_char="360" end_char="360">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="362" end_char="399">
<ORIGINAL_TEXT>Not rejected with knee-jerk reactions.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="362" end_char="364">Not</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="366" end_char="373">rejected</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="375" end_char="378">with</TOKEN>
<TOKEN id="token-4-3" pos="unknown" morph="none" start_char="380" end_char="388">knee-jerk</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="390" end_char="398">reactions</TOKEN>
<TOKEN id="token-4-5" pos="punct" morph="none" start_char="399" end_char="399">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="402" end_char="422">
<ORIGINAL_TEXT>"Nobody is listening.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="punct" morph="none" start_char="402" end_char="402">"</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="403" end_char="408">Nobody</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="410" end_char="411">is</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="413" end_char="421">listening</TOKEN>
<TOKEN id="token-5-4" pos="punct" morph="none" start_char="422" end_char="422">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="424" end_char="473">
<ORIGINAL_TEXT>They don't care what is happening to these people.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="424" end_char="427">They</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="429" end_char="433">don't</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="435" end_char="438">care</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="440" end_char="443">what</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="445" end_char="446">is</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="448" end_char="456">happening</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="458" end_char="459">to</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="461" end_char="465">these</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="467" end_char="472">people</TOKEN>
<TOKEN id="token-6-9" pos="punct" morph="none" start_char="473" end_char="473">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="475" end_char="507">
<ORIGINAL_TEXT>They don't." - Nicole Sirotek, RN</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="475" end_char="478">They</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="480" end_char="484">don't</TOKEN>
<TOKEN id="token-7-2" pos="punct" morph="none" start_char="485" end_char="486">."</TOKEN>
<TOKEN id="token-7-3" pos="punct" morph="none" start_char="488" end_char="488">-</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="490" end_char="495">Nicole</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="497" end_char="503">Sirotek</TOKEN>
<TOKEN id="token-7-6" pos="punct" morph="none" start_char="504" end_char="504">,</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="506" end_char="507">RN</TOKEN>
</SEG>
<SEG id="segment-8" start_char="510" end_char="545">
<ORIGINAL_TEXT>Link to Nevada Board of Nursing LINK</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="510" end_char="513">Link</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="515" end_char="516">to</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="518" end_char="523">Nevada</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="525" end_char="529">Board</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="531" end_char="532">of</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="534" end_char="540">Nursing</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="542" end_char="545">LINK</TOKEN>
</SEG>
<SEG id="segment-9" start_char="548" end_char="600">
<ORIGINAL_TEXT>Link to her video describing NYC hospital abses: LINK</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="548" end_char="551">Link</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="553" end_char="554">to</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="556" end_char="558">her</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="560" end_char="564">video</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="566" end_char="575">describing</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="577" end_char="579">NYC</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="581" end_char="588">hospital</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="590" end_char="594">abses</TOKEN>
<TOKEN id="token-9-8" pos="punct" morph="none" start_char="595" end_char="595">:</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="597" end_char="600">LINK</TOKEN>
</SEG>
<SEG id="segment-10" start_char="603" end_char="641">
<ORIGINAL_TEXT>This post was edited on 5/5 at 12:35 pm</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="603" end_char="606">This</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="608" end_char="611">post</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="613" end_char="615">was</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="617" end_char="622">edited</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="624" end_char="625">on</TOKEN>
<TOKEN id="token-10-5" pos="unknown" morph="none" start_char="627" end_char="629">5/5</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="631" end_char="632">at</TOKEN>
<TOKEN id="token-10-7" pos="unknown" morph="none" start_char="634" end_char="638">12:35</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="640" end_char="641">pm</TOKEN>
</SEG>
<SEG id="segment-11" start_char="645" end_char="677">
<ORIGINAL_TEXT>That still does not equal doctor.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="645" end_char="648">That</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="650" end_char="654">still</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="656" end_char="659">does</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="661" end_char="663">not</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="665" end_char="669">equal</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="671" end_char="676">doctor</TOKEN>
<TOKEN id="token-11-6" pos="punct" morph="none" start_char="677" end_char="677">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="679" end_char="707">
<ORIGINAL_TEXT>Does she deserve to be heard?</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="679" end_char="682">Does</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="684" end_char="686">she</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="688" end_char="694">deserve</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="696" end_char="697">to</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="699" end_char="700">be</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="702" end_char="706">heard</TOKEN>
<TOKEN id="token-12-6" pos="punct" morph="none" start_char="707" end_char="707">?</TOKEN>
</SEG>
<SEG id="segment-13" start_char="709" end_char="717">
<ORIGINAL_TEXT>Certainly</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="709" end_char="717">Certainly</TOKEN>
</SEG>
<SEG id="segment-14" start_char="721" end_char="816">
<ORIGINAL_TEXT>It puts her a cut above the Nurse Jackie Tik Tok crew who are the face of nursing at the moment.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="721" end_char="722">It</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="724" end_char="727">puts</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="729" end_char="731">her</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="733" end_char="733">a</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="735" end_char="737">cut</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="739" end_char="743">above</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="745" end_char="747">the</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="749" end_char="753">Nurse</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="755" end_char="760">Jackie</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="762" end_char="764">Tik</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="766" end_char="768">Tok</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="770" end_char="773">crew</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="775" end_char="777">who</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="779" end_char="781">are</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="783" end_char="785">the</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="787" end_char="790">face</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="792" end_char="793">of</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="795" end_char="801">nursing</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="803" end_char="804">at</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="806" end_char="808">the</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="810" end_char="815">moment</TOKEN>
<TOKEN id="token-14-21" pos="punct" morph="none" start_char="816" end_char="816">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="821" end_char="858">
<ORIGINAL_TEXT>quote:That still does not equal doctor</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="unknown" morph="none" start_char="821" end_char="830">quote:That</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="832" end_char="836">still</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="838" end_char="841">does</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="843" end_char="845">not</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="847" end_char="851">equal</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="853" end_char="858">doctor</TOKEN>
</SEG>
<SEG id="segment-16" start_char="861" end_char="932">
<ORIGINAL_TEXT>You don't have to be a doctor to recognize a death camp mentality, Slick</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="861" end_char="863">You</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="865" end_char="869">don't</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="871" end_char="874">have</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="876" end_char="877">to</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="879" end_char="880">be</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="882" end_char="882">a</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="884" end_char="889">doctor</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="891" end_char="892">to</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="894" end_char="902">recognize</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="904" end_char="904">a</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="906" end_char="910">death</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="912" end_char="915">camp</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="917" end_char="925">mentality</TOKEN>
<TOKEN id="token-16-13" pos="punct" morph="none" start_char="926" end_char="926">,</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="928" end_char="932">Slick</TOKEN>
</SEG>
<SEG id="segment-17" start_char="937" end_char="979">
<ORIGINAL_TEXT>Then she needs to clean up her nasty mouth.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="937" end_char="940">Then</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="942" end_char="944">she</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="946" end_char="950">needs</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="952" end_char="953">to</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="955" end_char="959">clean</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="961" end_char="962">up</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="964" end_char="966">her</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="968" end_char="972">nasty</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="974" end_char="978">mouth</TOKEN>
<TOKEN id="token-17-9" pos="punct" morph="none" start_char="979" end_char="979">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="981" end_char="1069">
<ORIGINAL_TEXT>IF she is on a nursing board then she should know the proper channels to make complaints.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="981" end_char="982">IF</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="984" end_char="986">she</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="988" end_char="989">is</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="991" end_char="992">on</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="994" end_char="994">a</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="996" end_char="1002">nursing</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="1004" end_char="1008">board</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="1010" end_char="1013">then</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="1015" end_char="1017">she</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="1019" end_char="1024">should</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="1026" end_char="1029">know</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="1031" end_char="1033">the</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="1035" end_char="1040">proper</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="1042" end_char="1049">channels</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="1051" end_char="1052">to</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="1054" end_char="1057">make</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="1059" end_char="1068">complaints</TOKEN>
<TOKEN id="token-18-17" pos="punct" morph="none" start_char="1069" end_char="1069">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="1071" end_char="1135">
<ORIGINAL_TEXT>Certainly an uncensored you tube video is not the proper channel.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="1071" end_char="1079">Certainly</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="1081" end_char="1082">an</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="1084" end_char="1093">uncensored</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="1095" end_char="1097">you</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="1099" end_char="1102">tube</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="1104" end_char="1108">video</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="1110" end_char="1111">is</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="1113" end_char="1115">not</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="1117" end_char="1119">the</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="1121" end_char="1126">proper</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="1128" end_char="1134">channel</TOKEN>
<TOKEN id="token-19-11" pos="punct" morph="none" start_char="1135" end_char="1135">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="1140" end_char="1178">
<ORIGINAL_TEXT>quote:That still does not equal doctor.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="unknown" morph="none" start_char="1140" end_char="1149">quote:That</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="1151" end_char="1155">still</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="1157" end_char="1160">does</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="1162" end_char="1164">not</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="1166" end_char="1170">equal</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="1172" end_char="1177">doctor</TOKEN>
<TOKEN id="token-20-6" pos="punct" morph="none" start_char="1178" end_char="1178">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="1180" end_char="1208">
<ORIGINAL_TEXT>Does she deserve to be heard?</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="1180" end_char="1183">Does</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="1185" end_char="1187">she</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="1189" end_char="1195">deserve</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="1197" end_char="1198">to</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="1200" end_char="1201">be</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="1203" end_char="1207">heard</TOKEN>
<TOKEN id="token-21-6" pos="punct" morph="none" start_char="1208" end_char="1208">?</TOKEN>
</SEG>
<SEG id="segment-22" start_char="1210" end_char="1218">
<ORIGINAL_TEXT>Certainly</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="1210" end_char="1218">Certainly</TOKEN>
</SEG>
<SEG id="segment-23" start_char="1221" end_char="1253">
<ORIGINAL_TEXT>Help me understand your position.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="1221" end_char="1224">Help</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="1226" end_char="1227">me</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="1229" end_char="1238">understand</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="1240" end_char="1243">your</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="1245" end_char="1252">position</TOKEN>
<TOKEN id="token-23-5" pos="punct" morph="none" start_char="1253" end_char="1253">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="1255" end_char="1332">
<ORIGINAL_TEXT>Why does one need to be a doctor to witness abuses and be able to report them?</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="1255" end_char="1257">Why</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="1259" end_char="1262">does</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="1264" end_char="1266">one</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="1268" end_char="1271">need</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="1273" end_char="1274">to</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="1276" end_char="1277">be</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="1279" end_char="1279">a</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="1281" end_char="1286">doctor</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="1288" end_char="1289">to</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="1291" end_char="1297">witness</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="1299" end_char="1304">abuses</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="1306" end_char="1308">and</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="1310" end_char="1311">be</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="1313" end_char="1316">able</TOKEN>
<TOKEN id="token-24-14" pos="word" morph="none" start_char="1318" end_char="1319">to</TOKEN>
<TOKEN id="token-24-15" pos="word" morph="none" start_char="1321" end_char="1326">report</TOKEN>
<TOKEN id="token-24-16" pos="word" morph="none" start_char="1328" end_char="1331">them</TOKEN>
<TOKEN id="token-24-17" pos="punct" morph="none" start_char="1332" end_char="1332">?</TOKEN>
</SEG>
<SEG id="segment-25" start_char="1334" end_char="1399">
<ORIGINAL_TEXT>Nurses are supposed to be familiar with procedures and treatments.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="1334" end_char="1339">Nurses</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="1341" end_char="1343">are</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="1345" end_char="1352">supposed</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="1354" end_char="1355">to</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="1357" end_char="1358">be</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="1360" end_char="1367">familiar</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="1369" end_char="1372">with</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="1374" end_char="1383">procedures</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="1385" end_char="1387">and</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="1389" end_char="1398">treatments</TOKEN>
<TOKEN id="token-25-10" pos="punct" morph="none" start_char="1399" end_char="1399">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="1401" end_char="1519">
<ORIGINAL_TEXT>So if something stood out as an obvious infraction to her, I don't know why we need to qualify that she isn't a doctor.</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="1401" end_char="1402">So</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="1404" end_char="1405">if</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="1407" end_char="1415">something</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="1417" end_char="1421">stood</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="1423" end_char="1425">out</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="1427" end_char="1428">as</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="1430" end_char="1431">an</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="1433" end_char="1439">obvious</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="1441" end_char="1450">infraction</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="1452" end_char="1453">to</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="1455" end_char="1457">her</TOKEN>
<TOKEN id="token-26-11" pos="punct" morph="none" start_char="1458" end_char="1458">,</TOKEN>
<TOKEN id="token-26-12" pos="word" morph="none" start_char="1460" end_char="1460">I</TOKEN>
<TOKEN id="token-26-13" pos="word" morph="none" start_char="1462" end_char="1466">don't</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="1468" end_char="1471">know</TOKEN>
<TOKEN id="token-26-15" pos="word" morph="none" start_char="1473" end_char="1475">why</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="1477" end_char="1478">we</TOKEN>
<TOKEN id="token-26-17" pos="word" morph="none" start_char="1480" end_char="1483">need</TOKEN>
<TOKEN id="token-26-18" pos="word" morph="none" start_char="1485" end_char="1486">to</TOKEN>
<TOKEN id="token-26-19" pos="word" morph="none" start_char="1488" end_char="1494">qualify</TOKEN>
<TOKEN id="token-26-20" pos="word" morph="none" start_char="1496" end_char="1499">that</TOKEN>
<TOKEN id="token-26-21" pos="word" morph="none" start_char="1501" end_char="1503">she</TOKEN>
<TOKEN id="token-26-22" pos="word" morph="none" start_char="1505" end_char="1509">isn't</TOKEN>
<TOKEN id="token-26-23" pos="word" morph="none" start_char="1511" end_char="1511">a</TOKEN>
<TOKEN id="token-26-24" pos="word" morph="none" start_char="1513" end_char="1518">doctor</TOKEN>
<TOKEN id="token-26-25" pos="punct" morph="none" start_char="1519" end_char="1519">.</TOKEN>
</SEG>
<SEG id="segment-27" start_char="1525" end_char="1626">
<ORIGINAL_TEXT>quote:It puts her a cut above the Nurse Jackie Tik Tok crew who are the face of nursing at the moment.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="unknown" morph="none" start_char="1525" end_char="1532">quote:It</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="1534" end_char="1537">puts</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="1539" end_char="1541">her</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="1543" end_char="1543">a</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="1545" end_char="1547">cut</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="1549" end_char="1553">above</TOKEN>
<TOKEN id="token-27-6" pos="word" morph="none" start_char="1555" end_char="1557">the</TOKEN>
<TOKEN id="token-27-7" pos="word" morph="none" start_char="1559" end_char="1563">Nurse</TOKEN>
<TOKEN id="token-27-8" pos="word" morph="none" start_char="1565" end_char="1570">Jackie</TOKEN>
<TOKEN id="token-27-9" pos="word" morph="none" start_char="1572" end_char="1574">Tik</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="1576" end_char="1578">Tok</TOKEN>
<TOKEN id="token-27-11" pos="word" morph="none" start_char="1580" end_char="1583">crew</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="1585" end_char="1587">who</TOKEN>
<TOKEN id="token-27-13" pos="word" morph="none" start_char="1589" end_char="1591">are</TOKEN>
<TOKEN id="token-27-14" pos="word" morph="none" start_char="1593" end_char="1595">the</TOKEN>
<TOKEN id="token-27-15" pos="word" morph="none" start_char="1597" end_char="1600">face</TOKEN>
<TOKEN id="token-27-16" pos="word" morph="none" start_char="1602" end_char="1603">of</TOKEN>
<TOKEN id="token-27-17" pos="word" morph="none" start_char="1605" end_char="1611">nursing</TOKEN>
<TOKEN id="token-27-18" pos="word" morph="none" start_char="1613" end_char="1614">at</TOKEN>
<TOKEN id="token-27-19" pos="word" morph="none" start_char="1616" end_char="1618">the</TOKEN>
<TOKEN id="token-27-20" pos="word" morph="none" start_char="1620" end_char="1625">moment</TOKEN>
<TOKEN id="token-27-21" pos="punct" morph="none" start_char="1626" end_char="1626">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="1629" end_char="1632">
<ORIGINAL_TEXT>Why?</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="1629" end_char="1631">Why</TOKEN>
<TOKEN id="token-28-1" pos="punct" morph="none" start_char="1632" end_char="1632">?</TOKEN>
</SEG>
<SEG id="segment-29" start_char="1634" end_char="1713">
<ORIGINAL_TEXT>Are doctors doing similar things to patients in hospitals you are familiar with?</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="1634" end_char="1636">Are</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="1638" end_char="1644">doctors</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="1646" end_char="1650">doing</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="1652" end_char="1658">similar</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="1660" end_char="1665">things</TOKEN>
<TOKEN id="token-29-5" pos="word" morph="none" start_char="1667" end_char="1668">to</TOKEN>
<TOKEN id="token-29-6" pos="word" morph="none" start_char="1670" end_char="1677">patients</TOKEN>
<TOKEN id="token-29-7" pos="word" morph="none" start_char="1679" end_char="1680">in</TOKEN>
<TOKEN id="token-29-8" pos="word" morph="none" start_char="1682" end_char="1690">hospitals</TOKEN>
<TOKEN id="token-29-9" pos="word" morph="none" start_char="1692" end_char="1694">you</TOKEN>
<TOKEN id="token-29-10" pos="word" morph="none" start_char="1696" end_char="1698">are</TOKEN>
<TOKEN id="token-29-11" pos="word" morph="none" start_char="1700" end_char="1707">familiar</TOKEN>
<TOKEN id="token-29-12" pos="word" morph="none" start_char="1709" end_char="1712">with</TOKEN>
<TOKEN id="token-29-13" pos="punct" morph="none" start_char="1713" end_char="1713">?</TOKEN>
</SEG>
<SEG id="segment-30" start_char="1715" end_char="1758">
<ORIGINAL_TEXT>Standard procedure in 2020 medical practice?</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="1715" end_char="1722">Standard</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="1724" end_char="1732">procedure</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="1734" end_char="1735">in</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="1737" end_char="1740">2020</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="1742" end_char="1748">medical</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="1750" end_char="1757">practice</TOKEN>
<TOKEN id="token-30-6" pos="punct" morph="none" start_char="1758" end_char="1758">?</TOKEN>
</SEG>
<SEG id="segment-31" start_char="1761" end_char="1809">
<ORIGINAL_TEXT>At least she has the raw courage to warn a nation</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="word" morph="none" start_char="1761" end_char="1762">At</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="1764" end_char="1768">least</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="1770" end_char="1772">she</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="1774" end_char="1776">has</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="1778" end_char="1780">the</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="1782" end_char="1784">raw</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="1786" end_char="1792">courage</TOKEN>
<TOKEN id="token-31-7" pos="word" morph="none" start_char="1794" end_char="1795">to</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="1797" end_char="1800">warn</TOKEN>
<TOKEN id="token-31-9" pos="word" morph="none" start_char="1802" end_char="1802">a</TOKEN>
<TOKEN id="token-31-10" pos="word" morph="none" start_char="1804" end_char="1809">nation</TOKEN>
</SEG>
<SEG id="segment-32" start_char="1812" end_char="1850">
<ORIGINAL_TEXT>This post was edited on 5/5 at 12:10 pm</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="1812" end_char="1815">This</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="1817" end_char="1820">post</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="1822" end_char="1824">was</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="1826" end_char="1831">edited</TOKEN>
<TOKEN id="token-32-4" pos="word" morph="none" start_char="1833" end_char="1834">on</TOKEN>
<TOKEN id="token-32-5" pos="unknown" morph="none" start_char="1836" end_char="1838">5/5</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="1840" end_char="1841">at</TOKEN>
<TOKEN id="token-32-7" pos="unknown" morph="none" start_char="1843" end_char="1847">12:10</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="1849" end_char="1850">pm</TOKEN>
</SEG>
<SEG id="segment-33" start_char="1855" end_char="1910">
<ORIGINAL_TEXT>Is this the woman accusing NY hospitals of being racist?</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="word" morph="none" start_char="1855" end_char="1856">Is</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="1858" end_char="1861">this</TOKEN>
<TOKEN id="token-33-2" pos="word" morph="none" start_char="1863" end_char="1865">the</TOKEN>
<TOKEN id="token-33-3" pos="word" morph="none" start_char="1867" end_char="1871">woman</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="1873" end_char="1880">accusing</TOKEN>
<TOKEN id="token-33-5" pos="word" morph="none" start_char="1882" end_char="1883">NY</TOKEN>
<TOKEN id="token-33-6" pos="word" morph="none" start_char="1885" end_char="1893">hospitals</TOKEN>
<TOKEN id="token-33-7" pos="word" morph="none" start_char="1895" end_char="1896">of</TOKEN>
<TOKEN id="token-33-8" pos="word" morph="none" start_char="1898" end_char="1902">being</TOKEN>
<TOKEN id="token-33-9" pos="word" morph="none" start_char="1904" end_char="1909">racist</TOKEN>
<TOKEN id="token-33-10" pos="punct" morph="none" start_char="1910" end_char="1910">?</TOKEN>
</SEG>
<SEG id="segment-34" start_char="1915" end_char="1953">
<ORIGINAL_TEXT>quote:That still does not equal doctor.</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="unknown" morph="none" start_char="1915" end_char="1924">quote:That</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="1926" end_char="1930">still</TOKEN>
<TOKEN id="token-34-2" pos="word" morph="none" start_char="1932" end_char="1935">does</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="1937" end_char="1939">not</TOKEN>
<TOKEN id="token-34-4" pos="word" morph="none" start_char="1941" end_char="1945">equal</TOKEN>
<TOKEN id="token-34-5" pos="word" morph="none" start_char="1947" end_char="1952">doctor</TOKEN>
<TOKEN id="token-34-6" pos="punct" morph="none" start_char="1953" end_char="1953">.</TOKEN>
</SEG>
<SEG id="segment-35" start_char="1955" end_char="1983">
<ORIGINAL_TEXT>Does she deserve to be heard?</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="1955" end_char="1958">Does</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="1960" end_char="1962">she</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="1964" end_char="1970">deserve</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="1972" end_char="1973">to</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="1975" end_char="1976">be</TOKEN>
<TOKEN id="token-35-5" pos="word" morph="none" start_char="1978" end_char="1982">heard</TOKEN>
<TOKEN id="token-35-6" pos="punct" morph="none" start_char="1983" end_char="1983">?</TOKEN>
</SEG>
<SEG id="segment-36" start_char="1985" end_char="1993">
<ORIGINAL_TEXT>Certainly</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="1985" end_char="1993">Certainly</TOKEN>
</SEG>
<SEG id="segment-37" start_char="1996" end_char="2089">
<ORIGINAL_TEXT>The opinion of a experienced RN on the treatment of patients is probably more valid than a MD.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="1996" end_char="1998">The</TOKEN>
<TOKEN id="token-37-1" pos="word" morph="none" start_char="2000" end_char="2006">opinion</TOKEN>
<TOKEN id="token-37-2" pos="word" morph="none" start_char="2008" end_char="2009">of</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="2011" end_char="2011">a</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="2013" end_char="2023">experienced</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="2025" end_char="2026">RN</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="2028" end_char="2029">on</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="2031" end_char="2033">the</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="2035" end_char="2043">treatment</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="2045" end_char="2046">of</TOKEN>
<TOKEN id="token-37-10" pos="word" morph="none" start_char="2048" end_char="2055">patients</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="2057" end_char="2058">is</TOKEN>
<TOKEN id="token-37-12" pos="word" morph="none" start_char="2060" end_char="2067">probably</TOKEN>
<TOKEN id="token-37-13" pos="word" morph="none" start_char="2069" end_char="2072">more</TOKEN>
<TOKEN id="token-37-14" pos="word" morph="none" start_char="2074" end_char="2078">valid</TOKEN>
<TOKEN id="token-37-15" pos="word" morph="none" start_char="2080" end_char="2083">than</TOKEN>
<TOKEN id="token-37-16" pos="word" morph="none" start_char="2085" end_char="2085">a</TOKEN>
<TOKEN id="token-37-17" pos="word" morph="none" start_char="2087" end_char="2088">MD</TOKEN>
<TOKEN id="token-37-18" pos="punct" morph="none" start_char="2089" end_char="2089">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="2094" end_char="2142">
<ORIGINAL_TEXT>I would expect a higher level of professionalism.</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="2094" end_char="2094">I</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="2096" end_char="2100">would</TOKEN>
<TOKEN id="token-38-2" pos="word" morph="none" start_char="2102" end_char="2107">expect</TOKEN>
<TOKEN id="token-38-3" pos="word" morph="none" start_char="2109" end_char="2109">a</TOKEN>
<TOKEN id="token-38-4" pos="word" morph="none" start_char="2111" end_char="2116">higher</TOKEN>
<TOKEN id="token-38-5" pos="word" morph="none" start_char="2118" end_char="2122">level</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="2124" end_char="2125">of</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="2127" end_char="2141">professionalism</TOKEN>
<TOKEN id="token-38-8" pos="punct" morph="none" start_char="2142" end_char="2142">.</TOKEN>
</SEG>
<SEG id="segment-39" start_char="2144" end_char="2218">
<ORIGINAL_TEXT>In her video she immediately stated that the oversight agencies don’t care.</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="word" morph="none" start_char="2144" end_char="2145">In</TOKEN>
<TOKEN id="token-39-1" pos="word" morph="none" start_char="2147" end_char="2149">her</TOKEN>
<TOKEN id="token-39-2" pos="word" morph="none" start_char="2151" end_char="2155">video</TOKEN>
<TOKEN id="token-39-3" pos="word" morph="none" start_char="2157" end_char="2159">she</TOKEN>
<TOKEN id="token-39-4" pos="word" morph="none" start_char="2161" end_char="2171">immediately</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="2173" end_char="2178">stated</TOKEN>
<TOKEN id="token-39-6" pos="word" morph="none" start_char="2180" end_char="2183">that</TOKEN>
<TOKEN id="token-39-7" pos="word" morph="none" start_char="2185" end_char="2187">the</TOKEN>
<TOKEN id="token-39-8" pos="word" morph="none" start_char="2189" end_char="2197">oversight</TOKEN>
<TOKEN id="token-39-9" pos="word" morph="none" start_char="2199" end_char="2206">agencies</TOKEN>
<TOKEN id="token-39-10" pos="word" morph="none" start_char="2208" end_char="2212">don’t</TOKEN>
<TOKEN id="token-39-11" pos="word" morph="none" start_char="2214" end_char="2217">care</TOKEN>
<TOKEN id="token-39-12" pos="punct" morph="none" start_char="2218" end_char="2218">.</TOKEN>
</SEG>
<SEG id="segment-40" start_char="2220" end_char="2253">
<ORIGINAL_TEXT>Has she approached those agencies?</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="word" morph="none" start_char="2220" end_char="2222">Has</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="2224" end_char="2226">she</TOKEN>
<TOKEN id="token-40-2" pos="word" morph="none" start_char="2228" end_char="2237">approached</TOKEN>
<TOKEN id="token-40-3" pos="word" morph="none" start_char="2239" end_char="2243">those</TOKEN>
<TOKEN id="token-40-4" pos="word" morph="none" start_char="2245" end_char="2252">agencies</TOKEN>
<TOKEN id="token-40-5" pos="punct" morph="none" start_char="2253" end_char="2253">?</TOKEN>
</SEG>
<SEG id="segment-41" start_char="2255" end_char="2297">
<ORIGINAL_TEXT>Does she have documented complaints logged?</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="2255" end_char="2258">Does</TOKEN>
<TOKEN id="token-41-1" pos="word" morph="none" start_char="2260" end_char="2262">she</TOKEN>
<TOKEN id="token-41-2" pos="word" morph="none" start_char="2264" end_char="2267">have</TOKEN>
<TOKEN id="token-41-3" pos="word" morph="none" start_char="2269" end_char="2278">documented</TOKEN>
<TOKEN id="token-41-4" pos="word" morph="none" start_char="2280" end_char="2289">complaints</TOKEN>
<TOKEN id="token-41-5" pos="word" morph="none" start_char="2291" end_char="2296">logged</TOKEN>
<TOKEN id="token-41-6" pos="punct" morph="none" start_char="2297" end_char="2297">?</TOKEN>
</SEG>
<SEG id="segment-42" start_char="2300" end_char="2362">
<ORIGINAL_TEXT>If not did she go strait to posting videos for internet points?</ORIGINAL_TEXT>
<TOKEN id="token-42-0" pos="word" morph="none" start_char="2300" end_char="2301">If</TOKEN>
<TOKEN id="token-42-1" pos="word" morph="none" start_char="2303" end_char="2305">not</TOKEN>
<TOKEN id="token-42-2" pos="word" morph="none" start_char="2307" end_char="2309">did</TOKEN>
<TOKEN id="token-42-3" pos="word" morph="none" start_char="2311" end_char="2313">she</TOKEN>
<TOKEN id="token-42-4" pos="word" morph="none" start_char="2315" end_char="2316">go</TOKEN>
<TOKEN id="token-42-5" pos="word" morph="none" start_char="2318" end_char="2323">strait</TOKEN>
<TOKEN id="token-42-6" pos="word" morph="none" start_char="2325" end_char="2326">to</TOKEN>
<TOKEN id="token-42-7" pos="word" morph="none" start_char="2328" end_char="2334">posting</TOKEN>
<TOKEN id="token-42-8" pos="word" morph="none" start_char="2336" end_char="2341">videos</TOKEN>
<TOKEN id="token-42-9" pos="word" morph="none" start_char="2343" end_char="2345">for</TOKEN>
<TOKEN id="token-42-10" pos="word" morph="none" start_char="2347" end_char="2354">internet</TOKEN>
<TOKEN id="token-42-11" pos="word" morph="none" start_char="2356" end_char="2361">points</TOKEN>
<TOKEN id="token-42-12" pos="punct" morph="none" start_char="2362" end_char="2362">?</TOKEN>
</SEG>
<SEG id="segment-43" start_char="2366" end_char="2397">
<ORIGINAL_TEXT>She is on a State Nursing board.</ORIGINAL_TEXT>
<TOKEN id="token-43-0" pos="word" morph="none" start_char="2366" end_char="2368">She</TOKEN>
<TOKEN id="token-43-1" pos="word" morph="none" start_char="2370" end_char="2371">is</TOKEN>
<TOKEN id="token-43-2" pos="word" morph="none" start_char="2373" end_char="2374">on</TOKEN>
<TOKEN id="token-43-3" pos="word" morph="none" start_char="2376" end_char="2376">a</TOKEN>
<TOKEN id="token-43-4" pos="word" morph="none" start_char="2378" end_char="2382">State</TOKEN>
<TOKEN id="token-43-5" pos="word" morph="none" start_char="2384" end_char="2390">Nursing</TOKEN>
<TOKEN id="token-43-6" pos="word" morph="none" start_char="2392" end_char="2396">board</TOKEN>
<TOKEN id="token-43-7" pos="punct" morph="none" start_char="2397" end_char="2397">.</TOKEN>
</SEG>
<SEG id="segment-44" start_char="2399" end_char="2473">
<ORIGINAL_TEXT>I would guess you have never been through a Joint Commission or CMS survey.</ORIGINAL_TEXT>
<TOKEN id="token-44-0" pos="word" morph="none" start_char="2399" end_char="2399">I</TOKEN>
<TOKEN id="token-44-1" pos="word" morph="none" start_char="2401" end_char="2405">would</TOKEN>
<TOKEN id="token-44-2" pos="word" morph="none" start_char="2407" end_char="2411">guess</TOKEN>
<TOKEN id="token-44-3" pos="word" morph="none" start_char="2413" end_char="2415">you</TOKEN>
<TOKEN id="token-44-4" pos="word" morph="none" start_char="2417" end_char="2420">have</TOKEN>
<TOKEN id="token-44-5" pos="word" morph="none" start_char="2422" end_char="2426">never</TOKEN>
<TOKEN id="token-44-6" pos="word" morph="none" start_char="2428" end_char="2431">been</TOKEN>
<TOKEN id="token-44-7" pos="word" morph="none" start_char="2433" end_char="2439">through</TOKEN>
<TOKEN id="token-44-8" pos="word" morph="none" start_char="2441" end_char="2441">a</TOKEN>
<TOKEN id="token-44-9" pos="word" morph="none" start_char="2443" end_char="2447">Joint</TOKEN>
<TOKEN id="token-44-10" pos="word" morph="none" start_char="2449" end_char="2458">Commission</TOKEN>
<TOKEN id="token-44-11" pos="word" morph="none" start_char="2460" end_char="2461">or</TOKEN>
<TOKEN id="token-44-12" pos="word" morph="none" start_char="2463" end_char="2465">CMS</TOKEN>
<TOKEN id="token-44-13" pos="word" morph="none" start_char="2467" end_char="2472">survey</TOKEN>
<TOKEN id="token-44-14" pos="punct" morph="none" start_char="2473" end_char="2473">.</TOKEN>
</SEG>
<SEG id="segment-45" start_char="2475" end_char="2525">
<ORIGINAL_TEXT>As a Director of Pharmacy I have been through many.</ORIGINAL_TEXT>
<TOKEN id="token-45-0" pos="word" morph="none" start_char="2475" end_char="2476">As</TOKEN>
<TOKEN id="token-45-1" pos="word" morph="none" start_char="2478" end_char="2478">a</TOKEN>
<TOKEN id="token-45-2" pos="word" morph="none" start_char="2480" end_char="2487">Director</TOKEN>
<TOKEN id="token-45-3" pos="word" morph="none" start_char="2489" end_char="2490">of</TOKEN>
<TOKEN id="token-45-4" pos="word" morph="none" start_char="2492" end_char="2499">Pharmacy</TOKEN>
<TOKEN id="token-45-5" pos="word" morph="none" start_char="2501" end_char="2501">I</TOKEN>
<TOKEN id="token-45-6" pos="word" morph="none" start_char="2503" end_char="2506">have</TOKEN>
<TOKEN id="token-45-7" pos="word" morph="none" start_char="2508" end_char="2511">been</TOKEN>
<TOKEN id="token-45-8" pos="word" morph="none" start_char="2513" end_char="2519">through</TOKEN>
<TOKEN id="token-45-9" pos="word" morph="none" start_char="2521" end_char="2524">many</TOKEN>
<TOKEN id="token-45-10" pos="punct" morph="none" start_char="2525" end_char="2525">.</TOKEN>
</SEG>
<SEG id="segment-46" start_char="2527" end_char="2565">
<ORIGINAL_TEXT>Nursing Leadership is heavily involved.</ORIGINAL_TEXT>
<TOKEN id="token-46-0" pos="word" morph="none" start_char="2527" end_char="2533">Nursing</TOKEN>
<TOKEN id="token-46-1" pos="word" morph="none" start_char="2535" end_char="2544">Leadership</TOKEN>
<TOKEN id="token-46-2" pos="word" morph="none" start_char="2546" end_char="2547">is</TOKEN>
<TOKEN id="token-46-3" pos="word" morph="none" start_char="2549" end_char="2555">heavily</TOKEN>
<TOKEN id="token-46-4" pos="word" morph="none" start_char="2557" end_char="2564">involved</TOKEN>
<TOKEN id="token-46-5" pos="punct" morph="none" start_char="2565" end_char="2565">.</TOKEN>
</SEG>
<SEG id="segment-47" start_char="2567" end_char="2607">
<ORIGINAL_TEXT>They know plenty about Standards of Care.</ORIGINAL_TEXT>
<TOKEN id="token-47-0" pos="word" morph="none" start_char="2567" end_char="2570">They</TOKEN>
<TOKEN id="token-47-1" pos="word" morph="none" start_char="2572" end_char="2575">know</TOKEN>
<TOKEN id="token-47-2" pos="word" morph="none" start_char="2577" end_char="2582">plenty</TOKEN>
<TOKEN id="token-47-3" pos="word" morph="none" start_char="2584" end_char="2588">about</TOKEN>
<TOKEN id="token-47-4" pos="word" morph="none" start_char="2590" end_char="2598">Standards</TOKEN>
<TOKEN id="token-47-5" pos="word" morph="none" start_char="2600" end_char="2601">of</TOKEN>
<TOKEN id="token-47-6" pos="word" morph="none" start_char="2603" end_char="2606">Care</TOKEN>
<TOKEN id="token-47-7" pos="punct" morph="none" start_char="2607" end_char="2607">.</TOKEN>
</SEG>
<SEG id="segment-48" start_char="2612" end_char="2673">
<ORIGINAL_TEXT>quote:Is this the woman accusing NY hospitals of being racist?</ORIGINAL_TEXT>
<TOKEN id="token-48-0" pos="unknown" morph="none" start_char="2612" end_char="2619">quote:Is</TOKEN>
<TOKEN id="token-48-1" pos="word" morph="none" start_char="2621" end_char="2624">this</TOKEN>
<TOKEN id="token-48-2" pos="word" morph="none" start_char="2626" end_char="2628">the</TOKEN>
<TOKEN id="token-48-3" pos="word" morph="none" start_char="2630" end_char="2634">woman</TOKEN>
<TOKEN id="token-48-4" pos="word" morph="none" start_char="2636" end_char="2643">accusing</TOKEN>
<TOKEN id="token-48-5" pos="word" morph="none" start_char="2645" end_char="2646">NY</TOKEN>
<TOKEN id="token-48-6" pos="word" morph="none" start_char="2648" end_char="2656">hospitals</TOKEN>
<TOKEN id="token-48-7" pos="word" morph="none" start_char="2658" end_char="2659">of</TOKEN>
<TOKEN id="token-48-8" pos="word" morph="none" start_char="2661" end_char="2665">being</TOKEN>
<TOKEN id="token-48-9" pos="word" morph="none" start_char="2667" end_char="2672">racist</TOKEN>
<TOKEN id="token-48-10" pos="punct" morph="none" start_char="2673" end_char="2673">?</TOKEN>
</SEG>
<SEG id="segment-49" start_char="2676" end_char="2678">
<ORIGINAL_TEXT>NO.</ORIGINAL_TEXT>
<TOKEN id="token-49-0" pos="word" morph="none" start_char="2676" end_char="2677">NO</TOKEN>
<TOKEN id="token-49-1" pos="punct" morph="none" start_char="2678" end_char="2678">.</TOKEN>
</SEG>
<SEG id="segment-50" start_char="2680" end_char="2748">
<ORIGINAL_TEXT>She is the nurse who is accusing NYC hospitals of Murdering patients.</ORIGINAL_TEXT>
<TOKEN id="token-50-0" pos="word" morph="none" start_char="2680" end_char="2682">She</TOKEN>
<TOKEN id="token-50-1" pos="word" morph="none" start_char="2684" end_char="2685">is</TOKEN>
<TOKEN id="token-50-2" pos="word" morph="none" start_char="2687" end_char="2689">the</TOKEN>
<TOKEN id="token-50-3" pos="word" morph="none" start_char="2691" end_char="2695">nurse</TOKEN>
<TOKEN id="token-50-4" pos="word" morph="none" start_char="2697" end_char="2699">who</TOKEN>
<TOKEN id="token-50-5" pos="word" morph="none" start_char="2701" end_char="2702">is</TOKEN>
<TOKEN id="token-50-6" pos="word" morph="none" start_char="2704" end_char="2711">accusing</TOKEN>
<TOKEN id="token-50-7" pos="word" morph="none" start_char="2713" end_char="2715">NYC</TOKEN>
<TOKEN id="token-50-8" pos="word" morph="none" start_char="2717" end_char="2725">hospitals</TOKEN>
<TOKEN id="token-50-9" pos="word" morph="none" start_char="2727" end_char="2728">of</TOKEN>
<TOKEN id="token-50-10" pos="word" morph="none" start_char="2730" end_char="2738">Murdering</TOKEN>
<TOKEN id="token-50-11" pos="word" morph="none" start_char="2740" end_char="2747">patients</TOKEN>
<TOKEN id="token-50-12" pos="punct" morph="none" start_char="2748" end_char="2748">.</TOKEN>
</SEG>
<SEG id="segment-51" start_char="2754" end_char="2853">
<ORIGINAL_TEXT>quote:The opinion of a experienced RN on the treatment of patients is probably more valid than a MD.</ORIGINAL_TEXT>
<TOKEN id="token-51-0" pos="unknown" morph="none" start_char="2754" end_char="2762">quote:The</TOKEN>
<TOKEN id="token-51-1" pos="word" morph="none" start_char="2764" end_char="2770">opinion</TOKEN>
<TOKEN id="token-51-2" pos="word" morph="none" start_char="2772" end_char="2773">of</TOKEN>
<TOKEN id="token-51-3" pos="word" morph="none" start_char="2775" end_char="2775">a</TOKEN>
<TOKEN id="token-51-4" pos="word" morph="none" start_char="2777" end_char="2787">experienced</TOKEN>
<TOKEN id="token-51-5" pos="word" morph="none" start_char="2789" end_char="2790">RN</TOKEN>
<TOKEN id="token-51-6" pos="word" morph="none" start_char="2792" end_char="2793">on</TOKEN>
<TOKEN id="token-51-7" pos="word" morph="none" start_char="2795" end_char="2797">the</TOKEN>
<TOKEN id="token-51-8" pos="word" morph="none" start_char="2799" end_char="2807">treatment</TOKEN>
<TOKEN id="token-51-9" pos="word" morph="none" start_char="2809" end_char="2810">of</TOKEN>
<TOKEN id="token-51-10" pos="word" morph="none" start_char="2812" end_char="2819">patients</TOKEN>
<TOKEN id="token-51-11" pos="word" morph="none" start_char="2821" end_char="2822">is</TOKEN>
<TOKEN id="token-51-12" pos="word" morph="none" start_char="2824" end_char="2831">probably</TOKEN>
<TOKEN id="token-51-13" pos="word" morph="none" start_char="2833" end_char="2836">more</TOKEN>
<TOKEN id="token-51-14" pos="word" morph="none" start_char="2838" end_char="2842">valid</TOKEN>
<TOKEN id="token-51-15" pos="word" morph="none" start_char="2844" end_char="2847">than</TOKEN>
<TOKEN id="token-51-16" pos="word" morph="none" start_char="2849" end_char="2849">a</TOKEN>
<TOKEN id="token-51-17" pos="word" morph="none" start_char="2851" end_char="2852">MD</TOKEN>
<TOKEN id="token-51-18" pos="punct" morph="none" start_char="2853" end_char="2853">.</TOKEN>
</SEG>
<SEG id="segment-52" start_char="2856" end_char="3132">
<ORIGINAL_TEXT>Most folks who apply Appeals to Authority logical fallacies seem to think that someone with letters behind their name are better at reading and critical thinking than other people who don't have letters behind their name ... but usually nothing could be further from the truth.</ORIGINAL_TEXT>
<TOKEN id="token-52-0" pos="word" morph="none" start_char="2856" end_char="2859">Most</TOKEN>
<TOKEN id="token-52-1" pos="word" morph="none" start_char="2861" end_char="2865">folks</TOKEN>
<TOKEN id="token-52-2" pos="word" morph="none" start_char="2867" end_char="2869">who</TOKEN>
<TOKEN id="token-52-3" pos="word" morph="none" start_char="2871" end_char="2875">apply</TOKEN>
<TOKEN id="token-52-4" pos="word" morph="none" start_char="2877" end_char="2883">Appeals</TOKEN>
<TOKEN id="token-52-5" pos="word" morph="none" start_char="2885" end_char="2886">to</TOKEN>
<TOKEN id="token-52-6" pos="word" morph="none" start_char="2888" end_char="2896">Authority</TOKEN>
<TOKEN id="token-52-7" pos="word" morph="none" start_char="2898" end_char="2904">logical</TOKEN>
<TOKEN id="token-52-8" pos="word" morph="none" start_char="2906" end_char="2914">fallacies</TOKEN>
<TOKEN id="token-52-9" pos="word" morph="none" start_char="2916" end_char="2919">seem</TOKEN>
<TOKEN id="token-52-10" pos="word" morph="none" start_char="2921" end_char="2922">to</TOKEN>
<TOKEN id="token-52-11" pos="word" morph="none" start_char="2924" end_char="2928">think</TOKEN>
<TOKEN id="token-52-12" pos="word" morph="none" start_char="2930" end_char="2933">that</TOKEN>
<TOKEN id="token-52-13" pos="word" morph="none" start_char="2935" end_char="2941">someone</TOKEN>
<TOKEN id="token-52-14" pos="word" morph="none" start_char="2943" end_char="2946">with</TOKEN>
<TOKEN id="token-52-15" pos="word" morph="none" start_char="2948" end_char="2954">letters</TOKEN>
<TOKEN id="token-52-16" pos="word" morph="none" start_char="2956" end_char="2961">behind</TOKEN>
<TOKEN id="token-52-17" pos="word" morph="none" start_char="2963" end_char="2967">their</TOKEN>
<TOKEN id="token-52-18" pos="word" morph="none" start_char="2969" end_char="2972">name</TOKEN>
<TOKEN id="token-52-19" pos="word" morph="none" start_char="2974" end_char="2976">are</TOKEN>
<TOKEN id="token-52-20" pos="word" morph="none" start_char="2978" end_char="2983">better</TOKEN>
<TOKEN id="token-52-21" pos="word" morph="none" start_char="2985" end_char="2986">at</TOKEN>
<TOKEN id="token-52-22" pos="word" morph="none" start_char="2988" end_char="2994">reading</TOKEN>
<TOKEN id="token-52-23" pos="word" morph="none" start_char="2996" end_char="2998">and</TOKEN>
<TOKEN id="token-52-24" pos="word" morph="none" start_char="3000" end_char="3007">critical</TOKEN>
<TOKEN id="token-52-25" pos="word" morph="none" start_char="3009" end_char="3016">thinking</TOKEN>
<TOKEN id="token-52-26" pos="word" morph="none" start_char="3018" end_char="3021">than</TOKEN>
<TOKEN id="token-52-27" pos="word" morph="none" start_char="3023" end_char="3027">other</TOKEN>
<TOKEN id="token-52-28" pos="word" morph="none" start_char="3029" end_char="3034">people</TOKEN>
<TOKEN id="token-52-29" pos="word" morph="none" start_char="3036" end_char="3038">who</TOKEN>
<TOKEN id="token-52-30" pos="word" morph="none" start_char="3040" end_char="3044">don't</TOKEN>
<TOKEN id="token-52-31" pos="word" morph="none" start_char="3046" end_char="3049">have</TOKEN>
<TOKEN id="token-52-32" pos="word" morph="none" start_char="3051" end_char="3057">letters</TOKEN>
<TOKEN id="token-52-33" pos="word" morph="none" start_char="3059" end_char="3064">behind</TOKEN>
<TOKEN id="token-52-34" pos="word" morph="none" start_char="3066" end_char="3070">their</TOKEN>
<TOKEN id="token-52-35" pos="word" morph="none" start_char="3072" end_char="3075">name</TOKEN>
<TOKEN id="token-52-36" pos="punct" morph="none" start_char="3077" end_char="3079">...</TOKEN>
<TOKEN id="token-52-37" pos="word" morph="none" start_char="3081" end_char="3083">but</TOKEN>
<TOKEN id="token-52-38" pos="word" morph="none" start_char="3085" end_char="3091">usually</TOKEN>
<TOKEN id="token-52-39" pos="word" morph="none" start_char="3093" end_char="3099">nothing</TOKEN>
<TOKEN id="token-52-40" pos="word" morph="none" start_char="3101" end_char="3105">could</TOKEN>
<TOKEN id="token-52-41" pos="word" morph="none" start_char="3107" end_char="3108">be</TOKEN>
<TOKEN id="token-52-42" pos="word" morph="none" start_char="3110" end_char="3116">further</TOKEN>
<TOKEN id="token-52-43" pos="word" morph="none" start_char="3118" end_char="3121">from</TOKEN>
<TOKEN id="token-52-44" pos="word" morph="none" start_char="3123" end_char="3125">the</TOKEN>
<TOKEN id="token-52-45" pos="word" morph="none" start_char="3127" end_char="3131">truth</TOKEN>
<TOKEN id="token-52-46" pos="punct" morph="none" start_char="3132" end_char="3132">.</TOKEN>
</SEG>
<SEG id="segment-53" start_char="3135" end_char="3237">
<ORIGINAL_TEXT>Piled High and Deep ... inability to critically think but usually well read and relies on consensus vs.</ORIGINAL_TEXT>
<TOKEN id="token-53-0" pos="word" morph="none" start_char="3135" end_char="3139">Piled</TOKEN>
<TOKEN id="token-53-1" pos="word" morph="none" start_char="3141" end_char="3144">High</TOKEN>
<TOKEN id="token-53-2" pos="word" morph="none" start_char="3146" end_char="3148">and</TOKEN>
<TOKEN id="token-53-3" pos="word" morph="none" start_char="3150" end_char="3153">Deep</TOKEN>
<TOKEN id="token-53-4" pos="punct" morph="none" start_char="3155" end_char="3157">...</TOKEN>
<TOKEN id="token-53-5" pos="word" morph="none" start_char="3159" end_char="3167">inability</TOKEN>
<TOKEN id="token-53-6" pos="word" morph="none" start_char="3169" end_char="3170">to</TOKEN>
<TOKEN id="token-53-7" pos="word" morph="none" start_char="3172" end_char="3181">critically</TOKEN>
<TOKEN id="token-53-8" pos="word" morph="none" start_char="3183" end_char="3187">think</TOKEN>
<TOKEN id="token-53-9" pos="word" morph="none" start_char="3189" end_char="3191">but</TOKEN>
<TOKEN id="token-53-10" pos="word" morph="none" start_char="3193" end_char="3199">usually</TOKEN>
<TOKEN id="token-53-11" pos="word" morph="none" start_char="3201" end_char="3204">well</TOKEN>
<TOKEN id="token-53-12" pos="word" morph="none" start_char="3206" end_char="3209">read</TOKEN>
<TOKEN id="token-53-13" pos="word" morph="none" start_char="3211" end_char="3213">and</TOKEN>
<TOKEN id="token-53-14" pos="word" morph="none" start_char="3215" end_char="3220">relies</TOKEN>
<TOKEN id="token-53-15" pos="word" morph="none" start_char="3222" end_char="3223">on</TOKEN>
<TOKEN id="token-53-16" pos="word" morph="none" start_char="3225" end_char="3233">consensus</TOKEN>
<TOKEN id="token-53-17" pos="word" morph="none" start_char="3235" end_char="3236">vs</TOKEN>
<TOKEN id="token-53-18" pos="punct" morph="none" start_char="3237" end_char="3237">.</TOKEN>
</SEG>
<SEG id="segment-54" start_char="3239" end_char="3251">
<ORIGINAL_TEXT>intuition ...</ORIGINAL_TEXT>
<TOKEN id="token-54-0" pos="word" morph="none" start_char="3239" end_char="3247">intuition</TOKEN>
<TOKEN id="token-54-1" pos="punct" morph="none" start_char="3249" end_char="3251">...</TOKEN>
</SEG>
<SEG id="segment-55" start_char="3256" end_char="3312">
<ORIGINAL_TEXT>Why did she say black lives don’t matter to these people?</ORIGINAL_TEXT>
<TOKEN id="token-55-0" pos="word" morph="none" start_char="3256" end_char="3258">Why</TOKEN>
<TOKEN id="token-55-1" pos="word" morph="none" start_char="3260" end_char="3262">did</TOKEN>
<TOKEN id="token-55-2" pos="word" morph="none" start_char="3264" end_char="3266">she</TOKEN>
<TOKEN id="token-55-3" pos="word" morph="none" start_char="3268" end_char="3270">say</TOKEN>
<TOKEN id="token-55-4" pos="word" morph="none" start_char="3272" end_char="3276">black</TOKEN>
<TOKEN id="token-55-5" pos="word" morph="none" start_char="3278" end_char="3282">lives</TOKEN>
<TOKEN id="token-55-6" pos="word" morph="none" start_char="3284" end_char="3288">don’t</TOKEN>
<TOKEN id="token-55-7" pos="word" morph="none" start_char="3290" end_char="3295">matter</TOKEN>
<TOKEN id="token-55-8" pos="word" morph="none" start_char="3297" end_char="3298">to</TOKEN>
<TOKEN id="token-55-9" pos="word" morph="none" start_char="3300" end_char="3304">these</TOKEN>
<TOKEN id="token-55-10" pos="word" morph="none" start_char="3306" end_char="3311">people</TOKEN>
<TOKEN id="token-55-11" pos="punct" morph="none" start_char="3312" end_char="3312">?</TOKEN>
</SEG>
<SEG id="segment-56" start_char="3316" end_char="3436">
<ORIGINAL_TEXT>Then why not go through the channels of reporting the complaints to the state health Department, Joint Commission or CMS?</ORIGINAL_TEXT>
<TOKEN id="token-56-0" pos="word" morph="none" start_char="3316" end_char="3319">Then</TOKEN>
<TOKEN id="token-56-1" pos="word" morph="none" start_char="3321" end_char="3323">why</TOKEN>
<TOKEN id="token-56-2" pos="word" morph="none" start_char="3325" end_char="3327">not</TOKEN>
<TOKEN id="token-56-3" pos="word" morph="none" start_char="3329" end_char="3330">go</TOKEN>
<TOKEN id="token-56-4" pos="word" morph="none" start_char="3332" end_char="3338">through</TOKEN>
<TOKEN id="token-56-5" pos="word" morph="none" start_char="3340" end_char="3342">the</TOKEN>
<TOKEN id="token-56-6" pos="word" morph="none" start_char="3344" end_char="3351">channels</TOKEN>
<TOKEN id="token-56-7" pos="word" morph="none" start_char="3353" end_char="3354">of</TOKEN>
<TOKEN id="token-56-8" pos="word" morph="none" start_char="3356" end_char="3364">reporting</TOKEN>
<TOKEN id="token-56-9" pos="word" morph="none" start_char="3366" end_char="3368">the</TOKEN>
<TOKEN id="token-56-10" pos="word" morph="none" start_char="3370" end_char="3379">complaints</TOKEN>
<TOKEN id="token-56-11" pos="word" morph="none" start_char="3381" end_char="3382">to</TOKEN>
<TOKEN id="token-56-12" pos="word" morph="none" start_char="3384" end_char="3386">the</TOKEN>
<TOKEN id="token-56-13" pos="word" morph="none" start_char="3388" end_char="3392">state</TOKEN>
<TOKEN id="token-56-14" pos="word" morph="none" start_char="3394" end_char="3399">health</TOKEN>
<TOKEN id="token-56-15" pos="word" morph="none" start_char="3401" end_char="3410">Department</TOKEN>
<TOKEN id="token-56-16" pos="punct" morph="none" start_char="3411" end_char="3411">,</TOKEN>
<TOKEN id="token-56-17" pos="word" morph="none" start_char="3413" end_char="3417">Joint</TOKEN>
<TOKEN id="token-56-18" pos="word" morph="none" start_char="3419" end_char="3428">Commission</TOKEN>
<TOKEN id="token-56-19" pos="word" morph="none" start_char="3430" end_char="3431">or</TOKEN>
<TOKEN id="token-56-20" pos="word" morph="none" start_char="3433" end_char="3435">CMS</TOKEN>
<TOKEN id="token-56-21" pos="punct" morph="none" start_char="3436" end_char="3436">?</TOKEN>
</SEG>
<SEG id="segment-57" start_char="3439" end_char="3546">
<ORIGINAL_TEXT>I find it extremely difficult to believe she reported potential wrongful deaths and JCO said nah we’re good.</ORIGINAL_TEXT>
<TOKEN id="token-57-0" pos="word" morph="none" start_char="3439" end_char="3439">I</TOKEN>
<TOKEN id="token-57-1" pos="word" morph="none" start_char="3441" end_char="3444">find</TOKEN>
<TOKEN id="token-57-2" pos="word" morph="none" start_char="3446" end_char="3447">it</TOKEN>
<TOKEN id="token-57-3" pos="word" morph="none" start_char="3449" end_char="3457">extremely</TOKEN>
<TOKEN id="token-57-4" pos="word" morph="none" start_char="3459" end_char="3467">difficult</TOKEN>
<TOKEN id="token-57-5" pos="word" morph="none" start_char="3469" end_char="3470">to</TOKEN>
<TOKEN id="token-57-6" pos="word" morph="none" start_char="3472" end_char="3478">believe</TOKEN>
<TOKEN id="token-57-7" pos="word" morph="none" start_char="3480" end_char="3482">she</TOKEN>
<TOKEN id="token-57-8" pos="word" morph="none" start_char="3484" end_char="3491">reported</TOKEN>
<TOKEN id="token-57-9" pos="word" morph="none" start_char="3493" end_char="3501">potential</TOKEN>
<TOKEN id="token-57-10" pos="word" morph="none" start_char="3503" end_char="3510">wrongful</TOKEN>
<TOKEN id="token-57-11" pos="word" morph="none" start_char="3512" end_char="3517">deaths</TOKEN>
<TOKEN id="token-57-12" pos="word" morph="none" start_char="3519" end_char="3521">and</TOKEN>
<TOKEN id="token-57-13" pos="word" morph="none" start_char="3523" end_char="3525">JCO</TOKEN>
<TOKEN id="token-57-14" pos="word" morph="none" start_char="3527" end_char="3530">said</TOKEN>
<TOKEN id="token-57-15" pos="word" morph="none" start_char="3532" end_char="3534">nah</TOKEN>
<TOKEN id="token-57-16" pos="word" morph="none" start_char="3536" end_char="3540">we’re</TOKEN>
<TOKEN id="token-57-17" pos="word" morph="none" start_char="3542" end_char="3545">good</TOKEN>
<TOKEN id="token-57-18" pos="punct" morph="none" start_char="3546" end_char="3546">.</TOKEN>
</SEG>
<SEG id="segment-58" start_char="3551" end_char="3620">
<ORIGINAL_TEXT>quote: If not did she go strait to posting videos for internet points?</ORIGINAL_TEXT>
<TOKEN id="token-58-0" pos="word" morph="none" start_char="3551" end_char="3555">quote</TOKEN>
<TOKEN id="token-58-1" pos="punct" morph="none" start_char="3556" end_char="3556">:</TOKEN>
<TOKEN id="token-58-2" pos="word" morph="none" start_char="3558" end_char="3559">If</TOKEN>
<TOKEN id="token-58-3" pos="word" morph="none" start_char="3561" end_char="3563">not</TOKEN>
<TOKEN id="token-58-4" pos="word" morph="none" start_char="3565" end_char="3567">did</TOKEN>
<TOKEN id="token-58-5" pos="word" morph="none" start_char="3569" end_char="3571">she</TOKEN>
<TOKEN id="token-58-6" pos="word" morph="none" start_char="3573" end_char="3574">go</TOKEN>
<TOKEN id="token-58-7" pos="word" morph="none" start_char="3576" end_char="3581">strait</TOKEN>
<TOKEN id="token-58-8" pos="word" morph="none" start_char="3583" end_char="3584">to</TOKEN>
<TOKEN id="token-58-9" pos="word" morph="none" start_char="3586" end_char="3592">posting</TOKEN>
<TOKEN id="token-58-10" pos="word" morph="none" start_char="3594" end_char="3599">videos</TOKEN>
<TOKEN id="token-58-11" pos="word" morph="none" start_char="3601" end_char="3603">for</TOKEN>
<TOKEN id="token-58-12" pos="word" morph="none" start_char="3605" end_char="3612">internet</TOKEN>
<TOKEN id="token-58-13" pos="word" morph="none" start_char="3614" end_char="3619">points</TOKEN>
<TOKEN id="token-58-14" pos="punct" morph="none" start_char="3620" end_char="3620">?</TOKEN>
</SEG>
<SEG id="segment-59" start_char="3623" end_char="3644">
<ORIGINAL_TEXT>This is 2020 not 1955.</ORIGINAL_TEXT>
<TOKEN id="token-59-0" pos="word" morph="none" start_char="3623" end_char="3626">This</TOKEN>
<TOKEN id="token-59-1" pos="word" morph="none" start_char="3628" end_char="3629">is</TOKEN>
<TOKEN id="token-59-2" pos="word" morph="none" start_char="3631" end_char="3634">2020</TOKEN>
<TOKEN id="token-59-3" pos="word" morph="none" start_char="3636" end_char="3638">not</TOKEN>
<TOKEN id="token-59-4" pos="word" morph="none" start_char="3640" end_char="3643">1955</TOKEN>
<TOKEN id="token-59-5" pos="punct" morph="none" start_char="3644" end_char="3644">.</TOKEN>
</SEG>
<SEG id="segment-60" start_char="3646" end_char="3694">
<ORIGINAL_TEXT>An effective YouTube video could save lives, NOW.</ORIGINAL_TEXT>
<TOKEN id="token-60-0" pos="word" morph="none" start_char="3646" end_char="3647">An</TOKEN>
<TOKEN id="token-60-1" pos="word" morph="none" start_char="3649" end_char="3657">effective</TOKEN>
<TOKEN id="token-60-2" pos="word" morph="none" start_char="3659" end_char="3665">YouTube</TOKEN>
<TOKEN id="token-60-3" pos="word" morph="none" start_char="3667" end_char="3671">video</TOKEN>
<TOKEN id="token-60-4" pos="word" morph="none" start_char="3673" end_char="3677">could</TOKEN>
<TOKEN id="token-60-5" pos="word" morph="none" start_char="3679" end_char="3682">save</TOKEN>
<TOKEN id="token-60-6" pos="word" morph="none" start_char="3684" end_char="3688">lives</TOKEN>
<TOKEN id="token-60-7" pos="punct" morph="none" start_char="3689" end_char="3689">,</TOKEN>
<TOKEN id="token-60-8" pos="word" morph="none" start_char="3691" end_char="3693">NOW</TOKEN>
<TOKEN id="token-60-9" pos="punct" morph="none" start_char="3694" end_char="3694">.</TOKEN>
</SEG>
<SEG id="segment-61" start_char="3696" end_char="3768">
<ORIGINAL_TEXT>Formal complaints may or may not be reviewed within the next year or two.</ORIGINAL_TEXT>
<TOKEN id="token-61-0" pos="word" morph="none" start_char="3696" end_char="3701">Formal</TOKEN>
<TOKEN id="token-61-1" pos="word" morph="none" start_char="3703" end_char="3712">complaints</TOKEN>
<TOKEN id="token-61-2" pos="word" morph="none" start_char="3714" end_char="3716">may</TOKEN>
<TOKEN id="token-61-3" pos="word" morph="none" start_char="3718" end_char="3719">or</TOKEN>
<TOKEN id="token-61-4" pos="word" morph="none" start_char="3721" end_char="3723">may</TOKEN>
<TOKEN id="token-61-5" pos="word" morph="none" start_char="3725" end_char="3727">not</TOKEN>
<TOKEN id="token-61-6" pos="word" morph="none" start_char="3729" end_char="3730">be</TOKEN>
<TOKEN id="token-61-7" pos="word" morph="none" start_char="3732" end_char="3739">reviewed</TOKEN>
<TOKEN id="token-61-8" pos="word" morph="none" start_char="3741" end_char="3746">within</TOKEN>
<TOKEN id="token-61-9" pos="word" morph="none" start_char="3748" end_char="3750">the</TOKEN>
<TOKEN id="token-61-10" pos="word" morph="none" start_char="3752" end_char="3755">next</TOKEN>
<TOKEN id="token-61-11" pos="word" morph="none" start_char="3757" end_char="3760">year</TOKEN>
<TOKEN id="token-61-12" pos="word" morph="none" start_char="3762" end_char="3763">or</TOKEN>
<TOKEN id="token-61-13" pos="word" morph="none" start_char="3765" end_char="3767">two</TOKEN>
<TOKEN id="token-61-14" pos="punct" morph="none" start_char="3768" end_char="3768">.</TOKEN>
</SEG>
<SEG id="segment-62" start_char="3771" end_char="3871">
<ORIGINAL_TEXT>Judging by the support that is mounting for her in Nevada, there will be complaints filed... aplenty.</ORIGINAL_TEXT>
<TOKEN id="token-62-0" pos="word" morph="none" start_char="3771" end_char="3777">Judging</TOKEN>
<TOKEN id="token-62-1" pos="word" morph="none" start_char="3779" end_char="3780">by</TOKEN>
<TOKEN id="token-62-2" pos="word" morph="none" start_char="3782" end_char="3784">the</TOKEN>
<TOKEN id="token-62-3" pos="word" morph="none" start_char="3786" end_char="3792">support</TOKEN>
<TOKEN id="token-62-4" pos="word" morph="none" start_char="3794" end_char="3797">that</TOKEN>
<TOKEN id="token-62-5" pos="word" morph="none" start_char="3799" end_char="3800">is</TOKEN>
<TOKEN id="token-62-6" pos="word" morph="none" start_char="3802" end_char="3809">mounting</TOKEN>
<TOKEN id="token-62-7" pos="word" morph="none" start_char="3811" end_char="3813">for</TOKEN>
<TOKEN id="token-62-8" pos="word" morph="none" start_char="3815" end_char="3817">her</TOKEN>
<TOKEN id="token-62-9" pos="word" morph="none" start_char="3819" end_char="3820">in</TOKEN>
<TOKEN id="token-62-10" pos="word" morph="none" start_char="3822" end_char="3827">Nevada</TOKEN>
<TOKEN id="token-62-11" pos="punct" morph="none" start_char="3828" end_char="3828">,</TOKEN>
<TOKEN id="token-62-12" pos="word" morph="none" start_char="3830" end_char="3834">there</TOKEN>
<TOKEN id="token-62-13" pos="word" morph="none" start_char="3836" end_char="3839">will</TOKEN>
<TOKEN id="token-62-14" pos="word" morph="none" start_char="3841" end_char="3842">be</TOKEN>
<TOKEN id="token-62-15" pos="word" morph="none" start_char="3844" end_char="3853">complaints</TOKEN>
<TOKEN id="token-62-16" pos="word" morph="none" start_char="3855" end_char="3859">filed</TOKEN>
<TOKEN id="token-62-17" pos="punct" morph="none" start_char="3860" end_char="3862">...</TOKEN>
<TOKEN id="token-62-18" pos="word" morph="none" start_char="3864" end_char="3870">aplenty</TOKEN>
<TOKEN id="token-62-19" pos="punct" morph="none" start_char="3871" end_char="3871">.</TOKEN>
</SEG>
<SEG id="segment-63" start_char="3877" end_char="3939">
<ORIGINAL_TEXT>quote:Why did she say black lives don’t matter to these people?</ORIGINAL_TEXT>
<TOKEN id="token-63-0" pos="unknown" morph="none" start_char="3877" end_char="3885">quote:Why</TOKEN>
<TOKEN id="token-63-1" pos="word" morph="none" start_char="3887" end_char="3889">did</TOKEN>
<TOKEN id="token-63-2" pos="word" morph="none" start_char="3891" end_char="3893">she</TOKEN>
<TOKEN id="token-63-3" pos="word" morph="none" start_char="3895" end_char="3897">say</TOKEN>
<TOKEN id="token-63-4" pos="word" morph="none" start_char="3899" end_char="3903">black</TOKEN>
<TOKEN id="token-63-5" pos="word" morph="none" start_char="3905" end_char="3909">lives</TOKEN>
<TOKEN id="token-63-6" pos="word" morph="none" start_char="3911" end_char="3915">don’t</TOKEN>
<TOKEN id="token-63-7" pos="word" morph="none" start_char="3917" end_char="3922">matter</TOKEN>
<TOKEN id="token-63-8" pos="word" morph="none" start_char="3924" end_char="3925">to</TOKEN>
<TOKEN id="token-63-9" pos="word" morph="none" start_char="3927" end_char="3931">these</TOKEN>
<TOKEN id="token-63-10" pos="word" morph="none" start_char="3933" end_char="3938">people</TOKEN>
<TOKEN id="token-63-11" pos="punct" morph="none" start_char="3939" end_char="3939">?</TOKEN>
</SEG>
<SEG id="segment-64" start_char="3942" end_char="3986">
<ORIGINAL_TEXT>Was that one of the many things she reported?</ORIGINAL_TEXT>
<TOKEN id="token-64-0" pos="word" morph="none" start_char="3942" end_char="3944">Was</TOKEN>
<TOKEN id="token-64-1" pos="word" morph="none" start_char="3946" end_char="3949">that</TOKEN>
<TOKEN id="token-64-2" pos="word" morph="none" start_char="3951" end_char="3953">one</TOKEN>
<TOKEN id="token-64-3" pos="word" morph="none" start_char="3955" end_char="3956">of</TOKEN>
<TOKEN id="token-64-4" pos="word" morph="none" start_char="3958" end_char="3960">the</TOKEN>
<TOKEN id="token-64-5" pos="word" morph="none" start_char="3962" end_char="3965">many</TOKEN>
<TOKEN id="token-64-6" pos="word" morph="none" start_char="3967" end_char="3972">things</TOKEN>
<TOKEN id="token-64-7" pos="word" morph="none" start_char="3974" end_char="3976">she</TOKEN>
<TOKEN id="token-64-8" pos="word" morph="none" start_char="3978" end_char="3985">reported</TOKEN>
<TOKEN id="token-64-9" pos="punct" morph="none" start_char="3986" end_char="3986">?</TOKEN>
</SEG>
<SEG id="segment-65" start_char="3988" end_char="4017">
<ORIGINAL_TEXT>Probably true, if she said it.</ORIGINAL_TEXT>
<TOKEN id="token-65-0" pos="word" morph="none" start_char="3988" end_char="3995">Probably</TOKEN>
<TOKEN id="token-65-1" pos="word" morph="none" start_char="3997" end_char="4000">true</TOKEN>
<TOKEN id="token-65-2" pos="punct" morph="none" start_char="4001" end_char="4001">,</TOKEN>
<TOKEN id="token-65-3" pos="word" morph="none" start_char="4003" end_char="4004">if</TOKEN>
<TOKEN id="token-65-4" pos="word" morph="none" start_char="4006" end_char="4008">she</TOKEN>
<TOKEN id="token-65-5" pos="word" morph="none" start_char="4010" end_char="4013">said</TOKEN>
<TOKEN id="token-65-6" pos="word" morph="none" start_char="4015" end_char="4016">it</TOKEN>
<TOKEN id="token-65-7" pos="punct" morph="none" start_char="4017" end_char="4017">.</TOKEN>
</SEG>
<SEG id="segment-66" start_char="4022" end_char="4084">
<ORIGINAL_TEXT>She is some bastion of truth or a prog looking for hero status?</ORIGINAL_TEXT>
<TOKEN id="token-66-0" pos="word" morph="none" start_char="4022" end_char="4024">She</TOKEN>
<TOKEN id="token-66-1" pos="word" morph="none" start_char="4026" end_char="4027">is</TOKEN>
<TOKEN id="token-66-2" pos="word" morph="none" start_char="4029" end_char="4032">some</TOKEN>
<TOKEN id="token-66-3" pos="word" morph="none" start_char="4034" end_char="4040">bastion</TOKEN>
<TOKEN id="token-66-4" pos="word" morph="none" start_char="4042" end_char="4043">of</TOKEN>
<TOKEN id="token-66-5" pos="word" morph="none" start_char="4045" end_char="4049">truth</TOKEN>
<TOKEN id="token-66-6" pos="word" morph="none" start_char="4051" end_char="4052">or</TOKEN>
<TOKEN id="token-66-7" pos="word" morph="none" start_char="4054" end_char="4054">a</TOKEN>
<TOKEN id="token-66-8" pos="word" morph="none" start_char="4056" end_char="4059">prog</TOKEN>
<TOKEN id="token-66-9" pos="word" morph="none" start_char="4061" end_char="4067">looking</TOKEN>
<TOKEN id="token-66-10" pos="word" morph="none" start_char="4069" end_char="4071">for</TOKEN>
<TOKEN id="token-66-11" pos="word" morph="none" start_char="4073" end_char="4076">hero</TOKEN>
<TOKEN id="token-66-12" pos="word" morph="none" start_char="4078" end_char="4083">status</TOKEN>
<TOKEN id="token-66-13" pos="punct" morph="none" start_char="4084" end_char="4084">?</TOKEN>
</SEG>
<SEG id="segment-67" start_char="4087" end_char="4131">
<ORIGINAL_TEXT>You think NYC, prog heaven, is a racist city?</ORIGINAL_TEXT>
<TOKEN id="token-67-0" pos="word" morph="none" start_char="4087" end_char="4089">You</TOKEN>
<TOKEN id="token-67-1" pos="word" morph="none" start_char="4091" end_char="4095">think</TOKEN>
<TOKEN id="token-67-2" pos="word" morph="none" start_char="4097" end_char="4099">NYC</TOKEN>
<TOKEN id="token-67-3" pos="punct" morph="none" start_char="4100" end_char="4100">,</TOKEN>
<TOKEN id="token-67-4" pos="word" morph="none" start_char="4102" end_char="4105">prog</TOKEN>
<TOKEN id="token-67-5" pos="word" morph="none" start_char="4107" end_char="4112">heaven</TOKEN>
<TOKEN id="token-67-6" pos="punct" morph="none" start_char="4113" end_char="4113">,</TOKEN>
<TOKEN id="token-67-7" pos="word" morph="none" start_char="4115" end_char="4116">is</TOKEN>
<TOKEN id="token-67-8" pos="word" morph="none" start_char="4118" end_char="4118">a</TOKEN>
<TOKEN id="token-67-9" pos="word" morph="none" start_char="4120" end_char="4125">racist</TOKEN>
<TOKEN id="token-67-10" pos="word" morph="none" start_char="4127" end_char="4130">city</TOKEN>
<TOKEN id="token-67-11" pos="punct" morph="none" start_char="4131" end_char="4131">?</TOKEN>
</SEG>
<SEG id="segment-68" start_char="4134" end_char="4172">
<ORIGINAL_TEXT>This post was edited on 5/5 at 12:21 pm</ORIGINAL_TEXT>
<TOKEN id="token-68-0" pos="word" morph="none" start_char="4134" end_char="4137">This</TOKEN>
<TOKEN id="token-68-1" pos="word" morph="none" start_char="4139" end_char="4142">post</TOKEN>
<TOKEN id="token-68-2" pos="word" morph="none" start_char="4144" end_char="4146">was</TOKEN>
<TOKEN id="token-68-3" pos="word" morph="none" start_char="4148" end_char="4153">edited</TOKEN>
<TOKEN id="token-68-4" pos="word" morph="none" start_char="4155" end_char="4156">on</TOKEN>
<TOKEN id="token-68-5" pos="unknown" morph="none" start_char="4158" end_char="4160">5/5</TOKEN>
<TOKEN id="token-68-6" pos="word" morph="none" start_char="4162" end_char="4163">at</TOKEN>
<TOKEN id="token-68-7" pos="unknown" morph="none" start_char="4165" end_char="4169">12:21</TOKEN>
<TOKEN id="token-68-8" pos="word" morph="none" start_char="4171" end_char="4172">pm</TOKEN>
</SEG>
<SEG id="segment-69" start_char="4177" end_char="4227">
<ORIGINAL_TEXT>quote:You think NYC, prog heaven, is a racist city?</ORIGINAL_TEXT>
<TOKEN id="token-69-0" pos="unknown" morph="none" start_char="4177" end_char="4185">quote:You</TOKEN>
<TOKEN id="token-69-1" pos="word" morph="none" start_char="4187" end_char="4191">think</TOKEN>
<TOKEN id="token-69-2" pos="word" morph="none" start_char="4193" end_char="4195">NYC</TOKEN>
<TOKEN id="token-69-3" pos="punct" morph="none" start_char="4196" end_char="4196">,</TOKEN>
<TOKEN id="token-69-4" pos="word" morph="none" start_char="4198" end_char="4201">prog</TOKEN>
<TOKEN id="token-69-5" pos="word" morph="none" start_char="4203" end_char="4208">heaven</TOKEN>
<TOKEN id="token-69-6" pos="punct" morph="none" start_char="4209" end_char="4209">,</TOKEN>
<TOKEN id="token-69-7" pos="word" morph="none" start_char="4211" end_char="4212">is</TOKEN>
<TOKEN id="token-69-8" pos="word" morph="none" start_char="4214" end_char="4214">a</TOKEN>
<TOKEN id="token-69-9" pos="word" morph="none" start_char="4216" end_char="4221">racist</TOKEN>
<TOKEN id="token-69-10" pos="word" morph="none" start_char="4223" end_char="4226">city</TOKEN>
<TOKEN id="token-69-11" pos="punct" morph="none" start_char="4227" end_char="4227">?</TOKEN>
</SEG>
<SEG id="segment-70" start_char="4230" end_char="4257">
<ORIGINAL_TEXT>You have never lived in NYC.</ORIGINAL_TEXT>
<TOKEN id="token-70-0" pos="word" morph="none" start_char="4230" end_char="4232">You</TOKEN>
<TOKEN id="token-70-1" pos="word" morph="none" start_char="4234" end_char="4237">have</TOKEN>
<TOKEN id="token-70-2" pos="word" morph="none" start_char="4239" end_char="4243">never</TOKEN>
<TOKEN id="token-70-3" pos="word" morph="none" start_char="4245" end_char="4249">lived</TOKEN>
<TOKEN id="token-70-4" pos="word" morph="none" start_char="4251" end_char="4252">in</TOKEN>
<TOKEN id="token-70-5" pos="word" morph="none" start_char="4254" end_char="4256">NYC</TOKEN>
<TOKEN id="token-70-6" pos="punct" morph="none" start_char="4257" end_char="4257">.</TOKEN>
</SEG>
<SEG id="segment-71" start_char="4259" end_char="4275">
<ORIGINAL_TEXT>Bless your heart.</ORIGINAL_TEXT>
<TOKEN id="token-71-0" pos="word" morph="none" start_char="4259" end_char="4263">Bless</TOKEN>
<TOKEN id="token-71-1" pos="word" morph="none" start_char="4265" end_char="4268">your</TOKEN>
<TOKEN id="token-71-2" pos="word" morph="none" start_char="4270" end_char="4274">heart</TOKEN>
<TOKEN id="token-71-3" pos="punct" morph="none" start_char="4275" end_char="4275">.</TOKEN>
</SEG>
<SEG id="segment-72" start_char="4281" end_char="4331">
<ORIGINAL_TEXT>quote:Was that one of the many things she reported?</ORIGINAL_TEXT>
<TOKEN id="token-72-0" pos="unknown" morph="none" start_char="4281" end_char="4289">quote:Was</TOKEN>
<TOKEN id="token-72-1" pos="word" morph="none" start_char="4291" end_char="4294">that</TOKEN>
<TOKEN id="token-72-2" pos="word" morph="none" start_char="4296" end_char="4298">one</TOKEN>
<TOKEN id="token-72-3" pos="word" morph="none" start_char="4300" end_char="4301">of</TOKEN>
<TOKEN id="token-72-4" pos="word" morph="none" start_char="4303" end_char="4305">the</TOKEN>
<TOKEN id="token-72-5" pos="word" morph="none" start_char="4307" end_char="4310">many</TOKEN>
<TOKEN id="token-72-6" pos="word" morph="none" start_char="4312" end_char="4317">things</TOKEN>
<TOKEN id="token-72-7" pos="word" morph="none" start_char="4319" end_char="4321">she</TOKEN>
<TOKEN id="token-72-8" pos="word" morph="none" start_char="4323" end_char="4330">reported</TOKEN>
<TOKEN id="token-72-9" pos="punct" morph="none" start_char="4331" end_char="4331">?</TOKEN>
</SEG>
<SEG id="segment-73" start_char="4334" end_char="4378">
<ORIGINAL_TEXT>Wait, you didn’t watch her now deleted video?</ORIGINAL_TEXT>
<TOKEN id="token-73-0" pos="word" morph="none" start_char="4334" end_char="4337">Wait</TOKEN>
<TOKEN id="token-73-1" pos="punct" morph="none" start_char="4338" end_char="4338">,</TOKEN>
<TOKEN id="token-73-2" pos="word" morph="none" start_char="4340" end_char="4342">you</TOKEN>
<TOKEN id="token-73-3" pos="word" morph="none" start_char="4344" end_char="4349">didn’t</TOKEN>
<TOKEN id="token-73-4" pos="word" morph="none" start_char="4351" end_char="4355">watch</TOKEN>
<TOKEN id="token-73-5" pos="word" morph="none" start_char="4357" end_char="4359">her</TOKEN>
<TOKEN id="token-73-6" pos="word" morph="none" start_char="4361" end_char="4363">now</TOKEN>
<TOKEN id="token-73-7" pos="word" morph="none" start_char="4365" end_char="4371">deleted</TOKEN>
<TOKEN id="token-73-8" pos="word" morph="none" start_char="4373" end_char="4377">video</TOKEN>
<TOKEN id="token-73-9" pos="punct" morph="none" start_char="4378" end_char="4378">?</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
