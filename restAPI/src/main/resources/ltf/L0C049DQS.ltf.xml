<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="eng">
<DOC id="L0C049DQS" lang="eng" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="4752" raw_text_md5="bd227d4cd679345e81a47eaeb9f34308">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="60">
<ORIGINAL_TEXT>A Nobel Laureate Said the New Coronavirus Was Made in a Lab.</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="1">A</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="3" end_char="7">Nobel</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="9" end_char="16">Laureate</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="18" end_char="21">Said</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="23" end_char="25">the</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="27" end_char="29">New</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="31" end_char="41">Coronavirus</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="43" end_char="45">Was</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="47" end_char="50">Made</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="52" end_char="53">in</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="55" end_char="55">a</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="57" end_char="59">Lab</TOKEN>
<TOKEN id="token-0-12" pos="punct" morph="none" start_char="60" end_char="60">.</TOKEN>
</SEG>
<SEG id="segment-1" start_char="62" end_char="72">
<ORIGINAL_TEXT>He’s Wrong.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="62" end_char="65">He’s</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="67" end_char="71">Wrong</TOKEN>
<TOKEN id="token-1-2" pos="punct" morph="none" start_char="72" end_char="72">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="76" end_char="114">
<ORIGINAL_TEXT>Luc Montagnier during the TV interview.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="76" end_char="78">Luc</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="80" end_char="89">Montagnier</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="91" end_char="96">during</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="98" end_char="100">the</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="102" end_char="103">TV</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="105" end_char="113">interview</TOKEN>
<TOKEN id="token-2-6" pos="punct" morph="none" start_char="114" end_char="114">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="116" end_char="130">
<ORIGINAL_TEXT>Photo: YouTube.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="116" end_char="120">Photo</TOKEN>
<TOKEN id="token-3-1" pos="punct" morph="none" start_char="121" end_char="121">:</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="123" end_char="129">YouTube</TOKEN>
<TOKEN id="token-3-3" pos="punct" morph="none" start_char="130" end_char="130">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="134" end_char="336">
<ORIGINAL_TEXT>The 2008 Nobel Laureate for physiology or medicine from France, Luc Antoine Montagnier, caught the media’s attention when he recently endorsed a COVID-19 conspiracy theory – that the virus is human-made.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="134" end_char="136">The</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="138" end_char="141">2008</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="143" end_char="147">Nobel</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="149" end_char="156">Laureate</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="158" end_char="160">for</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="162" end_char="171">physiology</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="173" end_char="174">or</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="176" end_char="183">medicine</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="185" end_char="188">from</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="190" end_char="195">France</TOKEN>
<TOKEN id="token-4-10" pos="punct" morph="none" start_char="196" end_char="196">,</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="198" end_char="200">Luc</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="202" end_char="208">Antoine</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="210" end_char="219">Montagnier</TOKEN>
<TOKEN id="token-4-14" pos="punct" morph="none" start_char="220" end_char="220">,</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="222" end_char="227">caught</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="229" end_char="231">the</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="233" end_char="239">media’s</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="241" end_char="249">attention</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="251" end_char="254">when</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="256" end_char="257">he</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="259" end_char="266">recently</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="268" end_char="275">endorsed</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="277" end_char="277">a</TOKEN>
<TOKEN id="token-4-24" pos="unknown" morph="none" start_char="279" end_char="286">COVID-19</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="288" end_char="297">conspiracy</TOKEN>
<TOKEN id="token-4-26" pos="word" morph="none" start_char="299" end_char="304">theory</TOKEN>
<TOKEN id="token-4-27" pos="punct" morph="none" start_char="306" end_char="306">–</TOKEN>
<TOKEN id="token-4-28" pos="word" morph="none" start_char="308" end_char="311">that</TOKEN>
<TOKEN id="token-4-29" pos="word" morph="none" start_char="313" end_char="315">the</TOKEN>
<TOKEN id="token-4-30" pos="word" morph="none" start_char="317" end_char="321">virus</TOKEN>
<TOKEN id="token-4-31" pos="word" morph="none" start_char="323" end_char="324">is</TOKEN>
<TOKEN id="token-4-32" pos="unknown" morph="none" start_char="326" end_char="335">human-made</TOKEN>
<TOKEN id="token-4-33" pos="punct" morph="none" start_char="336" end_char="336">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="338" end_char="436">
<ORIGINAL_TEXT>His proclamation was subsequently magnified by various news outlets, including many in India (e.g.,</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="338" end_char="340">His</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="342" end_char="353">proclamation</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="355" end_char="357">was</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="359" end_char="370">subsequently</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="372" end_char="380">magnified</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="382" end_char="383">by</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="385" end_char="391">various</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="393" end_char="396">news</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="398" end_char="404">outlets</TOKEN>
<TOKEN id="token-5-9" pos="punct" morph="none" start_char="405" end_char="405">,</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="407" end_char="415">including</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="417" end_char="420">many</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="422" end_char="423">in</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="425" end_char="429">India</TOKEN>
<TOKEN id="token-5-14" pos="punct" morph="none" start_char="431" end_char="431">(</TOKEN>
<TOKEN id="token-5-15" pos="unknown" morph="none" start_char="432" end_char="434">e.g</TOKEN>
<TOKEN id="token-5-16" pos="punct" morph="none" start_char="435" end_char="436">.,</TOKEN>
</SEG>
<SEG id="segment-6" start_char="439" end_char="446">
<ORIGINAL_TEXT>The Week</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="439" end_char="441">The</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="443" end_char="446">Week</TOKEN>
</SEG>
<SEG id="segment-7" start_char="449" end_char="449">
<ORIGINAL_TEXT>,</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="punct" morph="none" start_char="449" end_char="449">,</TOKEN>
</SEG>
<SEG id="segment-8" start_char="452" end_char="473">
<ORIGINAL_TEXT>The Hindu Businessline</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="452" end_char="454">The</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="456" end_char="460">Hindu</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="462" end_char="473">Businessline</TOKEN>
</SEG>
<SEG id="segment-9" start_char="476" end_char="480">
<ORIGINAL_TEXT>, and</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="punct" morph="none" start_char="476" end_char="476">,</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="478" end_char="480">and</TOKEN>
</SEG>
<SEG id="segment-10" start_char="483" end_char="496">
<ORIGINAL_TEXT>Times of India</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="483" end_char="487">Times</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="489" end_char="490">of</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="492" end_char="496">India</TOKEN>
</SEG>
<SEG id="segment-11" start_char="499" end_char="500">
<ORIGINAL_TEXT>).</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="punct" morph="none" start_char="499" end_char="500">).</TOKEN>
</SEG>
<SEG id="segment-12" start_char="503" end_char="688">
<ORIGINAL_TEXT>Montagnier argued during a TV interview with a French TV channel that elements of the HIV-1 retrovirus, which he co-discovered in 1983, can be found in the genome of the new coronavirus.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="503" end_char="512">Montagnier</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="514" end_char="519">argued</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="521" end_char="526">during</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="528" end_char="528">a</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="530" end_char="531">TV</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="533" end_char="541">interview</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="543" end_char="546">with</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="548" end_char="548">a</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="550" end_char="555">French</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="557" end_char="558">TV</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="560" end_char="566">channel</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="568" end_char="571">that</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="573" end_char="580">elements</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="582" end_char="583">of</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="585" end_char="587">the</TOKEN>
<TOKEN id="token-12-15" pos="unknown" morph="none" start_char="589" end_char="593">HIV-1</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="595" end_char="604">retrovirus</TOKEN>
<TOKEN id="token-12-17" pos="punct" morph="none" start_char="605" end_char="605">,</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="607" end_char="611">which</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="613" end_char="614">he</TOKEN>
<TOKEN id="token-12-20" pos="unknown" morph="none" start_char="616" end_char="628">co-discovered</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="630" end_char="631">in</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="633" end_char="636">1983</TOKEN>
<TOKEN id="token-12-23" pos="punct" morph="none" start_char="637" end_char="637">,</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="639" end_char="641">can</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="643" end_char="644">be</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="646" end_char="650">found</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="652" end_char="653">in</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="655" end_char="657">the</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="659" end_char="664">genome</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="666" end_char="667">of</TOKEN>
<TOKEN id="token-12-31" pos="word" morph="none" start_char="669" end_char="671">the</TOKEN>
<TOKEN id="token-12-32" pos="word" morph="none" start_char="673" end_char="675">new</TOKEN>
<TOKEN id="token-12-33" pos="word" morph="none" start_char="677" end_char="687">coronavirus</TOKEN>
<TOKEN id="token-12-34" pos="punct" morph="none" start_char="688" end_char="688">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="690" end_char="747">
<ORIGINAL_TEXT>He also said elements of the "malaria germ" – the parasite</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="690" end_char="691">He</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="693" end_char="696">also</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="698" end_char="701">said</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="703" end_char="710">elements</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="712" end_char="713">of</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="715" end_char="717">the</TOKEN>
<TOKEN id="token-13-6" pos="punct" morph="none" start_char="719" end_char="719">"</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="720" end_char="726">malaria</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="728" end_char="731">germ</TOKEN>
<TOKEN id="token-13-9" pos="punct" morph="none" start_char="732" end_char="732">"</TOKEN>
<TOKEN id="token-13-10" pos="punct" morph="none" start_char="734" end_char="734">–</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="736" end_char="738">the</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="740" end_char="747">parasite</TOKEN>
</SEG>
<SEG id="segment-14" start_char="750" end_char="770">
<ORIGINAL_TEXT>Plasmodium falciparum</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="750" end_char="759">Plasmodium</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="761" end_char="770">falciparum</TOKEN>
</SEG>
<SEG id="segment-15" start_char="773" end_char="813">
<ORIGINAL_TEXT>– can also be seen in the virus’s genome.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="punct" morph="none" start_char="773" end_char="773">–</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="775" end_char="777">can</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="779" end_char="782">also</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="784" end_char="785">be</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="787" end_char="790">seen</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="792" end_char="793">in</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="795" end_char="797">the</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="799" end_char="805">virus’s</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="807" end_char="812">genome</TOKEN>
<TOKEN id="token-15-9" pos="punct" morph="none" start_char="813" end_char="813">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="816" end_char="1021">
<ORIGINAL_TEXT>His full quote: "We were not the first since a group of Indian researchers tried to publish a study which showed that the complete genome of this coronavirus [has] sequences of another virus, which is HIV."</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="816" end_char="818">His</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="820" end_char="823">full</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="825" end_char="829">quote</TOKEN>
<TOKEN id="token-16-3" pos="punct" morph="none" start_char="830" end_char="830">:</TOKEN>
<TOKEN id="token-16-4" pos="punct" morph="none" start_char="832" end_char="832">"</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="833" end_char="834">We</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="836" end_char="839">were</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="841" end_char="843">not</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="845" end_char="847">the</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="849" end_char="853">first</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="855" end_char="859">since</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="861" end_char="861">a</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="863" end_char="867">group</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="869" end_char="870">of</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="872" end_char="877">Indian</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="879" end_char="889">researchers</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="891" end_char="895">tried</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="897" end_char="898">to</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="900" end_char="906">publish</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="908" end_char="908">a</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="910" end_char="914">study</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="916" end_char="920">which</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="922" end_char="927">showed</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="929" end_char="932">that</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="934" end_char="936">the</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="938" end_char="945">complete</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="947" end_char="952">genome</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="954" end_char="955">of</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="957" end_char="960">this</TOKEN>
<TOKEN id="token-16-29" pos="word" morph="none" start_char="962" end_char="972">coronavirus</TOKEN>
<TOKEN id="token-16-30" pos="punct" morph="none" start_char="974" end_char="974">[</TOKEN>
<TOKEN id="token-16-31" pos="word" morph="none" start_char="975" end_char="977">has</TOKEN>
<TOKEN id="token-16-32" pos="punct" morph="none" start_char="978" end_char="978">]</TOKEN>
<TOKEN id="token-16-33" pos="word" morph="none" start_char="980" end_char="988">sequences</TOKEN>
<TOKEN id="token-16-34" pos="word" morph="none" start_char="990" end_char="991">of</TOKEN>
<TOKEN id="token-16-35" pos="word" morph="none" start_char="993" end_char="999">another</TOKEN>
<TOKEN id="token-16-36" pos="word" morph="none" start_char="1001" end_char="1005">virus</TOKEN>
<TOKEN id="token-16-37" pos="punct" morph="none" start_char="1006" end_char="1006">,</TOKEN>
<TOKEN id="token-16-38" pos="word" morph="none" start_char="1008" end_char="1012">which</TOKEN>
<TOKEN id="token-16-39" pos="word" morph="none" start_char="1014" end_char="1015">is</TOKEN>
<TOKEN id="token-16-40" pos="word" morph="none" start_char="1017" end_char="1019">HIV</TOKEN>
<TOKEN id="token-16-41" pos="punct" morph="none" start_char="1020" end_char="1021">."</TOKEN>
</SEG>
<SEG id="segment-17" start_char="1024" end_char="1256">
<ORIGINAL_TEXT>In a separate podcast episode with a different outlet, Montagnier further said the virus had escaped in an "industrial accident" from the Wuhan city laboratory when Chinese scientists were attempting to develop a vaccine against HIV.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="1024" end_char="1025">In</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="1027" end_char="1027">a</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="1029" end_char="1036">separate</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="1038" end_char="1044">podcast</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="1046" end_char="1052">episode</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="1054" end_char="1057">with</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="1059" end_char="1059">a</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="1061" end_char="1069">different</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="1071" end_char="1076">outlet</TOKEN>
<TOKEN id="token-17-9" pos="punct" morph="none" start_char="1077" end_char="1077">,</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="1079" end_char="1088">Montagnier</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="1090" end_char="1096">further</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="1098" end_char="1101">said</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="1103" end_char="1105">the</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="1107" end_char="1111">virus</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="1113" end_char="1115">had</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="1117" end_char="1123">escaped</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="1125" end_char="1126">in</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="1128" end_char="1129">an</TOKEN>
<TOKEN id="token-17-19" pos="punct" morph="none" start_char="1131" end_char="1131">"</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="1132" end_char="1141">industrial</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="1143" end_char="1150">accident</TOKEN>
<TOKEN id="token-17-22" pos="punct" morph="none" start_char="1151" end_char="1151">"</TOKEN>
<TOKEN id="token-17-23" pos="word" morph="none" start_char="1153" end_char="1156">from</TOKEN>
<TOKEN id="token-17-24" pos="word" morph="none" start_char="1158" end_char="1160">the</TOKEN>
<TOKEN id="token-17-25" pos="word" morph="none" start_char="1162" end_char="1166">Wuhan</TOKEN>
<TOKEN id="token-17-26" pos="word" morph="none" start_char="1168" end_char="1171">city</TOKEN>
<TOKEN id="token-17-27" pos="word" morph="none" start_char="1173" end_char="1182">laboratory</TOKEN>
<TOKEN id="token-17-28" pos="word" morph="none" start_char="1184" end_char="1187">when</TOKEN>
<TOKEN id="token-17-29" pos="word" morph="none" start_char="1189" end_char="1195">Chinese</TOKEN>
<TOKEN id="token-17-30" pos="word" morph="none" start_char="1197" end_char="1206">scientists</TOKEN>
<TOKEN id="token-17-31" pos="word" morph="none" start_char="1208" end_char="1211">were</TOKEN>
<TOKEN id="token-17-32" pos="word" morph="none" start_char="1213" end_char="1222">attempting</TOKEN>
<TOKEN id="token-17-33" pos="word" morph="none" start_char="1224" end_char="1225">to</TOKEN>
<TOKEN id="token-17-34" pos="word" morph="none" start_char="1227" end_char="1233">develop</TOKEN>
<TOKEN id="token-17-35" pos="word" morph="none" start_char="1235" end_char="1235">a</TOKEN>
<TOKEN id="token-17-36" pos="word" morph="none" start_char="1237" end_char="1243">vaccine</TOKEN>
<TOKEN id="token-17-37" pos="word" morph="none" start_char="1245" end_char="1251">against</TOKEN>
<TOKEN id="token-17-38" pos="word" morph="none" start_char="1253" end_char="1255">HIV</TOKEN>
<TOKEN id="token-17-39" pos="punct" morph="none" start_char="1256" end_char="1256">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="1259" end_char="1304">
<ORIGINAL_TEXT>The new coronavirus is an RNA virus, like HIV.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="1259" end_char="1261">The</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="1263" end_char="1265">new</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="1267" end_char="1277">coronavirus</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="1279" end_char="1280">is</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="1282" end_char="1283">an</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="1285" end_char="1287">RNA</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="1289" end_char="1293">virus</TOKEN>
<TOKEN id="token-18-7" pos="punct" morph="none" start_char="1294" end_char="1294">,</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="1296" end_char="1299">like</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="1301" end_char="1303">HIV</TOKEN>
<TOKEN id="token-18-10" pos="punct" morph="none" start_char="1304" end_char="1304">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="1306" end_char="1461">
<ORIGINAL_TEXT>Scientists already know that many viruses incorporate pieces of other genomes into their own in the natural course of evolution, both of plants and animals.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="1306" end_char="1315">Scientists</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="1317" end_char="1323">already</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="1325" end_char="1328">know</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="1330" end_char="1333">that</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="1335" end_char="1338">many</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="1340" end_char="1346">viruses</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="1348" end_char="1358">incorporate</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="1360" end_char="1365">pieces</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="1367" end_char="1368">of</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="1370" end_char="1374">other</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="1376" end_char="1382">genomes</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="1384" end_char="1387">into</TOKEN>
<TOKEN id="token-19-12" pos="word" morph="none" start_char="1389" end_char="1393">their</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="1395" end_char="1397">own</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="1399" end_char="1400">in</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="1402" end_char="1404">the</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="1406" end_char="1412">natural</TOKEN>
<TOKEN id="token-19-17" pos="word" morph="none" start_char="1414" end_char="1419">course</TOKEN>
<TOKEN id="token-19-18" pos="word" morph="none" start_char="1421" end_char="1422">of</TOKEN>
<TOKEN id="token-19-19" pos="word" morph="none" start_char="1424" end_char="1432">evolution</TOKEN>
<TOKEN id="token-19-20" pos="punct" morph="none" start_char="1433" end_char="1433">,</TOKEN>
<TOKEN id="token-19-21" pos="word" morph="none" start_char="1435" end_char="1438">both</TOKEN>
<TOKEN id="token-19-22" pos="word" morph="none" start_char="1440" end_char="1441">of</TOKEN>
<TOKEN id="token-19-23" pos="word" morph="none" start_char="1443" end_char="1448">plants</TOKEN>
<TOKEN id="token-19-24" pos="word" morph="none" start_char="1450" end_char="1452">and</TOKEN>
<TOKEN id="token-19-25" pos="word" morph="none" start_char="1454" end_char="1460">animals</TOKEN>
<TOKEN id="token-19-26" pos="punct" morph="none" start_char="1461" end_char="1461">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="1463" end_char="1652">
<ORIGINAL_TEXT>Indeed, fully 43% of the human genome is composed of mobile genetic element sequences, which are the leftovers of viral infections that our ancestors experienced over the last 300,000 years.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="1463" end_char="1468">Indeed</TOKEN>
<TOKEN id="token-20-1" pos="punct" morph="none" start_char="1469" end_char="1469">,</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="1471" end_char="1475">fully</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="1477" end_char="1478">43</TOKEN>
<TOKEN id="token-20-4" pos="punct" morph="none" start_char="1479" end_char="1479">%</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="1481" end_char="1482">of</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="1484" end_char="1486">the</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="1488" end_char="1492">human</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="1494" end_char="1499">genome</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="1501" end_char="1502">is</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="1504" end_char="1511">composed</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="1513" end_char="1514">of</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="1516" end_char="1521">mobile</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="1523" end_char="1529">genetic</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="1531" end_char="1537">element</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="1539" end_char="1547">sequences</TOKEN>
<TOKEN id="token-20-16" pos="punct" morph="none" start_char="1548" end_char="1548">,</TOKEN>
<TOKEN id="token-20-17" pos="word" morph="none" start_char="1550" end_char="1554">which</TOKEN>
<TOKEN id="token-20-18" pos="word" morph="none" start_char="1556" end_char="1558">are</TOKEN>
<TOKEN id="token-20-19" pos="word" morph="none" start_char="1560" end_char="1562">the</TOKEN>
<TOKEN id="token-20-20" pos="word" morph="none" start_char="1564" end_char="1572">leftovers</TOKEN>
<TOKEN id="token-20-21" pos="word" morph="none" start_char="1574" end_char="1575">of</TOKEN>
<TOKEN id="token-20-22" pos="word" morph="none" start_char="1577" end_char="1581">viral</TOKEN>
<TOKEN id="token-20-23" pos="word" morph="none" start_char="1583" end_char="1592">infections</TOKEN>
<TOKEN id="token-20-24" pos="word" morph="none" start_char="1594" end_char="1597">that</TOKEN>
<TOKEN id="token-20-25" pos="word" morph="none" start_char="1599" end_char="1601">our</TOKEN>
<TOKEN id="token-20-26" pos="word" morph="none" start_char="1603" end_char="1611">ancestors</TOKEN>
<TOKEN id="token-20-27" pos="word" morph="none" start_char="1613" end_char="1623">experienced</TOKEN>
<TOKEN id="token-20-28" pos="word" morph="none" start_char="1625" end_char="1628">over</TOKEN>
<TOKEN id="token-20-29" pos="word" morph="none" start_char="1630" end_char="1632">the</TOKEN>
<TOKEN id="token-20-30" pos="word" morph="none" start_char="1634" end_char="1637">last</TOKEN>
<TOKEN id="token-20-31" pos="unknown" morph="none" start_char="1639" end_char="1645">300,000</TOKEN>
<TOKEN id="token-20-32" pos="word" morph="none" start_char="1647" end_char="1651">years</TOKEN>
<TOKEN id="token-20-33" pos="punct" morph="none" start_char="1652" end_char="1652">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="1655" end_char="1736">
<ORIGINAL_TEXT>The new virus also has an exceptionally large genome, of about 30,000 nucleobases.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="1655" end_char="1657">The</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="1659" end_char="1661">new</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="1663" end_char="1667">virus</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="1669" end_char="1672">also</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="1674" end_char="1676">has</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="1678" end_char="1679">an</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="1681" end_char="1693">exceptionally</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="1695" end_char="1699">large</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="1701" end_char="1706">genome</TOKEN>
<TOKEN id="token-21-9" pos="punct" morph="none" start_char="1707" end_char="1707">,</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="1709" end_char="1710">of</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="1712" end_char="1716">about</TOKEN>
<TOKEN id="token-21-12" pos="unknown" morph="none" start_char="1718" end_char="1723">30,000</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="1725" end_char="1735">nucleobases</TOKEN>
<TOKEN id="token-21-14" pos="punct" morph="none" start_char="1736" end_char="1736">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="1738" end_char="1842">
<ORIGINAL_TEXT>Mobile genetic elements have been discovered in many viruses with large genomes, including coronaviruses.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="1738" end_char="1743">Mobile</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="1745" end_char="1751">genetic</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="1753" end_char="1760">elements</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="1762" end_char="1765">have</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="1767" end_char="1770">been</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="1772" end_char="1781">discovered</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="1783" end_char="1784">in</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="1786" end_char="1789">many</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="1791" end_char="1797">viruses</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="1799" end_char="1802">with</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="1804" end_char="1808">large</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="1810" end_char="1816">genomes</TOKEN>
<TOKEN id="token-22-12" pos="punct" morph="none" start_char="1817" end_char="1817">,</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="1819" end_char="1827">including</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="1829" end_char="1841">coronaviruses</TOKEN>
<TOKEN id="token-22-15" pos="punct" morph="none" start_char="1842" end_char="1842">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="1844" end_char="1940">
<ORIGINAL_TEXT>The Indian study Montagnier referred to had been authored by a team from IIT Delhi, among others.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="1844" end_char="1846">The</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="1848" end_char="1853">Indian</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="1855" end_char="1859">study</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="1861" end_char="1870">Montagnier</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="1872" end_char="1879">referred</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="1881" end_char="1882">to</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="1884" end_char="1886">had</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="1888" end_char="1891">been</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="1893" end_char="1900">authored</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="1902" end_char="1903">by</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="1905" end_char="1905">a</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="1907" end_char="1910">team</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="1912" end_char="1915">from</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="1917" end_char="1919">IIT</TOKEN>
<TOKEN id="token-23-14" pos="word" morph="none" start_char="1921" end_char="1925">Delhi</TOKEN>
<TOKEN id="token-23-15" pos="punct" morph="none" start_char="1926" end_char="1926">,</TOKEN>
<TOKEN id="token-23-16" pos="word" morph="none" start_char="1928" end_char="1932">among</TOKEN>
<TOKEN id="token-23-17" pos="word" morph="none" start_char="1934" end_char="1939">others</TOKEN>
<TOKEN id="token-23-18" pos="punct" morph="none" start_char="1940" end_char="1940">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="1942" end_char="2105">
<ORIGINAL_TEXT>They had uploaded their manuscript to the bioRxiv preprint repository only to quickly take it down after commentators pointed out numerous errors in their analysis.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="1942" end_char="1945">They</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="1947" end_char="1949">had</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="1951" end_char="1958">uploaded</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="1960" end_char="1964">their</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="1966" end_char="1975">manuscript</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="1977" end_char="1978">to</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="1980" end_char="1982">the</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="1984" end_char="1990">bioRxiv</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="1992" end_char="1999">preprint</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="2001" end_char="2010">repository</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="2012" end_char="2015">only</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="2017" end_char="2018">to</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="2020" end_char="2026">quickly</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="2028" end_char="2031">take</TOKEN>
<TOKEN id="token-24-14" pos="word" morph="none" start_char="2033" end_char="2034">it</TOKEN>
<TOKEN id="token-24-15" pos="word" morph="none" start_char="2036" end_char="2039">down</TOKEN>
<TOKEN id="token-24-16" pos="word" morph="none" start_char="2041" end_char="2045">after</TOKEN>
<TOKEN id="token-24-17" pos="word" morph="none" start_char="2047" end_char="2058">commentators</TOKEN>
<TOKEN id="token-24-18" pos="word" morph="none" start_char="2060" end_char="2066">pointed</TOKEN>
<TOKEN id="token-24-19" pos="word" morph="none" start_char="2068" end_char="2070">out</TOKEN>
<TOKEN id="token-24-20" pos="word" morph="none" start_char="2072" end_char="2079">numerous</TOKEN>
<TOKEN id="token-24-21" pos="word" morph="none" start_char="2081" end_char="2086">errors</TOKEN>
<TOKEN id="token-24-22" pos="word" morph="none" start_char="2088" end_char="2089">in</TOKEN>
<TOKEN id="token-24-23" pos="word" morph="none" start_char="2091" end_char="2095">their</TOKEN>
<TOKEN id="token-24-24" pos="word" morph="none" start_char="2097" end_char="2104">analysis</TOKEN>
<TOKEN id="token-24-25" pos="punct" morph="none" start_char="2105" end_char="2105">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="2108" end_char="2156">
<ORIGINAL_TEXT>An article published more recently in the journal</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="2108" end_char="2109">An</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="2111" end_char="2117">article</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="2119" end_char="2127">published</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="2129" end_char="2132">more</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="2134" end_char="2141">recently</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="2143" end_char="2144">in</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="2146" end_char="2148">the</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="2150" end_char="2156">journal</TOKEN>
</SEG>
<SEG id="segment-26" start_char="2159" end_char="2173">
<ORIGINAL_TEXT>Nature Medicine</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="2159" end_char="2164">Nature</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="2166" end_char="2173">Medicine</TOKEN>
</SEG>
<SEG id="segment-27" start_char="2176" end_char="2334">
<ORIGINAL_TEXT>analysing the new virus’s genome concluded thus: "Our analyses clearly show that SARS-CoV-2 is not a laboratory construct or a purposefully manipulated virus."</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="2176" end_char="2184">analysing</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="2186" end_char="2188">the</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="2190" end_char="2192">new</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="2194" end_char="2200">virus’s</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="2202" end_char="2207">genome</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="2209" end_char="2217">concluded</TOKEN>
<TOKEN id="token-27-6" pos="word" morph="none" start_char="2219" end_char="2222">thus</TOKEN>
<TOKEN id="token-27-7" pos="punct" morph="none" start_char="2223" end_char="2223">:</TOKEN>
<TOKEN id="token-27-8" pos="punct" morph="none" start_char="2225" end_char="2225">"</TOKEN>
<TOKEN id="token-27-9" pos="word" morph="none" start_char="2226" end_char="2228">Our</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="2230" end_char="2237">analyses</TOKEN>
<TOKEN id="token-27-11" pos="word" morph="none" start_char="2239" end_char="2245">clearly</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="2247" end_char="2250">show</TOKEN>
<TOKEN id="token-27-13" pos="word" morph="none" start_char="2252" end_char="2255">that</TOKEN>
<TOKEN id="token-27-14" pos="unknown" morph="none" start_char="2257" end_char="2266">SARS-CoV-2</TOKEN>
<TOKEN id="token-27-15" pos="word" morph="none" start_char="2268" end_char="2269">is</TOKEN>
<TOKEN id="token-27-16" pos="word" morph="none" start_char="2271" end_char="2273">not</TOKEN>
<TOKEN id="token-27-17" pos="word" morph="none" start_char="2275" end_char="2275">a</TOKEN>
<TOKEN id="token-27-18" pos="word" morph="none" start_char="2277" end_char="2286">laboratory</TOKEN>
<TOKEN id="token-27-19" pos="word" morph="none" start_char="2288" end_char="2296">construct</TOKEN>
<TOKEN id="token-27-20" pos="word" morph="none" start_char="2298" end_char="2299">or</TOKEN>
<TOKEN id="token-27-21" pos="word" morph="none" start_char="2301" end_char="2301">a</TOKEN>
<TOKEN id="token-27-22" pos="word" morph="none" start_char="2303" end_char="2314">purposefully</TOKEN>
<TOKEN id="token-27-23" pos="word" morph="none" start_char="2316" end_char="2326">manipulated</TOKEN>
<TOKEN id="token-27-24" pos="word" morph="none" start_char="2328" end_char="2332">virus</TOKEN>
<TOKEN id="token-27-25" pos="punct" morph="none" start_char="2333" end_char="2334">."</TOKEN>
</SEG>
<SEG id="segment-28" start_char="2337" end_char="2411">
<ORIGINAL_TEXT>What Montagnier called the "elements" of HIV were short cis-acting elements</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="2337" end_char="2340">What</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="2342" end_char="2351">Montagnier</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="2353" end_char="2358">called</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="2360" end_char="2362">the</TOKEN>
<TOKEN id="token-28-4" pos="punct" morph="none" start_char="2364" end_char="2364">"</TOKEN>
<TOKEN id="token-28-5" pos="word" morph="none" start_char="2365" end_char="2372">elements</TOKEN>
<TOKEN id="token-28-6" pos="punct" morph="none" start_char="2373" end_char="2373">"</TOKEN>
<TOKEN id="token-28-7" pos="word" morph="none" start_char="2375" end_char="2376">of</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="2378" end_char="2380">HIV</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="2382" end_char="2385">were</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="2387" end_char="2391">short</TOKEN>
<TOKEN id="token-28-11" pos="unknown" morph="none" start_char="2393" end_char="2402">cis-acting</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="2404" end_char="2411">elements</TOKEN>
</SEG>
<SEG id="segment-29" start_char="2414" end_char="2483">
<ORIGINAL_TEXT>that scientists had discovered in the genome of coronaviruses in 2005.</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="2414" end_char="2417">that</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="2419" end_char="2428">scientists</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="2430" end_char="2432">had</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="2434" end_char="2443">discovered</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="2445" end_char="2446">in</TOKEN>
<TOKEN id="token-29-5" pos="word" morph="none" start_char="2448" end_char="2450">the</TOKEN>
<TOKEN id="token-29-6" pos="word" morph="none" start_char="2452" end_char="2457">genome</TOKEN>
<TOKEN id="token-29-7" pos="word" morph="none" start_char="2459" end_char="2460">of</TOKEN>
<TOKEN id="token-29-8" pos="word" morph="none" start_char="2462" end_char="2474">coronaviruses</TOKEN>
<TOKEN id="token-29-9" pos="word" morph="none" start_char="2476" end_char="2477">in</TOKEN>
<TOKEN id="token-29-10" pos="word" morph="none" start_char="2479" end_char="2482">2005</TOKEN>
<TOKEN id="token-29-11" pos="punct" morph="none" start_char="2483" end_char="2483">.</TOKEN>
</SEG>
<SEG id="segment-30" start_char="2485" end_char="2562">
<ORIGINAL_TEXT>They are required for genome replication and are shared by many coronaviruses.</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="2485" end_char="2488">They</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="2490" end_char="2492">are</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="2494" end_char="2501">required</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="2503" end_char="2505">for</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="2507" end_char="2512">genome</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="2514" end_char="2524">replication</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="2526" end_char="2528">and</TOKEN>
<TOKEN id="token-30-7" pos="word" morph="none" start_char="2530" end_char="2532">are</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="2534" end_char="2539">shared</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="2541" end_char="2542">by</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="2544" end_char="2547">many</TOKEN>
<TOKEN id="token-30-11" pos="word" morph="none" start_char="2549" end_char="2561">coronaviruses</TOKEN>
<TOKEN id="token-30-12" pos="punct" morph="none" start_char="2562" end_char="2562">.</TOKEN>
</SEG>
<SEG id="segment-31" start_char="2564" end_char="2737">
<ORIGINAL_TEXT>So if what Montagnier said is true, the whole family of coronaviruses – which originated over 10,000 years ago – would have to be lab-made, and this is obviously nonsensical.</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="word" morph="none" start_char="2564" end_char="2565">So</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="2567" end_char="2568">if</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="2570" end_char="2573">what</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="2575" end_char="2584">Montagnier</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="2586" end_char="2589">said</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="2591" end_char="2592">is</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="2594" end_char="2597">true</TOKEN>
<TOKEN id="token-31-7" pos="punct" morph="none" start_char="2598" end_char="2598">,</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="2600" end_char="2602">the</TOKEN>
<TOKEN id="token-31-9" pos="word" morph="none" start_char="2604" end_char="2608">whole</TOKEN>
<TOKEN id="token-31-10" pos="word" morph="none" start_char="2610" end_char="2615">family</TOKEN>
<TOKEN id="token-31-11" pos="word" morph="none" start_char="2617" end_char="2618">of</TOKEN>
<TOKEN id="token-31-12" pos="word" morph="none" start_char="2620" end_char="2632">coronaviruses</TOKEN>
<TOKEN id="token-31-13" pos="punct" morph="none" start_char="2634" end_char="2634">–</TOKEN>
<TOKEN id="token-31-14" pos="word" morph="none" start_char="2636" end_char="2640">which</TOKEN>
<TOKEN id="token-31-15" pos="word" morph="none" start_char="2642" end_char="2651">originated</TOKEN>
<TOKEN id="token-31-16" pos="word" morph="none" start_char="2653" end_char="2656">over</TOKEN>
<TOKEN id="token-31-17" pos="unknown" morph="none" start_char="2658" end_char="2663">10,000</TOKEN>
<TOKEN id="token-31-18" pos="word" morph="none" start_char="2665" end_char="2669">years</TOKEN>
<TOKEN id="token-31-19" pos="word" morph="none" start_char="2671" end_char="2673">ago</TOKEN>
<TOKEN id="token-31-20" pos="punct" morph="none" start_char="2675" end_char="2675">–</TOKEN>
<TOKEN id="token-31-21" pos="word" morph="none" start_char="2677" end_char="2681">would</TOKEN>
<TOKEN id="token-31-22" pos="word" morph="none" start_char="2683" end_char="2686">have</TOKEN>
<TOKEN id="token-31-23" pos="word" morph="none" start_char="2688" end_char="2689">to</TOKEN>
<TOKEN id="token-31-24" pos="word" morph="none" start_char="2691" end_char="2692">be</TOKEN>
<TOKEN id="token-31-25" pos="unknown" morph="none" start_char="2694" end_char="2701">lab-made</TOKEN>
<TOKEN id="token-31-26" pos="punct" morph="none" start_char="2702" end_char="2702">,</TOKEN>
<TOKEN id="token-31-27" pos="word" morph="none" start_char="2704" end_char="2706">and</TOKEN>
<TOKEN id="token-31-28" pos="word" morph="none" start_char="2708" end_char="2711">this</TOKEN>
<TOKEN id="token-31-29" pos="word" morph="none" start_char="2713" end_char="2714">is</TOKEN>
<TOKEN id="token-31-30" pos="word" morph="none" start_char="2716" end_char="2724">obviously</TOKEN>
<TOKEN id="token-31-31" pos="word" morph="none" start_char="2726" end_char="2736">nonsensical</TOKEN>
<TOKEN id="token-31-32" pos="punct" morph="none" start_char="2737" end_char="2737">.</TOKEN>
</SEG>
<SEG id="segment-32" start_char="2740" end_char="2820">
<ORIGINAL_TEXT>Many experts have already pointed out this obvious flaw in Montagnier’s argument.</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="2740" end_char="2743">Many</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="2745" end_char="2751">experts</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="2753" end_char="2756">have</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="2758" end_char="2764">already</TOKEN>
<TOKEN id="token-32-4" pos="word" morph="none" start_char="2766" end_char="2772">pointed</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="2774" end_char="2776">out</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="2778" end_char="2781">this</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="2783" end_char="2789">obvious</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="2791" end_char="2794">flaw</TOKEN>
<TOKEN id="token-32-9" pos="word" morph="none" start_char="2796" end_char="2797">in</TOKEN>
<TOKEN id="token-32-10" pos="word" morph="none" start_char="2799" end_char="2810">Montagnier’s</TOKEN>
<TOKEN id="token-32-11" pos="word" morph="none" start_char="2812" end_char="2819">argument</TOKEN>
<TOKEN id="token-32-12" pos="punct" morph="none" start_char="2820" end_char="2820">.</TOKEN>
</SEG>
<SEG id="segment-33" start_char="2822" end_char="3008">
<ORIGINAL_TEXT>As Étienne Simon-Lorière, a professor at the Institut Pasteur in Paris, said, "If we take a word from a book and it looks like another word, can we say that one has copied from the other?</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="word" morph="none" start_char="2822" end_char="2823">As</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="2825" end_char="2831">Étienne</TOKEN>
<TOKEN id="token-33-2" pos="unknown" morph="none" start_char="2833" end_char="2845">Simon-Lorière</TOKEN>
<TOKEN id="token-33-3" pos="punct" morph="none" start_char="2846" end_char="2846">,</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="2848" end_char="2848">a</TOKEN>
<TOKEN id="token-33-5" pos="word" morph="none" start_char="2850" end_char="2858">professor</TOKEN>
<TOKEN id="token-33-6" pos="word" morph="none" start_char="2860" end_char="2861">at</TOKEN>
<TOKEN id="token-33-7" pos="word" morph="none" start_char="2863" end_char="2865">the</TOKEN>
<TOKEN id="token-33-8" pos="word" morph="none" start_char="2867" end_char="2874">Institut</TOKEN>
<TOKEN id="token-33-9" pos="word" morph="none" start_char="2876" end_char="2882">Pasteur</TOKEN>
<TOKEN id="token-33-10" pos="word" morph="none" start_char="2884" end_char="2885">in</TOKEN>
<TOKEN id="token-33-11" pos="word" morph="none" start_char="2887" end_char="2891">Paris</TOKEN>
<TOKEN id="token-33-12" pos="punct" morph="none" start_char="2892" end_char="2892">,</TOKEN>
<TOKEN id="token-33-13" pos="word" morph="none" start_char="2894" end_char="2897">said</TOKEN>
<TOKEN id="token-33-14" pos="punct" morph="none" start_char="2898" end_char="2898">,</TOKEN>
<TOKEN id="token-33-15" pos="punct" morph="none" start_char="2900" end_char="2900">"</TOKEN>
<TOKEN id="token-33-16" pos="word" morph="none" start_char="2901" end_char="2902">If</TOKEN>
<TOKEN id="token-33-17" pos="word" morph="none" start_char="2904" end_char="2905">we</TOKEN>
<TOKEN id="token-33-18" pos="word" morph="none" start_char="2907" end_char="2910">take</TOKEN>
<TOKEN id="token-33-19" pos="word" morph="none" start_char="2912" end_char="2912">a</TOKEN>
<TOKEN id="token-33-20" pos="word" morph="none" start_char="2914" end_char="2917">word</TOKEN>
<TOKEN id="token-33-21" pos="word" morph="none" start_char="2919" end_char="2922">from</TOKEN>
<TOKEN id="token-33-22" pos="word" morph="none" start_char="2924" end_char="2924">a</TOKEN>
<TOKEN id="token-33-23" pos="word" morph="none" start_char="2926" end_char="2929">book</TOKEN>
<TOKEN id="token-33-24" pos="word" morph="none" start_char="2931" end_char="2933">and</TOKEN>
<TOKEN id="token-33-25" pos="word" morph="none" start_char="2935" end_char="2936">it</TOKEN>
<TOKEN id="token-33-26" pos="word" morph="none" start_char="2938" end_char="2942">looks</TOKEN>
<TOKEN id="token-33-27" pos="word" morph="none" start_char="2944" end_char="2947">like</TOKEN>
<TOKEN id="token-33-28" pos="word" morph="none" start_char="2949" end_char="2955">another</TOKEN>
<TOKEN id="token-33-29" pos="word" morph="none" start_char="2957" end_char="2960">word</TOKEN>
<TOKEN id="token-33-30" pos="punct" morph="none" start_char="2961" end_char="2961">,</TOKEN>
<TOKEN id="token-33-31" pos="word" morph="none" start_char="2963" end_char="2965">can</TOKEN>
<TOKEN id="token-33-32" pos="word" morph="none" start_char="2967" end_char="2968">we</TOKEN>
<TOKEN id="token-33-33" pos="word" morph="none" start_char="2970" end_char="2972">say</TOKEN>
<TOKEN id="token-33-34" pos="word" morph="none" start_char="2974" end_char="2977">that</TOKEN>
<TOKEN id="token-33-35" pos="word" morph="none" start_char="2979" end_char="2981">one</TOKEN>
<TOKEN id="token-33-36" pos="word" morph="none" start_char="2983" end_char="2985">has</TOKEN>
<TOKEN id="token-33-37" pos="word" morph="none" start_char="2987" end_char="2992">copied</TOKEN>
<TOKEN id="token-33-38" pos="word" morph="none" start_char="2994" end_char="2997">from</TOKEN>
<TOKEN id="token-33-39" pos="word" morph="none" start_char="2999" end_char="3001">the</TOKEN>
<TOKEN id="token-33-40" pos="word" morph="none" start_char="3003" end_char="3007">other</TOKEN>
<TOKEN id="token-33-41" pos="punct" morph="none" start_char="3008" end_char="3008">?</TOKEN>
</SEG>
<SEG id="segment-34" start_char="3010" end_char="3025">
<ORIGINAL_TEXT>This is absurd!"</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="3010" end_char="3013">This</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="3015" end_char="3016">is</TOKEN>
<TOKEN id="token-34-2" pos="word" morph="none" start_char="3018" end_char="3023">absurd</TOKEN>
<TOKEN id="token-34-3" pos="punct" morph="none" start_char="3024" end_char="3025">!"</TOKEN>
</SEG>
<SEG id="segment-35" start_char="3028" end_char="3179">
<ORIGINAL_TEXT>It is surprising to have a scientist of Montagnier’s stature utter such questionable statements – although Montagnier himself is a controversial figure.</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="3028" end_char="3029">It</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="3031" end_char="3032">is</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="3034" end_char="3043">surprising</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="3045" end_char="3046">to</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="3048" end_char="3051">have</TOKEN>
<TOKEN id="token-35-5" pos="word" morph="none" start_char="3053" end_char="3053">a</TOKEN>
<TOKEN id="token-35-6" pos="word" morph="none" start_char="3055" end_char="3063">scientist</TOKEN>
<TOKEN id="token-35-7" pos="word" morph="none" start_char="3065" end_char="3066">of</TOKEN>
<TOKEN id="token-35-8" pos="word" morph="none" start_char="3068" end_char="3079">Montagnier’s</TOKEN>
<TOKEN id="token-35-9" pos="word" morph="none" start_char="3081" end_char="3087">stature</TOKEN>
<TOKEN id="token-35-10" pos="word" morph="none" start_char="3089" end_char="3093">utter</TOKEN>
<TOKEN id="token-35-11" pos="word" morph="none" start_char="3095" end_char="3098">such</TOKEN>
<TOKEN id="token-35-12" pos="word" morph="none" start_char="3100" end_char="3111">questionable</TOKEN>
<TOKEN id="token-35-13" pos="word" morph="none" start_char="3113" end_char="3122">statements</TOKEN>
<TOKEN id="token-35-14" pos="punct" morph="none" start_char="3124" end_char="3124">–</TOKEN>
<TOKEN id="token-35-15" pos="word" morph="none" start_char="3126" end_char="3133">although</TOKEN>
<TOKEN id="token-35-16" pos="word" morph="none" start_char="3135" end_char="3144">Montagnier</TOKEN>
<TOKEN id="token-35-17" pos="word" morph="none" start_char="3146" end_char="3152">himself</TOKEN>
<TOKEN id="token-35-18" pos="word" morph="none" start_char="3154" end_char="3155">is</TOKEN>
<TOKEN id="token-35-19" pos="word" morph="none" start_char="3157" end_char="3157">a</TOKEN>
<TOKEN id="token-35-20" pos="word" morph="none" start_char="3159" end_char="3171">controversial</TOKEN>
<TOKEN id="token-35-21" pos="word" morph="none" start_char="3173" end_char="3178">figure</TOKEN>
<TOKEN id="token-35-22" pos="punct" morph="none" start_char="3179" end_char="3179">.</TOKEN>
</SEG>
<SEG id="segment-36" start_char="3181" end_char="3299">
<ORIGINAL_TEXT>Among other causes, he has supported anti-vaxxers, homeopathy and a silly claim that DNA emits "electromagnetic waves".</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="3181" end_char="3185">Among</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="3187" end_char="3191">other</TOKEN>
<TOKEN id="token-36-2" pos="word" morph="none" start_char="3193" end_char="3198">causes</TOKEN>
<TOKEN id="token-36-3" pos="punct" morph="none" start_char="3199" end_char="3199">,</TOKEN>
<TOKEN id="token-36-4" pos="word" morph="none" start_char="3201" end_char="3202">he</TOKEN>
<TOKEN id="token-36-5" pos="word" morph="none" start_char="3204" end_char="3206">has</TOKEN>
<TOKEN id="token-36-6" pos="word" morph="none" start_char="3208" end_char="3216">supported</TOKEN>
<TOKEN id="token-36-7" pos="unknown" morph="none" start_char="3218" end_char="3229">anti-vaxxers</TOKEN>
<TOKEN id="token-36-8" pos="punct" morph="none" start_char="3230" end_char="3230">,</TOKEN>
<TOKEN id="token-36-9" pos="word" morph="none" start_char="3232" end_char="3241">homeopathy</TOKEN>
<TOKEN id="token-36-10" pos="word" morph="none" start_char="3243" end_char="3245">and</TOKEN>
<TOKEN id="token-36-11" pos="word" morph="none" start_char="3247" end_char="3247">a</TOKEN>
<TOKEN id="token-36-12" pos="word" morph="none" start_char="3249" end_char="3253">silly</TOKEN>
<TOKEN id="token-36-13" pos="word" morph="none" start_char="3255" end_char="3259">claim</TOKEN>
<TOKEN id="token-36-14" pos="word" morph="none" start_char="3261" end_char="3264">that</TOKEN>
<TOKEN id="token-36-15" pos="word" morph="none" start_char="3266" end_char="3268">DNA</TOKEN>
<TOKEN id="token-36-16" pos="word" morph="none" start_char="3270" end_char="3274">emits</TOKEN>
<TOKEN id="token-36-17" pos="punct" morph="none" start_char="3276" end_char="3276">"</TOKEN>
<TOKEN id="token-36-18" pos="word" morph="none" start_char="3277" end_char="3291">electromagnetic</TOKEN>
<TOKEN id="token-36-19" pos="word" morph="none" start_char="3293" end_char="3297">waves</TOKEN>
<TOKEN id="token-36-20" pos="punct" morph="none" start_char="3298" end_char="3299">".</TOKEN>
</SEG>
<SEG id="segment-37" start_char="3302" end_char="3476">
<ORIGINAL_TEXT>As he lost credibility among his peers, scientific agencies around Europe began to reject his grant applications, and eventually he was left with no money to pursue his ideas.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="3302" end_char="3303">As</TOKEN>
<TOKEN id="token-37-1" pos="word" morph="none" start_char="3305" end_char="3306">he</TOKEN>
<TOKEN id="token-37-2" pos="word" morph="none" start_char="3308" end_char="3311">lost</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="3313" end_char="3323">credibility</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="3325" end_char="3329">among</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="3331" end_char="3333">his</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="3335" end_char="3339">peers</TOKEN>
<TOKEN id="token-37-7" pos="punct" morph="none" start_char="3340" end_char="3340">,</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="3342" end_char="3351">scientific</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="3353" end_char="3360">agencies</TOKEN>
<TOKEN id="token-37-10" pos="word" morph="none" start_char="3362" end_char="3367">around</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="3369" end_char="3374">Europe</TOKEN>
<TOKEN id="token-37-12" pos="word" morph="none" start_char="3376" end_char="3380">began</TOKEN>
<TOKEN id="token-37-13" pos="word" morph="none" start_char="3382" end_char="3383">to</TOKEN>
<TOKEN id="token-37-14" pos="word" morph="none" start_char="3385" end_char="3390">reject</TOKEN>
<TOKEN id="token-37-15" pos="word" morph="none" start_char="3392" end_char="3394">his</TOKEN>
<TOKEN id="token-37-16" pos="word" morph="none" start_char="3396" end_char="3400">grant</TOKEN>
<TOKEN id="token-37-17" pos="word" morph="none" start_char="3402" end_char="3413">applications</TOKEN>
<TOKEN id="token-37-18" pos="punct" morph="none" start_char="3414" end_char="3414">,</TOKEN>
<TOKEN id="token-37-19" pos="word" morph="none" start_char="3416" end_char="3418">and</TOKEN>
<TOKEN id="token-37-20" pos="word" morph="none" start_char="3420" end_char="3429">eventually</TOKEN>
<TOKEN id="token-37-21" pos="word" morph="none" start_char="3431" end_char="3432">he</TOKEN>
<TOKEN id="token-37-22" pos="word" morph="none" start_char="3434" end_char="3436">was</TOKEN>
<TOKEN id="token-37-23" pos="word" morph="none" start_char="3438" end_char="3441">left</TOKEN>
<TOKEN id="token-37-24" pos="word" morph="none" start_char="3443" end_char="3446">with</TOKEN>
<TOKEN id="token-37-25" pos="word" morph="none" start_char="3448" end_char="3449">no</TOKEN>
<TOKEN id="token-37-26" pos="word" morph="none" start_char="3451" end_char="3455">money</TOKEN>
<TOKEN id="token-37-27" pos="word" morph="none" start_char="3457" end_char="3458">to</TOKEN>
<TOKEN id="token-37-28" pos="word" morph="none" start_char="3460" end_char="3465">pursue</TOKEN>
<TOKEN id="token-37-29" pos="word" morph="none" start_char="3467" end_char="3469">his</TOKEN>
<TOKEN id="token-37-30" pos="word" morph="none" start_char="3471" end_char="3475">ideas</TOKEN>
<TOKEN id="token-37-31" pos="punct" morph="none" start_char="3476" end_char="3476">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="3478" end_char="3572">
<ORIGINAL_TEXT>In a 2010 interview, Montagnier said he was leaving Europe to "escape the intellectual terror."</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="3478" end_char="3479">In</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="3481" end_char="3481">a</TOKEN>
<TOKEN id="token-38-2" pos="word" morph="none" start_char="3483" end_char="3486">2010</TOKEN>
<TOKEN id="token-38-3" pos="word" morph="none" start_char="3488" end_char="3496">interview</TOKEN>
<TOKEN id="token-38-4" pos="punct" morph="none" start_char="3497" end_char="3497">,</TOKEN>
<TOKEN id="token-38-5" pos="word" morph="none" start_char="3499" end_char="3508">Montagnier</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="3510" end_char="3513">said</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="3515" end_char="3516">he</TOKEN>
<TOKEN id="token-38-8" pos="word" morph="none" start_char="3518" end_char="3520">was</TOKEN>
<TOKEN id="token-38-9" pos="word" morph="none" start_char="3522" end_char="3528">leaving</TOKEN>
<TOKEN id="token-38-10" pos="word" morph="none" start_char="3530" end_char="3535">Europe</TOKEN>
<TOKEN id="token-38-11" pos="word" morph="none" start_char="3537" end_char="3538">to</TOKEN>
<TOKEN id="token-38-12" pos="punct" morph="none" start_char="3540" end_char="3540">"</TOKEN>
<TOKEN id="token-38-13" pos="word" morph="none" start_char="3541" end_char="3546">escape</TOKEN>
<TOKEN id="token-38-14" pos="word" morph="none" start_char="3548" end_char="3550">the</TOKEN>
<TOKEN id="token-38-15" pos="word" morph="none" start_char="3552" end_char="3563">intellectual</TOKEN>
<TOKEN id="token-38-16" pos="word" morph="none" start_char="3565" end_char="3570">terror</TOKEN>
<TOKEN id="token-38-17" pos="punct" morph="none" start_char="3571" end_char="3572">."</TOKEN>
</SEG>
<SEG id="segment-39" start_char="3574" end_char="3648">
<ORIGINAL_TEXT>He added, "I’m no longer allowed to work at a public institute (in France).</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="word" morph="none" start_char="3574" end_char="3575">He</TOKEN>
<TOKEN id="token-39-1" pos="word" morph="none" start_char="3577" end_char="3581">added</TOKEN>
<TOKEN id="token-39-2" pos="punct" morph="none" start_char="3582" end_char="3582">,</TOKEN>
<TOKEN id="token-39-3" pos="punct" morph="none" start_char="3584" end_char="3584">"</TOKEN>
<TOKEN id="token-39-4" pos="word" morph="none" start_char="3585" end_char="3587">I’m</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="3589" end_char="3590">no</TOKEN>
<TOKEN id="token-39-6" pos="word" morph="none" start_char="3592" end_char="3597">longer</TOKEN>
<TOKEN id="token-39-7" pos="word" morph="none" start_char="3599" end_char="3605">allowed</TOKEN>
<TOKEN id="token-39-8" pos="word" morph="none" start_char="3607" end_char="3608">to</TOKEN>
<TOKEN id="token-39-9" pos="word" morph="none" start_char="3610" end_char="3613">work</TOKEN>
<TOKEN id="token-39-10" pos="word" morph="none" start_char="3615" end_char="3616">at</TOKEN>
<TOKEN id="token-39-11" pos="word" morph="none" start_char="3618" end_char="3618">a</TOKEN>
<TOKEN id="token-39-12" pos="word" morph="none" start_char="3620" end_char="3625">public</TOKEN>
<TOKEN id="token-39-13" pos="word" morph="none" start_char="3627" end_char="3635">institute</TOKEN>
<TOKEN id="token-39-14" pos="punct" morph="none" start_char="3637" end_char="3637">(</TOKEN>
<TOKEN id="token-39-15" pos="word" morph="none" start_char="3638" end_char="3639">in</TOKEN>
<TOKEN id="token-39-16" pos="word" morph="none" start_char="3641" end_char="3646">France</TOKEN>
<TOKEN id="token-39-17" pos="punct" morph="none" start_char="3647" end_char="3648">).</TOKEN>
</SEG>
<SEG id="segment-40" start_char="3650" end_char="3725">
<ORIGINAL_TEXT>I have applied for funding from other sources, but I have been turned down."</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="word" morph="none" start_char="3650" end_char="3650">I</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="3652" end_char="3655">have</TOKEN>
<TOKEN id="token-40-2" pos="word" morph="none" start_char="3657" end_char="3663">applied</TOKEN>
<TOKEN id="token-40-3" pos="word" morph="none" start_char="3665" end_char="3667">for</TOKEN>
<TOKEN id="token-40-4" pos="word" morph="none" start_char="3669" end_char="3675">funding</TOKEN>
<TOKEN id="token-40-5" pos="word" morph="none" start_char="3677" end_char="3680">from</TOKEN>
<TOKEN id="token-40-6" pos="word" morph="none" start_char="3682" end_char="3686">other</TOKEN>
<TOKEN id="token-40-7" pos="word" morph="none" start_char="3688" end_char="3694">sources</TOKEN>
<TOKEN id="token-40-8" pos="punct" morph="none" start_char="3695" end_char="3695">,</TOKEN>
<TOKEN id="token-40-9" pos="word" morph="none" start_char="3697" end_char="3699">but</TOKEN>
<TOKEN id="token-40-10" pos="word" morph="none" start_char="3701" end_char="3701">I</TOKEN>
<TOKEN id="token-40-11" pos="word" morph="none" start_char="3703" end_char="3706">have</TOKEN>
<TOKEN id="token-40-12" pos="word" morph="none" start_char="3708" end_char="3711">been</TOKEN>
<TOKEN id="token-40-13" pos="word" morph="none" start_char="3713" end_char="3718">turned</TOKEN>
<TOKEN id="token-40-14" pos="word" morph="none" start_char="3720" end_char="3723">down</TOKEN>
<TOKEN id="token-40-15" pos="punct" morph="none" start_char="3724" end_char="3725">."</TOKEN>
</SEG>
<SEG id="segment-41" start_char="3728" end_char="3815">
<ORIGINAL_TEXT>Pandemics have historically been breeding grounds for fake news and conspiracy theories.</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="3728" end_char="3736">Pandemics</TOKEN>
<TOKEN id="token-41-1" pos="word" morph="none" start_char="3738" end_char="3741">have</TOKEN>
<TOKEN id="token-41-2" pos="word" morph="none" start_char="3743" end_char="3754">historically</TOKEN>
<TOKEN id="token-41-3" pos="word" morph="none" start_char="3756" end_char="3759">been</TOKEN>
<TOKEN id="token-41-4" pos="word" morph="none" start_char="3761" end_char="3768">breeding</TOKEN>
<TOKEN id="token-41-5" pos="word" morph="none" start_char="3770" end_char="3776">grounds</TOKEN>
<TOKEN id="token-41-6" pos="word" morph="none" start_char="3778" end_char="3780">for</TOKEN>
<TOKEN id="token-41-7" pos="word" morph="none" start_char="3782" end_char="3785">fake</TOKEN>
<TOKEN id="token-41-8" pos="word" morph="none" start_char="3787" end_char="3790">news</TOKEN>
<TOKEN id="token-41-9" pos="word" morph="none" start_char="3792" end_char="3794">and</TOKEN>
<TOKEN id="token-41-10" pos="word" morph="none" start_char="3796" end_char="3805">conspiracy</TOKEN>
<TOKEN id="token-41-11" pos="word" morph="none" start_char="3807" end_char="3814">theories</TOKEN>
<TOKEN id="token-41-12" pos="punct" morph="none" start_char="3815" end_char="3815">.</TOKEN>
</SEG>
<SEG id="segment-42" start_char="3817" end_char="4050">
<ORIGINAL_TEXT>For example, in the 14th century, the bubonic plague epidemic in Europe fuelled a misbelief among Christians that the Jews were deliberately poisoning wells and rivers with infectious "miasma", leading to the mass persecution of Jews.</ORIGINAL_TEXT>
<TOKEN id="token-42-0" pos="word" morph="none" start_char="3817" end_char="3819">For</TOKEN>
<TOKEN id="token-42-1" pos="word" morph="none" start_char="3821" end_char="3827">example</TOKEN>
<TOKEN id="token-42-2" pos="punct" morph="none" start_char="3828" end_char="3828">,</TOKEN>
<TOKEN id="token-42-3" pos="word" morph="none" start_char="3830" end_char="3831">in</TOKEN>
<TOKEN id="token-42-4" pos="word" morph="none" start_char="3833" end_char="3835">the</TOKEN>
<TOKEN id="token-42-5" pos="word" morph="none" start_char="3837" end_char="3840">14th</TOKEN>
<TOKEN id="token-42-6" pos="word" morph="none" start_char="3842" end_char="3848">century</TOKEN>
<TOKEN id="token-42-7" pos="punct" morph="none" start_char="3849" end_char="3849">,</TOKEN>
<TOKEN id="token-42-8" pos="word" morph="none" start_char="3851" end_char="3853">the</TOKEN>
<TOKEN id="token-42-9" pos="word" morph="none" start_char="3855" end_char="3861">bubonic</TOKEN>
<TOKEN id="token-42-10" pos="word" morph="none" start_char="3863" end_char="3868">plague</TOKEN>
<TOKEN id="token-42-11" pos="word" morph="none" start_char="3870" end_char="3877">epidemic</TOKEN>
<TOKEN id="token-42-12" pos="word" morph="none" start_char="3879" end_char="3880">in</TOKEN>
<TOKEN id="token-42-13" pos="word" morph="none" start_char="3882" end_char="3887">Europe</TOKEN>
<TOKEN id="token-42-14" pos="word" morph="none" start_char="3889" end_char="3895">fuelled</TOKEN>
<TOKEN id="token-42-15" pos="word" morph="none" start_char="3897" end_char="3897">a</TOKEN>
<TOKEN id="token-42-16" pos="word" morph="none" start_char="3899" end_char="3907">misbelief</TOKEN>
<TOKEN id="token-42-17" pos="word" morph="none" start_char="3909" end_char="3913">among</TOKEN>
<TOKEN id="token-42-18" pos="word" morph="none" start_char="3915" end_char="3924">Christians</TOKEN>
<TOKEN id="token-42-19" pos="word" morph="none" start_char="3926" end_char="3929">that</TOKEN>
<TOKEN id="token-42-20" pos="word" morph="none" start_char="3931" end_char="3933">the</TOKEN>
<TOKEN id="token-42-21" pos="word" morph="none" start_char="3935" end_char="3938">Jews</TOKEN>
<TOKEN id="token-42-22" pos="word" morph="none" start_char="3940" end_char="3943">were</TOKEN>
<TOKEN id="token-42-23" pos="word" morph="none" start_char="3945" end_char="3956">deliberately</TOKEN>
<TOKEN id="token-42-24" pos="word" morph="none" start_char="3958" end_char="3966">poisoning</TOKEN>
<TOKEN id="token-42-25" pos="word" morph="none" start_char="3968" end_char="3972">wells</TOKEN>
<TOKEN id="token-42-26" pos="word" morph="none" start_char="3974" end_char="3976">and</TOKEN>
<TOKEN id="token-42-27" pos="word" morph="none" start_char="3978" end_char="3983">rivers</TOKEN>
<TOKEN id="token-42-28" pos="word" morph="none" start_char="3985" end_char="3988">with</TOKEN>
<TOKEN id="token-42-29" pos="word" morph="none" start_char="3990" end_char="3999">infectious</TOKEN>
<TOKEN id="token-42-30" pos="punct" morph="none" start_char="4001" end_char="4001">"</TOKEN>
<TOKEN id="token-42-31" pos="word" morph="none" start_char="4002" end_char="4007">miasma</TOKEN>
<TOKEN id="token-42-32" pos="punct" morph="none" start_char="4008" end_char="4009">",</TOKEN>
<TOKEN id="token-42-33" pos="word" morph="none" start_char="4011" end_char="4017">leading</TOKEN>
<TOKEN id="token-42-34" pos="word" morph="none" start_char="4019" end_char="4020">to</TOKEN>
<TOKEN id="token-42-35" pos="word" morph="none" start_char="4022" end_char="4024">the</TOKEN>
<TOKEN id="token-42-36" pos="word" morph="none" start_char="4026" end_char="4029">mass</TOKEN>
<TOKEN id="token-42-37" pos="word" morph="none" start_char="4031" end_char="4041">persecution</TOKEN>
<TOKEN id="token-42-38" pos="word" morph="none" start_char="4043" end_char="4044">of</TOKEN>
<TOKEN id="token-42-39" pos="word" morph="none" start_char="4046" end_char="4049">Jews</TOKEN>
<TOKEN id="token-42-40" pos="punct" morph="none" start_char="4050" end_char="4050">.</TOKEN>
</SEG>
<SEG id="segment-43" start_char="4052" end_char="4291">
<ORIGINAL_TEXT>Even when Montagnier helped discover the HIV virus (alongside Françoise Barré-Sinoussi) in the early 1980s, a prominent conspiracy theory in the US was that HIV is a human-made virus that the government had created to wipe out black people.</ORIGINAL_TEXT>
<TOKEN id="token-43-0" pos="word" morph="none" start_char="4052" end_char="4055">Even</TOKEN>
<TOKEN id="token-43-1" pos="word" morph="none" start_char="4057" end_char="4060">when</TOKEN>
<TOKEN id="token-43-2" pos="word" morph="none" start_char="4062" end_char="4071">Montagnier</TOKEN>
<TOKEN id="token-43-3" pos="word" morph="none" start_char="4073" end_char="4078">helped</TOKEN>
<TOKEN id="token-43-4" pos="word" morph="none" start_char="4080" end_char="4087">discover</TOKEN>
<TOKEN id="token-43-5" pos="word" morph="none" start_char="4089" end_char="4091">the</TOKEN>
<TOKEN id="token-43-6" pos="word" morph="none" start_char="4093" end_char="4095">HIV</TOKEN>
<TOKEN id="token-43-7" pos="word" morph="none" start_char="4097" end_char="4101">virus</TOKEN>
<TOKEN id="token-43-8" pos="punct" morph="none" start_char="4103" end_char="4103">(</TOKEN>
<TOKEN id="token-43-9" pos="word" morph="none" start_char="4104" end_char="4112">alongside</TOKEN>
<TOKEN id="token-43-10" pos="word" morph="none" start_char="4114" end_char="4122">Françoise</TOKEN>
<TOKEN id="token-43-11" pos="unknown" morph="none" start_char="4124" end_char="4137">Barré-Sinoussi</TOKEN>
<TOKEN id="token-43-12" pos="punct" morph="none" start_char="4138" end_char="4138">)</TOKEN>
<TOKEN id="token-43-13" pos="word" morph="none" start_char="4140" end_char="4141">in</TOKEN>
<TOKEN id="token-43-14" pos="word" morph="none" start_char="4143" end_char="4145">the</TOKEN>
<TOKEN id="token-43-15" pos="word" morph="none" start_char="4147" end_char="4151">early</TOKEN>
<TOKEN id="token-43-16" pos="word" morph="none" start_char="4153" end_char="4157">1980s</TOKEN>
<TOKEN id="token-43-17" pos="punct" morph="none" start_char="4158" end_char="4158">,</TOKEN>
<TOKEN id="token-43-18" pos="word" morph="none" start_char="4160" end_char="4160">a</TOKEN>
<TOKEN id="token-43-19" pos="word" morph="none" start_char="4162" end_char="4170">prominent</TOKEN>
<TOKEN id="token-43-20" pos="word" morph="none" start_char="4172" end_char="4181">conspiracy</TOKEN>
<TOKEN id="token-43-21" pos="word" morph="none" start_char="4183" end_char="4188">theory</TOKEN>
<TOKEN id="token-43-22" pos="word" morph="none" start_char="4190" end_char="4191">in</TOKEN>
<TOKEN id="token-43-23" pos="word" morph="none" start_char="4193" end_char="4195">the</TOKEN>
<TOKEN id="token-43-24" pos="word" morph="none" start_char="4197" end_char="4198">US</TOKEN>
<TOKEN id="token-43-25" pos="word" morph="none" start_char="4200" end_char="4202">was</TOKEN>
<TOKEN id="token-43-26" pos="word" morph="none" start_char="4204" end_char="4207">that</TOKEN>
<TOKEN id="token-43-27" pos="word" morph="none" start_char="4209" end_char="4211">HIV</TOKEN>
<TOKEN id="token-43-28" pos="word" morph="none" start_char="4213" end_char="4214">is</TOKEN>
<TOKEN id="token-43-29" pos="word" morph="none" start_char="4216" end_char="4216">a</TOKEN>
<TOKEN id="token-43-30" pos="unknown" morph="none" start_char="4218" end_char="4227">human-made</TOKEN>
<TOKEN id="token-43-31" pos="word" morph="none" start_char="4229" end_char="4233">virus</TOKEN>
<TOKEN id="token-43-32" pos="word" morph="none" start_char="4235" end_char="4238">that</TOKEN>
<TOKEN id="token-43-33" pos="word" morph="none" start_char="4240" end_char="4242">the</TOKEN>
<TOKEN id="token-43-34" pos="word" morph="none" start_char="4244" end_char="4253">government</TOKEN>
<TOKEN id="token-43-35" pos="word" morph="none" start_char="4255" end_char="4257">had</TOKEN>
<TOKEN id="token-43-36" pos="word" morph="none" start_char="4259" end_char="4265">created</TOKEN>
<TOKEN id="token-43-37" pos="word" morph="none" start_char="4267" end_char="4268">to</TOKEN>
<TOKEN id="token-43-38" pos="word" morph="none" start_char="4270" end_char="4273">wipe</TOKEN>
<TOKEN id="token-43-39" pos="word" morph="none" start_char="4275" end_char="4277">out</TOKEN>
<TOKEN id="token-43-40" pos="word" morph="none" start_char="4279" end_char="4283">black</TOKEN>
<TOKEN id="token-43-41" pos="word" morph="none" start_char="4285" end_char="4290">people</TOKEN>
<TOKEN id="token-43-42" pos="punct" morph="none" start_char="4291" end_char="4291">.</TOKEN>
</SEG>
<SEG id="segment-44" start_char="4294" end_char="4557">
<ORIGINAL_TEXT>And because pandemics are so fraught with misinformation, they also make for an important time to communicate good science, double-check suspicious comments, refuse to accept claims without good reason, and not amplify pseudoscience without suitable qualification.</ORIGINAL_TEXT>
<TOKEN id="token-44-0" pos="word" morph="none" start_char="4294" end_char="4296">And</TOKEN>
<TOKEN id="token-44-1" pos="word" morph="none" start_char="4298" end_char="4304">because</TOKEN>
<TOKEN id="token-44-2" pos="word" morph="none" start_char="4306" end_char="4314">pandemics</TOKEN>
<TOKEN id="token-44-3" pos="word" morph="none" start_char="4316" end_char="4318">are</TOKEN>
<TOKEN id="token-44-4" pos="word" morph="none" start_char="4320" end_char="4321">so</TOKEN>
<TOKEN id="token-44-5" pos="word" morph="none" start_char="4323" end_char="4329">fraught</TOKEN>
<TOKEN id="token-44-6" pos="word" morph="none" start_char="4331" end_char="4334">with</TOKEN>
<TOKEN id="token-44-7" pos="word" morph="none" start_char="4336" end_char="4349">misinformation</TOKEN>
<TOKEN id="token-44-8" pos="punct" morph="none" start_char="4350" end_char="4350">,</TOKEN>
<TOKEN id="token-44-9" pos="word" morph="none" start_char="4352" end_char="4355">they</TOKEN>
<TOKEN id="token-44-10" pos="word" morph="none" start_char="4357" end_char="4360">also</TOKEN>
<TOKEN id="token-44-11" pos="word" morph="none" start_char="4362" end_char="4365">make</TOKEN>
<TOKEN id="token-44-12" pos="word" morph="none" start_char="4367" end_char="4369">for</TOKEN>
<TOKEN id="token-44-13" pos="word" morph="none" start_char="4371" end_char="4372">an</TOKEN>
<TOKEN id="token-44-14" pos="word" morph="none" start_char="4374" end_char="4382">important</TOKEN>
<TOKEN id="token-44-15" pos="word" morph="none" start_char="4384" end_char="4387">time</TOKEN>
<TOKEN id="token-44-16" pos="word" morph="none" start_char="4389" end_char="4390">to</TOKEN>
<TOKEN id="token-44-17" pos="word" morph="none" start_char="4392" end_char="4402">communicate</TOKEN>
<TOKEN id="token-44-18" pos="word" morph="none" start_char="4404" end_char="4407">good</TOKEN>
<TOKEN id="token-44-19" pos="word" morph="none" start_char="4409" end_char="4415">science</TOKEN>
<TOKEN id="token-44-20" pos="punct" morph="none" start_char="4416" end_char="4416">,</TOKEN>
<TOKEN id="token-44-21" pos="unknown" morph="none" start_char="4418" end_char="4429">double-check</TOKEN>
<TOKEN id="token-44-22" pos="word" morph="none" start_char="4431" end_char="4440">suspicious</TOKEN>
<TOKEN id="token-44-23" pos="word" morph="none" start_char="4442" end_char="4449">comments</TOKEN>
<TOKEN id="token-44-24" pos="punct" morph="none" start_char="4450" end_char="4450">,</TOKEN>
<TOKEN id="token-44-25" pos="word" morph="none" start_char="4452" end_char="4457">refuse</TOKEN>
<TOKEN id="token-44-26" pos="word" morph="none" start_char="4459" end_char="4460">to</TOKEN>
<TOKEN id="token-44-27" pos="word" morph="none" start_char="4462" end_char="4467">accept</TOKEN>
<TOKEN id="token-44-28" pos="word" morph="none" start_char="4469" end_char="4474">claims</TOKEN>
<TOKEN id="token-44-29" pos="word" morph="none" start_char="4476" end_char="4482">without</TOKEN>
<TOKEN id="token-44-30" pos="word" morph="none" start_char="4484" end_char="4487">good</TOKEN>
<TOKEN id="token-44-31" pos="word" morph="none" start_char="4489" end_char="4494">reason</TOKEN>
<TOKEN id="token-44-32" pos="punct" morph="none" start_char="4495" end_char="4495">,</TOKEN>
<TOKEN id="token-44-33" pos="word" morph="none" start_char="4497" end_char="4499">and</TOKEN>
<TOKEN id="token-44-34" pos="word" morph="none" start_char="4501" end_char="4503">not</TOKEN>
<TOKEN id="token-44-35" pos="word" morph="none" start_char="4505" end_char="4511">amplify</TOKEN>
<TOKEN id="token-44-36" pos="word" morph="none" start_char="4513" end_char="4525">pseudoscience</TOKEN>
<TOKEN id="token-44-37" pos="word" morph="none" start_char="4527" end_char="4533">without</TOKEN>
<TOKEN id="token-44-38" pos="word" morph="none" start_char="4535" end_char="4542">suitable</TOKEN>
<TOKEN id="token-44-39" pos="word" morph="none" start_char="4544" end_char="4556">qualification</TOKEN>
<TOKEN id="token-44-40" pos="punct" morph="none" start_char="4557" end_char="4557">.</TOKEN>
</SEG>
<SEG id="segment-45" start_char="4560" end_char="4653">
<ORIGINAL_TEXT>Felix Bast is a science writer and an associate professor at the Central University of Punjab.</ORIGINAL_TEXT>
<TOKEN id="token-45-0" pos="word" morph="none" start_char="4560" end_char="4564">Felix</TOKEN>
<TOKEN id="token-45-1" pos="word" morph="none" start_char="4566" end_char="4569">Bast</TOKEN>
<TOKEN id="token-45-2" pos="word" morph="none" start_char="4571" end_char="4572">is</TOKEN>
<TOKEN id="token-45-3" pos="word" morph="none" start_char="4574" end_char="4574">a</TOKEN>
<TOKEN id="token-45-4" pos="word" morph="none" start_char="4576" end_char="4582">science</TOKEN>
<TOKEN id="token-45-5" pos="word" morph="none" start_char="4584" end_char="4589">writer</TOKEN>
<TOKEN id="token-45-6" pos="word" morph="none" start_char="4591" end_char="4593">and</TOKEN>
<TOKEN id="token-45-7" pos="word" morph="none" start_char="4595" end_char="4596">an</TOKEN>
<TOKEN id="token-45-8" pos="word" morph="none" start_char="4598" end_char="4606">associate</TOKEN>
<TOKEN id="token-45-9" pos="word" morph="none" start_char="4608" end_char="4616">professor</TOKEN>
<TOKEN id="token-45-10" pos="word" morph="none" start_char="4618" end_char="4619">at</TOKEN>
<TOKEN id="token-45-11" pos="word" morph="none" start_char="4621" end_char="4623">the</TOKEN>
<TOKEN id="token-45-12" pos="word" morph="none" start_char="4625" end_char="4631">Central</TOKEN>
<TOKEN id="token-45-13" pos="word" morph="none" start_char="4633" end_char="4642">University</TOKEN>
<TOKEN id="token-45-14" pos="word" morph="none" start_char="4644" end_char="4645">of</TOKEN>
<TOKEN id="token-45-15" pos="word" morph="none" start_char="4647" end_char="4652">Punjab</TOKEN>
<TOKEN id="token-45-16" pos="punct" morph="none" start_char="4653" end_char="4653">.</TOKEN>
</SEG>
<SEG id="segment-46" start_char="4655" end_char="4748">
<ORIGINAL_TEXT>This article was originally published on Medium and has been republished here with permission.</ORIGINAL_TEXT>
<TOKEN id="token-46-0" pos="word" morph="none" start_char="4655" end_char="4658">This</TOKEN>
<TOKEN id="token-46-1" pos="word" morph="none" start_char="4660" end_char="4666">article</TOKEN>
<TOKEN id="token-46-2" pos="word" morph="none" start_char="4668" end_char="4670">was</TOKEN>
<TOKEN id="token-46-3" pos="word" morph="none" start_char="4672" end_char="4681">originally</TOKEN>
<TOKEN id="token-46-4" pos="word" morph="none" start_char="4683" end_char="4691">published</TOKEN>
<TOKEN id="token-46-5" pos="word" morph="none" start_char="4693" end_char="4694">on</TOKEN>
<TOKEN id="token-46-6" pos="word" morph="none" start_char="4696" end_char="4701">Medium</TOKEN>
<TOKEN id="token-46-7" pos="word" morph="none" start_char="4703" end_char="4705">and</TOKEN>
<TOKEN id="token-46-8" pos="word" morph="none" start_char="4707" end_char="4709">has</TOKEN>
<TOKEN id="token-46-9" pos="word" morph="none" start_char="4711" end_char="4714">been</TOKEN>
<TOKEN id="token-46-10" pos="word" morph="none" start_char="4716" end_char="4726">republished</TOKEN>
<TOKEN id="token-46-11" pos="word" morph="none" start_char="4728" end_char="4731">here</TOKEN>
<TOKEN id="token-46-12" pos="word" morph="none" start_char="4733" end_char="4736">with</TOKEN>
<TOKEN id="token-46-13" pos="word" morph="none" start_char="4738" end_char="4747">permission</TOKEN>
<TOKEN id="token-46-14" pos="punct" morph="none" start_char="4748" end_char="4748">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
