<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C049DTG" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="2041" raw_text_md5="c4d082864e11b983fd735e768e45c361">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="87">
<ORIGINAL_TEXT>Un error podría ser el origen de la pandemia de coronavirus, según un Nobel de Medicina</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="2">Un</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="4" end_char="8">error</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="10" end_char="15">podría</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="17" end_char="19">ser</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="21" end_char="22">el</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="24" end_char="29">origen</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="31" end_char="32">de</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="34" end_char="35">la</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="37" end_char="44">pandemia</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="46" end_char="47">de</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="49" end_char="59">coronavirus</TOKEN>
<TOKEN id="token-0-11" pos="punct" morph="none" start_char="60" end_char="60">,</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="62" end_char="66">según</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="68" end_char="69">un</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="71" end_char="75">Nobel</TOKEN>
<TOKEN id="token-0-15" pos="word" morph="none" start_char="77" end_char="78">de</TOKEN>
<TOKEN id="token-0-16" pos="word" morph="none" start_char="80" end_char="87">Medicina</TOKEN>
</SEG>
<SEG id="segment-1" start_char="91" end_char="267">
<ORIGINAL_TEXT>El Premio Nobel de Medicina de 2008, el virólogo Luc Montagnier, es de los que defienden que el coronavirus​ se creó en un laboratorio y, además, que se contagió por un "error".</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="91" end_char="92">El</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="94" end_char="99">Premio</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="101" end_char="105">Nobel</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="107" end_char="108">de</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="110" end_char="117">Medicina</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="119" end_char="120">de</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="122" end_char="125">2008</TOKEN>
<TOKEN id="token-1-7" pos="punct" morph="none" start_char="126" end_char="126">,</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="128" end_char="129">el</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="131" end_char="138">virólogo</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="140" end_char="142">Luc</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="144" end_char="153">Montagnier</TOKEN>
<TOKEN id="token-1-12" pos="punct" morph="none" start_char="154" end_char="154">,</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="156" end_char="157">es</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="159" end_char="160">de</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="162" end_char="164">los</TOKEN>
<TOKEN id="token-1-16" pos="word" morph="none" start_char="166" end_char="168">que</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="170" end_char="178">defienden</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="180" end_char="182">que</TOKEN>
<TOKEN id="token-1-19" pos="word" morph="none" start_char="184" end_char="185">el</TOKEN>
<TOKEN id="token-1-20" pos="unknown" morph="none" start_char="187" end_char="198">coronavirus​</TOKEN>
<TOKEN id="token-1-21" pos="word" morph="none" start_char="200" end_char="201">se</TOKEN>
<TOKEN id="token-1-22" pos="word" morph="none" start_char="203" end_char="206">creó</TOKEN>
<TOKEN id="token-1-23" pos="word" morph="none" start_char="208" end_char="209">en</TOKEN>
<TOKEN id="token-1-24" pos="word" morph="none" start_char="211" end_char="212">un</TOKEN>
<TOKEN id="token-1-25" pos="word" morph="none" start_char="214" end_char="224">laboratorio</TOKEN>
<TOKEN id="token-1-26" pos="word" morph="none" start_char="226" end_char="226">y</TOKEN>
<TOKEN id="token-1-27" pos="punct" morph="none" start_char="227" end_char="227">,</TOKEN>
<TOKEN id="token-1-28" pos="word" morph="none" start_char="229" end_char="234">además</TOKEN>
<TOKEN id="token-1-29" pos="punct" morph="none" start_char="235" end_char="235">,</TOKEN>
<TOKEN id="token-1-30" pos="word" morph="none" start_char="237" end_char="239">que</TOKEN>
<TOKEN id="token-1-31" pos="word" morph="none" start_char="241" end_char="242">se</TOKEN>
<TOKEN id="token-1-32" pos="word" morph="none" start_char="244" end_char="251">contagió</TOKEN>
<TOKEN id="token-1-33" pos="word" morph="none" start_char="253" end_char="255">por</TOKEN>
<TOKEN id="token-1-34" pos="word" morph="none" start_char="257" end_char="258">un</TOKEN>
<TOKEN id="token-1-35" pos="punct" morph="none" start_char="260" end_char="260">"</TOKEN>
<TOKEN id="token-1-36" pos="word" morph="none" start_char="261" end_char="265">error</TOKEN>
<TOKEN id="token-1-37" pos="punct" morph="none" start_char="266" end_char="267">".</TOKEN>
</SEG>
<SEG id="segment-2" start_char="269" end_char="355">
<ORIGINAL_TEXT>El científico considera "imposible" que el SARS-CoV-2 se originara de forma espontánea.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="269" end_char="270">El</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="272" end_char="281">científico</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="283" end_char="291">considera</TOKEN>
<TOKEN id="token-2-3" pos="punct" morph="none" start_char="293" end_char="293">"</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="294" end_char="302">imposible</TOKEN>
<TOKEN id="token-2-5" pos="punct" morph="none" start_char="303" end_char="303">"</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="305" end_char="307">que</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="309" end_char="310">el</TOKEN>
<TOKEN id="token-2-8" pos="unknown" morph="none" start_char="312" end_char="321">SARS-CoV-2</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="323" end_char="324">se</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="326" end_char="334">originara</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="336" end_char="337">de</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="339" end_char="343">forma</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="345" end_char="354">espontánea</TOKEN>
<TOKEN id="token-2-14" pos="punct" morph="none" start_char="355" end_char="355">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="357" end_char="477">
<ORIGINAL_TEXT>"Eso de que el Covid-19 apareció tras una contaminación ocurrida en un mercado de animales salvajes es una bella leyenda.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="punct" morph="none" start_char="357" end_char="357">"</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="358" end_char="360">Eso</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="362" end_char="363">de</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="365" end_char="367">que</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="369" end_char="370">el</TOKEN>
<TOKEN id="token-3-5" pos="unknown" morph="none" start_char="372" end_char="379">Covid-19</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="381" end_char="388">apareció</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="390" end_char="393">tras</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="395" end_char="397">una</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="399" end_char="411">contaminación</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="413" end_char="420">ocurrida</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="422" end_char="423">en</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="425" end_char="426">un</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="428" end_char="434">mercado</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="436" end_char="437">de</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="439" end_char="446">animales</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="448" end_char="455">salvajes</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="457" end_char="458">es</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="460" end_char="462">una</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="464" end_char="468">bella</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="470" end_char="476">leyenda</TOKEN>
<TOKEN id="token-3-21" pos="punct" morph="none" start_char="477" end_char="477">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="479" end_char="521">
<ORIGINAL_TEXT>El virus salió de un laboratorio de Wuhan".</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="479" end_char="480">El</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="482" end_char="486">virus</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="488" end_char="492">salió</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="494" end_char="495">de</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="497" end_char="498">un</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="500" end_char="510">laboratorio</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="512" end_char="513">de</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="515" end_char="519">Wuhan</TOKEN>
<TOKEN id="token-4-8" pos="punct" morph="none" start_char="520" end_char="521">".</TOKEN>
</SEG>
<SEG id="segment-5" start_char="524" end_char="593">
<ORIGINAL_TEXT>Estas han sido sus declaraciones, realizadas a y publicadas por la web</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="524" end_char="528">Estas</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="530" end_char="532">han</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="534" end_char="537">sido</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="539" end_char="541">sus</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="543" end_char="555">declaraciones</TOKEN>
<TOKEN id="token-5-5" pos="punct" morph="none" start_char="556" end_char="556">,</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="558" end_char="567">realizadas</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="569" end_char="569">a</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="571" end_char="571">y</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="573" end_char="582">publicadas</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="584" end_char="586">por</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="588" end_char="589">la</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="591" end_char="593">web</TOKEN>
</SEG>
<SEG id="segment-6" start_char="596" end_char="611">
<ORIGINAL_TEXT>Porquoi docteur?</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="596" end_char="602">Porquoi</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="604" end_char="610">docteur</TOKEN>
<TOKEN id="token-6-2" pos="punct" morph="none" start_char="611" end_char="611">?</TOKEN>
</SEG>
<SEG id="segment-7" start_char="614" end_char="690">
<ORIGINAL_TEXT>, en las que también defiende el buen hacer científico de la comunidad China.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="punct" morph="none" start_char="614" end_char="614">,</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="616" end_char="617">en</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="619" end_char="621">las</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="623" end_char="625">que</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="627" end_char="633">también</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="635" end_char="642">defiende</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="644" end_char="645">el</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="647" end_char="650">buen</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="652" end_char="656">hacer</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="658" end_char="667">científico</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="669" end_char="670">de</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="672" end_char="673">la</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="675" end_char="683">comunidad</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="685" end_char="689">China</TOKEN>
<TOKEN id="token-7-14" pos="punct" morph="none" start_char="690" end_char="690">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="692" end_char="937">
<ORIGINAL_TEXT>"Los científicos chinos son grandes especialistas", ha asegurado, aunque a su vez piensa que un "error" generó que este virus se esparciera, causando más de 188.000 casos positivos en España y cerca de dos millones en todo el mundo --por ahora--.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="punct" morph="none" start_char="692" end_char="692">"</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="693" end_char="695">Los</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="697" end_char="707">científicos</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="709" end_char="714">chinos</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="716" end_char="718">son</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="720" end_char="726">grandes</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="728" end_char="740">especialistas</TOKEN>
<TOKEN id="token-8-7" pos="punct" morph="none" start_char="741" end_char="742">",</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="744" end_char="745">ha</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="747" end_char="755">asegurado</TOKEN>
<TOKEN id="token-8-10" pos="punct" morph="none" start_char="756" end_char="756">,</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="758" end_char="763">aunque</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="765" end_char="765">a</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="767" end_char="768">su</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="770" end_char="772">vez</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="774" end_char="779">piensa</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="781" end_char="783">que</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="785" end_char="786">un</TOKEN>
<TOKEN id="token-8-18" pos="punct" morph="none" start_char="788" end_char="788">"</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="789" end_char="793">error</TOKEN>
<TOKEN id="token-8-20" pos="punct" morph="none" start_char="794" end_char="794">"</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="796" end_char="801">generó</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="803" end_char="805">que</TOKEN>
<TOKEN id="token-8-23" pos="word" morph="none" start_char="807" end_char="810">este</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="812" end_char="816">virus</TOKEN>
<TOKEN id="token-8-25" pos="word" morph="none" start_char="818" end_char="819">se</TOKEN>
<TOKEN id="token-8-26" pos="word" morph="none" start_char="821" end_char="830">esparciera</TOKEN>
<TOKEN id="token-8-27" pos="punct" morph="none" start_char="831" end_char="831">,</TOKEN>
<TOKEN id="token-8-28" pos="word" morph="none" start_char="833" end_char="840">causando</TOKEN>
<TOKEN id="token-8-29" pos="word" morph="none" start_char="842" end_char="844">más</TOKEN>
<TOKEN id="token-8-30" pos="word" morph="none" start_char="846" end_char="847">de</TOKEN>
<TOKEN id="token-8-31" pos="word" morph="none" start_char="849" end_char="855">188.000</TOKEN>
<TOKEN id="token-8-32" pos="word" morph="none" start_char="857" end_char="861">casos</TOKEN>
<TOKEN id="token-8-33" pos="word" morph="none" start_char="863" end_char="871">positivos</TOKEN>
<TOKEN id="token-8-34" pos="word" morph="none" start_char="873" end_char="874">en</TOKEN>
<TOKEN id="token-8-35" pos="word" morph="none" start_char="876" end_char="881">España</TOKEN>
<TOKEN id="token-8-36" pos="word" morph="none" start_char="883" end_char="883">y</TOKEN>
<TOKEN id="token-8-37" pos="word" morph="none" start_char="885" end_char="889">cerca</TOKEN>
<TOKEN id="token-8-38" pos="word" morph="none" start_char="891" end_char="892">de</TOKEN>
<TOKEN id="token-8-39" pos="word" morph="none" start_char="894" end_char="896">dos</TOKEN>
<TOKEN id="token-8-40" pos="word" morph="none" start_char="898" end_char="905">millones</TOKEN>
<TOKEN id="token-8-41" pos="word" morph="none" start_char="907" end_char="908">en</TOKEN>
<TOKEN id="token-8-42" pos="word" morph="none" start_char="910" end_char="913">todo</TOKEN>
<TOKEN id="token-8-43" pos="word" morph="none" start_char="915" end_char="916">el</TOKEN>
<TOKEN id="token-8-44" pos="word" morph="none" start_char="918" end_char="922">mundo</TOKEN>
<TOKEN id="token-8-45" pos="punct" morph="none" start_char="924" end_char="925">--</TOKEN>
<TOKEN id="token-8-46" pos="word" morph="none" start_char="926" end_char="928">por</TOKEN>
<TOKEN id="token-8-47" pos="word" morph="none" start_char="930" end_char="934">ahora</TOKEN>
<TOKEN id="token-8-48" pos="punct" morph="none" start_char="935" end_char="937">--.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="940" end_char="965">
<ORIGINAL_TEXT>Virus "fabricado" en China</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="940" end_char="944">Virus</TOKEN>
<TOKEN id="token-9-1" pos="punct" morph="none" start_char="946" end_char="946">"</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="947" end_char="955">fabricado</TOKEN>
<TOKEN id="token-9-3" pos="punct" morph="none" start_char="956" end_char="956">"</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="958" end_char="959">en</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="961" end_char="965">China</TOKEN>
</SEG>
<SEG id="segment-10" start_char="969" end_char="1209">
<ORIGINAL_TEXT>Montagnier ha revelado que ha "analizado", junto al matemático Jean Claude Perrez, detalles de "la secuencia del descubrimiento y propagación del Covid-19" y consideran que "pudo ser fabricado", aunque no aclara si de manera total o parcial.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="969" end_char="978">Montagnier</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="980" end_char="981">ha</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="983" end_char="990">revelado</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="992" end_char="994">que</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="996" end_char="997">ha</TOKEN>
<TOKEN id="token-10-5" pos="punct" morph="none" start_char="999" end_char="999">"</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1000" end_char="1008">analizado</TOKEN>
<TOKEN id="token-10-7" pos="punct" morph="none" start_char="1009" end_char="1010">",</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1012" end_char="1016">junto</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1018" end_char="1019">al</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1021" end_char="1030">matemático</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1032" end_char="1035">Jean</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1037" end_char="1042">Claude</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1044" end_char="1049">Perrez</TOKEN>
<TOKEN id="token-10-14" pos="punct" morph="none" start_char="1050" end_char="1050">,</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1052" end_char="1059">detalles</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1061" end_char="1062">de</TOKEN>
<TOKEN id="token-10-17" pos="punct" morph="none" start_char="1064" end_char="1064">"</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1065" end_char="1066">la</TOKEN>
<TOKEN id="token-10-19" pos="word" morph="none" start_char="1068" end_char="1076">secuencia</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="1078" end_char="1080">del</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="1082" end_char="1095">descubrimiento</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="1097" end_char="1097">y</TOKEN>
<TOKEN id="token-10-23" pos="word" morph="none" start_char="1099" end_char="1109">propagación</TOKEN>
<TOKEN id="token-10-24" pos="word" morph="none" start_char="1111" end_char="1113">del</TOKEN>
<TOKEN id="token-10-25" pos="unknown" morph="none" start_char="1115" end_char="1122">Covid-19</TOKEN>
<TOKEN id="token-10-26" pos="punct" morph="none" start_char="1123" end_char="1123">"</TOKEN>
<TOKEN id="token-10-27" pos="word" morph="none" start_char="1125" end_char="1125">y</TOKEN>
<TOKEN id="token-10-28" pos="word" morph="none" start_char="1127" end_char="1136">consideran</TOKEN>
<TOKEN id="token-10-29" pos="word" morph="none" start_char="1138" end_char="1140">que</TOKEN>
<TOKEN id="token-10-30" pos="punct" morph="none" start_char="1142" end_char="1142">"</TOKEN>
<TOKEN id="token-10-31" pos="word" morph="none" start_char="1143" end_char="1146">pudo</TOKEN>
<TOKEN id="token-10-32" pos="word" morph="none" start_char="1148" end_char="1150">ser</TOKEN>
<TOKEN id="token-10-33" pos="word" morph="none" start_char="1152" end_char="1160">fabricado</TOKEN>
<TOKEN id="token-10-34" pos="punct" morph="none" start_char="1161" end_char="1162">",</TOKEN>
<TOKEN id="token-10-35" pos="word" morph="none" start_char="1164" end_char="1169">aunque</TOKEN>
<TOKEN id="token-10-36" pos="word" morph="none" start_char="1171" end_char="1172">no</TOKEN>
<TOKEN id="token-10-37" pos="word" morph="none" start_char="1174" end_char="1179">aclara</TOKEN>
<TOKEN id="token-10-38" pos="word" morph="none" start_char="1181" end_char="1182">si</TOKEN>
<TOKEN id="token-10-39" pos="word" morph="none" start_char="1184" end_char="1185">de</TOKEN>
<TOKEN id="token-10-40" pos="word" morph="none" start_char="1187" end_char="1192">manera</TOKEN>
<TOKEN id="token-10-41" pos="word" morph="none" start_char="1194" end_char="1198">total</TOKEN>
<TOKEN id="token-10-42" pos="word" morph="none" start_char="1200" end_char="1200">o</TOKEN>
<TOKEN id="token-10-43" pos="word" morph="none" start_char="1202" end_char="1208">parcial</TOKEN>
<TOKEN id="token-10-44" pos="punct" morph="none" start_char="1209" end_char="1209">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1212" end_char="1357">
<ORIGINAL_TEXT>Asimismo, ha sostenido que la dispersión de la enfermedad pandémica pudiera haberse tratado de un "fallo" tanto humano, como técnico o científico.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1212" end_char="1219">Asimismo</TOKEN>
<TOKEN id="token-11-1" pos="punct" morph="none" start_char="1220" end_char="1220">,</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1222" end_char="1223">ha</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1225" end_char="1233">sostenido</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1235" end_char="1237">que</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1239" end_char="1240">la</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1242" end_char="1251">dispersión</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1253" end_char="1254">de</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1256" end_char="1257">la</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1259" end_char="1268">enfermedad</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1270" end_char="1278">pandémica</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1280" end_char="1286">pudiera</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1288" end_char="1294">haberse</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1296" end_char="1302">tratado</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1304" end_char="1305">de</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1307" end_char="1308">un</TOKEN>
<TOKEN id="token-11-16" pos="punct" morph="none" start_char="1310" end_char="1310">"</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1311" end_char="1315">fallo</TOKEN>
<TOKEN id="token-11-18" pos="punct" morph="none" start_char="1316" end_char="1316">"</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1318" end_char="1322">tanto</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1324" end_char="1329">humano</TOKEN>
<TOKEN id="token-11-21" pos="punct" morph="none" start_char="1330" end_char="1330">,</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1332" end_char="1335">como</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1337" end_char="1343">técnico</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1345" end_char="1345">o</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1347" end_char="1356">científico</TOKEN>
<TOKEN id="token-11-26" pos="punct" morph="none" start_char="1357" end_char="1357">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1359" end_char="1573">
<ORIGINAL_TEXT>Según Montagnier, los científicos que trabajan en el laboratorio de Wuhan son "grandes especialistas en los coronavirus" y detalla que la investigación sobre esta tipología vírica se remonta a "principios del 2000".</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1359" end_char="1363">Según</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1365" end_char="1374">Montagnier</TOKEN>
<TOKEN id="token-12-2" pos="punct" morph="none" start_char="1375" end_char="1375">,</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1377" end_char="1379">los</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1381" end_char="1391">científicos</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1393" end_char="1395">que</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1397" end_char="1404">trabajan</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1406" end_char="1407">en</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1409" end_char="1410">el</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1412" end_char="1422">laboratorio</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1424" end_char="1425">de</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1427" end_char="1431">Wuhan</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1433" end_char="1435">son</TOKEN>
<TOKEN id="token-12-13" pos="punct" morph="none" start_char="1437" end_char="1437">"</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1438" end_char="1444">grandes</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1446" end_char="1458">especialistas</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1460" end_char="1461">en</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1463" end_char="1465">los</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="1467" end_char="1477">coronavirus</TOKEN>
<TOKEN id="token-12-19" pos="punct" morph="none" start_char="1478" end_char="1478">"</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1480" end_char="1480">y</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1482" end_char="1488">detalla</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1490" end_char="1492">que</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1494" end_char="1495">la</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1497" end_char="1509">investigación</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1511" end_char="1515">sobre</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="1517" end_char="1520">esta</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="1522" end_char="1530">tipología</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="1532" end_char="1537">vírica</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="1539" end_char="1540">se</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="1542" end_char="1548">remonta</TOKEN>
<TOKEN id="token-12-31" pos="word" morph="none" start_char="1550" end_char="1550">a</TOKEN>
<TOKEN id="token-12-32" pos="punct" morph="none" start_char="1552" end_char="1552">"</TOKEN>
<TOKEN id="token-12-33" pos="word" morph="none" start_char="1553" end_char="1562">principios</TOKEN>
<TOKEN id="token-12-34" pos="word" morph="none" start_char="1564" end_char="1566">del</TOKEN>
<TOKEN id="token-12-35" pos="word" morph="none" start_char="1568" end_char="1571">2000</TOKEN>
<TOKEN id="token-12-36" pos="punct" morph="none" start_char="1572" end_char="1573">".</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1576" end_char="1596">
<ORIGINAL_TEXT>Investigación del VIH</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1576" end_char="1588">Investigación</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1590" end_char="1592">del</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1594" end_char="1596">VIH</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1600" end_char="1782">
<ORIGINAL_TEXT>Montagnier fue galardonado con el Nobel de Medicina gracias a su participación en el descubrimiento y aislamiento del virus de la inmunodeficiencia humana (VIH), responsable del sida.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1600" end_char="1609">Montagnier</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1611" end_char="1613">fue</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1615" end_char="1625">galardonado</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1627" end_char="1629">con</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1631" end_char="1632">el</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1634" end_char="1638">Nobel</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1640" end_char="1641">de</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1643" end_char="1650">Medicina</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1652" end_char="1658">gracias</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1660" end_char="1660">a</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1662" end_char="1663">su</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1665" end_char="1677">participación</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="1679" end_char="1680">en</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="1682" end_char="1683">el</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="1685" end_char="1698">descubrimiento</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="1700" end_char="1700">y</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="1702" end_char="1712">aislamiento</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="1714" end_char="1716">del</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="1718" end_char="1722">virus</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="1724" end_char="1725">de</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="1727" end_char="1728">la</TOKEN>
<TOKEN id="token-14-21" pos="word" morph="none" start_char="1730" end_char="1746">inmunodeficiencia</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="1748" end_char="1753">humana</TOKEN>
<TOKEN id="token-14-23" pos="punct" morph="none" start_char="1755" end_char="1755">(</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="1756" end_char="1758">VIH</TOKEN>
<TOKEN id="token-14-25" pos="punct" morph="none" start_char="1759" end_char="1760">),</TOKEN>
<TOKEN id="token-14-26" pos="word" morph="none" start_char="1762" end_char="1772">responsable</TOKEN>
<TOKEN id="token-14-27" pos="word" morph="none" start_char="1774" end_char="1776">del</TOKEN>
<TOKEN id="token-14-28" pos="word" morph="none" start_char="1778" end_char="1781">sida</TOKEN>
<TOKEN id="token-14-29" pos="punct" morph="none" start_char="1782" end_char="1782">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1785" end_char="1921">
<ORIGINAL_TEXT>Compartió el mérito con el también científico Françoise Barré-Sinoussi --también descubridor del VIH-- y con el aleman Harald zur Hausen.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1785" end_char="1793">Compartió</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1795" end_char="1796">el</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1798" end_char="1803">mérito</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="1805" end_char="1807">con</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1809" end_char="1810">el</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1812" end_char="1818">también</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="1820" end_char="1829">científico</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1831" end_char="1839">Françoise</TOKEN>
<TOKEN id="token-15-8" pos="unknown" morph="none" start_char="1841" end_char="1854">Barré-Sinoussi</TOKEN>
<TOKEN id="token-15-9" pos="punct" morph="none" start_char="1856" end_char="1857">--</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1858" end_char="1864">también</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="1866" end_char="1876">descubridor</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="1878" end_char="1880">del</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="1882" end_char="1884">VIH</TOKEN>
<TOKEN id="token-15-14" pos="punct" morph="none" start_char="1885" end_char="1886">--</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="1888" end_char="1888">y</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="1890" end_char="1892">con</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="1894" end_char="1895">el</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="1897" end_char="1902">aleman</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="1904" end_char="1909">Harald</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="1911" end_char="1913">zur</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="1915" end_char="1920">Hausen</TOKEN>
<TOKEN id="token-15-22" pos="punct" morph="none" start_char="1921" end_char="1921">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1923" end_char="2037">
<ORIGINAL_TEXT>Este último descubrió el virus del papiloma humano, que causa enfermedades de transmisión sexual y cáncer cervical.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1923" end_char="1926">Este</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1928" end_char="1933">último</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1935" end_char="1943">descubrió</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1945" end_char="1946">el</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1948" end_char="1952">virus</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1954" end_char="1956">del</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="1958" end_char="1965">papiloma</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="1967" end_char="1972">humano</TOKEN>
<TOKEN id="token-16-8" pos="punct" morph="none" start_char="1973" end_char="1973">,</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="1975" end_char="1977">que</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="1979" end_char="1983">causa</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="1985" end_char="1996">enfermedades</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="1998" end_char="1999">de</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2001" end_char="2011">transmisión</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="2013" end_char="2018">sexual</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="2020" end_char="2020">y</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="2022" end_char="2027">cáncer</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="2029" end_char="2036">cervical</TOKEN>
<TOKEN id="token-16-18" pos="punct" morph="none" start_char="2037" end_char="2037">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
