<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATG7" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="2534" raw_text_md5="8b56755370d87c789faeb13479411011">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="115">
<ORIGINAL_TEXT>What will be the after effects of social distancing and complete lockdown imposed due to the spread of coronavirus?</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="4">What</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="6" end_char="9">will</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="11" end_char="12">be</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="14" end_char="16">the</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="18" end_char="22">after</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="24" end_char="30">effects</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="32" end_char="33">of</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="35" end_char="40">social</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="42" end_char="51">distancing</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="53" end_char="55">and</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="57" end_char="64">complete</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="66" end_char="73">lockdown</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="75" end_char="81">imposed</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="83" end_char="85">due</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="87" end_char="88">to</TOKEN>
<TOKEN id="token-0-15" pos="word" morph="none" start_char="90" end_char="92">the</TOKEN>
<TOKEN id="token-0-16" pos="word" morph="none" start_char="94" end_char="99">spread</TOKEN>
<TOKEN id="token-0-17" pos="word" morph="none" start_char="101" end_char="102">of</TOKEN>
<TOKEN id="token-0-18" pos="word" morph="none" start_char="104" end_char="114">coronavirus</TOKEN>
<TOKEN id="token-0-19" pos="punct" morph="none" start_char="115" end_char="115">?</TOKEN>
</SEG>
<SEG id="segment-1" start_char="119" end_char="189">
<ORIGINAL_TEXT>The pathogens will keep coming just as they have for billions of years.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="119" end_char="121">The</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="123" end_char="131">pathogens</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="133" end_char="136">will</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="138" end_char="141">keep</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="143" end_char="148">coming</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="150" end_char="153">just</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="155" end_char="156">as</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="158" end_char="161">they</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="163" end_char="166">have</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="168" end_char="170">for</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="172" end_char="179">billions</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="181" end_char="182">of</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="184" end_char="188">years</TOKEN>
<TOKEN id="token-1-13" pos="punct" morph="none" start_char="189" end_char="189">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="191" end_char="268">
<ORIGINAL_TEXT>It's valuable to remember that microbes once had the planet all to themselves.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="191" end_char="194">It's</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="196" end_char="203">valuable</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="205" end_char="206">to</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="208" end_char="215">remember</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="217" end_char="220">that</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="222" end_char="229">microbes</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="231" end_char="234">once</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="236" end_char="238">had</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="240" end_char="242">the</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="244" end_char="249">planet</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="251" end_char="253">all</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="255" end_char="256">to</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="258" end_char="267">themselves</TOKEN>
<TOKEN id="token-2-13" pos="punct" morph="none" start_char="268" end_char="268">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="270" end_char="290">
<ORIGINAL_TEXT>Bacteria and viruses.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="270" end_char="277">Bacteria</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="279" end_char="281">and</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="283" end_char="289">viruses</TOKEN>
<TOKEN id="token-3-3" pos="punct" morph="none" start_char="290" end_char="290">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="293" end_char="297">
<ORIGINAL_TEXT>Also?</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="293" end_char="296">Also</TOKEN>
<TOKEN id="token-4-1" pos="punct" morph="none" start_char="297" end_char="297">?</TOKEN>
</SEG>
<SEG id="segment-5" start_char="299" end_char="350">
<ORIGINAL_TEXT>90+% of what each of us calls 'my self' is microbes.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="unknown" morph="none" start_char="299" end_char="302">90+%</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="304" end_char="305">of</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="307" end_char="310">what</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="312" end_char="315">each</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="317" end_char="318">of</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="320" end_char="321">us</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="323" end_char="327">calls</TOKEN>
<TOKEN id="token-5-7" pos="punct" morph="none" start_char="329" end_char="329">'</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="330" end_char="331">my</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="333" end_char="336">self</TOKEN>
<TOKEN id="token-5-10" pos="punct" morph="none" start_char="337" end_char="337">'</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="339" end_char="340">is</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="342" end_char="349">microbes</TOKEN>
<TOKEN id="token-5-13" pos="punct" morph="none" start_char="350" end_char="350">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="352" end_char="402">
<ORIGINAL_TEXT>90+% of all cells in our bodies are bacteria et al.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="unknown" morph="none" start_char="352" end_char="355">90+%</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="357" end_char="358">of</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="360" end_char="362">all</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="364" end_char="368">cells</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="370" end_char="371">in</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="373" end_char="375">our</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="377" end_char="382">bodies</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="384" end_char="386">are</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="388" end_char="395">bacteria</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="397" end_char="398">et</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="400" end_char="401">al</TOKEN>
<TOKEN id="token-6-11" pos="punct" morph="none" start_char="402" end_char="402">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="405" end_char="408">
<ORIGINAL_TEXT>And?</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="405" end_char="407">And</TOKEN>
<TOKEN id="token-7-1" pos="punct" morph="none" start_char="408" end_char="408">?</TOKEN>
</SEG>
<SEG id="segment-8" start_char="411" end_char="524">
<ORIGINAL_TEXT>We owe most of our current nature to both our benign microbial tennants - and the ones that have tried to kill us.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="411" end_char="412">We</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="414" end_char="416">owe</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="418" end_char="421">most</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="423" end_char="424">of</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="426" end_char="428">our</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="430" end_char="436">current</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="438" end_char="443">nature</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="445" end_char="446">to</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="448" end_char="451">both</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="453" end_char="455">our</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="457" end_char="462">benign</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="464" end_char="472">microbial</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="474" end_char="481">tennants</TOKEN>
<TOKEN id="token-8-13" pos="punct" morph="none" start_char="483" end_char="483">-</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="485" end_char="487">and</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="489" end_char="491">the</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="493" end_char="496">ones</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="498" end_char="501">that</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="503" end_char="506">have</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="508" end_char="512">tried</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="514" end_char="515">to</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="517" end_char="520">kill</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="522" end_char="523">us</TOKEN>
<TOKEN id="token-8-23" pos="punct" morph="none" start_char="524" end_char="524">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="527" end_char="529">
<ORIGINAL_TEXT>So.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="527" end_char="528">So</TOKEN>
<TOKEN id="token-9-1" pos="punct" morph="none" start_char="529" end_char="529">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="531" end_char="557">
<ORIGINAL_TEXT>Viruses are always with us.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="531" end_char="537">Viruses</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="539" end_char="541">are</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="543" end_char="548">always</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="550" end_char="553">with</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="555" end_char="556">us</TOKEN>
<TOKEN id="token-10-5" pos="punct" morph="none" start_char="557" end_char="557">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="559" end_char="598">
<ORIGINAL_TEXT>They were here long long long before us.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="559" end_char="562">They</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="564" end_char="567">were</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="569" end_char="572">here</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="574" end_char="577">long</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="579" end_char="582">long</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="584" end_char="587">long</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="589" end_char="594">before</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="596" end_char="597">us</TOKEN>
<TOKEN id="token-11-8" pos="punct" morph="none" start_char="598" end_char="598">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="600" end_char="674">
<ORIGINAL_TEXT>There's no preventing them inventing new expressions of their ‘inner self'.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="600" end_char="606">There's</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="608" end_char="609">no</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="611" end_char="620">preventing</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="622" end_char="625">them</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="627" end_char="635">inventing</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="637" end_char="639">new</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="641" end_char="651">expressions</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="653" end_char="654">of</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="656" end_char="660">their</TOKEN>
<TOKEN id="token-12-9" pos="punct" morph="none" start_char="662" end_char="662">‘</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="663" end_char="667">inner</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="669" end_char="672">self</TOKEN>
<TOKEN id="token-12-12" pos="punct" morph="none" start_char="673" end_char="674">'.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="677" end_char="725">
<ORIGINAL_TEXT>The best we can do is be as vigilant as possible.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="677" end_char="679">The</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="681" end_char="684">best</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="686" end_char="687">we</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="689" end_char="691">can</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="693" end_char="694">do</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="696" end_char="697">is</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="699" end_char="700">be</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="702" end_char="703">as</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="705" end_char="712">vigilant</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="714" end_char="715">as</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="717" end_char="724">possible</TOKEN>
<TOKEN id="token-13-11" pos="punct" morph="none" start_char="725" end_char="725">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="727" end_char="728">
<ORIGINAL_TEXT>Th</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="727" end_char="728">Th</TOKEN>
</SEG>
<SEG id="segment-15" start_char="731" end_char="785">
<ORIGINAL_TEXT>The previous SARS was far more dangerous if you got it.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="731" end_char="733">The</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="735" end_char="742">previous</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="744" end_char="747">SARS</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="749" end_char="751">was</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="753" end_char="755">far</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="757" end_char="760">more</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="762" end_char="770">dangerous</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="772" end_char="773">if</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="775" end_char="777">you</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="779" end_char="781">got</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="783" end_char="784">it</TOKEN>
<TOKEN id="token-15-11" pos="punct" morph="none" start_char="785" end_char="785">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="787" end_char="881">
<ORIGINAL_TEXT>This new that that causes the COVID-19 disease, the virus that causes it, is also a SARS virus.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="787" end_char="790">This</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="792" end_char="794">new</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="796" end_char="799">that</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="801" end_char="804">that</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="806" end_char="811">causes</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="813" end_char="815">the</TOKEN>
<TOKEN id="token-16-6" pos="unknown" morph="none" start_char="817" end_char="824">COVID-19</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="826" end_char="832">disease</TOKEN>
<TOKEN id="token-16-8" pos="punct" morph="none" start_char="833" end_char="833">,</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="835" end_char="837">the</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="839" end_char="843">virus</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="845" end_char="848">that</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="850" end_char="855">causes</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="857" end_char="858">it</TOKEN>
<TOKEN id="token-16-14" pos="punct" morph="none" start_char="859" end_char="859">,</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="861" end_char="862">is</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="864" end_char="867">also</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="869" end_char="869">a</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="871" end_char="874">SARS</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="876" end_char="880">virus</TOKEN>
<TOKEN id="token-16-20" pos="punct" morph="none" start_char="881" end_char="881">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="883" end_char="905">
<ORIGINAL_TEXT>It’s called SARS-CoV-2.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="883" end_char="886">It’s</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="888" end_char="893">called</TOKEN>
<TOKEN id="token-17-2" pos="unknown" morph="none" start_char="895" end_char="904">SARS-CoV-2</TOKEN>
<TOKEN id="token-17-3" pos="punct" morph="none" start_char="905" end_char="905">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="907" end_char="936">
<ORIGINAL_TEXT>Also called novel coronavirus.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="907" end_char="910">Also</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="912" end_char="917">called</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="919" end_char="923">novel</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="925" end_char="935">coronavirus</TOKEN>
<TOKEN id="token-18-4" pos="punct" morph="none" start_char="936" end_char="936">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="938" end_char="976">
<ORIGINAL_TEXT>The virus itself isn’t called COVID-19.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="938" end_char="940">The</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="942" end_char="946">virus</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="948" end_char="953">itself</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="955" end_char="959">isn’t</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="961" end_char="966">called</TOKEN>
<TOKEN id="token-19-5" pos="unknown" morph="none" start_char="968" end_char="975">COVID-19</TOKEN>
<TOKEN id="token-19-6" pos="punct" morph="none" start_char="976" end_char="976">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="978" end_char="1018">
<ORIGINAL_TEXT>The disease it causes is called COVID-19.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="978" end_char="980">The</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="982" end_char="988">disease</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="990" end_char="991">it</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="993" end_char="998">causes</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="1000" end_char="1001">is</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="1003" end_char="1008">called</TOKEN>
<TOKEN id="token-20-6" pos="unknown" morph="none" start_char="1010" end_char="1017">COVID-19</TOKEN>
<TOKEN id="token-20-7" pos="punct" morph="none" start_char="1018" end_char="1018">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="1021" end_char="1122">
<ORIGINAL_TEXT>So anyway, this new novel coronavirus, can spread when no symptoms are there or before symptoms arise.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="1021" end_char="1022">So</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="1024" end_char="1029">anyway</TOKEN>
<TOKEN id="token-21-2" pos="punct" morph="none" start_char="1030" end_char="1030">,</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="1032" end_char="1035">this</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="1037" end_char="1039">new</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="1041" end_char="1045">novel</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="1047" end_char="1057">coronavirus</TOKEN>
<TOKEN id="token-21-7" pos="punct" morph="none" start_char="1058" end_char="1058">,</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="1060" end_char="1062">can</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="1064" end_char="1069">spread</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="1071" end_char="1074">when</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="1076" end_char="1077">no</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="1079" end_char="1086">symptoms</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="1088" end_char="1090">are</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="1092" end_char="1096">there</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="1098" end_char="1099">or</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="1101" end_char="1106">before</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="1108" end_char="1115">symptoms</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="1117" end_char="1121">arise</TOKEN>
<TOKEN id="token-21-19" pos="punct" morph="none" start_char="1122" end_char="1122">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="1124" end_char="1150">
<ORIGINAL_TEXT>It makes it rather stealth.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="1124" end_char="1125">It</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="1127" end_char="1131">makes</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="1133" end_char="1134">it</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="1136" end_char="1141">rather</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="1143" end_char="1149">stealth</TOKEN>
<TOKEN id="token-22-5" pos="punct" morph="none" start_char="1150" end_char="1150">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="1152" end_char="1257">
<ORIGINAL_TEXT>Also… and this is not discussed much, many people actually *DO* get symptoms, but those symptoms are mild.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="1152" end_char="1155">Also</TOKEN>
<TOKEN id="token-23-1" pos="punct" morph="none" start_char="1156" end_char="1156">…</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="1158" end_char="1160">and</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="1162" end_char="1165">this</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="1167" end_char="1168">is</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="1170" end_char="1172">not</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="1174" end_char="1182">discussed</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="1184" end_char="1187">much</TOKEN>
<TOKEN id="token-23-8" pos="punct" morph="none" start_char="1188" end_char="1188">,</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="1190" end_char="1193">many</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="1195" end_char="1200">people</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="1202" end_char="1209">actually</TOKEN>
<TOKEN id="token-23-12" pos="punct" morph="none" start_char="1211" end_char="1211">*</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="1212" end_char="1213">DO</TOKEN>
<TOKEN id="token-23-14" pos="punct" morph="none" start_char="1214" end_char="1214">*</TOKEN>
<TOKEN id="token-23-15" pos="word" morph="none" start_char="1216" end_char="1218">get</TOKEN>
<TOKEN id="token-23-16" pos="word" morph="none" start_char="1220" end_char="1227">symptoms</TOKEN>
<TOKEN id="token-23-17" pos="punct" morph="none" start_char="1228" end_char="1228">,</TOKEN>
<TOKEN id="token-23-18" pos="word" morph="none" start_char="1230" end_char="1232">but</TOKEN>
<TOKEN id="token-23-19" pos="word" morph="none" start_char="1234" end_char="1238">those</TOKEN>
<TOKEN id="token-23-20" pos="word" morph="none" start_char="1240" end_char="1247">symptoms</TOKEN>
<TOKEN id="token-23-21" pos="word" morph="none" start_char="1249" end_char="1251">are</TOKEN>
<TOKEN id="token-23-22" pos="word" morph="none" start_char="1253" end_char="1256">mild</TOKEN>
<TOKEN id="token-23-23" pos="punct" morph="none" start_char="1257" end_char="1257">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="1259" end_char="1283">
<ORIGINAL_TEXT>A mild fever for example.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="1259" end_char="1259">A</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="1261" end_char="1264">mild</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="1266" end_char="1270">fever</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="1272" end_char="1274">for</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="1276" end_char="1282">example</TOKEN>
<TOKEN id="token-24-5" pos="punct" morph="none" start_char="1283" end_char="1283">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="1285" end_char="1331">
<ORIGINAL_TEXT>The problem is that people can’t be bothered or</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="1285" end_char="1287">The</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="1289" end_char="1295">problem</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="1297" end_char="1298">is</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="1300" end_char="1303">that</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="1305" end_char="1310">people</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="1312" end_char="1316">can’t</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="1318" end_char="1319">be</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="1321" end_char="1328">bothered</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="1330" end_char="1331">or</TOKEN>
</SEG>
<SEG id="segment-26" start_char="1334" end_char="1381">
<ORIGINAL_TEXT>You know the answer already as you said…"like it</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="1334" end_char="1336">You</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="1338" end_char="1341">know</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="1343" end_char="1345">the</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="1347" end_char="1352">answer</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="1354" end_char="1360">already</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="1362" end_char="1363">as</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="1365" end_char="1367">you</TOKEN>
<TOKEN id="token-26-7" pos="unknown" morph="none" start_char="1369" end_char="1378">said…"like</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="1380" end_char="1381">it</TOKEN>
</SEG>
<SEG id="segment-27" start_char="1384" end_char="1408">
<ORIGINAL_TEXT>Here is my 2 cents worth…</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="1384" end_char="1387">Here</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="1389" end_char="1390">is</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="1392" end_char="1393">my</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="1395" end_char="1395">2</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="1397" end_char="1401">cents</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="1403" end_char="1407">worth</TOKEN>
<TOKEN id="token-27-6" pos="punct" morph="none" start_char="1408" end_char="1408">…</TOKEN>
</SEG>
<SEG id="segment-28" start_char="1411" end_char="1530">
<ORIGINAL_TEXT>China has the political will, discipline, and resources to do what they have to do to solve whatever problems they face.</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="1411" end_char="1415">China</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="1417" end_char="1419">has</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="1421" end_char="1423">the</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="1425" end_char="1433">political</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="1435" end_char="1438">will</TOKEN>
<TOKEN id="token-28-5" pos="punct" morph="none" start_char="1439" end_char="1439">,</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="1441" end_char="1450">discipline</TOKEN>
<TOKEN id="token-28-7" pos="punct" morph="none" start_char="1451" end_char="1451">,</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="1453" end_char="1455">and</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="1457" end_char="1465">resources</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="1467" end_char="1468">to</TOKEN>
<TOKEN id="token-28-11" pos="word" morph="none" start_char="1470" end_char="1471">do</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="1473" end_char="1476">what</TOKEN>
<TOKEN id="token-28-13" pos="word" morph="none" start_char="1478" end_char="1481">they</TOKEN>
<TOKEN id="token-28-14" pos="word" morph="none" start_char="1483" end_char="1486">have</TOKEN>
<TOKEN id="token-28-15" pos="word" morph="none" start_char="1488" end_char="1489">to</TOKEN>
<TOKEN id="token-28-16" pos="word" morph="none" start_char="1491" end_char="1492">do</TOKEN>
<TOKEN id="token-28-17" pos="word" morph="none" start_char="1494" end_char="1495">to</TOKEN>
<TOKEN id="token-28-18" pos="word" morph="none" start_char="1497" end_char="1501">solve</TOKEN>
<TOKEN id="token-28-19" pos="word" morph="none" start_char="1503" end_char="1510">whatever</TOKEN>
<TOKEN id="token-28-20" pos="word" morph="none" start_char="1512" end_char="1519">problems</TOKEN>
<TOKEN id="token-28-21" pos="word" morph="none" start_char="1521" end_char="1524">they</TOKEN>
<TOKEN id="token-28-22" pos="word" morph="none" start_char="1526" end_char="1529">face</TOKEN>
<TOKEN id="token-28-23" pos="punct" morph="none" start_char="1530" end_char="1530">.</TOKEN>
</SEG>
<SEG id="segment-29" start_char="1532" end_char="1625">
<ORIGINAL_TEXT>Once they realise the coronavirus flu is not just your common flu, they mobilized into action.</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="1532" end_char="1535">Once</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="1537" end_char="1540">they</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="1542" end_char="1548">realise</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="1550" end_char="1552">the</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="1554" end_char="1564">coronavirus</TOKEN>
<TOKEN id="token-29-5" pos="word" morph="none" start_char="1566" end_char="1568">flu</TOKEN>
<TOKEN id="token-29-6" pos="word" morph="none" start_char="1570" end_char="1571">is</TOKEN>
<TOKEN id="token-29-7" pos="word" morph="none" start_char="1573" end_char="1575">not</TOKEN>
<TOKEN id="token-29-8" pos="word" morph="none" start_char="1577" end_char="1580">just</TOKEN>
<TOKEN id="token-29-9" pos="word" morph="none" start_char="1582" end_char="1585">your</TOKEN>
<TOKEN id="token-29-10" pos="word" morph="none" start_char="1587" end_char="1592">common</TOKEN>
<TOKEN id="token-29-11" pos="word" morph="none" start_char="1594" end_char="1596">flu</TOKEN>
<TOKEN id="token-29-12" pos="punct" morph="none" start_char="1597" end_char="1597">,</TOKEN>
<TOKEN id="token-29-13" pos="word" morph="none" start_char="1599" end_char="1602">they</TOKEN>
<TOKEN id="token-29-14" pos="word" morph="none" start_char="1604" end_char="1612">mobilized</TOKEN>
<TOKEN id="token-29-15" pos="word" morph="none" start_char="1614" end_char="1617">into</TOKEN>
<TOKEN id="token-29-16" pos="word" morph="none" start_char="1619" end_char="1624">action</TOKEN>
<TOKEN id="token-29-17" pos="punct" morph="none" start_char="1625" end_char="1625">.</TOKEN>
</SEG>
<SEG id="segment-30" start_char="1627" end_char="1701">
<ORIGINAL_TEXT>They shutdown and literally quarantined Wuhan, a city of 11 million people.</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="1627" end_char="1630">They</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="1632" end_char="1639">shutdown</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="1641" end_char="1643">and</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="1645" end_char="1653">literally</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="1655" end_char="1665">quarantined</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="1667" end_char="1671">Wuhan</TOKEN>
<TOKEN id="token-30-6" pos="punct" morph="none" start_char="1672" end_char="1672">,</TOKEN>
<TOKEN id="token-30-7" pos="word" morph="none" start_char="1674" end_char="1674">a</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="1676" end_char="1679">city</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="1681" end_char="1682">of</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="1684" end_char="1685">11</TOKEN>
<TOKEN id="token-30-11" pos="word" morph="none" start_char="1687" end_char="1693">million</TOKEN>
<TOKEN id="token-30-12" pos="word" morph="none" start_char="1695" end_char="1700">people</TOKEN>
<TOKEN id="token-30-13" pos="punct" morph="none" start_char="1701" end_char="1701">.</TOKEN>
</SEG>
<SEG id="segment-31" start_char="1703" end_char="1729">
<ORIGINAL_TEXT>Which country can do this ?</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="word" morph="none" start_char="1703" end_char="1707">Which</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="1709" end_char="1715">country</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="1717" end_char="1719">can</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="1721" end_char="1722">do</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="1724" end_char="1727">this</TOKEN>
<TOKEN id="token-31-5" pos="punct" morph="none" start_char="1729" end_char="1729">?</TOKEN>
</SEG>
<SEG id="segment-32" start_char="1731" end_char="1844">
<ORIGINAL_TEXT>They mobilized their medical resources into Hubei province immediately, and built a 1000 bed hospital in 6 days !!</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="1731" end_char="1734">They</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="1736" end_char="1744">mobilized</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="1746" end_char="1750">their</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="1752" end_char="1758">medical</TOKEN>
<TOKEN id="token-32-4" pos="word" morph="none" start_char="1760" end_char="1768">resources</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="1770" end_char="1773">into</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="1775" end_char="1779">Hubei</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="1781" end_char="1788">province</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="1790" end_char="1800">immediately</TOKEN>
<TOKEN id="token-32-9" pos="punct" morph="none" start_char="1801" end_char="1801">,</TOKEN>
<TOKEN id="token-32-10" pos="word" morph="none" start_char="1803" end_char="1805">and</TOKEN>
<TOKEN id="token-32-11" pos="word" morph="none" start_char="1807" end_char="1811">built</TOKEN>
<TOKEN id="token-32-12" pos="word" morph="none" start_char="1813" end_char="1813">a</TOKEN>
<TOKEN id="token-32-13" pos="word" morph="none" start_char="1815" end_char="1818">1000</TOKEN>
<TOKEN id="token-32-14" pos="word" morph="none" start_char="1820" end_char="1822">bed</TOKEN>
<TOKEN id="token-32-15" pos="word" morph="none" start_char="1824" end_char="1831">hospital</TOKEN>
<TOKEN id="token-32-16" pos="word" morph="none" start_char="1833" end_char="1834">in</TOKEN>
<TOKEN id="token-32-17" pos="word" morph="none" start_char="1836" end_char="1836">6</TOKEN>
<TOKEN id="token-32-18" pos="word" morph="none" start_char="1838" end_char="1841">days</TOKEN>
<TOKEN id="token-32-19" pos="punct" morph="none" start_char="1843" end_char="1844">!!</TOKEN>
</SEG>
<SEG id="segment-33" start_char="1847" end_char="1923">
<ORIGINAL_TEXT>Of course they will have to discipline and change the poor personal hygiene h</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="word" morph="none" start_char="1847" end_char="1848">Of</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="1850" end_char="1855">course</TOKEN>
<TOKEN id="token-33-2" pos="word" morph="none" start_char="1857" end_char="1860">they</TOKEN>
<TOKEN id="token-33-3" pos="word" morph="none" start_char="1862" end_char="1865">will</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="1867" end_char="1870">have</TOKEN>
<TOKEN id="token-33-5" pos="word" morph="none" start_char="1872" end_char="1873">to</TOKEN>
<TOKEN id="token-33-6" pos="word" morph="none" start_char="1875" end_char="1884">discipline</TOKEN>
<TOKEN id="token-33-7" pos="word" morph="none" start_char="1886" end_char="1888">and</TOKEN>
<TOKEN id="token-33-8" pos="word" morph="none" start_char="1890" end_char="1895">change</TOKEN>
<TOKEN id="token-33-9" pos="word" morph="none" start_char="1897" end_char="1899">the</TOKEN>
<TOKEN id="token-33-10" pos="word" morph="none" start_char="1901" end_char="1904">poor</TOKEN>
<TOKEN id="token-33-11" pos="word" morph="none" start_char="1906" end_char="1913">personal</TOKEN>
<TOKEN id="token-33-12" pos="word" morph="none" start_char="1915" end_char="1921">hygiene</TOKEN>
<TOKEN id="token-33-13" pos="word" morph="none" start_char="1923" end_char="1923">h</TOKEN>
</SEG>
<SEG id="segment-34" start_char="1926" end_char="1947">
<ORIGINAL_TEXT>Thank you for the A2A:</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="1926" end_char="1930">Thank</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="1932" end_char="1934">you</TOKEN>
<TOKEN id="token-34-2" pos="word" morph="none" start_char="1936" end_char="1938">for</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="1940" end_char="1942">the</TOKEN>
<TOKEN id="token-34-4" pos="word" morph="none" start_char="1944" end_char="1946">A2A</TOKEN>
<TOKEN id="token-34-5" pos="punct" morph="none" start_char="1947" end_char="1947">:</TOKEN>
</SEG>
<SEG id="segment-35" start_char="1950" end_char="2103">
<ORIGINAL_TEXT>Is it true that unlike SARS and MERS, the coronavirus can yield mild or asymptomatic cases that are more difficult to detect, making it harder to contain?</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="1950" end_char="1951">Is</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="1953" end_char="1954">it</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="1956" end_char="1959">true</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="1961" end_char="1964">that</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="1966" end_char="1971">unlike</TOKEN>
<TOKEN id="token-35-5" pos="word" morph="none" start_char="1973" end_char="1976">SARS</TOKEN>
<TOKEN id="token-35-6" pos="word" morph="none" start_char="1978" end_char="1980">and</TOKEN>
<TOKEN id="token-35-7" pos="word" morph="none" start_char="1982" end_char="1985">MERS</TOKEN>
<TOKEN id="token-35-8" pos="punct" morph="none" start_char="1986" end_char="1986">,</TOKEN>
<TOKEN id="token-35-9" pos="word" morph="none" start_char="1988" end_char="1990">the</TOKEN>
<TOKEN id="token-35-10" pos="word" morph="none" start_char="1992" end_char="2002">coronavirus</TOKEN>
<TOKEN id="token-35-11" pos="word" morph="none" start_char="2004" end_char="2006">can</TOKEN>
<TOKEN id="token-35-12" pos="word" morph="none" start_char="2008" end_char="2012">yield</TOKEN>
<TOKEN id="token-35-13" pos="word" morph="none" start_char="2014" end_char="2017">mild</TOKEN>
<TOKEN id="token-35-14" pos="word" morph="none" start_char="2019" end_char="2020">or</TOKEN>
<TOKEN id="token-35-15" pos="word" morph="none" start_char="2022" end_char="2033">asymptomatic</TOKEN>
<TOKEN id="token-35-16" pos="word" morph="none" start_char="2035" end_char="2039">cases</TOKEN>
<TOKEN id="token-35-17" pos="word" morph="none" start_char="2041" end_char="2044">that</TOKEN>
<TOKEN id="token-35-18" pos="word" morph="none" start_char="2046" end_char="2048">are</TOKEN>
<TOKEN id="token-35-19" pos="word" morph="none" start_char="2050" end_char="2053">more</TOKEN>
<TOKEN id="token-35-20" pos="word" morph="none" start_char="2055" end_char="2063">difficult</TOKEN>
<TOKEN id="token-35-21" pos="word" morph="none" start_char="2065" end_char="2066">to</TOKEN>
<TOKEN id="token-35-22" pos="word" morph="none" start_char="2068" end_char="2073">detect</TOKEN>
<TOKEN id="token-35-23" pos="punct" morph="none" start_char="2074" end_char="2074">,</TOKEN>
<TOKEN id="token-35-24" pos="word" morph="none" start_char="2076" end_char="2081">making</TOKEN>
<TOKEN id="token-35-25" pos="word" morph="none" start_char="2083" end_char="2084">it</TOKEN>
<TOKEN id="token-35-26" pos="word" morph="none" start_char="2086" end_char="2091">harder</TOKEN>
<TOKEN id="token-35-27" pos="word" morph="none" start_char="2093" end_char="2094">to</TOKEN>
<TOKEN id="token-35-28" pos="word" morph="none" start_char="2096" end_char="2102">contain</TOKEN>
<TOKEN id="token-35-29" pos="punct" morph="none" start_char="2103" end_char="2103">?</TOKEN>
</SEG>
<SEG id="segment-36" start_char="2106" end_char="2270">
<ORIGINAL_TEXT>Yes, this is highly likely as a number of cases in Europe have no apparent contact with known cases, have not traveled to areas where known cases have been detected.</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="2106" end_char="2108">Yes</TOKEN>
<TOKEN id="token-36-1" pos="punct" morph="none" start_char="2109" end_char="2109">,</TOKEN>
<TOKEN id="token-36-2" pos="word" morph="none" start_char="2111" end_char="2114">this</TOKEN>
<TOKEN id="token-36-3" pos="word" morph="none" start_char="2116" end_char="2117">is</TOKEN>
<TOKEN id="token-36-4" pos="word" morph="none" start_char="2119" end_char="2124">highly</TOKEN>
<TOKEN id="token-36-5" pos="word" morph="none" start_char="2126" end_char="2131">likely</TOKEN>
<TOKEN id="token-36-6" pos="word" morph="none" start_char="2133" end_char="2134">as</TOKEN>
<TOKEN id="token-36-7" pos="word" morph="none" start_char="2136" end_char="2136">a</TOKEN>
<TOKEN id="token-36-8" pos="word" morph="none" start_char="2138" end_char="2143">number</TOKEN>
<TOKEN id="token-36-9" pos="word" morph="none" start_char="2145" end_char="2146">of</TOKEN>
<TOKEN id="token-36-10" pos="word" morph="none" start_char="2148" end_char="2152">cases</TOKEN>
<TOKEN id="token-36-11" pos="word" morph="none" start_char="2154" end_char="2155">in</TOKEN>
<TOKEN id="token-36-12" pos="word" morph="none" start_char="2157" end_char="2162">Europe</TOKEN>
<TOKEN id="token-36-13" pos="word" morph="none" start_char="2164" end_char="2167">have</TOKEN>
<TOKEN id="token-36-14" pos="word" morph="none" start_char="2169" end_char="2170">no</TOKEN>
<TOKEN id="token-36-15" pos="word" morph="none" start_char="2172" end_char="2179">apparent</TOKEN>
<TOKEN id="token-36-16" pos="word" morph="none" start_char="2181" end_char="2187">contact</TOKEN>
<TOKEN id="token-36-17" pos="word" morph="none" start_char="2189" end_char="2192">with</TOKEN>
<TOKEN id="token-36-18" pos="word" morph="none" start_char="2194" end_char="2198">known</TOKEN>
<TOKEN id="token-36-19" pos="word" morph="none" start_char="2200" end_char="2204">cases</TOKEN>
<TOKEN id="token-36-20" pos="punct" morph="none" start_char="2205" end_char="2205">,</TOKEN>
<TOKEN id="token-36-21" pos="word" morph="none" start_char="2207" end_char="2210">have</TOKEN>
<TOKEN id="token-36-22" pos="word" morph="none" start_char="2212" end_char="2214">not</TOKEN>
<TOKEN id="token-36-23" pos="word" morph="none" start_char="2216" end_char="2223">traveled</TOKEN>
<TOKEN id="token-36-24" pos="word" morph="none" start_char="2225" end_char="2226">to</TOKEN>
<TOKEN id="token-36-25" pos="word" morph="none" start_char="2228" end_char="2232">areas</TOKEN>
<TOKEN id="token-36-26" pos="word" morph="none" start_char="2234" end_char="2238">where</TOKEN>
<TOKEN id="token-36-27" pos="word" morph="none" start_char="2240" end_char="2244">known</TOKEN>
<TOKEN id="token-36-28" pos="word" morph="none" start_char="2246" end_char="2250">cases</TOKEN>
<TOKEN id="token-36-29" pos="word" morph="none" start_char="2252" end_char="2255">have</TOKEN>
<TOKEN id="token-36-30" pos="word" morph="none" start_char="2257" end_char="2260">been</TOKEN>
<TOKEN id="token-36-31" pos="word" morph="none" start_char="2262" end_char="2269">detected</TOKEN>
<TOKEN id="token-36-32" pos="punct" morph="none" start_char="2270" end_char="2270">.</TOKEN>
</SEG>
<SEG id="segment-37" start_char="2272" end_char="2373">
<ORIGINAL_TEXT>We also know that quite a few cases are mild diseases that mimic a normal upper respiratory infection.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="2272" end_char="2273">We</TOKEN>
<TOKEN id="token-37-1" pos="word" morph="none" start_char="2275" end_char="2278">also</TOKEN>
<TOKEN id="token-37-2" pos="word" morph="none" start_char="2280" end_char="2283">know</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="2285" end_char="2288">that</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="2290" end_char="2294">quite</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="2296" end_char="2296">a</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="2298" end_char="2300">few</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="2302" end_char="2306">cases</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="2308" end_char="2310">are</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="2312" end_char="2315">mild</TOKEN>
<TOKEN id="token-37-10" pos="word" morph="none" start_char="2317" end_char="2324">diseases</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="2326" end_char="2329">that</TOKEN>
<TOKEN id="token-37-12" pos="word" morph="none" start_char="2331" end_char="2335">mimic</TOKEN>
<TOKEN id="token-37-13" pos="word" morph="none" start_char="2337" end_char="2337">a</TOKEN>
<TOKEN id="token-37-14" pos="word" morph="none" start_char="2339" end_char="2344">normal</TOKEN>
<TOKEN id="token-37-15" pos="word" morph="none" start_char="2346" end_char="2350">upper</TOKEN>
<TOKEN id="token-37-16" pos="word" morph="none" start_char="2352" end_char="2362">respiratory</TOKEN>
<TOKEN id="token-37-17" pos="word" morph="none" start_char="2364" end_char="2372">infection</TOKEN>
<TOKEN id="token-37-18" pos="punct" morph="none" start_char="2373" end_char="2373">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="2376" end_char="2530">
<ORIGINAL_TEXT>Statistical models that have been applied to the current data from the outbreak indicate that there is probably a considerable number of asymptomatic cases</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="2376" end_char="2386">Statistical</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="2388" end_char="2393">models</TOKEN>
<TOKEN id="token-38-2" pos="word" morph="none" start_char="2395" end_char="2398">that</TOKEN>
<TOKEN id="token-38-3" pos="word" morph="none" start_char="2400" end_char="2403">have</TOKEN>
<TOKEN id="token-38-4" pos="word" morph="none" start_char="2405" end_char="2408">been</TOKEN>
<TOKEN id="token-38-5" pos="word" morph="none" start_char="2410" end_char="2416">applied</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="2418" end_char="2419">to</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="2421" end_char="2423">the</TOKEN>
<TOKEN id="token-38-8" pos="word" morph="none" start_char="2425" end_char="2431">current</TOKEN>
<TOKEN id="token-38-9" pos="word" morph="none" start_char="2433" end_char="2436">data</TOKEN>
<TOKEN id="token-38-10" pos="word" morph="none" start_char="2438" end_char="2441">from</TOKEN>
<TOKEN id="token-38-11" pos="word" morph="none" start_char="2443" end_char="2445">the</TOKEN>
<TOKEN id="token-38-12" pos="word" morph="none" start_char="2447" end_char="2454">outbreak</TOKEN>
<TOKEN id="token-38-13" pos="word" morph="none" start_char="2456" end_char="2463">indicate</TOKEN>
<TOKEN id="token-38-14" pos="word" morph="none" start_char="2465" end_char="2468">that</TOKEN>
<TOKEN id="token-38-15" pos="word" morph="none" start_char="2470" end_char="2474">there</TOKEN>
<TOKEN id="token-38-16" pos="word" morph="none" start_char="2476" end_char="2477">is</TOKEN>
<TOKEN id="token-38-17" pos="word" morph="none" start_char="2479" end_char="2486">probably</TOKEN>
<TOKEN id="token-38-18" pos="word" morph="none" start_char="2488" end_char="2488">a</TOKEN>
<TOKEN id="token-38-19" pos="word" morph="none" start_char="2490" end_char="2501">considerable</TOKEN>
<TOKEN id="token-38-20" pos="word" morph="none" start_char="2503" end_char="2508">number</TOKEN>
<TOKEN id="token-38-21" pos="word" morph="none" start_char="2510" end_char="2511">of</TOKEN>
<TOKEN id="token-38-22" pos="word" morph="none" start_char="2513" end_char="2524">asymptomatic</TOKEN>
<TOKEN id="token-38-23" pos="word" morph="none" start_char="2526" end_char="2530">cases</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
