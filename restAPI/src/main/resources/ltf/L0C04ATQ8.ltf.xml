<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATQ8" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="1455" raw_text_md5="097a16bc4c05e462dabff7c9131476ec">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="78">
<ORIGINAL_TEXT>El coronavirus ya circulaba por Europa en noviembre de 2019, revela un estudio</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="2">El</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="4" end_char="14">coronavirus</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="16" end_char="17">ya</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="19" end_char="27">circulaba</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="29" end_char="31">por</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="33" end_char="38">Europa</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="40" end_char="41">en</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="43" end_char="51">noviembre</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="53" end_char="54">de</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="56" end_char="59">2019</TOKEN>
<TOKEN id="token-0-10" pos="punct" morph="none" start_char="60" end_char="60">,</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="62" end_char="67">revela</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="69" end_char="70">un</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="72" end_char="78">estudio</TOKEN>
</SEG>
<SEG id="segment-1" start_char="82" end_char="93">
<ORIGINAL_TEXT>EUROPA PRESS</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="82" end_char="87">EUROPA</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="89" end_char="93">PRESS</TOKEN>
</SEG>
<SEG id="segment-2" start_char="96" end_char="164">
<ORIGINAL_TEXT>Un hombre con mascarilla frente a la Torre Eiffel en París (Francia).</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="96" end_char="97">Un</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="99" end_char="104">hombre</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="106" end_char="108">con</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="110" end_char="119">mascarilla</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="121" end_char="126">frente</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="128" end_char="128">a</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="130" end_char="131">la</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="133" end_char="137">Torre</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="139" end_char="144">Eiffel</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="146" end_char="147">en</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="149" end_char="153">París</TOKEN>
<TOKEN id="token-2-11" pos="punct" morph="none" start_char="155" end_char="155">(</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="156" end_char="162">Francia</TOKEN>
<TOKEN id="token-2-13" pos="punct" morph="none" start_char="163" end_char="164">).</TOKEN>
</SEG>
<SEG id="segment-3" start_char="168" end_char="318">
<ORIGINAL_TEXT>La covid-19 ya circulaba por Europa en noviembre de 2019, en concreto en Francia, según ha revelado un estudio publicado a principios de este mes en el</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="168" end_char="169">La</TOKEN>
<TOKEN id="token-3-1" pos="unknown" morph="none" start_char="171" end_char="178">covid-19</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="180" end_char="181">ya</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="183" end_char="191">circulaba</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="193" end_char="195">por</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="197" end_char="202">Europa</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="204" end_char="205">en</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="207" end_char="215">noviembre</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="217" end_char="218">de</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="220" end_char="223">2019</TOKEN>
<TOKEN id="token-3-10" pos="punct" morph="none" start_char="224" end_char="224">,</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="226" end_char="227">en</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="229" end_char="236">concreto</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="238" end_char="239">en</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="241" end_char="247">Francia</TOKEN>
<TOKEN id="token-3-15" pos="punct" morph="none" start_char="248" end_char="248">,</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="250" end_char="254">según</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="256" end_char="257">ha</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="259" end_char="266">revelado</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="268" end_char="269">un</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="271" end_char="277">estudio</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="279" end_char="287">publicado</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="289" end_char="289">a</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="291" end_char="300">principios</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="302" end_char="303">de</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="305" end_char="308">este</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="310" end_char="312">mes</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="314" end_char="315">en</TOKEN>
<TOKEN id="token-3-28" pos="word" morph="none" start_char="317" end_char="318">el</TOKEN>
</SEG>
<SEG id="segment-4" start_char="321" end_char="353">
<ORIGINAL_TEXT>European Journal of Epidemiology.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="321" end_char="328">European</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="330" end_char="336">Journal</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="338" end_char="339">of</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="341" end_char="352">Epidemiology</TOKEN>
<TOKEN id="token-4-4" pos="punct" morph="none" start_char="353" end_char="353">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="357" end_char="533">
<ORIGINAL_TEXT>Este estudio desafía la teoría de que la pandemia se inició a finales del año pasado en la ciudad china de Wuhan, en concreto en los alrededores del mercado húmedo de la ciudad.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="357" end_char="360">Este</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="362" end_char="368">estudio</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="370" end_char="376">desafía</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="378" end_char="379">la</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="381" end_char="386">teoría</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="388" end_char="389">de</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="391" end_char="393">que</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="395" end_char="396">la</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="398" end_char="405">pandemia</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="407" end_char="408">se</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="410" end_char="415">inició</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="417" end_char="417">a</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="419" end_char="425">finales</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="427" end_char="429">del</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="431" end_char="433">año</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="435" end_char="440">pasado</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="442" end_char="443">en</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="445" end_char="446">la</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="448" end_char="453">ciudad</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="455" end_char="459">china</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="461" end_char="462">de</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="464" end_char="468">Wuhan</TOKEN>
<TOKEN id="token-5-22" pos="punct" morph="none" start_char="469" end_char="469">,</TOKEN>
<TOKEN id="token-5-23" pos="word" morph="none" start_char="471" end_char="472">en</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="474" end_char="481">concreto</TOKEN>
<TOKEN id="token-5-25" pos="word" morph="none" start_char="483" end_char="484">en</TOKEN>
<TOKEN id="token-5-26" pos="word" morph="none" start_char="486" end_char="488">los</TOKEN>
<TOKEN id="token-5-27" pos="word" morph="none" start_char="490" end_char="500">alrededores</TOKEN>
<TOKEN id="token-5-28" pos="word" morph="none" start_char="502" end_char="504">del</TOKEN>
<TOKEN id="token-5-29" pos="word" morph="none" start_char="506" end_char="512">mercado</TOKEN>
<TOKEN id="token-5-30" pos="word" morph="none" start_char="514" end_char="519">húmedo</TOKEN>
<TOKEN id="token-5-31" pos="word" morph="none" start_char="521" end_char="522">de</TOKEN>
<TOKEN id="token-5-32" pos="word" morph="none" start_char="524" end_char="525">la</TOKEN>
<TOKEN id="token-5-33" pos="word" morph="none" start_char="527" end_char="532">ciudad</TOKEN>
<TOKEN id="token-5-34" pos="punct" morph="none" start_char="533" end_char="533">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="536" end_char="766">
<ORIGINAL_TEXT>"Este informe sugiere que la infección por SARS-CoV-2 puede haber ocurrido ya en noviembre de 2019 en Francia", dice el documento, firmado por científicos de la Universidad de París, de la Sorbona y de la Universidad Aix-Marseille.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="punct" morph="none" start_char="536" end_char="536">"</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="537" end_char="540">Este</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="542" end_char="548">informe</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="550" end_char="556">sugiere</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="558" end_char="560">que</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="562" end_char="563">la</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="565" end_char="573">infección</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="575" end_char="577">por</TOKEN>
<TOKEN id="token-6-8" pos="unknown" morph="none" start_char="579" end_char="588">SARS-CoV-2</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="590" end_char="594">puede</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="596" end_char="600">haber</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="602" end_char="609">ocurrido</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="611" end_char="612">ya</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="614" end_char="615">en</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="617" end_char="625">noviembre</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="627" end_char="628">de</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="630" end_char="633">2019</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="635" end_char="636">en</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="638" end_char="644">Francia</TOKEN>
<TOKEN id="token-6-19" pos="punct" morph="none" start_char="645" end_char="646">",</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="648" end_char="651">dice</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="653" end_char="654">el</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="656" end_char="664">documento</TOKEN>
<TOKEN id="token-6-23" pos="punct" morph="none" start_char="665" end_char="665">,</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="667" end_char="673">firmado</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="675" end_char="677">por</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="679" end_char="689">científicos</TOKEN>
<TOKEN id="token-6-27" pos="word" morph="none" start_char="691" end_char="692">de</TOKEN>
<TOKEN id="token-6-28" pos="word" morph="none" start_char="694" end_char="695">la</TOKEN>
<TOKEN id="token-6-29" pos="word" morph="none" start_char="697" end_char="707">Universidad</TOKEN>
<TOKEN id="token-6-30" pos="word" morph="none" start_char="709" end_char="710">de</TOKEN>
<TOKEN id="token-6-31" pos="word" morph="none" start_char="712" end_char="716">París</TOKEN>
<TOKEN id="token-6-32" pos="punct" morph="none" start_char="717" end_char="717">,</TOKEN>
<TOKEN id="token-6-33" pos="word" morph="none" start_char="719" end_char="720">de</TOKEN>
<TOKEN id="token-6-34" pos="word" morph="none" start_char="722" end_char="723">la</TOKEN>
<TOKEN id="token-6-35" pos="word" morph="none" start_char="725" end_char="731">Sorbona</TOKEN>
<TOKEN id="token-6-36" pos="word" morph="none" start_char="733" end_char="733">y</TOKEN>
<TOKEN id="token-6-37" pos="word" morph="none" start_char="735" end_char="736">de</TOKEN>
<TOKEN id="token-6-38" pos="word" morph="none" start_char="738" end_char="739">la</TOKEN>
<TOKEN id="token-6-39" pos="word" morph="none" start_char="741" end_char="751">Universidad</TOKEN>
<TOKEN id="token-6-40" pos="unknown" morph="none" start_char="753" end_char="765">Aix-Marseille</TOKEN>
<TOKEN id="token-6-41" pos="punct" morph="none" start_char="766" end_char="766">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="769" end_char="956">
<ORIGINAL_TEXT>"En varios participantes identificamos síntomas, antecedentes de posibles exposiciones o eventos específicos compatibles con la infección temprana por SARS-CoV-2", revela la investigación.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="punct" morph="none" start_char="769" end_char="769">"</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="770" end_char="771">En</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="773" end_char="778">varios</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="780" end_char="792">participantes</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="794" end_char="806">identificamos</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="808" end_char="815">síntomas</TOKEN>
<TOKEN id="token-7-6" pos="punct" morph="none" start_char="816" end_char="816">,</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="818" end_char="829">antecedentes</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="831" end_char="832">de</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="834" end_char="841">posibles</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="843" end_char="854">exposiciones</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="856" end_char="856">o</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="858" end_char="864">eventos</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="866" end_char="876">específicos</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="878" end_char="888">compatibles</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="890" end_char="892">con</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="894" end_char="895">la</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="897" end_char="905">infección</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="907" end_char="914">temprana</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="916" end_char="918">por</TOKEN>
<TOKEN id="token-7-20" pos="unknown" morph="none" start_char="920" end_char="929">SARS-CoV-2</TOKEN>
<TOKEN id="token-7-21" pos="punct" morph="none" start_char="930" end_char="931">",</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="933" end_char="938">revela</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="940" end_char="941">la</TOKEN>
<TOKEN id="token-7-24" pos="word" morph="none" start_char="943" end_char="955">investigación</TOKEN>
<TOKEN id="token-7-25" pos="punct" morph="none" start_char="956" end_char="956">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="959" end_char="1266">
<ORIGINAL_TEXT>El estudio, dirigido por el profesor Fabrice Carrat, director del Instituto Pierre-Louis de Epidemiología y Salud Pública para Inserm en la Universidad de la Sorbona, involucró la recolección de muestras de suero de 9.144 adultos en la población francesa, de los cuales 353 habían dado positivo por covid-19.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="959" end_char="960">El</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="962" end_char="968">estudio</TOKEN>
<TOKEN id="token-8-2" pos="punct" morph="none" start_char="969" end_char="969">,</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="971" end_char="978">dirigido</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="980" end_char="982">por</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="984" end_char="985">el</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="987" end_char="994">profesor</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="996" end_char="1002">Fabrice</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1004" end_char="1009">Carrat</TOKEN>
<TOKEN id="token-8-9" pos="punct" morph="none" start_char="1010" end_char="1010">,</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="1012" end_char="1019">director</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="1021" end_char="1023">del</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="1025" end_char="1033">Instituto</TOKEN>
<TOKEN id="token-8-13" pos="unknown" morph="none" start_char="1035" end_char="1046">Pierre-Louis</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="1048" end_char="1049">de</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="1051" end_char="1063">Epidemiología</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="1065" end_char="1065">y</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="1067" end_char="1071">Salud</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="1073" end_char="1079">Pública</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="1081" end_char="1084">para</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="1086" end_char="1091">Inserm</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="1093" end_char="1094">en</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="1096" end_char="1097">la</TOKEN>
<TOKEN id="token-8-23" pos="word" morph="none" start_char="1099" end_char="1109">Universidad</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="1111" end_char="1112">de</TOKEN>
<TOKEN id="token-8-25" pos="word" morph="none" start_char="1114" end_char="1115">la</TOKEN>
<TOKEN id="token-8-26" pos="word" morph="none" start_char="1117" end_char="1123">Sorbona</TOKEN>
<TOKEN id="token-8-27" pos="punct" morph="none" start_char="1124" end_char="1124">,</TOKEN>
<TOKEN id="token-8-28" pos="word" morph="none" start_char="1126" end_char="1134">involucró</TOKEN>
<TOKEN id="token-8-29" pos="word" morph="none" start_char="1136" end_char="1137">la</TOKEN>
<TOKEN id="token-8-30" pos="word" morph="none" start_char="1139" end_char="1149">recolección</TOKEN>
<TOKEN id="token-8-31" pos="word" morph="none" start_char="1151" end_char="1152">de</TOKEN>
<TOKEN id="token-8-32" pos="word" morph="none" start_char="1154" end_char="1161">muestras</TOKEN>
<TOKEN id="token-8-33" pos="word" morph="none" start_char="1163" end_char="1164">de</TOKEN>
<TOKEN id="token-8-34" pos="word" morph="none" start_char="1166" end_char="1170">suero</TOKEN>
<TOKEN id="token-8-35" pos="word" morph="none" start_char="1172" end_char="1173">de</TOKEN>
<TOKEN id="token-8-36" pos="word" morph="none" start_char="1175" end_char="1179">9.144</TOKEN>
<TOKEN id="token-8-37" pos="word" morph="none" start_char="1181" end_char="1187">adultos</TOKEN>
<TOKEN id="token-8-38" pos="word" morph="none" start_char="1189" end_char="1190">en</TOKEN>
<TOKEN id="token-8-39" pos="word" morph="none" start_char="1192" end_char="1193">la</TOKEN>
<TOKEN id="token-8-40" pos="word" morph="none" start_char="1195" end_char="1203">población</TOKEN>
<TOKEN id="token-8-41" pos="word" morph="none" start_char="1205" end_char="1212">francesa</TOKEN>
<TOKEN id="token-8-42" pos="punct" morph="none" start_char="1213" end_char="1213">,</TOKEN>
<TOKEN id="token-8-43" pos="word" morph="none" start_char="1215" end_char="1216">de</TOKEN>
<TOKEN id="token-8-44" pos="word" morph="none" start_char="1218" end_char="1220">los</TOKEN>
<TOKEN id="token-8-45" pos="word" morph="none" start_char="1222" end_char="1227">cuales</TOKEN>
<TOKEN id="token-8-46" pos="word" morph="none" start_char="1229" end_char="1231">353</TOKEN>
<TOKEN id="token-8-47" pos="word" morph="none" start_char="1233" end_char="1238">habían</TOKEN>
<TOKEN id="token-8-48" pos="word" morph="none" start_char="1240" end_char="1243">dado</TOKEN>
<TOKEN id="token-8-49" pos="word" morph="none" start_char="1245" end_char="1252">positivo</TOKEN>
<TOKEN id="token-8-50" pos="word" morph="none" start_char="1254" end_char="1256">por</TOKEN>
<TOKEN id="token-8-51" pos="unknown" morph="none" start_char="1258" end_char="1265">covid-19</TOKEN>
<TOKEN id="token-8-52" pos="punct" morph="none" start_char="1266" end_char="1266">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1269" end_char="1451">
<ORIGINAL_TEXT>De esos positivos, 13 fueron muestreados entre noviembre de 2019 y enero de 2020 y fueron confirmados mediante pruebas de anticuerpos neutralizantes, revela la investigación francesa.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1269" end_char="1270">De</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1272" end_char="1275">esos</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="1277" end_char="1285">positivos</TOKEN>
<TOKEN id="token-9-3" pos="punct" morph="none" start_char="1286" end_char="1286">,</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1288" end_char="1289">13</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1291" end_char="1296">fueron</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1298" end_char="1308">muestreados</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1310" end_char="1314">entre</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1316" end_char="1324">noviembre</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="1326" end_char="1327">de</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1329" end_char="1332">2019</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1334" end_char="1334">y</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="1336" end_char="1340">enero</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1342" end_char="1343">de</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1345" end_char="1348">2020</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1350" end_char="1350">y</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="1352" end_char="1357">fueron</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1359" end_char="1369">confirmados</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1371" end_char="1378">mediante</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1380" end_char="1386">pruebas</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1388" end_char="1389">de</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1391" end_char="1401">anticuerpos</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1403" end_char="1416">neutralizantes</TOKEN>
<TOKEN id="token-9-23" pos="punct" morph="none" start_char="1417" end_char="1417">,</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1419" end_char="1424">revela</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="1426" end_char="1427">la</TOKEN>
<TOKEN id="token-9-26" pos="word" morph="none" start_char="1429" end_char="1441">investigación</TOKEN>
<TOKEN id="token-9-27" pos="word" morph="none" start_char="1443" end_char="1450">francesa</TOKEN>
<TOKEN id="token-9-28" pos="punct" morph="none" start_char="1451" end_char="1451">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
