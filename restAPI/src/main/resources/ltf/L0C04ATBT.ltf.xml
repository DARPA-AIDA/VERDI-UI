<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATBT" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="3172" raw_text_md5="109586661602811217b9ed7b9f6847aa">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="68">
<ORIGINAL_TEXT>Análisis genéticos apuntan al murciélago como origen del coronavirus</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="8">Análisis</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="10" end_char="18">genéticos</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="20" end_char="26">apuntan</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="28" end_char="29">al</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="31" end_char="40">murciélago</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="42" end_char="45">como</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="47" end_char="52">origen</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="54" end_char="56">del</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="58" end_char="68">coronavirus</TOKEN>
</SEG>
<SEG id="segment-1" start_char="73" end_char="352">
<ORIGINAL_TEXT>Los murciélagos de la especie Rhinolophus Affinis, muy común en China y el sureste asiático, se han vuelto los principales sospechosos del origen del coronavirus, debido a un análisis comparativo del genoma del virus, hecho en Italia y publicado en el Journal of Medical Virology.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="73" end_char="75">Los</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="77" end_char="87">murciélagos</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="89" end_char="90">de</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="92" end_char="93">la</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="95" end_char="101">especie</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="103" end_char="113">Rhinolophus</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="115" end_char="121">Affinis</TOKEN>
<TOKEN id="token-1-7" pos="punct" morph="none" start_char="122" end_char="122">,</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="124" end_char="126">muy</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="128" end_char="132">común</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="134" end_char="135">en</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="137" end_char="141">China</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="143" end_char="143">y</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="145" end_char="146">el</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="148" end_char="154">sureste</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="156" end_char="163">asiático</TOKEN>
<TOKEN id="token-1-16" pos="punct" morph="none" start_char="164" end_char="164">,</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="166" end_char="167">se</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="169" end_char="171">han</TOKEN>
<TOKEN id="token-1-19" pos="word" morph="none" start_char="173" end_char="178">vuelto</TOKEN>
<TOKEN id="token-1-20" pos="word" morph="none" start_char="180" end_char="182">los</TOKEN>
<TOKEN id="token-1-21" pos="word" morph="none" start_char="184" end_char="194">principales</TOKEN>
<TOKEN id="token-1-22" pos="word" morph="none" start_char="196" end_char="206">sospechosos</TOKEN>
<TOKEN id="token-1-23" pos="word" morph="none" start_char="208" end_char="210">del</TOKEN>
<TOKEN id="token-1-24" pos="word" morph="none" start_char="212" end_char="217">origen</TOKEN>
<TOKEN id="token-1-25" pos="word" morph="none" start_char="219" end_char="221">del</TOKEN>
<TOKEN id="token-1-26" pos="word" morph="none" start_char="223" end_char="233">coronavirus</TOKEN>
<TOKEN id="token-1-27" pos="punct" morph="none" start_char="234" end_char="234">,</TOKEN>
<TOKEN id="token-1-28" pos="word" morph="none" start_char="236" end_char="241">debido</TOKEN>
<TOKEN id="token-1-29" pos="word" morph="none" start_char="243" end_char="243">a</TOKEN>
<TOKEN id="token-1-30" pos="word" morph="none" start_char="245" end_char="246">un</TOKEN>
<TOKEN id="token-1-31" pos="word" morph="none" start_char="248" end_char="255">análisis</TOKEN>
<TOKEN id="token-1-32" pos="word" morph="none" start_char="257" end_char="267">comparativo</TOKEN>
<TOKEN id="token-1-33" pos="word" morph="none" start_char="269" end_char="271">del</TOKEN>
<TOKEN id="token-1-34" pos="word" morph="none" start_char="273" end_char="278">genoma</TOKEN>
<TOKEN id="token-1-35" pos="word" morph="none" start_char="280" end_char="282">del</TOKEN>
<TOKEN id="token-1-36" pos="word" morph="none" start_char="284" end_char="288">virus</TOKEN>
<TOKEN id="token-1-37" pos="punct" morph="none" start_char="289" end_char="289">,</TOKEN>
<TOKEN id="token-1-38" pos="word" morph="none" start_char="291" end_char="295">hecho</TOKEN>
<TOKEN id="token-1-39" pos="word" morph="none" start_char="297" end_char="298">en</TOKEN>
<TOKEN id="token-1-40" pos="word" morph="none" start_char="300" end_char="305">Italia</TOKEN>
<TOKEN id="token-1-41" pos="word" morph="none" start_char="307" end_char="307">y</TOKEN>
<TOKEN id="token-1-42" pos="word" morph="none" start_char="309" end_char="317">publicado</TOKEN>
<TOKEN id="token-1-43" pos="word" morph="none" start_char="319" end_char="320">en</TOKEN>
<TOKEN id="token-1-44" pos="word" morph="none" start_char="322" end_char="323">el</TOKEN>
<TOKEN id="token-1-45" pos="word" morph="none" start_char="325" end_char="331">Journal</TOKEN>
<TOKEN id="token-1-46" pos="word" morph="none" start_char="333" end_char="334">of</TOKEN>
<TOKEN id="token-1-47" pos="word" morph="none" start_char="336" end_char="342">Medical</TOKEN>
<TOKEN id="token-1-48" pos="word" morph="none" start_char="344" end_char="351">Virology</TOKEN>
<TOKEN id="token-1-49" pos="punct" morph="none" start_char="352" end_char="352">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="355" end_char="521">
<ORIGINAL_TEXT>Los Rhinolophus Affini ya eran sospechosos desde el 23 de enero, cuando el Instituto de Virología de Wuhan lanzó el primer identikit genético de coronavirus 2019-nCoV.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="355" end_char="357">Los</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="359" end_char="369">Rhinolophus</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="371" end_char="376">Affini</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="378" end_char="379">ya</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="381" end_char="384">eran</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="386" end_char="396">sospechosos</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="398" end_char="402">desde</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="404" end_char="405">el</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="407" end_char="408">23</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="410" end_char="411">de</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="413" end_char="417">enero</TOKEN>
<TOKEN id="token-2-11" pos="punct" morph="none" start_char="418" end_char="418">,</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="420" end_char="425">cuando</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="427" end_char="428">el</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="430" end_char="438">Instituto</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="440" end_char="441">de</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="443" end_char="451">Virología</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="453" end_char="454">de</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="456" end_char="460">Wuhan</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="462" end_char="466">lanzó</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="468" end_char="469">el</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="471" end_char="476">primer</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="478" end_char="486">identikit</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="488" end_char="495">genético</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="497" end_char="498">de</TOKEN>
<TOKEN id="token-2-25" pos="word" morph="none" start_char="500" end_char="510">coronavirus</TOKEN>
<TOKEN id="token-2-26" pos="unknown" morph="none" start_char="512" end_char="520">2019-nCoV</TOKEN>
<TOKEN id="token-2-27" pos="punct" morph="none" start_char="521" end_char="521">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="524" end_char="867">
<ORIGINAL_TEXT>"El domingo pasado descargamos los seis genomas de coronavirus contenido en las bases de datos Gisaid y Genbank y buscamos secuencias similares en bases a datos públicos", dijo el coordinador de la investigación, el experto en bioinformática Federico Giorgi, del Departamento de Farmacia y Biotecnología de Universidad de Bologna, informó ANSA.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="punct" morph="none" start_char="524" end_char="524">"</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="525" end_char="526">El</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="528" end_char="534">domingo</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="536" end_char="541">pasado</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="543" end_char="553">descargamos</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="555" end_char="557">los</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="559" end_char="562">seis</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="564" end_char="570">genomas</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="572" end_char="573">de</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="575" end_char="585">coronavirus</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="587" end_char="595">contenido</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="597" end_char="598">en</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="600" end_char="602">las</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="604" end_char="608">bases</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="610" end_char="611">de</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="613" end_char="617">datos</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="619" end_char="624">Gisaid</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="626" end_char="626">y</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="628" end_char="634">Genbank</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="636" end_char="636">y</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="638" end_char="645">buscamos</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="647" end_char="656">secuencias</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="658" end_char="666">similares</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="668" end_char="669">en</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="671" end_char="675">bases</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="677" end_char="677">a</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="679" end_char="683">datos</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="685" end_char="692">públicos</TOKEN>
<TOKEN id="token-3-28" pos="punct" morph="none" start_char="693" end_char="694">",</TOKEN>
<TOKEN id="token-3-29" pos="word" morph="none" start_char="696" end_char="699">dijo</TOKEN>
<TOKEN id="token-3-30" pos="word" morph="none" start_char="701" end_char="702">el</TOKEN>
<TOKEN id="token-3-31" pos="word" morph="none" start_char="704" end_char="714">coordinador</TOKEN>
<TOKEN id="token-3-32" pos="word" morph="none" start_char="716" end_char="717">de</TOKEN>
<TOKEN id="token-3-33" pos="word" morph="none" start_char="719" end_char="720">la</TOKEN>
<TOKEN id="token-3-34" pos="word" morph="none" start_char="722" end_char="734">investigación</TOKEN>
<TOKEN id="token-3-35" pos="punct" morph="none" start_char="735" end_char="735">,</TOKEN>
<TOKEN id="token-3-36" pos="word" morph="none" start_char="737" end_char="738">el</TOKEN>
<TOKEN id="token-3-37" pos="word" morph="none" start_char="740" end_char="746">experto</TOKEN>
<TOKEN id="token-3-38" pos="word" morph="none" start_char="748" end_char="749">en</TOKEN>
<TOKEN id="token-3-39" pos="word" morph="none" start_char="751" end_char="764">bioinformática</TOKEN>
<TOKEN id="token-3-40" pos="word" morph="none" start_char="766" end_char="773">Federico</TOKEN>
<TOKEN id="token-3-41" pos="word" morph="none" start_char="775" end_char="780">Giorgi</TOKEN>
<TOKEN id="token-3-42" pos="punct" morph="none" start_char="781" end_char="781">,</TOKEN>
<TOKEN id="token-3-43" pos="word" morph="none" start_char="783" end_char="785">del</TOKEN>
<TOKEN id="token-3-44" pos="word" morph="none" start_char="787" end_char="798">Departamento</TOKEN>
<TOKEN id="token-3-45" pos="word" morph="none" start_char="800" end_char="801">de</TOKEN>
<TOKEN id="token-3-46" pos="word" morph="none" start_char="803" end_char="810">Farmacia</TOKEN>
<TOKEN id="token-3-47" pos="word" morph="none" start_char="812" end_char="812">y</TOKEN>
<TOKEN id="token-3-48" pos="word" morph="none" start_char="814" end_char="826">Biotecnología</TOKEN>
<TOKEN id="token-3-49" pos="word" morph="none" start_char="828" end_char="829">de</TOKEN>
<TOKEN id="token-3-50" pos="word" morph="none" start_char="831" end_char="841">Universidad</TOKEN>
<TOKEN id="token-3-51" pos="word" morph="none" start_char="843" end_char="844">de</TOKEN>
<TOKEN id="token-3-52" pos="word" morph="none" start_char="846" end_char="852">Bologna</TOKEN>
<TOKEN id="token-3-53" pos="punct" morph="none" start_char="853" end_char="853">,</TOKEN>
<TOKEN id="token-3-54" pos="word" morph="none" start_char="855" end_char="861">informó</TOKEN>
<TOKEN id="token-3-55" pos="word" morph="none" start_char="863" end_char="866">ANSA</TOKEN>
<TOKEN id="token-3-56" pos="punct" morph="none" start_char="867" end_char="867">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="870" end_char="1003">
<ORIGINAL_TEXT>"El virus no es muy heterogéneo y cambiante" y eso implica que "es posible una terapia farmacológica, que debería funcionar en todos".</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="punct" morph="none" start_char="870" end_char="870">"</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="871" end_char="872">El</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="874" end_char="878">virus</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="880" end_char="881">no</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="883" end_char="884">es</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="886" end_char="888">muy</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="890" end_char="900">heterogéneo</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="902" end_char="902">y</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="904" end_char="912">cambiante</TOKEN>
<TOKEN id="token-4-9" pos="punct" morph="none" start_char="913" end_char="913">"</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="915" end_char="915">y</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="917" end_char="919">eso</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="921" end_char="927">implica</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="929" end_char="931">que</TOKEN>
<TOKEN id="token-4-14" pos="punct" morph="none" start_char="933" end_char="933">"</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="934" end_char="935">es</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="937" end_char="943">posible</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="945" end_char="947">una</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="949" end_char="955">terapia</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="957" end_char="969">farmacológica</TOKEN>
<TOKEN id="token-4-20" pos="punct" morph="none" start_char="970" end_char="970">,</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="972" end_char="974">que</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="976" end_char="982">debería</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="984" end_char="992">funcionar</TOKEN>
<TOKEN id="token-4-24" pos="word" morph="none" start_char="994" end_char="995">en</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="997" end_char="1001">todos</TOKEN>
<TOKEN id="token-4-26" pos="punct" morph="none" start_char="1002" end_char="1003">".</TOKEN>
</SEG>
<SEG id="segment-5" start_char="1007" end_char="1285">
<ORIGINAL_TEXT>El análisis también indica que el virus está cambiando muy lentamente porque todos los coronavirus humanos secuenciados hasta ahora "son muchos, similares entre sí, incluso si provienen de diferentes regiones de China y del mundo", hasta el punto de superponerse a más de un 99%.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="1007" end_char="1008">El</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="1010" end_char="1017">análisis</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="1019" end_char="1025">también</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="1027" end_char="1032">indica</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="1034" end_char="1036">que</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="1038" end_char="1039">el</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="1041" end_char="1045">virus</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="1047" end_char="1050">está</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="1052" end_char="1060">cambiando</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="1062" end_char="1064">muy</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="1066" end_char="1075">lentamente</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="1077" end_char="1082">porque</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="1084" end_char="1088">todos</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="1090" end_char="1092">los</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="1094" end_char="1104">coronavirus</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="1106" end_char="1112">humanos</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="1114" end_char="1125">secuenciados</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="1127" end_char="1131">hasta</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="1133" end_char="1137">ahora</TOKEN>
<TOKEN id="token-5-19" pos="punct" morph="none" start_char="1139" end_char="1139">"</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="1140" end_char="1142">son</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="1144" end_char="1149">muchos</TOKEN>
<TOKEN id="token-5-22" pos="punct" morph="none" start_char="1150" end_char="1150">,</TOKEN>
<TOKEN id="token-5-23" pos="word" morph="none" start_char="1152" end_char="1160">similares</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="1162" end_char="1166">entre</TOKEN>
<TOKEN id="token-5-25" pos="word" morph="none" start_char="1168" end_char="1169">sí</TOKEN>
<TOKEN id="token-5-26" pos="punct" morph="none" start_char="1170" end_char="1170">,</TOKEN>
<TOKEN id="token-5-27" pos="word" morph="none" start_char="1172" end_char="1178">incluso</TOKEN>
<TOKEN id="token-5-28" pos="word" morph="none" start_char="1180" end_char="1181">si</TOKEN>
<TOKEN id="token-5-29" pos="word" morph="none" start_char="1183" end_char="1191">provienen</TOKEN>
<TOKEN id="token-5-30" pos="word" morph="none" start_char="1193" end_char="1194">de</TOKEN>
<TOKEN id="token-5-31" pos="word" morph="none" start_char="1196" end_char="1205">diferentes</TOKEN>
<TOKEN id="token-5-32" pos="word" morph="none" start_char="1207" end_char="1214">regiones</TOKEN>
<TOKEN id="token-5-33" pos="word" morph="none" start_char="1216" end_char="1217">de</TOKEN>
<TOKEN id="token-5-34" pos="word" morph="none" start_char="1219" end_char="1223">China</TOKEN>
<TOKEN id="token-5-35" pos="word" morph="none" start_char="1225" end_char="1225">y</TOKEN>
<TOKEN id="token-5-36" pos="word" morph="none" start_char="1227" end_char="1229">del</TOKEN>
<TOKEN id="token-5-37" pos="word" morph="none" start_char="1231" end_char="1235">mundo</TOKEN>
<TOKEN id="token-5-38" pos="punct" morph="none" start_char="1236" end_char="1237">",</TOKEN>
<TOKEN id="token-5-39" pos="word" morph="none" start_char="1239" end_char="1243">hasta</TOKEN>
<TOKEN id="token-5-40" pos="word" morph="none" start_char="1245" end_char="1246">el</TOKEN>
<TOKEN id="token-5-41" pos="word" morph="none" start_char="1248" end_char="1252">punto</TOKEN>
<TOKEN id="token-5-42" pos="word" morph="none" start_char="1254" end_char="1255">de</TOKEN>
<TOKEN id="token-5-43" pos="word" morph="none" start_char="1257" end_char="1268">superponerse</TOKEN>
<TOKEN id="token-5-44" pos="word" morph="none" start_char="1270" end_char="1270">a</TOKEN>
<TOKEN id="token-5-45" pos="word" morph="none" start_char="1272" end_char="1274">más</TOKEN>
<TOKEN id="token-5-46" pos="word" morph="none" start_char="1276" end_char="1277">de</TOKEN>
<TOKEN id="token-5-47" pos="word" morph="none" start_char="1279" end_char="1280">un</TOKEN>
<TOKEN id="token-5-48" pos="word" morph="none" start_char="1282" end_char="1283">99</TOKEN>
<TOKEN id="token-5-49" pos="punct" morph="none" start_char="1284" end_char="1285">%.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="1288" end_char="1456">
<ORIGINAL_TEXT>Esto significa, señaló Giorgi, que "el virus no es muy heterogéneo y cambiante" y eso implica que "es posible una terapia farmacológica, que debería funcionar en todos".</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="1288" end_char="1291">Esto</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="1293" end_char="1301">significa</TOKEN>
<TOKEN id="token-6-2" pos="punct" morph="none" start_char="1302" end_char="1302">,</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="1304" end_char="1309">señaló</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="1311" end_char="1316">Giorgi</TOKEN>
<TOKEN id="token-6-5" pos="punct" morph="none" start_char="1317" end_char="1317">,</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="1319" end_char="1321">que</TOKEN>
<TOKEN id="token-6-7" pos="punct" morph="none" start_char="1323" end_char="1323">"</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="1324" end_char="1325">el</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="1327" end_char="1331">virus</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="1333" end_char="1334">no</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="1336" end_char="1337">es</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="1339" end_char="1341">muy</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="1343" end_char="1353">heterogéneo</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="1355" end_char="1355">y</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="1357" end_char="1365">cambiante</TOKEN>
<TOKEN id="token-6-16" pos="punct" morph="none" start_char="1366" end_char="1366">"</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="1368" end_char="1368">y</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="1370" end_char="1372">eso</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="1374" end_char="1380">implica</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="1382" end_char="1384">que</TOKEN>
<TOKEN id="token-6-21" pos="punct" morph="none" start_char="1386" end_char="1386">"</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="1387" end_char="1388">es</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="1390" end_char="1396">posible</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="1398" end_char="1400">una</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="1402" end_char="1408">terapia</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="1410" end_char="1422">farmacológica</TOKEN>
<TOKEN id="token-6-27" pos="punct" morph="none" start_char="1423" end_char="1423">,</TOKEN>
<TOKEN id="token-6-28" pos="word" morph="none" start_char="1425" end_char="1427">que</TOKEN>
<TOKEN id="token-6-29" pos="word" morph="none" start_char="1429" end_char="1435">debería</TOKEN>
<TOKEN id="token-6-30" pos="word" morph="none" start_char="1437" end_char="1445">funcionar</TOKEN>
<TOKEN id="token-6-31" pos="word" morph="none" start_char="1447" end_char="1448">en</TOKEN>
<TOKEN id="token-6-32" pos="word" morph="none" start_char="1450" end_char="1454">todos</TOKEN>
<TOKEN id="token-6-33" pos="punct" morph="none" start_char="1455" end_char="1456">".</TOKEN>
</SEG>
<SEG id="segment-7" start_char="1459" end_char="1547">
<ORIGINAL_TEXT>Igualmente, el material genético del coronavirus muestra "un alto grado de variabilidad".</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="1459" end_char="1468">Igualmente</TOKEN>
<TOKEN id="token-7-1" pos="punct" morph="none" start_char="1469" end_char="1469">,</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="1471" end_char="1472">el</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="1474" end_char="1481">material</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="1483" end_char="1490">genético</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="1492" end_char="1494">del</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="1496" end_char="1506">coronavirus</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="1508" end_char="1514">muestra</TOKEN>
<TOKEN id="token-7-8" pos="punct" morph="none" start_char="1516" end_char="1516">"</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="1517" end_char="1518">un</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="1520" end_char="1523">alto</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="1525" end_char="1529">grado</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="1531" end_char="1532">de</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="1534" end_char="1545">variabilidad</TOKEN>
<TOKEN id="token-7-14" pos="punct" morph="none" start_char="1546" end_char="1547">".</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1549" end_char="1592">
<ORIGINAL_TEXT>Aprender más es el próximo objetivo, agregó.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="1549" end_char="1556">Aprender</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="1558" end_char="1560">más</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="1562" end_char="1563">es</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1565" end_char="1566">el</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1568" end_char="1574">próximo</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1576" end_char="1583">objetivo</TOKEN>
<TOKEN id="token-8-6" pos="punct" morph="none" start_char="1584" end_char="1584">,</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="1586" end_char="1591">agregó</TOKEN>
<TOKEN id="token-8-8" pos="punct" morph="none" start_char="1592" end_char="1592">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1595" end_char="1661">
<ORIGINAL_TEXT>No hay idea de como el animal podría haberlo transmitido al hombre.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1595" end_char="1596">No</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1598" end_char="1600">hay</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="1602" end_char="1605">idea</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1607" end_char="1608">de</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1610" end_char="1613">como</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1615" end_char="1616">el</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1618" end_char="1623">animal</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1625" end_char="1630">podría</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1632" end_char="1638">haberlo</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="1640" end_char="1650">transmitido</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1652" end_char="1653">al</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1655" end_char="1660">hombre</TOKEN>
<TOKEN id="token-9-12" pos="punct" morph="none" start_char="1661" end_char="1661">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1663" end_char="1728">
<ORIGINAL_TEXT>Acaso, por intermedio de otro animal, o por una mordedura directa.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1663" end_char="1667">Acaso</TOKEN>
<TOKEN id="token-10-1" pos="punct" morph="none" start_char="1668" end_char="1668">,</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1670" end_char="1672">por</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1674" end_char="1683">intermedio</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1685" end_char="1686">de</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1688" end_char="1691">otro</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1693" end_char="1698">animal</TOKEN>
<TOKEN id="token-10-7" pos="punct" morph="none" start_char="1699" end_char="1699">,</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1701" end_char="1701">o</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1703" end_char="1705">por</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1707" end_char="1709">una</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1711" end_char="1719">mordedura</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1721" end_char="1727">directa</TOKEN>
<TOKEN id="token-10-13" pos="punct" morph="none" start_char="1728" end_char="1728">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1732" end_char="1970">
<ORIGINAL_TEXT>El equipo de Bologna también descubrió que el genoma del coronavirus en humanos comparte el 96,2% de sus activos genéticos con el del murciélago Rhinolophus Affinis, cuya secuencia se había obtenido en 2013 en la provincia china de Yunnan.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1732" end_char="1733">El</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1735" end_char="1740">equipo</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1742" end_char="1743">de</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1745" end_char="1751">Bologna</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1753" end_char="1759">también</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1761" end_char="1769">descubrió</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1771" end_char="1773">que</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1775" end_char="1776">el</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1778" end_char="1783">genoma</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1785" end_char="1787">del</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1789" end_char="1799">coronavirus</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1801" end_char="1802">en</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1804" end_char="1810">humanos</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1812" end_char="1819">comparte</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1821" end_char="1822">el</TOKEN>
<TOKEN id="token-11-15" pos="unknown" morph="none" start_char="1824" end_char="1827">96,2</TOKEN>
<TOKEN id="token-11-16" pos="punct" morph="none" start_char="1828" end_char="1828">%</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1830" end_char="1831">de</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1833" end_char="1835">sus</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1837" end_char="1843">activos</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1845" end_char="1853">genéticos</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1855" end_char="1857">con</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1859" end_char="1860">el</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1862" end_char="1864">del</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1866" end_char="1875">murciélago</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1877" end_char="1887">Rhinolophus</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="1889" end_char="1895">Affinis</TOKEN>
<TOKEN id="token-11-27" pos="punct" morph="none" start_char="1896" end_char="1896">,</TOKEN>
<TOKEN id="token-11-28" pos="word" morph="none" start_char="1898" end_char="1901">cuya</TOKEN>
<TOKEN id="token-11-29" pos="word" morph="none" start_char="1903" end_char="1911">secuencia</TOKEN>
<TOKEN id="token-11-30" pos="word" morph="none" start_char="1913" end_char="1914">se</TOKEN>
<TOKEN id="token-11-31" pos="word" morph="none" start_char="1916" end_char="1920">había</TOKEN>
<TOKEN id="token-11-32" pos="word" morph="none" start_char="1922" end_char="1929">obtenido</TOKEN>
<TOKEN id="token-11-33" pos="word" morph="none" start_char="1931" end_char="1932">en</TOKEN>
<TOKEN id="token-11-34" pos="word" morph="none" start_char="1934" end_char="1937">2013</TOKEN>
<TOKEN id="token-11-35" pos="word" morph="none" start_char="1939" end_char="1940">en</TOKEN>
<TOKEN id="token-11-36" pos="word" morph="none" start_char="1942" end_char="1943">la</TOKEN>
<TOKEN id="token-11-37" pos="word" morph="none" start_char="1945" end_char="1953">provincia</TOKEN>
<TOKEN id="token-11-38" pos="word" morph="none" start_char="1955" end_char="1959">china</TOKEN>
<TOKEN id="token-11-39" pos="word" morph="none" start_char="1961" end_char="1962">de</TOKEN>
<TOKEN id="token-11-40" pos="word" morph="none" start_char="1964" end_char="1969">Yunnan</TOKEN>
<TOKEN id="token-11-41" pos="punct" morph="none" start_char="1970" end_char="1970">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1974" end_char="2147">
<ORIGINAL_TEXT>Realizzata all’Università di Bologna, la ricerca conferma l’origine nei pipistrelli e mostra che il virus è poco mutabile, ma individua anche un punto di elevata variabilità.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1974" end_char="1983">Realizzata</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1985" end_char="1998">all’Università</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="2000" end_char="2001">di</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="2003" end_char="2009">Bologna</TOKEN>
<TOKEN id="token-12-4" pos="punct" morph="none" start_char="2010" end_char="2010">,</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="2012" end_char="2013">la</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="2015" end_char="2021">ricerca</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="2023" end_char="2030">conferma</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="2032" end_char="2040">l’origine</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="2042" end_char="2044">nei</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="2046" end_char="2056">pipistrelli</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="2058" end_char="2058">e</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="2060" end_char="2065">mostra</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="2067" end_char="2069">che</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="2071" end_char="2072">il</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="2074" end_char="2078">virus</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="2080" end_char="2080">è</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="2082" end_char="2085">poco</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="2087" end_char="2094">mutabile</TOKEN>
<TOKEN id="token-12-19" pos="punct" morph="none" start_char="2095" end_char="2095">,</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="2097" end_char="2098">ma</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="2100" end_char="2108">individua</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="2110" end_char="2114">anche</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="2116" end_char="2117">un</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="2119" end_char="2123">punto</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="2125" end_char="2126">di</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="2128" end_char="2134">elevata</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="2136" end_char="2146">variabilità</TOKEN>
<TOKEN id="token-12-28" pos="punct" morph="none" start_char="2147" end_char="2147">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="2149" end_char="2249">
<ORIGINAL_TEXT>https://t.co/CsXLG3vkn8 #Unibo #ricerca #coronavirus— UniboMagazine (@UniboMagazine) February 7, 2020</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="url" morph="none" start_char="2149" end_char="2171">https://t.co/CsXLG3vkn8</TOKEN>
<TOKEN id="token-13-1" pos="tag" morph="none" start_char="2173" end_char="2178">#Unibo</TOKEN>
<TOKEN id="token-13-2" pos="tag" morph="none" start_char="2180" end_char="2187">#ricerca</TOKEN>
<TOKEN id="token-13-3" pos="tag" morph="none" start_char="2189" end_char="2201">#coronavirus—</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="2203" end_char="2215">UniboMagazine</TOKEN>
<TOKEN id="token-13-5" pos="punct" morph="none" start_char="2217" end_char="2218">(@</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="2219" end_char="2231">UniboMagazine</TOKEN>
<TOKEN id="token-13-7" pos="punct" morph="none" start_char="2232" end_char="2232">)</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="2234" end_char="2241">February</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="2243" end_char="2243">7</TOKEN>
<TOKEN id="token-13-10" pos="punct" morph="none" start_char="2244" end_char="2244">,</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="2246" end_char="2249">2020</TOKEN>
</SEG>
<SEG id="segment-14" start_char="2253" end_char="2458">
<ORIGINAL_TEXT>Definitivamente, sostiene el equipo científico, es más bajo al 80,3% del parecido al virus Sars (Síndrome Respiratorio Agudo Severo), la enfermedad también causada por coronavirus que apareció en 2002-2003.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="2253" end_char="2267">Definitivamente</TOKEN>
<TOKEN id="token-14-1" pos="punct" morph="none" start_char="2268" end_char="2268">,</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="2270" end_char="2277">sostiene</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="2279" end_char="2280">el</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="2282" end_char="2287">equipo</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="2289" end_char="2298">científico</TOKEN>
<TOKEN id="token-14-6" pos="punct" morph="none" start_char="2299" end_char="2299">,</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="2301" end_char="2302">es</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="2304" end_char="2306">más</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="2308" end_char="2311">bajo</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="2313" end_char="2314">al</TOKEN>
<TOKEN id="token-14-11" pos="unknown" morph="none" start_char="2316" end_char="2319">80,3</TOKEN>
<TOKEN id="token-14-12" pos="punct" morph="none" start_char="2320" end_char="2320">%</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="2322" end_char="2324">del</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="2326" end_char="2333">parecido</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="2335" end_char="2336">al</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="2338" end_char="2342">virus</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="2344" end_char="2347">Sars</TOKEN>
<TOKEN id="token-14-18" pos="punct" morph="none" start_char="2349" end_char="2349">(</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="2350" end_char="2357">Síndrome</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="2359" end_char="2370">Respiratorio</TOKEN>
<TOKEN id="token-14-21" pos="word" morph="none" start_char="2372" end_char="2376">Agudo</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="2378" end_char="2383">Severo</TOKEN>
<TOKEN id="token-14-23" pos="punct" morph="none" start_char="2384" end_char="2385">),</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="2387" end_char="2388">la</TOKEN>
<TOKEN id="token-14-25" pos="word" morph="none" start_char="2390" end_char="2399">enfermedad</TOKEN>
<TOKEN id="token-14-26" pos="word" morph="none" start_char="2401" end_char="2407">también</TOKEN>
<TOKEN id="token-14-27" pos="word" morph="none" start_char="2409" end_char="2415">causada</TOKEN>
<TOKEN id="token-14-28" pos="word" morph="none" start_char="2417" end_char="2419">por</TOKEN>
<TOKEN id="token-14-29" pos="word" morph="none" start_char="2421" end_char="2431">coronavirus</TOKEN>
<TOKEN id="token-14-30" pos="word" morph="none" start_char="2433" end_char="2435">que</TOKEN>
<TOKEN id="token-14-31" pos="word" morph="none" start_char="2437" end_char="2444">apareció</TOKEN>
<TOKEN id="token-14-32" pos="word" morph="none" start_char="2446" end_char="2447">en</TOKEN>
<TOKEN id="token-14-33" pos="unknown" morph="none" start_char="2449" end_char="2457">2002-2003</TOKEN>
<TOKEN id="token-14-34" pos="punct" morph="none" start_char="2458" end_char="2458">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="2461" end_char="2618">
<ORIGINAL_TEXT>Es el segundo análisis genético más grande publicado hasta ahora, tras un primer árbol genealógico de coronavirus que en la revista Lancet comparó 10 genomas.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="2461" end_char="2462">Es</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="2464" end_char="2465">el</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="2467" end_char="2473">segundo</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="2475" end_char="2482">análisis</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="2484" end_char="2491">genético</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="2493" end_char="2495">más</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="2497" end_char="2502">grande</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="2504" end_char="2512">publicado</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="2514" end_char="2518">hasta</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="2520" end_char="2524">ahora</TOKEN>
<TOKEN id="token-15-10" pos="punct" morph="none" start_char="2525" end_char="2525">,</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="2527" end_char="2530">tras</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="2532" end_char="2533">un</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="2535" end_char="2540">primer</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="2542" end_char="2546">árbol</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="2548" end_char="2558">genealógico</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="2560" end_char="2561">de</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="2563" end_char="2573">coronavirus</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="2575" end_char="2577">que</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="2579" end_char="2580">en</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="2582" end_char="2583">la</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="2585" end_char="2591">revista</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="2593" end_char="2598">Lancet</TOKEN>
<TOKEN id="token-15-23" pos="word" morph="none" start_char="2600" end_char="2606">comparó</TOKEN>
<TOKEN id="token-15-24" pos="word" morph="none" start_char="2608" end_char="2609">10</TOKEN>
<TOKEN id="token-15-25" pos="word" morph="none" start_char="2611" end_char="2617">genomas</TOKEN>
<TOKEN id="token-15-26" pos="punct" morph="none" start_char="2618" end_char="2618">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="2621" end_char="2754">
<ORIGINAL_TEXT>En el trabajo ya han aumentado a 56 y desde el domingo 2 de febrero hasta la fecha, las secuencias publicadas se han convertido en 74.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="2621" end_char="2622">En</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="2624" end_char="2625">el</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="2627" end_char="2633">trabajo</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="2635" end_char="2636">ya</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="2638" end_char="2640">han</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="2642" end_char="2650">aumentado</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="2652" end_char="2652">a</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="2654" end_char="2655">56</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="2657" end_char="2657">y</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="2659" end_char="2663">desde</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="2665" end_char="2666">el</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="2668" end_char="2674">domingo</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="2676" end_char="2676">2</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2678" end_char="2679">de</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="2681" end_char="2687">febrero</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="2689" end_char="2693">hasta</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="2695" end_char="2696">la</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="2698" end_char="2702">fecha</TOKEN>
<TOKEN id="token-16-18" pos="punct" morph="none" start_char="2703" end_char="2703">,</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="2705" end_char="2707">las</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="2709" end_char="2718">secuencias</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="2720" end_char="2729">publicadas</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="2731" end_char="2732">se</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="2734" end_char="2736">han</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="2738" end_char="2747">convertido</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="2749" end_char="2750">en</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="2752" end_char="2753">74</TOKEN>
<TOKEN id="token-16-27" pos="punct" morph="none" start_char="2754" end_char="2754">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2756" end_char="2873">
<ORIGINAL_TEXT>También están los del coronavirus aislados de dos turistas chinos hospitalizados en Roma, en el Instituto Spallanzani.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2756" end_char="2762">También</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2764" end_char="2768">están</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2770" end_char="2772">los</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2774" end_char="2776">del</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2778" end_char="2788">coronavirus</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2790" end_char="2797">aislados</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2799" end_char="2800">de</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="2802" end_char="2804">dos</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="2806" end_char="2813">turistas</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="2815" end_char="2820">chinos</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="2822" end_char="2835">hospitalizados</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="2837" end_char="2838">en</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="2840" end_char="2843">Roma</TOKEN>
<TOKEN id="token-17-13" pos="punct" morph="none" start_char="2844" end_char="2844">,</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="2846" end_char="2847">en</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="2849" end_char="2850">el</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="2852" end_char="2860">Instituto</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="2862" end_char="2872">Spallanzani</TOKEN>
<TOKEN id="token-17-18" pos="punct" morph="none" start_char="2873" end_char="2873">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2876" end_char="2946">
<ORIGINAL_TEXT>Ninguna secuencia, en cambio, hasta ahora llegó de Africa y Sudamérica.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2876" end_char="2882">Ninguna</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2884" end_char="2892">secuencia</TOKEN>
<TOKEN id="token-18-2" pos="punct" morph="none" start_char="2893" end_char="2893">,</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="2895" end_char="2896">en</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2898" end_char="2903">cambio</TOKEN>
<TOKEN id="token-18-5" pos="punct" morph="none" start_char="2904" end_char="2904">,</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2906" end_char="2910">hasta</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2912" end_char="2916">ahora</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2918" end_char="2922">llegó</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2924" end_char="2925">de</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2927" end_char="2932">Africa</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="2934" end_char="2934">y</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="2936" end_char="2945">Sudamérica</TOKEN>
<TOKEN id="token-18-13" pos="punct" morph="none" start_char="2946" end_char="2946">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2949" end_char="3015">
<ORIGINAL_TEXT>No hay idea de como el animal podría haberlo transmitido al hombre.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2949" end_char="2950">No</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2952" end_char="2954">hay</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2956" end_char="2959">idea</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="2961" end_char="2962">de</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2964" end_char="2967">como</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="2969" end_char="2970">el</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2972" end_char="2977">animal</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="2979" end_char="2984">podría</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="2986" end_char="2992">haberlo</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="2994" end_char="3004">transmitido</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="3006" end_char="3007">al</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="3009" end_char="3014">hombre</TOKEN>
<TOKEN id="token-19-12" pos="punct" morph="none" start_char="3015" end_char="3015">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="3017" end_char="3082">
<ORIGINAL_TEXT>Acaso, por intermedio de otro animal, o por una mordedura directa.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="3017" end_char="3021">Acaso</TOKEN>
<TOKEN id="token-20-1" pos="punct" morph="none" start_char="3022" end_char="3022">,</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="3024" end_char="3026">por</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="3028" end_char="3037">intermedio</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="3039" end_char="3040">de</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="3042" end_char="3045">otro</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="3047" end_char="3052">animal</TOKEN>
<TOKEN id="token-20-7" pos="punct" morph="none" start_char="3053" end_char="3053">,</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="3055" end_char="3055">o</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="3057" end_char="3059">por</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="3061" end_char="3063">una</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="3065" end_char="3073">mordedura</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="3075" end_char="3081">directa</TOKEN>
<TOKEN id="token-20-13" pos="punct" morph="none" start_char="3082" end_char="3082">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="3085" end_char="3168">
<ORIGINAL_TEXT>Según Giorgi, por el momento "no hay evidencia de un posible invitado en la cadena".</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="3085" end_char="3089">Según</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="3091" end_char="3096">Giorgi</TOKEN>
<TOKEN id="token-21-2" pos="punct" morph="none" start_char="3097" end_char="3097">,</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="3099" end_char="3101">por</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="3103" end_char="3104">el</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="3106" end_char="3112">momento</TOKEN>
<TOKEN id="token-21-6" pos="punct" morph="none" start_char="3114" end_char="3114">"</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="3115" end_char="3116">no</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="3118" end_char="3120">hay</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="3122" end_char="3130">evidencia</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="3132" end_char="3133">de</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="3135" end_char="3136">un</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="3138" end_char="3144">posible</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="3146" end_char="3153">invitado</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="3155" end_char="3156">en</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="3158" end_char="3159">la</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="3161" end_char="3166">cadena</TOKEN>
<TOKEN id="token-21-17" pos="punct" morph="none" start_char="3167" end_char="3168">".</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
