<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CAB5" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="16112" raw_text_md5="8ea504c2dc34cd3a1472631173bb92e9">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="75">
<ORIGINAL_TEXT>Satellite data suggests coronavirus may have hit China earlier: Researchers</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="9">Satellite</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="11" end_char="14">data</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="16" end_char="23">suggests</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="25" end_char="35">coronavirus</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="37" end_char="39">may</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="41" end_char="44">have</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="46" end_char="48">hit</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="50" end_char="54">China</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="56" end_char="62">earlier</TOKEN>
<TOKEN id="token-0-9" pos="punct" morph="none" start_char="63" end_char="63">:</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="65" end_char="75">Researchers</TOKEN>
</SEG>
<SEG id="segment-1" start_char="80" end_char="347">
<ORIGINAL_TEXT>Dramatic spikes in auto traffic around major hospitals in Wuhan last fall suggest the novel coronavirus may have been present and spreading through central China long before the outbreak was first reported to the world, according to a new Harvard Medical School study.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="80" end_char="87">Dramatic</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="89" end_char="94">spikes</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="96" end_char="97">in</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="99" end_char="102">auto</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="104" end_char="110">traffic</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="112" end_char="117">around</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="119" end_char="123">major</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="125" end_char="133">hospitals</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="135" end_char="136">in</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="138" end_char="142">Wuhan</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="144" end_char="147">last</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="149" end_char="152">fall</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="154" end_char="160">suggest</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="162" end_char="164">the</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="166" end_char="170">novel</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="172" end_char="182">coronavirus</TOKEN>
<TOKEN id="token-1-16" pos="word" morph="none" start_char="184" end_char="186">may</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="188" end_char="191">have</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="193" end_char="196">been</TOKEN>
<TOKEN id="token-1-19" pos="word" morph="none" start_char="198" end_char="204">present</TOKEN>
<TOKEN id="token-1-20" pos="word" morph="none" start_char="206" end_char="208">and</TOKEN>
<TOKEN id="token-1-21" pos="word" morph="none" start_char="210" end_char="218">spreading</TOKEN>
<TOKEN id="token-1-22" pos="word" morph="none" start_char="220" end_char="226">through</TOKEN>
<TOKEN id="token-1-23" pos="word" morph="none" start_char="228" end_char="234">central</TOKEN>
<TOKEN id="token-1-24" pos="word" morph="none" start_char="236" end_char="240">China</TOKEN>
<TOKEN id="token-1-25" pos="word" morph="none" start_char="242" end_char="245">long</TOKEN>
<TOKEN id="token-1-26" pos="word" morph="none" start_char="247" end_char="252">before</TOKEN>
<TOKEN id="token-1-27" pos="word" morph="none" start_char="254" end_char="256">the</TOKEN>
<TOKEN id="token-1-28" pos="word" morph="none" start_char="258" end_char="265">outbreak</TOKEN>
<TOKEN id="token-1-29" pos="word" morph="none" start_char="267" end_char="269">was</TOKEN>
<TOKEN id="token-1-30" pos="word" morph="none" start_char="271" end_char="275">first</TOKEN>
<TOKEN id="token-1-31" pos="word" morph="none" start_char="277" end_char="284">reported</TOKEN>
<TOKEN id="token-1-32" pos="word" morph="none" start_char="286" end_char="287">to</TOKEN>
<TOKEN id="token-1-33" pos="word" morph="none" start_char="289" end_char="291">the</TOKEN>
<TOKEN id="token-1-34" pos="word" morph="none" start_char="293" end_char="297">world</TOKEN>
<TOKEN id="token-1-35" pos="punct" morph="none" start_char="298" end_char="298">,</TOKEN>
<TOKEN id="token-1-36" pos="word" morph="none" start_char="300" end_char="308">according</TOKEN>
<TOKEN id="token-1-37" pos="word" morph="none" start_char="310" end_char="311">to</TOKEN>
<TOKEN id="token-1-38" pos="word" morph="none" start_char="313" end_char="313">a</TOKEN>
<TOKEN id="token-1-39" pos="word" morph="none" start_char="315" end_char="317">new</TOKEN>
<TOKEN id="token-1-40" pos="word" morph="none" start_char="319" end_char="325">Harvard</TOKEN>
<TOKEN id="token-1-41" pos="word" morph="none" start_char="327" end_char="333">Medical</TOKEN>
<TOKEN id="token-1-42" pos="word" morph="none" start_char="335" end_char="340">School</TOKEN>
<TOKEN id="token-1-43" pos="word" morph="none" start_char="342" end_char="346">study</TOKEN>
<TOKEN id="token-1-44" pos="punct" morph="none" start_char="347" end_char="347">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="350" end_char="640">
<ORIGINAL_TEXT>Using techniques similar to those employed by intelligence agencies, the research team behind the study analyzed commercial satellite imagery and "observed a dramatic increase in hospital traffic outside five major Wuhan hospitals beginning late summer and early fall 2019," according to Dr.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="350" end_char="354">Using</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="356" end_char="365">techniques</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="367" end_char="373">similar</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="375" end_char="376">to</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="378" end_char="382">those</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="384" end_char="391">employed</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="393" end_char="394">by</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="396" end_char="407">intelligence</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="409" end_char="416">agencies</TOKEN>
<TOKEN id="token-2-9" pos="punct" morph="none" start_char="417" end_char="417">,</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="419" end_char="421">the</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="423" end_char="430">research</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="432" end_char="435">team</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="437" end_char="442">behind</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="444" end_char="446">the</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="448" end_char="452">study</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="454" end_char="461">analyzed</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="463" end_char="472">commercial</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="474" end_char="482">satellite</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="484" end_char="490">imagery</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="492" end_char="494">and</TOKEN>
<TOKEN id="token-2-21" pos="punct" morph="none" start_char="496" end_char="496">"</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="497" end_char="504">observed</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="506" end_char="506">a</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="508" end_char="515">dramatic</TOKEN>
<TOKEN id="token-2-25" pos="word" morph="none" start_char="517" end_char="524">increase</TOKEN>
<TOKEN id="token-2-26" pos="word" morph="none" start_char="526" end_char="527">in</TOKEN>
<TOKEN id="token-2-27" pos="word" morph="none" start_char="529" end_char="536">hospital</TOKEN>
<TOKEN id="token-2-28" pos="word" morph="none" start_char="538" end_char="544">traffic</TOKEN>
<TOKEN id="token-2-29" pos="word" morph="none" start_char="546" end_char="552">outside</TOKEN>
<TOKEN id="token-2-30" pos="word" morph="none" start_char="554" end_char="557">five</TOKEN>
<TOKEN id="token-2-31" pos="word" morph="none" start_char="559" end_char="563">major</TOKEN>
<TOKEN id="token-2-32" pos="word" morph="none" start_char="565" end_char="569">Wuhan</TOKEN>
<TOKEN id="token-2-33" pos="word" morph="none" start_char="571" end_char="579">hospitals</TOKEN>
<TOKEN id="token-2-34" pos="word" morph="none" start_char="581" end_char="589">beginning</TOKEN>
<TOKEN id="token-2-35" pos="word" morph="none" start_char="591" end_char="594">late</TOKEN>
<TOKEN id="token-2-36" pos="word" morph="none" start_char="596" end_char="601">summer</TOKEN>
<TOKEN id="token-2-37" pos="word" morph="none" start_char="603" end_char="605">and</TOKEN>
<TOKEN id="token-2-38" pos="word" morph="none" start_char="607" end_char="611">early</TOKEN>
<TOKEN id="token-2-39" pos="word" morph="none" start_char="613" end_char="616">fall</TOKEN>
<TOKEN id="token-2-40" pos="word" morph="none" start_char="618" end_char="621">2019</TOKEN>
<TOKEN id="token-2-41" pos="punct" morph="none" start_char="622" end_char="623">,"</TOKEN>
<TOKEN id="token-2-42" pos="word" morph="none" start_char="625" end_char="633">according</TOKEN>
<TOKEN id="token-2-43" pos="word" morph="none" start_char="635" end_char="636">to</TOKEN>
<TOKEN id="token-2-44" pos="word" morph="none" start_char="638" end_char="639">Dr</TOKEN>
<TOKEN id="token-2-45" pos="punct" morph="none" start_char="640" end_char="640">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="642" end_char="709">
<ORIGINAL_TEXT>John Brownstein, the Harvard Medical professor who led the research.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="642" end_char="645">John</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="647" end_char="656">Brownstein</TOKEN>
<TOKEN id="token-3-2" pos="punct" morph="none" start_char="657" end_char="657">,</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="659" end_char="661">the</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="663" end_char="669">Harvard</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="671" end_char="677">Medical</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="679" end_char="687">professor</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="689" end_char="691">who</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="693" end_char="695">led</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="697" end_char="699">the</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="701" end_char="708">research</TOKEN>
<TOKEN id="token-3-11" pos="punct" morph="none" start_char="709" end_char="709">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="712" end_char="945">
<ORIGINAL_TEXT>Brownstein, an ABC News contributor, said the traffic increase also "coincided with" elevated queries on a Chinese internet search for "certain symptoms that would later be determined as closely associated with the novel coronavirus."</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="712" end_char="721">Brownstein</TOKEN>
<TOKEN id="token-4-1" pos="punct" morph="none" start_char="722" end_char="722">,</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="724" end_char="725">an</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="727" end_char="729">ABC</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="731" end_char="734">News</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="736" end_char="746">contributor</TOKEN>
<TOKEN id="token-4-6" pos="punct" morph="none" start_char="747" end_char="747">,</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="749" end_char="752">said</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="754" end_char="756">the</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="758" end_char="764">traffic</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="766" end_char="773">increase</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="775" end_char="778">also</TOKEN>
<TOKEN id="token-4-12" pos="punct" morph="none" start_char="780" end_char="780">"</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="781" end_char="789">coincided</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="791" end_char="794">with</TOKEN>
<TOKEN id="token-4-15" pos="punct" morph="none" start_char="795" end_char="795">"</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="797" end_char="804">elevated</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="806" end_char="812">queries</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="814" end_char="815">on</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="817" end_char="817">a</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="819" end_char="825">Chinese</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="827" end_char="834">internet</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="836" end_char="841">search</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="843" end_char="845">for</TOKEN>
<TOKEN id="token-4-24" pos="punct" morph="none" start_char="847" end_char="847">"</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="848" end_char="854">certain</TOKEN>
<TOKEN id="token-4-26" pos="word" morph="none" start_char="856" end_char="863">symptoms</TOKEN>
<TOKEN id="token-4-27" pos="word" morph="none" start_char="865" end_char="868">that</TOKEN>
<TOKEN id="token-4-28" pos="word" morph="none" start_char="870" end_char="874">would</TOKEN>
<TOKEN id="token-4-29" pos="word" morph="none" start_char="876" end_char="880">later</TOKEN>
<TOKEN id="token-4-30" pos="word" morph="none" start_char="882" end_char="883">be</TOKEN>
<TOKEN id="token-4-31" pos="word" morph="none" start_char="885" end_char="894">determined</TOKEN>
<TOKEN id="token-4-32" pos="word" morph="none" start_char="896" end_char="897">as</TOKEN>
<TOKEN id="token-4-33" pos="word" morph="none" start_char="899" end_char="905">closely</TOKEN>
<TOKEN id="token-4-34" pos="word" morph="none" start_char="907" end_char="916">associated</TOKEN>
<TOKEN id="token-4-35" pos="word" morph="none" start_char="918" end_char="921">with</TOKEN>
<TOKEN id="token-4-36" pos="word" morph="none" start_char="923" end_char="925">the</TOKEN>
<TOKEN id="token-4-37" pos="word" morph="none" start_char="927" end_char="931">novel</TOKEN>
<TOKEN id="token-4-38" pos="word" morph="none" start_char="933" end_char="943">coronavirus</TOKEN>
<TOKEN id="token-4-39" pos="punct" morph="none" start_char="944" end_char="945">."</TOKEN>
</SEG>
<SEG id="segment-5" start_char="948" end_char="1103">
<ORIGINAL_TEXT>Though Brownstein acknowledged the evidence is circumstantial, he said the study makes for an important new data point in the mystery of COVID-19's origins.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="948" end_char="953">Though</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="955" end_char="964">Brownstein</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="966" end_char="977">acknowledged</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="979" end_char="981">the</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="983" end_char="990">evidence</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="992" end_char="993">is</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="995" end_char="1008">circumstantial</TOKEN>
<TOKEN id="token-5-7" pos="punct" morph="none" start_char="1009" end_char="1009">,</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="1011" end_char="1012">he</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="1014" end_char="1017">said</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="1019" end_char="1021">the</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="1023" end_char="1027">study</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="1029" end_char="1033">makes</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="1035" end_char="1037">for</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="1039" end_char="1040">an</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="1042" end_char="1050">important</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="1052" end_char="1054">new</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="1056" end_char="1059">data</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="1061" end_char="1065">point</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="1067" end_char="1068">in</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="1070" end_char="1072">the</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="1074" end_char="1080">mystery</TOKEN>
<TOKEN id="token-5-22" pos="word" morph="none" start_char="1082" end_char="1083">of</TOKEN>
<TOKEN id="token-5-23" pos="unknown" morph="none" start_char="1085" end_char="1094">COVID-19's</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="1096" end_char="1102">origins</TOKEN>
<TOKEN id="token-5-25" pos="punct" morph="none" start_char="1103" end_char="1103">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="1106" end_char="1287">
<ORIGINAL_TEXT>"Something was happening in October," said Brownstein, the chief innovation officer at Boston Children’s Hospital and director of the medical center’s Computational Epidemiology Lab.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="punct" morph="none" start_char="1106" end_char="1106">"</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="1107" end_char="1115">Something</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="1117" end_char="1119">was</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="1121" end_char="1129">happening</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="1131" end_char="1132">in</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="1134" end_char="1140">October</TOKEN>
<TOKEN id="token-6-6" pos="punct" morph="none" start_char="1141" end_char="1142">,"</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="1144" end_char="1147">said</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="1149" end_char="1158">Brownstein</TOKEN>
<TOKEN id="token-6-9" pos="punct" morph="none" start_char="1159" end_char="1159">,</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="1161" end_char="1163">the</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="1165" end_char="1169">chief</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="1171" end_char="1180">innovation</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="1182" end_char="1188">officer</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="1190" end_char="1191">at</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="1193" end_char="1198">Boston</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="1200" end_char="1209">Children’s</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="1211" end_char="1218">Hospital</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="1220" end_char="1222">and</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="1224" end_char="1231">director</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="1233" end_char="1234">of</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="1236" end_char="1238">the</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="1240" end_char="1246">medical</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="1248" end_char="1255">center’s</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="1257" end_char="1269">Computational</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="1271" end_char="1282">Epidemiology</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="1284" end_char="1286">Lab</TOKEN>
<TOKEN id="token-6-27" pos="punct" morph="none" start_char="1287" end_char="1287">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="1289" end_char="1444">
<ORIGINAL_TEXT>"Clearly, there was some level of social disruption taking place well before what was previously identified as the start of the novel coronavirus pandemic."</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="punct" morph="none" start_char="1289" end_char="1289">"</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="1290" end_char="1296">Clearly</TOKEN>
<TOKEN id="token-7-2" pos="punct" morph="none" start_char="1297" end_char="1297">,</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="1299" end_char="1303">there</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="1305" end_char="1307">was</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="1309" end_char="1312">some</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="1314" end_char="1318">level</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="1320" end_char="1321">of</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="1323" end_char="1328">social</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="1330" end_char="1339">disruption</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="1341" end_char="1346">taking</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="1348" end_char="1352">place</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="1354" end_char="1357">well</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="1359" end_char="1364">before</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="1366" end_char="1369">what</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="1371" end_char="1373">was</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="1375" end_char="1384">previously</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="1386" end_char="1395">identified</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="1397" end_char="1398">as</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="1400" end_char="1402">the</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="1404" end_char="1408">start</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="1410" end_char="1411">of</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="1413" end_char="1415">the</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="1417" end_char="1421">novel</TOKEN>
<TOKEN id="token-7-24" pos="word" morph="none" start_char="1423" end_char="1433">coronavirus</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="1435" end_char="1442">pandemic</TOKEN>
<TOKEN id="token-7-26" pos="punct" morph="none" start_char="1443" end_char="1444">."</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1447" end_char="1645">
<ORIGINAL_TEXT>Since the outbreak in China last year, the coronavirus has swept across the globe infecting nearly 7 million and killing more than 400,000 worldwide, according to a count by Johns Hopkins University.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="1447" end_char="1451">Since</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="1453" end_char="1455">the</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="1457" end_char="1464">outbreak</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1466" end_char="1467">in</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1469" end_char="1473">China</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1475" end_char="1478">last</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="1480" end_char="1483">year</TOKEN>
<TOKEN id="token-8-7" pos="punct" morph="none" start_char="1484" end_char="1484">,</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1486" end_char="1488">the</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="1490" end_char="1500">coronavirus</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="1502" end_char="1504">has</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="1506" end_char="1510">swept</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="1512" end_char="1517">across</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="1519" end_char="1521">the</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="1523" end_char="1527">globe</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="1529" end_char="1537">infecting</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="1539" end_char="1544">nearly</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="1546" end_char="1546">7</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="1548" end_char="1554">million</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="1556" end_char="1558">and</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="1560" end_char="1566">killing</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="1568" end_char="1571">more</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="1573" end_char="1576">than</TOKEN>
<TOKEN id="token-8-23" pos="unknown" morph="none" start_char="1578" end_char="1584">400,000</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="1586" end_char="1594">worldwide</TOKEN>
<TOKEN id="token-8-25" pos="punct" morph="none" start_char="1595" end_char="1595">,</TOKEN>
<TOKEN id="token-8-26" pos="word" morph="none" start_char="1597" end_char="1605">according</TOKEN>
<TOKEN id="token-8-27" pos="word" morph="none" start_char="1607" end_char="1608">to</TOKEN>
<TOKEN id="token-8-28" pos="word" morph="none" start_char="1610" end_char="1610">a</TOKEN>
<TOKEN id="token-8-29" pos="word" morph="none" start_char="1612" end_char="1616">count</TOKEN>
<TOKEN id="token-8-30" pos="word" morph="none" start_char="1618" end_char="1619">by</TOKEN>
<TOKEN id="token-8-31" pos="word" morph="none" start_char="1621" end_char="1625">Johns</TOKEN>
<TOKEN id="token-8-32" pos="word" morph="none" start_char="1627" end_char="1633">Hopkins</TOKEN>
<TOKEN id="token-8-33" pos="word" morph="none" start_char="1635" end_char="1644">University</TOKEN>
<TOKEN id="token-8-34" pos="punct" morph="none" start_char="1645" end_char="1645">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1647" end_char="1839">
<ORIGINAL_TEXT>It is believed that the virus jumped from animal species, where it had little effect, to humans, where it has become the most potent natural killer since the Spanish flu pandemic a century ago.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1647" end_char="1648">It</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1650" end_char="1651">is</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="1653" end_char="1660">believed</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1662" end_char="1665">that</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1667" end_char="1669">the</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1671" end_char="1675">virus</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1677" end_char="1682">jumped</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1684" end_char="1687">from</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1689" end_char="1694">animal</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="1696" end_char="1702">species</TOKEN>
<TOKEN id="token-9-10" pos="punct" morph="none" start_char="1703" end_char="1703">,</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1705" end_char="1709">where</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="1711" end_char="1712">it</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1714" end_char="1716">had</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1718" end_char="1723">little</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1725" end_char="1730">effect</TOKEN>
<TOKEN id="token-9-16" pos="punct" morph="none" start_char="1731" end_char="1731">,</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1733" end_char="1734">to</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1736" end_char="1741">humans</TOKEN>
<TOKEN id="token-9-19" pos="punct" morph="none" start_char="1742" end_char="1742">,</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1744" end_char="1748">where</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1750" end_char="1751">it</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1753" end_char="1755">has</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="1757" end_char="1762">become</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1764" end_char="1766">the</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="1768" end_char="1771">most</TOKEN>
<TOKEN id="token-9-26" pos="word" morph="none" start_char="1773" end_char="1778">potent</TOKEN>
<TOKEN id="token-9-27" pos="word" morph="none" start_char="1780" end_char="1786">natural</TOKEN>
<TOKEN id="token-9-28" pos="word" morph="none" start_char="1788" end_char="1793">killer</TOKEN>
<TOKEN id="token-9-29" pos="word" morph="none" start_char="1795" end_char="1799">since</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1801" end_char="1803">the</TOKEN>
<TOKEN id="token-9-31" pos="word" morph="none" start_char="1805" end_char="1811">Spanish</TOKEN>
<TOKEN id="token-9-32" pos="word" morph="none" start_char="1813" end_char="1815">flu</TOKEN>
<TOKEN id="token-9-33" pos="word" morph="none" start_char="1817" end_char="1824">pandemic</TOKEN>
<TOKEN id="token-9-34" pos="word" morph="none" start_char="1826" end_char="1826">a</TOKEN>
<TOKEN id="token-9-35" pos="word" morph="none" start_char="1828" end_char="1834">century</TOKEN>
<TOKEN id="token-9-36" pos="word" morph="none" start_char="1836" end_char="1838">ago</TOKEN>
<TOKEN id="token-9-37" pos="punct" morph="none" start_char="1839" end_char="1839">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1842" end_char="1932">
<ORIGINAL_TEXT>Though Chinese officials would not formally notify the World Health Organization until Dec.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1842" end_char="1847">Though</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1849" end_char="1855">Chinese</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1857" end_char="1865">officials</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1867" end_char="1871">would</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1873" end_char="1875">not</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1877" end_char="1884">formally</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1886" end_char="1891">notify</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1893" end_char="1895">the</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1897" end_char="1901">World</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1903" end_char="1908">Health</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1910" end_char="1921">Organization</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1923" end_char="1927">until</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1929" end_char="1931">Dec</TOKEN>
<TOKEN id="token-10-13" pos="punct" morph="none" start_char="1932" end_char="1932">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1934" end_char="2158">
<ORIGINAL_TEXT>31 that a new respiratory pathogen was coursing through Wuhan, U.S. intelligence caught wind of a problem as early as late November and notified the Pentagon, according to four sources briefed on the confidential information.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1934" end_char="1935">31</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1937" end_char="1940">that</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1942" end_char="1942">a</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1944" end_char="1946">new</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1948" end_char="1958">respiratory</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1960" end_char="1967">pathogen</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1969" end_char="1971">was</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1973" end_char="1980">coursing</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1982" end_char="1988">through</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1990" end_char="1994">Wuhan</TOKEN>
<TOKEN id="token-11-10" pos="punct" morph="none" start_char="1995" end_char="1995">,</TOKEN>
<TOKEN id="token-11-11" pos="unknown" morph="none" start_char="1997" end_char="1999">U.S</TOKEN>
<TOKEN id="token-11-12" pos="punct" morph="none" start_char="2000" end_char="2000">.</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="2002" end_char="2013">intelligence</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="2015" end_char="2020">caught</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="2022" end_char="2025">wind</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="2027" end_char="2028">of</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="2030" end_char="2030">a</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="2032" end_char="2038">problem</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="2040" end_char="2041">as</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="2043" end_char="2047">early</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="2049" end_char="2050">as</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="2052" end_char="2055">late</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="2057" end_char="2064">November</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="2066" end_char="2068">and</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="2070" end_char="2077">notified</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="2079" end_char="2081">the</TOKEN>
<TOKEN id="token-11-27" pos="word" morph="none" start_char="2083" end_char="2090">Pentagon</TOKEN>
<TOKEN id="token-11-28" pos="punct" morph="none" start_char="2091" end_char="2091">,</TOKEN>
<TOKEN id="token-11-29" pos="word" morph="none" start_char="2093" end_char="2101">according</TOKEN>
<TOKEN id="token-11-30" pos="word" morph="none" start_char="2103" end_char="2104">to</TOKEN>
<TOKEN id="token-11-31" pos="word" morph="none" start_char="2106" end_char="2109">four</TOKEN>
<TOKEN id="token-11-32" pos="word" morph="none" start_char="2111" end_char="2117">sources</TOKEN>
<TOKEN id="token-11-33" pos="word" morph="none" start_char="2119" end_char="2125">briefed</TOKEN>
<TOKEN id="token-11-34" pos="word" morph="none" start_char="2127" end_char="2128">on</TOKEN>
<TOKEN id="token-11-35" pos="word" morph="none" start_char="2130" end_char="2132">the</TOKEN>
<TOKEN id="token-11-36" pos="word" morph="none" start_char="2134" end_char="2145">confidential</TOKEN>
<TOKEN id="token-11-37" pos="word" morph="none" start_char="2147" end_char="2157">information</TOKEN>
<TOKEN id="token-11-38" pos="punct" morph="none" start_char="2158" end_char="2158">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="2161" end_char="2380">
<ORIGINAL_TEXT>Because the origin of a novel virus is so hard to pin down but so critically important for scientists to understand, experts around the world are racing to uncover the secrets of the pathogen formally known as SARS-CoV2.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="2161" end_char="2167">Because</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="2169" end_char="2171">the</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="2173" end_char="2178">origin</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="2180" end_char="2181">of</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="2183" end_char="2183">a</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="2185" end_char="2189">novel</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="2191" end_char="2195">virus</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="2197" end_char="2198">is</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="2200" end_char="2201">so</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="2203" end_char="2206">hard</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="2208" end_char="2209">to</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="2211" end_char="2213">pin</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="2215" end_char="2218">down</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="2220" end_char="2222">but</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="2224" end_char="2225">so</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="2227" end_char="2236">critically</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="2238" end_char="2246">important</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="2248" end_char="2250">for</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="2252" end_char="2261">scientists</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="2263" end_char="2264">to</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="2266" end_char="2275">understand</TOKEN>
<TOKEN id="token-12-21" pos="punct" morph="none" start_char="2276" end_char="2276">,</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="2278" end_char="2284">experts</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="2286" end_char="2291">around</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="2293" end_char="2295">the</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="2297" end_char="2301">world</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="2303" end_char="2305">are</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="2307" end_char="2312">racing</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="2314" end_char="2315">to</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="2317" end_char="2323">uncover</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="2325" end_char="2327">the</TOKEN>
<TOKEN id="token-12-31" pos="word" morph="none" start_char="2329" end_char="2335">secrets</TOKEN>
<TOKEN id="token-12-32" pos="word" morph="none" start_char="2337" end_char="2338">of</TOKEN>
<TOKEN id="token-12-33" pos="word" morph="none" start_char="2340" end_char="2342">the</TOKEN>
<TOKEN id="token-12-34" pos="word" morph="none" start_char="2344" end_char="2351">pathogen</TOKEN>
<TOKEN id="token-12-35" pos="word" morph="none" start_char="2353" end_char="2360">formally</TOKEN>
<TOKEN id="token-12-36" pos="word" morph="none" start_char="2362" end_char="2366">known</TOKEN>
<TOKEN id="token-12-37" pos="word" morph="none" start_char="2368" end_char="2369">as</TOKEN>
<TOKEN id="token-12-38" pos="unknown" morph="none" start_char="2371" end_char="2379">SARS-CoV2</TOKEN>
<TOKEN id="token-12-39" pos="punct" morph="none" start_char="2380" end_char="2380">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="2382" end_char="2578">
<ORIGINAL_TEXT>The task for researchers is made far more complicated by the Chinese government’s refusal to fully cooperate with Western and international health authorities, American and WHO officials have said.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="2382" end_char="2384">The</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="2386" end_char="2389">task</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="2391" end_char="2393">for</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="2395" end_char="2405">researchers</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="2407" end_char="2408">is</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="2410" end_char="2413">made</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="2415" end_char="2417">far</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="2419" end_char="2422">more</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="2424" end_char="2434">complicated</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="2436" end_char="2437">by</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="2439" end_char="2441">the</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="2443" end_char="2449">Chinese</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="2451" end_char="2462">government’s</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="2464" end_char="2470">refusal</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="2472" end_char="2473">to</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="2475" end_char="2479">fully</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="2481" end_char="2489">cooperate</TOKEN>
<TOKEN id="token-13-17" pos="word" morph="none" start_char="2491" end_char="2494">with</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="2496" end_char="2502">Western</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="2504" end_char="2506">and</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="2508" end_char="2520">international</TOKEN>
<TOKEN id="token-13-21" pos="word" morph="none" start_char="2522" end_char="2527">health</TOKEN>
<TOKEN id="token-13-22" pos="word" morph="none" start_char="2529" end_char="2539">authorities</TOKEN>
<TOKEN id="token-13-23" pos="punct" morph="none" start_char="2540" end_char="2540">,</TOKEN>
<TOKEN id="token-13-24" pos="word" morph="none" start_char="2542" end_char="2549">American</TOKEN>
<TOKEN id="token-13-25" pos="word" morph="none" start_char="2551" end_char="2553">and</TOKEN>
<TOKEN id="token-13-26" pos="word" morph="none" start_char="2555" end_char="2557">WHO</TOKEN>
<TOKEN id="token-13-27" pos="word" morph="none" start_char="2559" end_char="2567">officials</TOKEN>
<TOKEN id="token-13-28" pos="word" morph="none" start_char="2569" end_char="2572">have</TOKEN>
<TOKEN id="token-13-29" pos="word" morph="none" start_char="2574" end_char="2577">said</TOKEN>
<TOKEN id="token-13-30" pos="punct" morph="none" start_char="2578" end_char="2578">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="2581" end_char="2825">
<ORIGINAL_TEXT>Brownstein and his team, which included researchers from Boston University and Boston Children’s Hospital, have spent more than a month trying to pin down the signs for when the population of Hubei province in China first started to be stricken.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="2581" end_char="2590">Brownstein</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="2592" end_char="2594">and</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="2596" end_char="2598">his</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="2600" end_char="2603">team</TOKEN>
<TOKEN id="token-14-4" pos="punct" morph="none" start_char="2604" end_char="2604">,</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="2606" end_char="2610">which</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="2612" end_char="2619">included</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="2621" end_char="2631">researchers</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="2633" end_char="2636">from</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="2638" end_char="2643">Boston</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="2645" end_char="2654">University</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="2656" end_char="2658">and</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="2660" end_char="2665">Boston</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="2667" end_char="2676">Children’s</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="2678" end_char="2685">Hospital</TOKEN>
<TOKEN id="token-14-15" pos="punct" morph="none" start_char="2686" end_char="2686">,</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="2688" end_char="2691">have</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="2693" end_char="2697">spent</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="2699" end_char="2702">more</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="2704" end_char="2707">than</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="2709" end_char="2709">a</TOKEN>
<TOKEN id="token-14-21" pos="word" morph="none" start_char="2711" end_char="2715">month</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="2717" end_char="2722">trying</TOKEN>
<TOKEN id="token-14-23" pos="word" morph="none" start_char="2724" end_char="2725">to</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="2727" end_char="2729">pin</TOKEN>
<TOKEN id="token-14-25" pos="word" morph="none" start_char="2731" end_char="2734">down</TOKEN>
<TOKEN id="token-14-26" pos="word" morph="none" start_char="2736" end_char="2738">the</TOKEN>
<TOKEN id="token-14-27" pos="word" morph="none" start_char="2740" end_char="2744">signs</TOKEN>
<TOKEN id="token-14-28" pos="word" morph="none" start_char="2746" end_char="2748">for</TOKEN>
<TOKEN id="token-14-29" pos="word" morph="none" start_char="2750" end_char="2753">when</TOKEN>
<TOKEN id="token-14-30" pos="word" morph="none" start_char="2755" end_char="2757">the</TOKEN>
<TOKEN id="token-14-31" pos="word" morph="none" start_char="2759" end_char="2768">population</TOKEN>
<TOKEN id="token-14-32" pos="word" morph="none" start_char="2770" end_char="2771">of</TOKEN>
<TOKEN id="token-14-33" pos="word" morph="none" start_char="2773" end_char="2777">Hubei</TOKEN>
<TOKEN id="token-14-34" pos="word" morph="none" start_char="2779" end_char="2786">province</TOKEN>
<TOKEN id="token-14-35" pos="word" morph="none" start_char="2788" end_char="2789">in</TOKEN>
<TOKEN id="token-14-36" pos="word" morph="none" start_char="2791" end_char="2795">China</TOKEN>
<TOKEN id="token-14-37" pos="word" morph="none" start_char="2797" end_char="2801">first</TOKEN>
<TOKEN id="token-14-38" pos="word" morph="none" start_char="2803" end_char="2809">started</TOKEN>
<TOKEN id="token-14-39" pos="word" morph="none" start_char="2811" end_char="2812">to</TOKEN>
<TOKEN id="token-14-40" pos="word" morph="none" start_char="2814" end_char="2815">be</TOKEN>
<TOKEN id="token-14-41" pos="word" morph="none" start_char="2817" end_char="2824">stricken</TOKEN>
<TOKEN id="token-14-42" pos="punct" morph="none" start_char="2825" end_char="2825">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="2828" end_char="2991">
<ORIGINAL_TEXT>The logic of Brownstein’s research project was straightforward: respiratory diseases lead to very specific types of behavior in communities where they’re spreading.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="2828" end_char="2830">The</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="2832" end_char="2836">logic</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="2838" end_char="2839">of</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="2841" end_char="2852">Brownstein’s</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="2854" end_char="2861">research</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="2863" end_char="2869">project</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="2871" end_char="2873">was</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="2875" end_char="2889">straightforward</TOKEN>
<TOKEN id="token-15-8" pos="punct" morph="none" start_char="2890" end_char="2890">:</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="2892" end_char="2902">respiratory</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="2904" end_char="2911">diseases</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="2913" end_char="2916">lead</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="2918" end_char="2919">to</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="2921" end_char="2924">very</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="2926" end_char="2933">specific</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="2935" end_char="2939">types</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="2941" end_char="2942">of</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="2944" end_char="2951">behavior</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="2953" end_char="2954">in</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="2956" end_char="2966">communities</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="2968" end_char="2972">where</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="2974" end_char="2980">they’re</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="2982" end_char="2990">spreading</TOKEN>
<TOKEN id="token-15-23" pos="punct" morph="none" start_char="2991" end_char="2991">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="2993" end_char="3165">
<ORIGINAL_TEXT>So, pictures that show those patterns of behavior could help explain what was happening even if the people who were sickened did not realize the broader problem at the time.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="2993" end_char="2994">So</TOKEN>
<TOKEN id="token-16-1" pos="punct" morph="none" start_char="2995" end_char="2995">,</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="2997" end_char="3004">pictures</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="3006" end_char="3009">that</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="3011" end_char="3014">show</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="3016" end_char="3020">those</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="3022" end_char="3029">patterns</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="3031" end_char="3032">of</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="3034" end_char="3041">behavior</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="3043" end_char="3047">could</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="3049" end_char="3052">help</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="3054" end_char="3060">explain</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="3062" end_char="3065">what</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="3067" end_char="3069">was</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="3071" end_char="3079">happening</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="3081" end_char="3084">even</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="3086" end_char="3087">if</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="3089" end_char="3091">the</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="3093" end_char="3098">people</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="3100" end_char="3102">who</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="3104" end_char="3107">were</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="3109" end_char="3116">sickened</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="3118" end_char="3120">did</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="3122" end_char="3124">not</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="3126" end_char="3132">realize</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="3134" end_char="3136">the</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="3138" end_char="3144">broader</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="3146" end_char="3152">problem</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="3154" end_char="3155">at</TOKEN>
<TOKEN id="token-16-29" pos="word" morph="none" start_char="3157" end_char="3159">the</TOKEN>
<TOKEN id="token-16-30" pos="word" morph="none" start_char="3161" end_char="3164">time</TOKEN>
<TOKEN id="token-16-31" pos="punct" morph="none" start_char="3165" end_char="3165">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="3168" end_char="3258">
<ORIGINAL_TEXT>"What we're trying to do is look at the activity, how busy a hospital is," Brownstein said.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="punct" morph="none" start_char="3168" end_char="3168">"</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="3169" end_char="3172">What</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="3174" end_char="3178">we're</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="3180" end_char="3185">trying</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="3187" end_char="3188">to</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="3190" end_char="3191">do</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="3193" end_char="3194">is</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="3196" end_char="3199">look</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="3201" end_char="3202">at</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="3204" end_char="3206">the</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="3208" end_char="3215">activity</TOKEN>
<TOKEN id="token-17-11" pos="punct" morph="none" start_char="3216" end_char="3216">,</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="3218" end_char="3220">how</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="3222" end_char="3225">busy</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="3227" end_char="3227">a</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="3229" end_char="3236">hospital</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="3238" end_char="3239">is</TOKEN>
<TOKEN id="token-17-17" pos="punct" morph="none" start_char="3240" end_char="3241">,"</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="3243" end_char="3252">Brownstein</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="3254" end_char="3257">said</TOKEN>
<TOKEN id="token-17-20" pos="punct" morph="none" start_char="3258" end_char="3258">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="3260" end_char="3333">
<ORIGINAL_TEXT>"And the way we do that is by counting the cars that are at that hospital.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="punct" morph="none" start_char="3260" end_char="3260">"</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="3261" end_char="3263">And</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="3265" end_char="3267">the</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="3269" end_char="3271">way</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="3273" end_char="3274">we</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="3276" end_char="3277">do</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="3279" end_char="3282">that</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="3284" end_char="3285">is</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="3287" end_char="3288">by</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="3290" end_char="3297">counting</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="3299" end_char="3301">the</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="3303" end_char="3306">cars</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="3308" end_char="3311">that</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="3313" end_char="3315">are</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="3317" end_char="3318">at</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="3320" end_char="3323">that</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="3325" end_char="3332">hospital</TOKEN>
<TOKEN id="token-18-17" pos="punct" morph="none" start_char="3333" end_char="3333">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="3335" end_char="3385">
<ORIGINAL_TEXT>Parking lots will get full as a hospital gets busy.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="3335" end_char="3341">Parking</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="3343" end_char="3346">lots</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="3348" end_char="3351">will</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="3353" end_char="3355">get</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="3357" end_char="3360">full</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="3362" end_char="3363">as</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="3365" end_char="3365">a</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="3367" end_char="3374">hospital</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="3376" end_char="3379">gets</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="3381" end_char="3384">busy</TOKEN>
<TOKEN id="token-19-10" pos="punct" morph="none" start_char="3385" end_char="3385">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="3387" end_char="3548">
<ORIGINAL_TEXT>So more cars in a hospital, the hospital's busier, likely because something's happening in the community, an infection is growing and people have to see a doctor.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="3387" end_char="3388">So</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="3390" end_char="3393">more</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="3395" end_char="3398">cars</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="3400" end_char="3401">in</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="3403" end_char="3403">a</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="3405" end_char="3412">hospital</TOKEN>
<TOKEN id="token-20-6" pos="punct" morph="none" start_char="3413" end_char="3413">,</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="3415" end_char="3417">the</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="3419" end_char="3428">hospital's</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="3430" end_char="3435">busier</TOKEN>
<TOKEN id="token-20-10" pos="punct" morph="none" start_char="3436" end_char="3436">,</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="3438" end_char="3443">likely</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="3445" end_char="3451">because</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="3453" end_char="3463">something's</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="3465" end_char="3473">happening</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="3475" end_char="3476">in</TOKEN>
<TOKEN id="token-20-16" pos="word" morph="none" start_char="3478" end_char="3480">the</TOKEN>
<TOKEN id="token-20-17" pos="word" morph="none" start_char="3482" end_char="3490">community</TOKEN>
<TOKEN id="token-20-18" pos="punct" morph="none" start_char="3491" end_char="3491">,</TOKEN>
<TOKEN id="token-20-19" pos="word" morph="none" start_char="3493" end_char="3494">an</TOKEN>
<TOKEN id="token-20-20" pos="word" morph="none" start_char="3496" end_char="3504">infection</TOKEN>
<TOKEN id="token-20-21" pos="word" morph="none" start_char="3506" end_char="3507">is</TOKEN>
<TOKEN id="token-20-22" pos="word" morph="none" start_char="3509" end_char="3515">growing</TOKEN>
<TOKEN id="token-20-23" pos="word" morph="none" start_char="3517" end_char="3519">and</TOKEN>
<TOKEN id="token-20-24" pos="word" morph="none" start_char="3521" end_char="3526">people</TOKEN>
<TOKEN id="token-20-25" pos="word" morph="none" start_char="3528" end_char="3531">have</TOKEN>
<TOKEN id="token-20-26" pos="word" morph="none" start_char="3533" end_char="3534">to</TOKEN>
<TOKEN id="token-20-27" pos="word" morph="none" start_char="3536" end_char="3538">see</TOKEN>
<TOKEN id="token-20-28" pos="word" morph="none" start_char="3540" end_char="3540">a</TOKEN>
<TOKEN id="token-20-29" pos="word" morph="none" start_char="3542" end_char="3547">doctor</TOKEN>
<TOKEN id="token-20-30" pos="punct" morph="none" start_char="3548" end_char="3548">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="3550" end_char="3659">
<ORIGINAL_TEXT>So you see the increases in the hospital business through the cars… We saw this across multiple institutions."</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="3550" end_char="3551">So</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="3553" end_char="3555">you</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="3557" end_char="3559">see</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="3561" end_char="3563">the</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="3565" end_char="3573">increases</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="3575" end_char="3576">in</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="3578" end_char="3580">the</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="3582" end_char="3589">hospital</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="3591" end_char="3598">business</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="3600" end_char="3606">through</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="3608" end_char="3610">the</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="3612" end_char="3615">cars</TOKEN>
<TOKEN id="token-21-12" pos="punct" morph="none" start_char="3616" end_char="3616">…</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="3618" end_char="3619">We</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="3621" end_char="3623">saw</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="3625" end_char="3628">this</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="3630" end_char="3635">across</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="3637" end_char="3644">multiple</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="3646" end_char="3657">institutions</TOKEN>
<TOKEN id="token-21-19" pos="punct" morph="none" start_char="3658" end_char="3659">."</TOKEN>
</SEG>
<SEG id="segment-22" start_char="3662" end_char="3783">
<ORIGINAL_TEXT>The picture painted by the data is not in itself conclusive, Brownstein acknowledged, but he said the numbers are telling.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="3662" end_char="3664">The</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="3666" end_char="3672">picture</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="3674" end_char="3680">painted</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="3682" end_char="3683">by</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="3685" end_char="3687">the</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="3689" end_char="3692">data</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="3694" end_char="3695">is</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="3697" end_char="3699">not</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="3701" end_char="3702">in</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="3704" end_char="3709">itself</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="3711" end_char="3720">conclusive</TOKEN>
<TOKEN id="token-22-11" pos="punct" morph="none" start_char="3721" end_char="3721">,</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="3723" end_char="3732">Brownstein</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="3734" end_char="3745">acknowledged</TOKEN>
<TOKEN id="token-22-14" pos="punct" morph="none" start_char="3746" end_char="3746">,</TOKEN>
<TOKEN id="token-22-15" pos="word" morph="none" start_char="3748" end_char="3750">but</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="3752" end_char="3753">he</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="3755" end_char="3758">said</TOKEN>
<TOKEN id="token-22-18" pos="word" morph="none" start_char="3760" end_char="3762">the</TOKEN>
<TOKEN id="token-22-19" pos="word" morph="none" start_char="3764" end_char="3770">numbers</TOKEN>
<TOKEN id="token-22-20" pos="word" morph="none" start_char="3772" end_char="3774">are</TOKEN>
<TOKEN id="token-22-21" pos="word" morph="none" start_char="3776" end_char="3782">telling</TOKEN>
<TOKEN id="token-22-22" pos="punct" morph="none" start_char="3783" end_char="3783">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="3786" end_char="3908">
<ORIGINAL_TEXT>"This is all about a growing body of information pointing to something taking place in Wuhan at the time," Brownstein said.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="punct" morph="none" start_char="3786" end_char="3786">"</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="3787" end_char="3790">This</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="3792" end_char="3793">is</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="3795" end_char="3797">all</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="3799" end_char="3803">about</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="3805" end_char="3805">a</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="3807" end_char="3813">growing</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="3815" end_char="3818">body</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="3820" end_char="3821">of</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="3823" end_char="3833">information</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="3835" end_char="3842">pointing</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="3844" end_char="3845">to</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="3847" end_char="3855">something</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="3857" end_char="3862">taking</TOKEN>
<TOKEN id="token-23-14" pos="word" morph="none" start_char="3864" end_char="3868">place</TOKEN>
<TOKEN id="token-23-15" pos="word" morph="none" start_char="3870" end_char="3871">in</TOKEN>
<TOKEN id="token-23-16" pos="word" morph="none" start_char="3873" end_char="3877">Wuhan</TOKEN>
<TOKEN id="token-23-17" pos="word" morph="none" start_char="3879" end_char="3880">at</TOKEN>
<TOKEN id="token-23-18" pos="word" morph="none" start_char="3882" end_char="3884">the</TOKEN>
<TOKEN id="token-23-19" pos="word" morph="none" start_char="3886" end_char="3889">time</TOKEN>
<TOKEN id="token-23-20" pos="punct" morph="none" start_char="3890" end_char="3891">,"</TOKEN>
<TOKEN id="token-23-21" pos="word" morph="none" start_char="3893" end_char="3902">Brownstein</TOKEN>
<TOKEN id="token-23-22" pos="word" morph="none" start_char="3904" end_char="3907">said</TOKEN>
<TOKEN id="token-23-23" pos="punct" morph="none" start_char="3908" end_char="3908">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="3910" end_char="4071">
<ORIGINAL_TEXT>"Many studies are still needed to fully uncover what took place and for people to really learn about how these disease outbreaks unfold and emerge in populations.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="punct" morph="none" start_char="3910" end_char="3910">"</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="3911" end_char="3914">Many</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="3916" end_char="3922">studies</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="3924" end_char="3926">are</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="3928" end_char="3932">still</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="3934" end_char="3939">needed</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="3941" end_char="3942">to</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="3944" end_char="3948">fully</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="3950" end_char="3956">uncover</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="3958" end_char="3961">what</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="3963" end_char="3966">took</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="3968" end_char="3972">place</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="3974" end_char="3976">and</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="3978" end_char="3980">for</TOKEN>
<TOKEN id="token-24-14" pos="word" morph="none" start_char="3982" end_char="3987">people</TOKEN>
<TOKEN id="token-24-15" pos="word" morph="none" start_char="3989" end_char="3990">to</TOKEN>
<TOKEN id="token-24-16" pos="word" morph="none" start_char="3992" end_char="3997">really</TOKEN>
<TOKEN id="token-24-17" pos="word" morph="none" start_char="3999" end_char="4003">learn</TOKEN>
<TOKEN id="token-24-18" pos="word" morph="none" start_char="4005" end_char="4009">about</TOKEN>
<TOKEN id="token-24-19" pos="word" morph="none" start_char="4011" end_char="4013">how</TOKEN>
<TOKEN id="token-24-20" pos="word" morph="none" start_char="4015" end_char="4019">these</TOKEN>
<TOKEN id="token-24-21" pos="word" morph="none" start_char="4021" end_char="4027">disease</TOKEN>
<TOKEN id="token-24-22" pos="word" morph="none" start_char="4029" end_char="4037">outbreaks</TOKEN>
<TOKEN id="token-24-23" pos="word" morph="none" start_char="4039" end_char="4044">unfold</TOKEN>
<TOKEN id="token-24-24" pos="word" morph="none" start_char="4046" end_char="4048">and</TOKEN>
<TOKEN id="token-24-25" pos="word" morph="none" start_char="4050" end_char="4055">emerge</TOKEN>
<TOKEN id="token-24-26" pos="word" morph="none" start_char="4057" end_char="4058">in</TOKEN>
<TOKEN id="token-24-27" pos="word" morph="none" start_char="4060" end_char="4070">populations</TOKEN>
<TOKEN id="token-24-28" pos="punct" morph="none" start_char="4071" end_char="4071">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="4073" end_char="4115">
<ORIGINAL_TEXT>So this is just another point of evidence."</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="4073" end_char="4074">So</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="4076" end_char="4079">this</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="4081" end_char="4082">is</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="4084" end_char="4087">just</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="4089" end_char="4095">another</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="4097" end_char="4101">point</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="4103" end_char="4104">of</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="4106" end_char="4113">evidence</TOKEN>
<TOKEN id="token-25-8" pos="punct" morph="none" start_char="4114" end_char="4115">."</TOKEN>
</SEG>
<SEG id="segment-26" start_char="4118" end_char="4260">
<ORIGINAL_TEXT>Disease ecologist Peter Daszak, president of the nonprofit EcoHealth Alliance in Manhattan, said the Harvard study "is absolutely fascinating."</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="4118" end_char="4124">Disease</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="4126" end_char="4134">ecologist</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="4136" end_char="4140">Peter</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="4142" end_char="4147">Daszak</TOKEN>
<TOKEN id="token-26-4" pos="punct" morph="none" start_char="4148" end_char="4148">,</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="4150" end_char="4158">president</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="4160" end_char="4161">of</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="4163" end_char="4165">the</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="4167" end_char="4175">nonprofit</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="4177" end_char="4185">EcoHealth</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="4187" end_char="4194">Alliance</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="4196" end_char="4197">in</TOKEN>
<TOKEN id="token-26-12" pos="word" morph="none" start_char="4199" end_char="4207">Manhattan</TOKEN>
<TOKEN id="token-26-13" pos="punct" morph="none" start_char="4208" end_char="4208">,</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="4210" end_char="4213">said</TOKEN>
<TOKEN id="token-26-15" pos="word" morph="none" start_char="4215" end_char="4217">the</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="4219" end_char="4225">Harvard</TOKEN>
<TOKEN id="token-26-17" pos="word" morph="none" start_char="4227" end_char="4231">study</TOKEN>
<TOKEN id="token-26-18" pos="punct" morph="none" start_char="4233" end_char="4233">"</TOKEN>
<TOKEN id="token-26-19" pos="word" morph="none" start_char="4234" end_char="4235">is</TOKEN>
<TOKEN id="token-26-20" pos="word" morph="none" start_char="4237" end_char="4246">absolutely</TOKEN>
<TOKEN id="token-26-21" pos="word" morph="none" start_char="4248" end_char="4258">fascinating</TOKEN>
<TOKEN id="token-26-22" pos="punct" morph="none" start_char="4259" end_char="4260">."</TOKEN>
</SEG>
<SEG id="segment-27" start_char="4263" end_char="4440">
<ORIGINAL_TEXT>"You need to look at every possible bit of evidence, where it came from and when it emerged," said Daszak, whose organization works to understand the origin of emerging diseases.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="punct" morph="none" start_char="4263" end_char="4263">"</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="4264" end_char="4266">You</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="4268" end_char="4271">need</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="4273" end_char="4274">to</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="4276" end_char="4279">look</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="4281" end_char="4282">at</TOKEN>
<TOKEN id="token-27-6" pos="word" morph="none" start_char="4284" end_char="4288">every</TOKEN>
<TOKEN id="token-27-7" pos="word" morph="none" start_char="4290" end_char="4297">possible</TOKEN>
<TOKEN id="token-27-8" pos="word" morph="none" start_char="4299" end_char="4301">bit</TOKEN>
<TOKEN id="token-27-9" pos="word" morph="none" start_char="4303" end_char="4304">of</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="4306" end_char="4313">evidence</TOKEN>
<TOKEN id="token-27-11" pos="punct" morph="none" start_char="4314" end_char="4314">,</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="4316" end_char="4320">where</TOKEN>
<TOKEN id="token-27-13" pos="word" morph="none" start_char="4322" end_char="4323">it</TOKEN>
<TOKEN id="token-27-14" pos="word" morph="none" start_char="4325" end_char="4328">came</TOKEN>
<TOKEN id="token-27-15" pos="word" morph="none" start_char="4330" end_char="4333">from</TOKEN>
<TOKEN id="token-27-16" pos="word" morph="none" start_char="4335" end_char="4337">and</TOKEN>
<TOKEN id="token-27-17" pos="word" morph="none" start_char="4339" end_char="4342">when</TOKEN>
<TOKEN id="token-27-18" pos="word" morph="none" start_char="4344" end_char="4345">it</TOKEN>
<TOKEN id="token-27-19" pos="word" morph="none" start_char="4347" end_char="4353">emerged</TOKEN>
<TOKEN id="token-27-20" pos="punct" morph="none" start_char="4354" end_char="4355">,"</TOKEN>
<TOKEN id="token-27-21" pos="word" morph="none" start_char="4357" end_char="4360">said</TOKEN>
<TOKEN id="token-27-22" pos="word" morph="none" start_char="4362" end_char="4367">Daszak</TOKEN>
<TOKEN id="token-27-23" pos="punct" morph="none" start_char="4368" end_char="4368">,</TOKEN>
<TOKEN id="token-27-24" pos="word" morph="none" start_char="4370" end_char="4374">whose</TOKEN>
<TOKEN id="token-27-25" pos="word" morph="none" start_char="4376" end_char="4387">organization</TOKEN>
<TOKEN id="token-27-26" pos="word" morph="none" start_char="4389" end_char="4393">works</TOKEN>
<TOKEN id="token-27-27" pos="word" morph="none" start_char="4395" end_char="4396">to</TOKEN>
<TOKEN id="token-27-28" pos="word" morph="none" start_char="4398" end_char="4407">understand</TOKEN>
<TOKEN id="token-27-29" pos="word" morph="none" start_char="4409" end_char="4411">the</TOKEN>
<TOKEN id="token-27-30" pos="word" morph="none" start_char="4413" end_char="4418">origin</TOKEN>
<TOKEN id="token-27-31" pos="word" morph="none" start_char="4420" end_char="4421">of</TOKEN>
<TOKEN id="token-27-32" pos="word" morph="none" start_char="4423" end_char="4430">emerging</TOKEN>
<TOKEN id="token-27-33" pos="word" morph="none" start_char="4432" end_char="4439">diseases</TOKEN>
<TOKEN id="token-27-34" pos="punct" morph="none" start_char="4440" end_char="4440">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="4442" end_char="4563">
<ORIGINAL_TEXT>"When we do analysis after outbreaks, we find that the diseases had been in circulation days, weeks, months, years before.</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="punct" morph="none" start_char="4442" end_char="4442">"</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="4443" end_char="4446">When</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="4448" end_char="4449">we</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="4451" end_char="4452">do</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="4454" end_char="4461">analysis</TOKEN>
<TOKEN id="token-28-5" pos="word" morph="none" start_char="4463" end_char="4467">after</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="4469" end_char="4477">outbreaks</TOKEN>
<TOKEN id="token-28-7" pos="punct" morph="none" start_char="4478" end_char="4478">,</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="4480" end_char="4481">we</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="4483" end_char="4486">find</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="4488" end_char="4491">that</TOKEN>
<TOKEN id="token-28-11" pos="word" morph="none" start_char="4493" end_char="4495">the</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="4497" end_char="4504">diseases</TOKEN>
<TOKEN id="token-28-13" pos="word" morph="none" start_char="4506" end_char="4508">had</TOKEN>
<TOKEN id="token-28-14" pos="word" morph="none" start_char="4510" end_char="4513">been</TOKEN>
<TOKEN id="token-28-15" pos="word" morph="none" start_char="4515" end_char="4516">in</TOKEN>
<TOKEN id="token-28-16" pos="word" morph="none" start_char="4518" end_char="4528">circulation</TOKEN>
<TOKEN id="token-28-17" pos="word" morph="none" start_char="4530" end_char="4533">days</TOKEN>
<TOKEN id="token-28-18" pos="punct" morph="none" start_char="4534" end_char="4534">,</TOKEN>
<TOKEN id="token-28-19" pos="word" morph="none" start_char="4536" end_char="4540">weeks</TOKEN>
<TOKEN id="token-28-20" pos="punct" morph="none" start_char="4541" end_char="4541">,</TOKEN>
<TOKEN id="token-28-21" pos="word" morph="none" start_char="4543" end_char="4548">months</TOKEN>
<TOKEN id="token-28-22" pos="punct" morph="none" start_char="4549" end_char="4549">,</TOKEN>
<TOKEN id="token-28-23" pos="word" morph="none" start_char="4551" end_char="4555">years</TOKEN>
<TOKEN id="token-28-24" pos="word" morph="none" start_char="4557" end_char="4562">before</TOKEN>
<TOKEN id="token-28-25" pos="punct" morph="none" start_char="4563" end_char="4563">.</TOKEN>
</SEG>
<SEG id="segment-29" start_char="4565" end_char="4628">
<ORIGINAL_TEXT>I really believe that’s what we’re going to find with COVID-19."</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="4565" end_char="4565">I</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="4567" end_char="4572">really</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="4574" end_char="4580">believe</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="4582" end_char="4587">that’s</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="4589" end_char="4592">what</TOKEN>
<TOKEN id="token-29-5" pos="word" morph="none" start_char="4594" end_char="4598">we’re</TOKEN>
<TOKEN id="token-29-6" pos="word" morph="none" start_char="4600" end_char="4604">going</TOKEN>
<TOKEN id="token-29-7" pos="word" morph="none" start_char="4606" end_char="4607">to</TOKEN>
<TOKEN id="token-29-8" pos="word" morph="none" start_char="4609" end_char="4612">find</TOKEN>
<TOKEN id="token-29-9" pos="word" morph="none" start_char="4614" end_char="4617">with</TOKEN>
<TOKEN id="token-29-10" pos="unknown" morph="none" start_char="4619" end_char="4626">COVID-19</TOKEN>
<TOKEN id="token-29-11" pos="punct" morph="none" start_char="4627" end_char="4628">."</TOKEN>
</SEG>
<SEG id="segment-30" start_char="4631" end_char="4807">
<ORIGINAL_TEXT>David Perlin, chief science officer at the Center for Discovery and Innovation in New Jersey, said he was intrigued by Brownstein’s research, though he wasn’t totally convinced.</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="4631" end_char="4635">David</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="4637" end_char="4642">Perlin</TOKEN>
<TOKEN id="token-30-2" pos="punct" morph="none" start_char="4643" end_char="4643">,</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="4645" end_char="4649">chief</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="4651" end_char="4657">science</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="4659" end_char="4665">officer</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="4667" end_char="4668">at</TOKEN>
<TOKEN id="token-30-7" pos="word" morph="none" start_char="4670" end_char="4672">the</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="4674" end_char="4679">Center</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="4681" end_char="4683">for</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="4685" end_char="4693">Discovery</TOKEN>
<TOKEN id="token-30-11" pos="word" morph="none" start_char="4695" end_char="4697">and</TOKEN>
<TOKEN id="token-30-12" pos="word" morph="none" start_char="4699" end_char="4708">Innovation</TOKEN>
<TOKEN id="token-30-13" pos="word" morph="none" start_char="4710" end_char="4711">in</TOKEN>
<TOKEN id="token-30-14" pos="word" morph="none" start_char="4713" end_char="4715">New</TOKEN>
<TOKEN id="token-30-15" pos="word" morph="none" start_char="4717" end_char="4722">Jersey</TOKEN>
<TOKEN id="token-30-16" pos="punct" morph="none" start_char="4723" end_char="4723">,</TOKEN>
<TOKEN id="token-30-17" pos="word" morph="none" start_char="4725" end_char="4728">said</TOKEN>
<TOKEN id="token-30-18" pos="word" morph="none" start_char="4730" end_char="4731">he</TOKEN>
<TOKEN id="token-30-19" pos="word" morph="none" start_char="4733" end_char="4735">was</TOKEN>
<TOKEN id="token-30-20" pos="word" morph="none" start_char="4737" end_char="4745">intrigued</TOKEN>
<TOKEN id="token-30-21" pos="word" morph="none" start_char="4747" end_char="4748">by</TOKEN>
<TOKEN id="token-30-22" pos="word" morph="none" start_char="4750" end_char="4761">Brownstein’s</TOKEN>
<TOKEN id="token-30-23" pos="word" morph="none" start_char="4763" end_char="4770">research</TOKEN>
<TOKEN id="token-30-24" pos="punct" morph="none" start_char="4771" end_char="4771">,</TOKEN>
<TOKEN id="token-30-25" pos="word" morph="none" start_char="4773" end_char="4778">though</TOKEN>
<TOKEN id="token-30-26" pos="word" morph="none" start_char="4780" end_char="4781">he</TOKEN>
<TOKEN id="token-30-27" pos="word" morph="none" start_char="4783" end_char="4788">wasn’t</TOKEN>
<TOKEN id="token-30-28" pos="word" morph="none" start_char="4790" end_char="4796">totally</TOKEN>
<TOKEN id="token-30-29" pos="word" morph="none" start_char="4798" end_char="4806">convinced</TOKEN>
<TOKEN id="token-30-30" pos="punct" morph="none" start_char="4807" end_char="4807">.</TOKEN>
</SEG>
<SEG id="segment-31" start_char="4810" end_char="4923">
<ORIGINAL_TEXT>"I think some of the methods are questionable and their interpretation is slightly over-interpreted," Perlin said.</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="punct" morph="none" start_char="4810" end_char="4810">"</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="4811" end_char="4811">I</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="4813" end_char="4817">think</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="4819" end_char="4822">some</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="4824" end_char="4825">of</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="4827" end_char="4829">the</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="4831" end_char="4837">methods</TOKEN>
<TOKEN id="token-31-7" pos="word" morph="none" start_char="4839" end_char="4841">are</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="4843" end_char="4854">questionable</TOKEN>
<TOKEN id="token-31-9" pos="word" morph="none" start_char="4856" end_char="4858">and</TOKEN>
<TOKEN id="token-31-10" pos="word" morph="none" start_char="4860" end_char="4864">their</TOKEN>
<TOKEN id="token-31-11" pos="word" morph="none" start_char="4866" end_char="4879">interpretation</TOKEN>
<TOKEN id="token-31-12" pos="word" morph="none" start_char="4881" end_char="4882">is</TOKEN>
<TOKEN id="token-31-13" pos="word" morph="none" start_char="4884" end_char="4891">slightly</TOKEN>
<TOKEN id="token-31-14" pos="unknown" morph="none" start_char="4893" end_char="4908">over-interpreted</TOKEN>
<TOKEN id="token-31-15" pos="punct" morph="none" start_char="4909" end_char="4910">,"</TOKEN>
<TOKEN id="token-31-16" pos="word" morph="none" start_char="4912" end_char="4917">Perlin</TOKEN>
<TOKEN id="token-31-17" pos="word" morph="none" start_char="4919" end_char="4922">said</TOKEN>
<TOKEN id="token-31-18" pos="punct" morph="none" start_char="4923" end_char="4923">.</TOKEN>
</SEG>
<SEG id="segment-32" start_char="4925" end_char="4975">
<ORIGINAL_TEXT>"The problem is we only have a subset of data here.</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="punct" morph="none" start_char="4925" end_char="4925">"</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="4926" end_char="4928">The</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="4930" end_char="4936">problem</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="4938" end_char="4939">is</TOKEN>
<TOKEN id="token-32-4" pos="word" morph="none" start_char="4941" end_char="4942">we</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="4944" end_char="4947">only</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="4949" end_char="4952">have</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="4954" end_char="4954">a</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="4956" end_char="4961">subset</TOKEN>
<TOKEN id="token-32-9" pos="word" morph="none" start_char="4963" end_char="4964">of</TOKEN>
<TOKEN id="token-32-10" pos="word" morph="none" start_char="4966" end_char="4969">data</TOKEN>
<TOKEN id="token-32-11" pos="word" morph="none" start_char="4971" end_char="4974">here</TOKEN>
<TOKEN id="token-32-12" pos="punct" morph="none" start_char="4975" end_char="4975">.</TOKEN>
</SEG>
<SEG id="segment-33" start_char="4977" end_char="5096">
<ORIGINAL_TEXT>I always worry when people start drawing inferences from data subsets, cherry-picking data [like the internet searches].</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="word" morph="none" start_char="4977" end_char="4977">I</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="4979" end_char="4984">always</TOKEN>
<TOKEN id="token-33-2" pos="word" morph="none" start_char="4986" end_char="4990">worry</TOKEN>
<TOKEN id="token-33-3" pos="word" morph="none" start_char="4992" end_char="4995">when</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="4997" end_char="5002">people</TOKEN>
<TOKEN id="token-33-5" pos="word" morph="none" start_char="5004" end_char="5008">start</TOKEN>
<TOKEN id="token-33-6" pos="word" morph="none" start_char="5010" end_char="5016">drawing</TOKEN>
<TOKEN id="token-33-7" pos="word" morph="none" start_char="5018" end_char="5027">inferences</TOKEN>
<TOKEN id="token-33-8" pos="word" morph="none" start_char="5029" end_char="5032">from</TOKEN>
<TOKEN id="token-33-9" pos="word" morph="none" start_char="5034" end_char="5037">data</TOKEN>
<TOKEN id="token-33-10" pos="word" morph="none" start_char="5039" end_char="5045">subsets</TOKEN>
<TOKEN id="token-33-11" pos="punct" morph="none" start_char="5046" end_char="5046">,</TOKEN>
<TOKEN id="token-33-12" pos="unknown" morph="none" start_char="5048" end_char="5061">cherry-picking</TOKEN>
<TOKEN id="token-33-13" pos="word" morph="none" start_char="5063" end_char="5066">data</TOKEN>
<TOKEN id="token-33-14" pos="punct" morph="none" start_char="5068" end_char="5068">[</TOKEN>
<TOKEN id="token-33-15" pos="word" morph="none" start_char="5069" end_char="5072">like</TOKEN>
<TOKEN id="token-33-16" pos="word" morph="none" start_char="5074" end_char="5076">the</TOKEN>
<TOKEN id="token-33-17" pos="word" morph="none" start_char="5078" end_char="5085">internet</TOKEN>
<TOKEN id="token-33-18" pos="word" morph="none" start_char="5087" end_char="5094">searches</TOKEN>
<TOKEN id="token-33-19" pos="punct" morph="none" start_char="5095" end_char="5096">].</TOKEN>
</SEG>
<SEG id="segment-34" start_char="5098" end_char="5114">
<ORIGINAL_TEXT>It’s suggestive."</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="5098" end_char="5101">It’s</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="5103" end_char="5112">suggestive</TOKEN>
<TOKEN id="token-34-2" pos="punct" morph="none" start_char="5113" end_char="5114">."</TOKEN>
</SEG>
<SEG id="segment-35" start_char="5117" end_char="5168">
<ORIGINAL_TEXT>Photographs taken from space suggests a crisis below</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="5117" end_char="5127">Photographs</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="5129" end_char="5133">taken</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="5135" end_char="5138">from</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="5140" end_char="5144">space</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="5146" end_char="5153">suggests</TOKEN>
<TOKEN id="token-35-5" pos="word" morph="none" start_char="5155" end_char="5155">a</TOKEN>
<TOKEN id="token-35-6" pos="word" morph="none" start_char="5157" end_char="5162">crisis</TOKEN>
<TOKEN id="token-35-7" pos="word" morph="none" start_char="5164" end_char="5168">below</TOKEN>
</SEG>
<SEG id="segment-36" start_char="5171" end_char="5363">
<ORIGINAL_TEXT>Starting with nearly 350 images captured by private satellites circling the globe, Brownstein’s study first examined traffic and parking outside major hospitals in Wuhan for the past two years.</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="5171" end_char="5178">Starting</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="5180" end_char="5183">with</TOKEN>
<TOKEN id="token-36-2" pos="word" morph="none" start_char="5185" end_char="5190">nearly</TOKEN>
<TOKEN id="token-36-3" pos="word" morph="none" start_char="5192" end_char="5194">350</TOKEN>
<TOKEN id="token-36-4" pos="word" morph="none" start_char="5196" end_char="5201">images</TOKEN>
<TOKEN id="token-36-5" pos="word" morph="none" start_char="5203" end_char="5210">captured</TOKEN>
<TOKEN id="token-36-6" pos="word" morph="none" start_char="5212" end_char="5213">by</TOKEN>
<TOKEN id="token-36-7" pos="word" morph="none" start_char="5215" end_char="5221">private</TOKEN>
<TOKEN id="token-36-8" pos="word" morph="none" start_char="5223" end_char="5232">satellites</TOKEN>
<TOKEN id="token-36-9" pos="word" morph="none" start_char="5234" end_char="5241">circling</TOKEN>
<TOKEN id="token-36-10" pos="word" morph="none" start_char="5243" end_char="5245">the</TOKEN>
<TOKEN id="token-36-11" pos="word" morph="none" start_char="5247" end_char="5251">globe</TOKEN>
<TOKEN id="token-36-12" pos="punct" morph="none" start_char="5252" end_char="5252">,</TOKEN>
<TOKEN id="token-36-13" pos="word" morph="none" start_char="5254" end_char="5265">Brownstein’s</TOKEN>
<TOKEN id="token-36-14" pos="word" morph="none" start_char="5267" end_char="5271">study</TOKEN>
<TOKEN id="token-36-15" pos="word" morph="none" start_char="5273" end_char="5277">first</TOKEN>
<TOKEN id="token-36-16" pos="word" morph="none" start_char="5279" end_char="5286">examined</TOKEN>
<TOKEN id="token-36-17" pos="word" morph="none" start_char="5288" end_char="5294">traffic</TOKEN>
<TOKEN id="token-36-18" pos="word" morph="none" start_char="5296" end_char="5298">and</TOKEN>
<TOKEN id="token-36-19" pos="word" morph="none" start_char="5300" end_char="5306">parking</TOKEN>
<TOKEN id="token-36-20" pos="word" morph="none" start_char="5308" end_char="5314">outside</TOKEN>
<TOKEN id="token-36-21" pos="word" morph="none" start_char="5316" end_char="5320">major</TOKEN>
<TOKEN id="token-36-22" pos="word" morph="none" start_char="5322" end_char="5330">hospitals</TOKEN>
<TOKEN id="token-36-23" pos="word" morph="none" start_char="5332" end_char="5333">in</TOKEN>
<TOKEN id="token-36-24" pos="word" morph="none" start_char="5335" end_char="5339">Wuhan</TOKEN>
<TOKEN id="token-36-25" pos="word" morph="none" start_char="5341" end_char="5343">for</TOKEN>
<TOKEN id="token-36-26" pos="word" morph="none" start_char="5345" end_char="5347">the</TOKEN>
<TOKEN id="token-36-27" pos="word" morph="none" start_char="5349" end_char="5352">past</TOKEN>
<TOKEN id="token-36-28" pos="word" morph="none" start_char="5354" end_char="5356">two</TOKEN>
<TOKEN id="token-36-29" pos="word" morph="none" start_char="5358" end_char="5362">years</TOKEN>
<TOKEN id="token-36-30" pos="punct" morph="none" start_char="5363" end_char="5363">.</TOKEN>
</SEG>
<SEG id="segment-37" start_char="5365" end_char="5481">
<ORIGINAL_TEXT>Among them were photographs snapped from space approximately every week or every other week through the fall of 2019.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="5365" end_char="5369">Among</TOKEN>
<TOKEN id="token-37-1" pos="word" morph="none" start_char="5371" end_char="5374">them</TOKEN>
<TOKEN id="token-37-2" pos="word" morph="none" start_char="5376" end_char="5379">were</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="5381" end_char="5391">photographs</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="5393" end_char="5399">snapped</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="5401" end_char="5404">from</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="5406" end_char="5410">space</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="5412" end_char="5424">approximately</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="5426" end_char="5430">every</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="5432" end_char="5435">week</TOKEN>
<TOKEN id="token-37-10" pos="word" morph="none" start_char="5437" end_char="5438">or</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="5440" end_char="5444">every</TOKEN>
<TOKEN id="token-37-12" pos="word" morph="none" start_char="5446" end_char="5450">other</TOKEN>
<TOKEN id="token-37-13" pos="word" morph="none" start_char="5452" end_char="5455">week</TOKEN>
<TOKEN id="token-37-14" pos="word" morph="none" start_char="5457" end_char="5463">through</TOKEN>
<TOKEN id="token-37-15" pos="word" morph="none" start_char="5465" end_char="5467">the</TOKEN>
<TOKEN id="token-37-16" pos="word" morph="none" start_char="5469" end_char="5472">fall</TOKEN>
<TOKEN id="token-37-17" pos="word" morph="none" start_char="5474" end_char="5475">of</TOKEN>
<TOKEN id="token-37-18" pos="word" morph="none" start_char="5477" end_char="5480">2019</TOKEN>
<TOKEN id="token-37-19" pos="punct" morph="none" start_char="5481" end_char="5481">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="5483" end_char="5685">
<ORIGINAL_TEXT>From the approximately 350 frames, researchers found 108 usable images, showing locations without obstruction from smog, tall buildings, clouds or other features that could complicate satellite analysis.</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="5483" end_char="5486">From</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="5488" end_char="5490">the</TOKEN>
<TOKEN id="token-38-2" pos="word" morph="none" start_char="5492" end_char="5504">approximately</TOKEN>
<TOKEN id="token-38-3" pos="word" morph="none" start_char="5506" end_char="5508">350</TOKEN>
<TOKEN id="token-38-4" pos="word" morph="none" start_char="5510" end_char="5515">frames</TOKEN>
<TOKEN id="token-38-5" pos="punct" morph="none" start_char="5516" end_char="5516">,</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="5518" end_char="5528">researchers</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="5530" end_char="5534">found</TOKEN>
<TOKEN id="token-38-8" pos="word" morph="none" start_char="5536" end_char="5538">108</TOKEN>
<TOKEN id="token-38-9" pos="word" morph="none" start_char="5540" end_char="5545">usable</TOKEN>
<TOKEN id="token-38-10" pos="word" morph="none" start_char="5547" end_char="5552">images</TOKEN>
<TOKEN id="token-38-11" pos="punct" morph="none" start_char="5553" end_char="5553">,</TOKEN>
<TOKEN id="token-38-12" pos="word" morph="none" start_char="5555" end_char="5561">showing</TOKEN>
<TOKEN id="token-38-13" pos="word" morph="none" start_char="5563" end_char="5571">locations</TOKEN>
<TOKEN id="token-38-14" pos="word" morph="none" start_char="5573" end_char="5579">without</TOKEN>
<TOKEN id="token-38-15" pos="word" morph="none" start_char="5581" end_char="5591">obstruction</TOKEN>
<TOKEN id="token-38-16" pos="word" morph="none" start_char="5593" end_char="5596">from</TOKEN>
<TOKEN id="token-38-17" pos="word" morph="none" start_char="5598" end_char="5601">smog</TOKEN>
<TOKEN id="token-38-18" pos="punct" morph="none" start_char="5602" end_char="5602">,</TOKEN>
<TOKEN id="token-38-19" pos="word" morph="none" start_char="5604" end_char="5607">tall</TOKEN>
<TOKEN id="token-38-20" pos="word" morph="none" start_char="5609" end_char="5617">buildings</TOKEN>
<TOKEN id="token-38-21" pos="punct" morph="none" start_char="5618" end_char="5618">,</TOKEN>
<TOKEN id="token-38-22" pos="word" morph="none" start_char="5620" end_char="5625">clouds</TOKEN>
<TOKEN id="token-38-23" pos="word" morph="none" start_char="5627" end_char="5628">or</TOKEN>
<TOKEN id="token-38-24" pos="word" morph="none" start_char="5630" end_char="5634">other</TOKEN>
<TOKEN id="token-38-25" pos="word" morph="none" start_char="5636" end_char="5643">features</TOKEN>
<TOKEN id="token-38-26" pos="word" morph="none" start_char="5645" end_char="5648">that</TOKEN>
<TOKEN id="token-38-27" pos="word" morph="none" start_char="5650" end_char="5654">could</TOKEN>
<TOKEN id="token-38-28" pos="word" morph="none" start_char="5656" end_char="5665">complicate</TOKEN>
<TOKEN id="token-38-29" pos="word" morph="none" start_char="5667" end_char="5675">satellite</TOKEN>
<TOKEN id="token-38-30" pos="word" morph="none" start_char="5677" end_char="5684">analysis</TOKEN>
<TOKEN id="token-38-31" pos="punct" morph="none" start_char="5685" end_char="5685">.</TOKEN>
</SEG>
<SEG id="segment-39" start_char="5688" end_char="5778">
<ORIGINAL_TEXT>"It has to be right at noon," Brownstein said, "because you basically want direct sunlight.</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="punct" morph="none" start_char="5688" end_char="5688">"</TOKEN>
<TOKEN id="token-39-1" pos="word" morph="none" start_char="5689" end_char="5690">It</TOKEN>
<TOKEN id="token-39-2" pos="word" morph="none" start_char="5692" end_char="5694">has</TOKEN>
<TOKEN id="token-39-3" pos="word" morph="none" start_char="5696" end_char="5697">to</TOKEN>
<TOKEN id="token-39-4" pos="word" morph="none" start_char="5699" end_char="5700">be</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="5702" end_char="5706">right</TOKEN>
<TOKEN id="token-39-6" pos="word" morph="none" start_char="5708" end_char="5709">at</TOKEN>
<TOKEN id="token-39-7" pos="word" morph="none" start_char="5711" end_char="5714">noon</TOKEN>
<TOKEN id="token-39-8" pos="punct" morph="none" start_char="5715" end_char="5716">,"</TOKEN>
<TOKEN id="token-39-9" pos="word" morph="none" start_char="5718" end_char="5727">Brownstein</TOKEN>
<TOKEN id="token-39-10" pos="word" morph="none" start_char="5729" end_char="5732">said</TOKEN>
<TOKEN id="token-39-11" pos="punct" morph="none" start_char="5733" end_char="5733">,</TOKEN>
<TOKEN id="token-39-12" pos="punct" morph="none" start_char="5735" end_char="5735">"</TOKEN>
<TOKEN id="token-39-13" pos="word" morph="none" start_char="5736" end_char="5742">because</TOKEN>
<TOKEN id="token-39-14" pos="word" morph="none" start_char="5744" end_char="5746">you</TOKEN>
<TOKEN id="token-39-15" pos="word" morph="none" start_char="5748" end_char="5756">basically</TOKEN>
<TOKEN id="token-39-16" pos="word" morph="none" start_char="5758" end_char="5761">want</TOKEN>
<TOKEN id="token-39-17" pos="word" morph="none" start_char="5763" end_char="5768">direct</TOKEN>
<TOKEN id="token-39-18" pos="word" morph="none" start_char="5770" end_char="5777">sunlight</TOKEN>
<TOKEN id="token-39-19" pos="punct" morph="none" start_char="5778" end_char="5778">.</TOKEN>
</SEG>
<SEG id="segment-40" start_char="5780" end_char="5844">
<ORIGINAL_TEXT>You don’t want shadows to prevent our ability to count the cars."</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="word" morph="none" start_char="5780" end_char="5782">You</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="5784" end_char="5788">don’t</TOKEN>
<TOKEN id="token-40-2" pos="word" morph="none" start_char="5790" end_char="5793">want</TOKEN>
<TOKEN id="token-40-3" pos="word" morph="none" start_char="5795" end_char="5801">shadows</TOKEN>
<TOKEN id="token-40-4" pos="word" morph="none" start_char="5803" end_char="5804">to</TOKEN>
<TOKEN id="token-40-5" pos="word" morph="none" start_char="5806" end_char="5812">prevent</TOKEN>
<TOKEN id="token-40-6" pos="word" morph="none" start_char="5814" end_char="5816">our</TOKEN>
<TOKEN id="token-40-7" pos="word" morph="none" start_char="5818" end_char="5824">ability</TOKEN>
<TOKEN id="token-40-8" pos="word" morph="none" start_char="5826" end_char="5827">to</TOKEN>
<TOKEN id="token-40-9" pos="word" morph="none" start_char="5829" end_char="5833">count</TOKEN>
<TOKEN id="token-40-10" pos="word" morph="none" start_char="5835" end_char="5837">the</TOKEN>
<TOKEN id="token-40-11" pos="word" morph="none" start_char="5839" end_char="5842">cars</TOKEN>
<TOKEN id="token-40-12" pos="punct" morph="none" start_char="5843" end_char="5844">."</TOKEN>
</SEG>
<SEG id="segment-41" start_char="5847" end_char="5853">
<ORIGINAL_TEXT>On Oct.</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="5847" end_char="5848">On</TOKEN>
<TOKEN id="token-41-1" pos="word" morph="none" start_char="5850" end_char="5852">Oct</TOKEN>
<TOKEN id="token-41-2" pos="punct" morph="none" start_char="5853" end_char="5853">.</TOKEN>
</SEG>
<SEG id="segment-42" start_char="5855" end_char="5958">
<ORIGINAL_TEXT>10, 2018, there were 171 cars in the parking lot of Wuhan’s Tianyou Hospital, one of the city’s largest.</ORIGINAL_TEXT>
<TOKEN id="token-42-0" pos="word" morph="none" start_char="5855" end_char="5856">10</TOKEN>
<TOKEN id="token-42-1" pos="punct" morph="none" start_char="5857" end_char="5857">,</TOKEN>
<TOKEN id="token-42-2" pos="word" morph="none" start_char="5859" end_char="5862">2018</TOKEN>
<TOKEN id="token-42-3" pos="punct" morph="none" start_char="5863" end_char="5863">,</TOKEN>
<TOKEN id="token-42-4" pos="word" morph="none" start_char="5865" end_char="5869">there</TOKEN>
<TOKEN id="token-42-5" pos="word" morph="none" start_char="5871" end_char="5874">were</TOKEN>
<TOKEN id="token-42-6" pos="word" morph="none" start_char="5876" end_char="5878">171</TOKEN>
<TOKEN id="token-42-7" pos="word" morph="none" start_char="5880" end_char="5883">cars</TOKEN>
<TOKEN id="token-42-8" pos="word" morph="none" start_char="5885" end_char="5886">in</TOKEN>
<TOKEN id="token-42-9" pos="word" morph="none" start_char="5888" end_char="5890">the</TOKEN>
<TOKEN id="token-42-10" pos="word" morph="none" start_char="5892" end_char="5898">parking</TOKEN>
<TOKEN id="token-42-11" pos="word" morph="none" start_char="5900" end_char="5902">lot</TOKEN>
<TOKEN id="token-42-12" pos="word" morph="none" start_char="5904" end_char="5905">of</TOKEN>
<TOKEN id="token-42-13" pos="word" morph="none" start_char="5907" end_char="5913">Wuhan’s</TOKEN>
<TOKEN id="token-42-14" pos="word" morph="none" start_char="5915" end_char="5921">Tianyou</TOKEN>
<TOKEN id="token-42-15" pos="word" morph="none" start_char="5923" end_char="5930">Hospital</TOKEN>
<TOKEN id="token-42-16" pos="punct" morph="none" start_char="5931" end_char="5931">,</TOKEN>
<TOKEN id="token-42-17" pos="word" morph="none" start_char="5933" end_char="5935">one</TOKEN>
<TOKEN id="token-42-18" pos="word" morph="none" start_char="5937" end_char="5938">of</TOKEN>
<TOKEN id="token-42-19" pos="word" morph="none" start_char="5940" end_char="5942">the</TOKEN>
<TOKEN id="token-42-20" pos="word" morph="none" start_char="5944" end_char="5949">city’s</TOKEN>
<TOKEN id="token-42-21" pos="word" morph="none" start_char="5951" end_char="5957">largest</TOKEN>
<TOKEN id="token-42-22" pos="punct" morph="none" start_char="5958" end_char="5958">.</TOKEN>
</SEG>
<SEG id="segment-43" start_char="5960" end_char="6096">
<ORIGINAL_TEXT>A year later, satellites recorded 285 cars -- a 67% increase, according to the data reviewed by the researchers and shared with ABC News.</ORIGINAL_TEXT>
<TOKEN id="token-43-0" pos="word" morph="none" start_char="5960" end_char="5960">A</TOKEN>
<TOKEN id="token-43-1" pos="word" morph="none" start_char="5962" end_char="5965">year</TOKEN>
<TOKEN id="token-43-2" pos="word" morph="none" start_char="5967" end_char="5971">later</TOKEN>
<TOKEN id="token-43-3" pos="punct" morph="none" start_char="5972" end_char="5972">,</TOKEN>
<TOKEN id="token-43-4" pos="word" morph="none" start_char="5974" end_char="5983">satellites</TOKEN>
<TOKEN id="token-43-5" pos="word" morph="none" start_char="5985" end_char="5992">recorded</TOKEN>
<TOKEN id="token-43-6" pos="word" morph="none" start_char="5994" end_char="5996">285</TOKEN>
<TOKEN id="token-43-7" pos="word" morph="none" start_char="5998" end_char="6001">cars</TOKEN>
<TOKEN id="token-43-8" pos="punct" morph="none" start_char="6003" end_char="6004">--</TOKEN>
<TOKEN id="token-43-9" pos="word" morph="none" start_char="6006" end_char="6006">a</TOKEN>
<TOKEN id="token-43-10" pos="word" morph="none" start_char="6008" end_char="6009">67</TOKEN>
<TOKEN id="token-43-11" pos="punct" morph="none" start_char="6010" end_char="6010">%</TOKEN>
<TOKEN id="token-43-12" pos="word" morph="none" start_char="6012" end_char="6019">increase</TOKEN>
<TOKEN id="token-43-13" pos="punct" morph="none" start_char="6020" end_char="6020">,</TOKEN>
<TOKEN id="token-43-14" pos="word" morph="none" start_char="6022" end_char="6030">according</TOKEN>
<TOKEN id="token-43-15" pos="word" morph="none" start_char="6032" end_char="6033">to</TOKEN>
<TOKEN id="token-43-16" pos="word" morph="none" start_char="6035" end_char="6037">the</TOKEN>
<TOKEN id="token-43-17" pos="word" morph="none" start_char="6039" end_char="6042">data</TOKEN>
<TOKEN id="token-43-18" pos="word" morph="none" start_char="6044" end_char="6051">reviewed</TOKEN>
<TOKEN id="token-43-19" pos="word" morph="none" start_char="6053" end_char="6054">by</TOKEN>
<TOKEN id="token-43-20" pos="word" morph="none" start_char="6056" end_char="6058">the</TOKEN>
<TOKEN id="token-43-21" pos="word" morph="none" start_char="6060" end_char="6070">researchers</TOKEN>
<TOKEN id="token-43-22" pos="word" morph="none" start_char="6072" end_char="6074">and</TOKEN>
<TOKEN id="token-43-23" pos="word" morph="none" start_char="6076" end_char="6081">shared</TOKEN>
<TOKEN id="token-43-24" pos="word" morph="none" start_char="6083" end_char="6086">with</TOKEN>
<TOKEN id="token-43-25" pos="word" morph="none" start_char="6088" end_char="6090">ABC</TOKEN>
<TOKEN id="token-43-26" pos="word" morph="none" start_char="6092" end_char="6095">News</TOKEN>
<TOKEN id="token-43-27" pos="punct" morph="none" start_char="6096" end_char="6096">.</TOKEN>
</SEG>
<SEG id="segment-44" start_char="6099" end_char="6219">
<ORIGINAL_TEXT>Other hospitals showed up to a 90% increase when comparing traffic between fall of 2018 and 2019, according to the study.</ORIGINAL_TEXT>
<TOKEN id="token-44-0" pos="word" morph="none" start_char="6099" end_char="6103">Other</TOKEN>
<TOKEN id="token-44-1" pos="word" morph="none" start_char="6105" end_char="6113">hospitals</TOKEN>
<TOKEN id="token-44-2" pos="word" morph="none" start_char="6115" end_char="6120">showed</TOKEN>
<TOKEN id="token-44-3" pos="word" morph="none" start_char="6122" end_char="6123">up</TOKEN>
<TOKEN id="token-44-4" pos="word" morph="none" start_char="6125" end_char="6126">to</TOKEN>
<TOKEN id="token-44-5" pos="word" morph="none" start_char="6128" end_char="6128">a</TOKEN>
<TOKEN id="token-44-6" pos="word" morph="none" start_char="6130" end_char="6131">90</TOKEN>
<TOKEN id="token-44-7" pos="punct" morph="none" start_char="6132" end_char="6132">%</TOKEN>
<TOKEN id="token-44-8" pos="word" morph="none" start_char="6134" end_char="6141">increase</TOKEN>
<TOKEN id="token-44-9" pos="word" morph="none" start_char="6143" end_char="6146">when</TOKEN>
<TOKEN id="token-44-10" pos="word" morph="none" start_char="6148" end_char="6156">comparing</TOKEN>
<TOKEN id="token-44-11" pos="word" morph="none" start_char="6158" end_char="6164">traffic</TOKEN>
<TOKEN id="token-44-12" pos="word" morph="none" start_char="6166" end_char="6172">between</TOKEN>
<TOKEN id="token-44-13" pos="word" morph="none" start_char="6174" end_char="6177">fall</TOKEN>
<TOKEN id="token-44-14" pos="word" morph="none" start_char="6179" end_char="6180">of</TOKEN>
<TOKEN id="token-44-15" pos="word" morph="none" start_char="6182" end_char="6185">2018</TOKEN>
<TOKEN id="token-44-16" pos="word" morph="none" start_char="6187" end_char="6189">and</TOKEN>
<TOKEN id="token-44-17" pos="word" morph="none" start_char="6191" end_char="6194">2019</TOKEN>
<TOKEN id="token-44-18" pos="punct" morph="none" start_char="6195" end_char="6195">,</TOKEN>
<TOKEN id="token-44-19" pos="word" morph="none" start_char="6197" end_char="6205">according</TOKEN>
<TOKEN id="token-44-20" pos="word" morph="none" start_char="6207" end_char="6208">to</TOKEN>
<TOKEN id="token-44-21" pos="word" morph="none" start_char="6210" end_char="6212">the</TOKEN>
<TOKEN id="token-44-22" pos="word" morph="none" start_char="6214" end_char="6218">study</TOKEN>
<TOKEN id="token-44-23" pos="punct" morph="none" start_char="6219" end_char="6219">.</TOKEN>
</SEG>
<SEG id="segment-45" start_char="6221" end_char="6330">
<ORIGINAL_TEXT>At Wuhan Tongji Medical University, the spike in car traffic was found to have occurred in mid-September 2019.</ORIGINAL_TEXT>
<TOKEN id="token-45-0" pos="word" morph="none" start_char="6221" end_char="6222">At</TOKEN>
<TOKEN id="token-45-1" pos="word" morph="none" start_char="6224" end_char="6228">Wuhan</TOKEN>
<TOKEN id="token-45-2" pos="word" morph="none" start_char="6230" end_char="6235">Tongji</TOKEN>
<TOKEN id="token-45-3" pos="word" morph="none" start_char="6237" end_char="6243">Medical</TOKEN>
<TOKEN id="token-45-4" pos="word" morph="none" start_char="6245" end_char="6254">University</TOKEN>
<TOKEN id="token-45-5" pos="punct" morph="none" start_char="6255" end_char="6255">,</TOKEN>
<TOKEN id="token-45-6" pos="word" morph="none" start_char="6257" end_char="6259">the</TOKEN>
<TOKEN id="token-45-7" pos="word" morph="none" start_char="6261" end_char="6265">spike</TOKEN>
<TOKEN id="token-45-8" pos="word" morph="none" start_char="6267" end_char="6268">in</TOKEN>
<TOKEN id="token-45-9" pos="word" morph="none" start_char="6270" end_char="6272">car</TOKEN>
<TOKEN id="token-45-10" pos="word" morph="none" start_char="6274" end_char="6280">traffic</TOKEN>
<TOKEN id="token-45-11" pos="word" morph="none" start_char="6282" end_char="6284">was</TOKEN>
<TOKEN id="token-45-12" pos="word" morph="none" start_char="6286" end_char="6290">found</TOKEN>
<TOKEN id="token-45-13" pos="word" morph="none" start_char="6292" end_char="6293">to</TOKEN>
<TOKEN id="token-45-14" pos="word" morph="none" start_char="6295" end_char="6298">have</TOKEN>
<TOKEN id="token-45-15" pos="word" morph="none" start_char="6300" end_char="6307">occurred</TOKEN>
<TOKEN id="token-45-16" pos="word" morph="none" start_char="6309" end_char="6310">in</TOKEN>
<TOKEN id="token-45-17" pos="unknown" morph="none" start_char="6312" end_char="6324">mid-September</TOKEN>
<TOKEN id="token-45-18" pos="word" morph="none" start_char="6326" end_char="6329">2019</TOKEN>
<TOKEN id="token-45-19" pos="punct" morph="none" start_char="6330" end_char="6330">.</TOKEN>
</SEG>
<SEG id="segment-46" start_char="6333" end_char="6563">
<ORIGINAL_TEXT>To ensure they were not reaching faulty conclusions, researchers said they took into account everything that could explain away traffic surges -- from large public gatherings to the possibility of new construction at the hospitals.</ORIGINAL_TEXT>
<TOKEN id="token-46-0" pos="word" morph="none" start_char="6333" end_char="6334">To</TOKEN>
<TOKEN id="token-46-1" pos="word" morph="none" start_char="6336" end_char="6341">ensure</TOKEN>
<TOKEN id="token-46-2" pos="word" morph="none" start_char="6343" end_char="6346">they</TOKEN>
<TOKEN id="token-46-3" pos="word" morph="none" start_char="6348" end_char="6351">were</TOKEN>
<TOKEN id="token-46-4" pos="word" morph="none" start_char="6353" end_char="6355">not</TOKEN>
<TOKEN id="token-46-5" pos="word" morph="none" start_char="6357" end_char="6364">reaching</TOKEN>
<TOKEN id="token-46-6" pos="word" morph="none" start_char="6366" end_char="6371">faulty</TOKEN>
<TOKEN id="token-46-7" pos="word" morph="none" start_char="6373" end_char="6383">conclusions</TOKEN>
<TOKEN id="token-46-8" pos="punct" morph="none" start_char="6384" end_char="6384">,</TOKEN>
<TOKEN id="token-46-9" pos="word" morph="none" start_char="6386" end_char="6396">researchers</TOKEN>
<TOKEN id="token-46-10" pos="word" morph="none" start_char="6398" end_char="6401">said</TOKEN>
<TOKEN id="token-46-11" pos="word" morph="none" start_char="6403" end_char="6406">they</TOKEN>
<TOKEN id="token-46-12" pos="word" morph="none" start_char="6408" end_char="6411">took</TOKEN>
<TOKEN id="token-46-13" pos="word" morph="none" start_char="6413" end_char="6416">into</TOKEN>
<TOKEN id="token-46-14" pos="word" morph="none" start_char="6418" end_char="6424">account</TOKEN>
<TOKEN id="token-46-15" pos="word" morph="none" start_char="6426" end_char="6435">everything</TOKEN>
<TOKEN id="token-46-16" pos="word" morph="none" start_char="6437" end_char="6440">that</TOKEN>
<TOKEN id="token-46-17" pos="word" morph="none" start_char="6442" end_char="6446">could</TOKEN>
<TOKEN id="token-46-18" pos="word" morph="none" start_char="6448" end_char="6454">explain</TOKEN>
<TOKEN id="token-46-19" pos="word" morph="none" start_char="6456" end_char="6459">away</TOKEN>
<TOKEN id="token-46-20" pos="word" morph="none" start_char="6461" end_char="6467">traffic</TOKEN>
<TOKEN id="token-46-21" pos="word" morph="none" start_char="6469" end_char="6474">surges</TOKEN>
<TOKEN id="token-46-22" pos="punct" morph="none" start_char="6476" end_char="6477">--</TOKEN>
<TOKEN id="token-46-23" pos="word" morph="none" start_char="6479" end_char="6482">from</TOKEN>
<TOKEN id="token-46-24" pos="word" morph="none" start_char="6484" end_char="6488">large</TOKEN>
<TOKEN id="token-46-25" pos="word" morph="none" start_char="6490" end_char="6495">public</TOKEN>
<TOKEN id="token-46-26" pos="word" morph="none" start_char="6497" end_char="6506">gatherings</TOKEN>
<TOKEN id="token-46-27" pos="word" morph="none" start_char="6508" end_char="6509">to</TOKEN>
<TOKEN id="token-46-28" pos="word" morph="none" start_char="6511" end_char="6513">the</TOKEN>
<TOKEN id="token-46-29" pos="word" morph="none" start_char="6515" end_char="6525">possibility</TOKEN>
<TOKEN id="token-46-30" pos="word" morph="none" start_char="6527" end_char="6528">of</TOKEN>
<TOKEN id="token-46-31" pos="word" morph="none" start_char="6530" end_char="6532">new</TOKEN>
<TOKEN id="token-46-32" pos="word" morph="none" start_char="6534" end_char="6545">construction</TOKEN>
<TOKEN id="token-46-33" pos="word" morph="none" start_char="6547" end_char="6548">at</TOKEN>
<TOKEN id="token-46-34" pos="word" morph="none" start_char="6550" end_char="6552">the</TOKEN>
<TOKEN id="token-46-35" pos="word" morph="none" start_char="6554" end_char="6562">hospitals</TOKEN>
<TOKEN id="token-46-36" pos="punct" morph="none" start_char="6563" end_char="6563">.</TOKEN>
</SEG>
<SEG id="segment-47" start_char="6565" end_char="6659">
<ORIGINAL_TEXT>Still, they said they found statistically significant increases in the numbers of cars present.</ORIGINAL_TEXT>
<TOKEN id="token-47-0" pos="word" morph="none" start_char="6565" end_char="6569">Still</TOKEN>
<TOKEN id="token-47-1" pos="punct" morph="none" start_char="6570" end_char="6570">,</TOKEN>
<TOKEN id="token-47-2" pos="word" morph="none" start_char="6572" end_char="6575">they</TOKEN>
<TOKEN id="token-47-3" pos="word" morph="none" start_char="6577" end_char="6580">said</TOKEN>
<TOKEN id="token-47-4" pos="word" morph="none" start_char="6582" end_char="6585">they</TOKEN>
<TOKEN id="token-47-5" pos="word" morph="none" start_char="6587" end_char="6591">found</TOKEN>
<TOKEN id="token-47-6" pos="word" morph="none" start_char="6593" end_char="6605">statistically</TOKEN>
<TOKEN id="token-47-7" pos="word" morph="none" start_char="6607" end_char="6617">significant</TOKEN>
<TOKEN id="token-47-8" pos="word" morph="none" start_char="6619" end_char="6627">increases</TOKEN>
<TOKEN id="token-47-9" pos="word" morph="none" start_char="6629" end_char="6630">in</TOKEN>
<TOKEN id="token-47-10" pos="word" morph="none" start_char="6632" end_char="6634">the</TOKEN>
<TOKEN id="token-47-11" pos="word" morph="none" start_char="6636" end_char="6642">numbers</TOKEN>
<TOKEN id="token-47-12" pos="word" morph="none" start_char="6644" end_char="6645">of</TOKEN>
<TOKEN id="token-47-13" pos="word" morph="none" start_char="6647" end_char="6650">cars</TOKEN>
<TOKEN id="token-47-14" pos="word" morph="none" start_char="6652" end_char="6658">present</TOKEN>
<TOKEN id="token-47-15" pos="punct" morph="none" start_char="6659" end_char="6659">.</TOKEN>
</SEG>
<SEG id="segment-48" start_char="6662" end_char="6949">
<ORIGINAL_TEXT>"If you look at all of the images, observations we've ever had of all of these locations since 2018, almost all of the highest car counts are all in the September through December 2019 time frame," said Tom Diamond, president of RS Metrics, which worked with the Brownstein research team.</ORIGINAL_TEXT>
<TOKEN id="token-48-0" pos="punct" morph="none" start_char="6662" end_char="6662">"</TOKEN>
<TOKEN id="token-48-1" pos="word" morph="none" start_char="6663" end_char="6664">If</TOKEN>
<TOKEN id="token-48-2" pos="word" morph="none" start_char="6666" end_char="6668">you</TOKEN>
<TOKEN id="token-48-3" pos="word" morph="none" start_char="6670" end_char="6673">look</TOKEN>
<TOKEN id="token-48-4" pos="word" morph="none" start_char="6675" end_char="6676">at</TOKEN>
<TOKEN id="token-48-5" pos="word" morph="none" start_char="6678" end_char="6680">all</TOKEN>
<TOKEN id="token-48-6" pos="word" morph="none" start_char="6682" end_char="6683">of</TOKEN>
<TOKEN id="token-48-7" pos="word" morph="none" start_char="6685" end_char="6687">the</TOKEN>
<TOKEN id="token-48-8" pos="word" morph="none" start_char="6689" end_char="6694">images</TOKEN>
<TOKEN id="token-48-9" pos="punct" morph="none" start_char="6695" end_char="6695">,</TOKEN>
<TOKEN id="token-48-10" pos="word" morph="none" start_char="6697" end_char="6708">observations</TOKEN>
<TOKEN id="token-48-11" pos="word" morph="none" start_char="6710" end_char="6714">we've</TOKEN>
<TOKEN id="token-48-12" pos="word" morph="none" start_char="6716" end_char="6719">ever</TOKEN>
<TOKEN id="token-48-13" pos="word" morph="none" start_char="6721" end_char="6723">had</TOKEN>
<TOKEN id="token-48-14" pos="word" morph="none" start_char="6725" end_char="6726">of</TOKEN>
<TOKEN id="token-48-15" pos="word" morph="none" start_char="6728" end_char="6730">all</TOKEN>
<TOKEN id="token-48-16" pos="word" morph="none" start_char="6732" end_char="6733">of</TOKEN>
<TOKEN id="token-48-17" pos="word" morph="none" start_char="6735" end_char="6739">these</TOKEN>
<TOKEN id="token-48-18" pos="word" morph="none" start_char="6741" end_char="6749">locations</TOKEN>
<TOKEN id="token-48-19" pos="word" morph="none" start_char="6751" end_char="6755">since</TOKEN>
<TOKEN id="token-48-20" pos="word" morph="none" start_char="6757" end_char="6760">2018</TOKEN>
<TOKEN id="token-48-21" pos="punct" morph="none" start_char="6761" end_char="6761">,</TOKEN>
<TOKEN id="token-48-22" pos="word" morph="none" start_char="6763" end_char="6768">almost</TOKEN>
<TOKEN id="token-48-23" pos="word" morph="none" start_char="6770" end_char="6772">all</TOKEN>
<TOKEN id="token-48-24" pos="word" morph="none" start_char="6774" end_char="6775">of</TOKEN>
<TOKEN id="token-48-25" pos="word" morph="none" start_char="6777" end_char="6779">the</TOKEN>
<TOKEN id="token-48-26" pos="word" morph="none" start_char="6781" end_char="6787">highest</TOKEN>
<TOKEN id="token-48-27" pos="word" morph="none" start_char="6789" end_char="6791">car</TOKEN>
<TOKEN id="token-48-28" pos="word" morph="none" start_char="6793" end_char="6798">counts</TOKEN>
<TOKEN id="token-48-29" pos="word" morph="none" start_char="6800" end_char="6802">are</TOKEN>
<TOKEN id="token-48-30" pos="word" morph="none" start_char="6804" end_char="6806">all</TOKEN>
<TOKEN id="token-48-31" pos="word" morph="none" start_char="6808" end_char="6809">in</TOKEN>
<TOKEN id="token-48-32" pos="word" morph="none" start_char="6811" end_char="6813">the</TOKEN>
<TOKEN id="token-48-33" pos="word" morph="none" start_char="6815" end_char="6823">September</TOKEN>
<TOKEN id="token-48-34" pos="word" morph="none" start_char="6825" end_char="6831">through</TOKEN>
<TOKEN id="token-48-35" pos="word" morph="none" start_char="6833" end_char="6840">December</TOKEN>
<TOKEN id="token-48-36" pos="word" morph="none" start_char="6842" end_char="6845">2019</TOKEN>
<TOKEN id="token-48-37" pos="word" morph="none" start_char="6847" end_char="6850">time</TOKEN>
<TOKEN id="token-48-38" pos="word" morph="none" start_char="6852" end_char="6856">frame</TOKEN>
<TOKEN id="token-48-39" pos="punct" morph="none" start_char="6857" end_char="6858">,"</TOKEN>
<TOKEN id="token-48-40" pos="word" morph="none" start_char="6860" end_char="6863">said</TOKEN>
<TOKEN id="token-48-41" pos="word" morph="none" start_char="6865" end_char="6867">Tom</TOKEN>
<TOKEN id="token-48-42" pos="word" morph="none" start_char="6869" end_char="6875">Diamond</TOKEN>
<TOKEN id="token-48-43" pos="punct" morph="none" start_char="6876" end_char="6876">,</TOKEN>
<TOKEN id="token-48-44" pos="word" morph="none" start_char="6878" end_char="6886">president</TOKEN>
<TOKEN id="token-48-45" pos="word" morph="none" start_char="6888" end_char="6889">of</TOKEN>
<TOKEN id="token-48-46" pos="word" morph="none" start_char="6891" end_char="6892">RS</TOKEN>
<TOKEN id="token-48-47" pos="word" morph="none" start_char="6894" end_char="6900">Metrics</TOKEN>
<TOKEN id="token-48-48" pos="punct" morph="none" start_char="6901" end_char="6901">,</TOKEN>
<TOKEN id="token-48-49" pos="word" morph="none" start_char="6903" end_char="6907">which</TOKEN>
<TOKEN id="token-48-50" pos="word" morph="none" start_char="6909" end_char="6914">worked</TOKEN>
<TOKEN id="token-48-51" pos="word" morph="none" start_char="6916" end_char="6919">with</TOKEN>
<TOKEN id="token-48-52" pos="word" morph="none" start_char="6921" end_char="6923">the</TOKEN>
<TOKEN id="token-48-53" pos="word" morph="none" start_char="6925" end_char="6934">Brownstein</TOKEN>
<TOKEN id="token-48-54" pos="word" morph="none" start_char="6936" end_char="6943">research</TOKEN>
<TOKEN id="token-48-55" pos="word" morph="none" start_char="6945" end_char="6948">team</TOKEN>
<TOKEN id="token-48-56" pos="punct" morph="none" start_char="6949" end_char="6949">.</TOKEN>
</SEG>
<SEG id="segment-49" start_char="6952" end_char="7361">
<ORIGINAL_TEXT>As an initial "validation" of their methodology of extrapolating information about movement through the review of satellite images, researchers said they compared parking lot activity at the Huanan Seafood Market in mid-September, when the market was busy, and after the market was shut down by authorities after reports emerged that the wet market may have been ground zero for the novel coronavirus outbreak.</ORIGINAL_TEXT>
<TOKEN id="token-49-0" pos="word" morph="none" start_char="6952" end_char="6953">As</TOKEN>
<TOKEN id="token-49-1" pos="word" morph="none" start_char="6955" end_char="6956">an</TOKEN>
<TOKEN id="token-49-2" pos="word" morph="none" start_char="6958" end_char="6964">initial</TOKEN>
<TOKEN id="token-49-3" pos="punct" morph="none" start_char="6966" end_char="6966">"</TOKEN>
<TOKEN id="token-49-4" pos="word" morph="none" start_char="6967" end_char="6976">validation</TOKEN>
<TOKEN id="token-49-5" pos="punct" morph="none" start_char="6977" end_char="6977">"</TOKEN>
<TOKEN id="token-49-6" pos="word" morph="none" start_char="6979" end_char="6980">of</TOKEN>
<TOKEN id="token-49-7" pos="word" morph="none" start_char="6982" end_char="6986">their</TOKEN>
<TOKEN id="token-49-8" pos="word" morph="none" start_char="6988" end_char="6998">methodology</TOKEN>
<TOKEN id="token-49-9" pos="word" morph="none" start_char="7000" end_char="7001">of</TOKEN>
<TOKEN id="token-49-10" pos="word" morph="none" start_char="7003" end_char="7015">extrapolating</TOKEN>
<TOKEN id="token-49-11" pos="word" morph="none" start_char="7017" end_char="7027">information</TOKEN>
<TOKEN id="token-49-12" pos="word" morph="none" start_char="7029" end_char="7033">about</TOKEN>
<TOKEN id="token-49-13" pos="word" morph="none" start_char="7035" end_char="7042">movement</TOKEN>
<TOKEN id="token-49-14" pos="word" morph="none" start_char="7044" end_char="7050">through</TOKEN>
<TOKEN id="token-49-15" pos="word" morph="none" start_char="7052" end_char="7054">the</TOKEN>
<TOKEN id="token-49-16" pos="word" morph="none" start_char="7056" end_char="7061">review</TOKEN>
<TOKEN id="token-49-17" pos="word" morph="none" start_char="7063" end_char="7064">of</TOKEN>
<TOKEN id="token-49-18" pos="word" morph="none" start_char="7066" end_char="7074">satellite</TOKEN>
<TOKEN id="token-49-19" pos="word" morph="none" start_char="7076" end_char="7081">images</TOKEN>
<TOKEN id="token-49-20" pos="punct" morph="none" start_char="7082" end_char="7082">,</TOKEN>
<TOKEN id="token-49-21" pos="word" morph="none" start_char="7084" end_char="7094">researchers</TOKEN>
<TOKEN id="token-49-22" pos="word" morph="none" start_char="7096" end_char="7099">said</TOKEN>
<TOKEN id="token-49-23" pos="word" morph="none" start_char="7101" end_char="7104">they</TOKEN>
<TOKEN id="token-49-24" pos="word" morph="none" start_char="7106" end_char="7113">compared</TOKEN>
<TOKEN id="token-49-25" pos="word" morph="none" start_char="7115" end_char="7121">parking</TOKEN>
<TOKEN id="token-49-26" pos="word" morph="none" start_char="7123" end_char="7125">lot</TOKEN>
<TOKEN id="token-49-27" pos="word" morph="none" start_char="7127" end_char="7134">activity</TOKEN>
<TOKEN id="token-49-28" pos="word" morph="none" start_char="7136" end_char="7137">at</TOKEN>
<TOKEN id="token-49-29" pos="word" morph="none" start_char="7139" end_char="7141">the</TOKEN>
<TOKEN id="token-49-30" pos="word" morph="none" start_char="7143" end_char="7148">Huanan</TOKEN>
<TOKEN id="token-49-31" pos="word" morph="none" start_char="7150" end_char="7156">Seafood</TOKEN>
<TOKEN id="token-49-32" pos="word" morph="none" start_char="7158" end_char="7163">Market</TOKEN>
<TOKEN id="token-49-33" pos="word" morph="none" start_char="7165" end_char="7166">in</TOKEN>
<TOKEN id="token-49-34" pos="unknown" morph="none" start_char="7168" end_char="7180">mid-September</TOKEN>
<TOKEN id="token-49-35" pos="punct" morph="none" start_char="7181" end_char="7181">,</TOKEN>
<TOKEN id="token-49-36" pos="word" morph="none" start_char="7183" end_char="7186">when</TOKEN>
<TOKEN id="token-49-37" pos="word" morph="none" start_char="7188" end_char="7190">the</TOKEN>
<TOKEN id="token-49-38" pos="word" morph="none" start_char="7192" end_char="7197">market</TOKEN>
<TOKEN id="token-49-39" pos="word" morph="none" start_char="7199" end_char="7201">was</TOKEN>
<TOKEN id="token-49-40" pos="word" morph="none" start_char="7203" end_char="7206">busy</TOKEN>
<TOKEN id="token-49-41" pos="punct" morph="none" start_char="7207" end_char="7207">,</TOKEN>
<TOKEN id="token-49-42" pos="word" morph="none" start_char="7209" end_char="7211">and</TOKEN>
<TOKEN id="token-49-43" pos="word" morph="none" start_char="7213" end_char="7217">after</TOKEN>
<TOKEN id="token-49-44" pos="word" morph="none" start_char="7219" end_char="7221">the</TOKEN>
<TOKEN id="token-49-45" pos="word" morph="none" start_char="7223" end_char="7228">market</TOKEN>
<TOKEN id="token-49-46" pos="word" morph="none" start_char="7230" end_char="7232">was</TOKEN>
<TOKEN id="token-49-47" pos="word" morph="none" start_char="7234" end_char="7237">shut</TOKEN>
<TOKEN id="token-49-48" pos="word" morph="none" start_char="7239" end_char="7242">down</TOKEN>
<TOKEN id="token-49-49" pos="word" morph="none" start_char="7244" end_char="7245">by</TOKEN>
<TOKEN id="token-49-50" pos="word" morph="none" start_char="7247" end_char="7257">authorities</TOKEN>
<TOKEN id="token-49-51" pos="word" morph="none" start_char="7259" end_char="7263">after</TOKEN>
<TOKEN id="token-49-52" pos="word" morph="none" start_char="7265" end_char="7271">reports</TOKEN>
<TOKEN id="token-49-53" pos="word" morph="none" start_char="7273" end_char="7279">emerged</TOKEN>
<TOKEN id="token-49-54" pos="word" morph="none" start_char="7281" end_char="7284">that</TOKEN>
<TOKEN id="token-49-55" pos="word" morph="none" start_char="7286" end_char="7288">the</TOKEN>
<TOKEN id="token-49-56" pos="word" morph="none" start_char="7290" end_char="7292">wet</TOKEN>
<TOKEN id="token-49-57" pos="word" morph="none" start_char="7294" end_char="7299">market</TOKEN>
<TOKEN id="token-49-58" pos="word" morph="none" start_char="7301" end_char="7303">may</TOKEN>
<TOKEN id="token-49-59" pos="word" morph="none" start_char="7305" end_char="7308">have</TOKEN>
<TOKEN id="token-49-60" pos="word" morph="none" start_char="7310" end_char="7313">been</TOKEN>
<TOKEN id="token-49-61" pos="word" morph="none" start_char="7315" end_char="7320">ground</TOKEN>
<TOKEN id="token-49-62" pos="word" morph="none" start_char="7322" end_char="7325">zero</TOKEN>
<TOKEN id="token-49-63" pos="word" morph="none" start_char="7327" end_char="7329">for</TOKEN>
<TOKEN id="token-49-64" pos="word" morph="none" start_char="7331" end_char="7333">the</TOKEN>
<TOKEN id="token-49-65" pos="word" morph="none" start_char="7335" end_char="7339">novel</TOKEN>
<TOKEN id="token-49-66" pos="word" morph="none" start_char="7341" end_char="7351">coronavirus</TOKEN>
<TOKEN id="token-49-67" pos="word" morph="none" start_char="7353" end_char="7360">outbreak</TOKEN>
<TOKEN id="token-49-68" pos="punct" morph="none" start_char="7361" end_char="7361">.</TOKEN>
</SEG>
<SEG id="segment-50" start_char="7363" end_char="7399">
<ORIGINAL_TEXT>They said they found a marked change.</ORIGINAL_TEXT>
<TOKEN id="token-50-0" pos="word" morph="none" start_char="7363" end_char="7366">They</TOKEN>
<TOKEN id="token-50-1" pos="word" morph="none" start_char="7368" end_char="7371">said</TOKEN>
<TOKEN id="token-50-2" pos="word" morph="none" start_char="7373" end_char="7376">they</TOKEN>
<TOKEN id="token-50-3" pos="word" morph="none" start_char="7378" end_char="7382">found</TOKEN>
<TOKEN id="token-50-4" pos="word" morph="none" start_char="7384" end_char="7384">a</TOKEN>
<TOKEN id="token-50-5" pos="word" morph="none" start_char="7386" end_char="7391">marked</TOKEN>
<TOKEN id="token-50-6" pos="word" morph="none" start_char="7393" end_char="7398">change</TOKEN>
<TOKEN id="token-50-7" pos="punct" morph="none" start_char="7399" end_char="7399">.</TOKEN>
</SEG>
<SEG id="segment-51" start_char="7401" end_char="7534">
<ORIGINAL_TEXT>"The images validate the concept that activity and movement is shown through the lens of these sort of parking lots," said Brownstein.</ORIGINAL_TEXT>
<TOKEN id="token-51-0" pos="punct" morph="none" start_char="7401" end_char="7401">"</TOKEN>
<TOKEN id="token-51-1" pos="word" morph="none" start_char="7402" end_char="7404">The</TOKEN>
<TOKEN id="token-51-2" pos="word" morph="none" start_char="7406" end_char="7411">images</TOKEN>
<TOKEN id="token-51-3" pos="word" morph="none" start_char="7413" end_char="7420">validate</TOKEN>
<TOKEN id="token-51-4" pos="word" morph="none" start_char="7422" end_char="7424">the</TOKEN>
<TOKEN id="token-51-5" pos="word" morph="none" start_char="7426" end_char="7432">concept</TOKEN>
<TOKEN id="token-51-6" pos="word" morph="none" start_char="7434" end_char="7437">that</TOKEN>
<TOKEN id="token-51-7" pos="word" morph="none" start_char="7439" end_char="7446">activity</TOKEN>
<TOKEN id="token-51-8" pos="word" morph="none" start_char="7448" end_char="7450">and</TOKEN>
<TOKEN id="token-51-9" pos="word" morph="none" start_char="7452" end_char="7459">movement</TOKEN>
<TOKEN id="token-51-10" pos="word" morph="none" start_char="7461" end_char="7462">is</TOKEN>
<TOKEN id="token-51-11" pos="word" morph="none" start_char="7464" end_char="7468">shown</TOKEN>
<TOKEN id="token-51-12" pos="word" morph="none" start_char="7470" end_char="7476">through</TOKEN>
<TOKEN id="token-51-13" pos="word" morph="none" start_char="7478" end_char="7480">the</TOKEN>
<TOKEN id="token-51-14" pos="word" morph="none" start_char="7482" end_char="7485">lens</TOKEN>
<TOKEN id="token-51-15" pos="word" morph="none" start_char="7487" end_char="7488">of</TOKEN>
<TOKEN id="token-51-16" pos="word" morph="none" start_char="7490" end_char="7494">these</TOKEN>
<TOKEN id="token-51-17" pos="word" morph="none" start_char="7496" end_char="7499">sort</TOKEN>
<TOKEN id="token-51-18" pos="word" morph="none" start_char="7501" end_char="7502">of</TOKEN>
<TOKEN id="token-51-19" pos="word" morph="none" start_char="7504" end_char="7510">parking</TOKEN>
<TOKEN id="token-51-20" pos="word" morph="none" start_char="7512" end_char="7515">lots</TOKEN>
<TOKEN id="token-51-21" pos="punct" morph="none" start_char="7516" end_char="7517">,"</TOKEN>
<TOKEN id="token-51-22" pos="word" morph="none" start_char="7519" end_char="7522">said</TOKEN>
<TOKEN id="token-51-23" pos="word" morph="none" start_char="7524" end_char="7533">Brownstein</TOKEN>
<TOKEN id="token-51-24" pos="punct" morph="none" start_char="7534" end_char="7534">.</TOKEN>
</SEG>
<SEG id="segment-52" start_char="7537" end_char="7629">
<ORIGINAL_TEXT>The study has been submitted to the journal Nature Digital Medicine and is under peer review.</ORIGINAL_TEXT>
<TOKEN id="token-52-0" pos="word" morph="none" start_char="7537" end_char="7539">The</TOKEN>
<TOKEN id="token-52-1" pos="word" morph="none" start_char="7541" end_char="7545">study</TOKEN>
<TOKEN id="token-52-2" pos="word" morph="none" start_char="7547" end_char="7549">has</TOKEN>
<TOKEN id="token-52-3" pos="word" morph="none" start_char="7551" end_char="7554">been</TOKEN>
<TOKEN id="token-52-4" pos="word" morph="none" start_char="7556" end_char="7564">submitted</TOKEN>
<TOKEN id="token-52-5" pos="word" morph="none" start_char="7566" end_char="7567">to</TOKEN>
<TOKEN id="token-52-6" pos="word" morph="none" start_char="7569" end_char="7571">the</TOKEN>
<TOKEN id="token-52-7" pos="word" morph="none" start_char="7573" end_char="7579">journal</TOKEN>
<TOKEN id="token-52-8" pos="word" morph="none" start_char="7581" end_char="7586">Nature</TOKEN>
<TOKEN id="token-52-9" pos="word" morph="none" start_char="7588" end_char="7594">Digital</TOKEN>
<TOKEN id="token-52-10" pos="word" morph="none" start_char="7596" end_char="7603">Medicine</TOKEN>
<TOKEN id="token-52-11" pos="word" morph="none" start_char="7605" end_char="7607">and</TOKEN>
<TOKEN id="token-52-12" pos="word" morph="none" start_char="7609" end_char="7610">is</TOKEN>
<TOKEN id="token-52-13" pos="word" morph="none" start_char="7612" end_char="7616">under</TOKEN>
<TOKEN id="token-52-14" pos="word" morph="none" start_char="7618" end_char="7621">peer</TOKEN>
<TOKEN id="token-52-15" pos="word" morph="none" start_char="7623" end_char="7628">review</TOKEN>
<TOKEN id="token-52-16" pos="punct" morph="none" start_char="7629" end_char="7629">.</TOKEN>
</SEG>
<SEG id="segment-53" start_char="7631" end_char="7730">
<ORIGINAL_TEXT>It is scheduled to be posted Monday morning on "Dash," Harvard’s preprint server for medical papers.</ORIGINAL_TEXT>
<TOKEN id="token-53-0" pos="word" morph="none" start_char="7631" end_char="7632">It</TOKEN>
<TOKEN id="token-53-1" pos="word" morph="none" start_char="7634" end_char="7635">is</TOKEN>
<TOKEN id="token-53-2" pos="word" morph="none" start_char="7637" end_char="7645">scheduled</TOKEN>
<TOKEN id="token-53-3" pos="word" morph="none" start_char="7647" end_char="7648">to</TOKEN>
<TOKEN id="token-53-4" pos="word" morph="none" start_char="7650" end_char="7651">be</TOKEN>
<TOKEN id="token-53-5" pos="word" morph="none" start_char="7653" end_char="7658">posted</TOKEN>
<TOKEN id="token-53-6" pos="word" morph="none" start_char="7660" end_char="7665">Monday</TOKEN>
<TOKEN id="token-53-7" pos="word" morph="none" start_char="7667" end_char="7673">morning</TOKEN>
<TOKEN id="token-53-8" pos="word" morph="none" start_char="7675" end_char="7676">on</TOKEN>
<TOKEN id="token-53-9" pos="punct" morph="none" start_char="7678" end_char="7678">"</TOKEN>
<TOKEN id="token-53-10" pos="word" morph="none" start_char="7679" end_char="7682">Dash</TOKEN>
<TOKEN id="token-53-11" pos="punct" morph="none" start_char="7683" end_char="7684">,"</TOKEN>
<TOKEN id="token-53-12" pos="word" morph="none" start_char="7686" end_char="7694">Harvard’s</TOKEN>
<TOKEN id="token-53-13" pos="word" morph="none" start_char="7696" end_char="7703">preprint</TOKEN>
<TOKEN id="token-53-14" pos="word" morph="none" start_char="7705" end_char="7710">server</TOKEN>
<TOKEN id="token-53-15" pos="word" morph="none" start_char="7712" end_char="7714">for</TOKEN>
<TOKEN id="token-53-16" pos="word" morph="none" start_char="7716" end_char="7722">medical</TOKEN>
<TOKEN id="token-53-17" pos="word" morph="none" start_char="7724" end_char="7729">papers</TOKEN>
<TOKEN id="token-53-18" pos="punct" morph="none" start_char="7730" end_char="7730">.</TOKEN>
</SEG>
<SEG id="segment-54" start_char="7733" end_char="7801">
<ORIGINAL_TEXT>On Monday morning the website for "Dash" suffered a temporary outage.</ORIGINAL_TEXT>
<TOKEN id="token-54-0" pos="word" morph="none" start_char="7733" end_char="7734">On</TOKEN>
<TOKEN id="token-54-1" pos="word" morph="none" start_char="7736" end_char="7741">Monday</TOKEN>
<TOKEN id="token-54-2" pos="word" morph="none" start_char="7743" end_char="7749">morning</TOKEN>
<TOKEN id="token-54-3" pos="word" morph="none" start_char="7751" end_char="7753">the</TOKEN>
<TOKEN id="token-54-4" pos="word" morph="none" start_char="7755" end_char="7761">website</TOKEN>
<TOKEN id="token-54-5" pos="word" morph="none" start_char="7763" end_char="7765">for</TOKEN>
<TOKEN id="token-54-6" pos="punct" morph="none" start_char="7767" end_char="7767">"</TOKEN>
<TOKEN id="token-54-7" pos="word" morph="none" start_char="7768" end_char="7771">Dash</TOKEN>
<TOKEN id="token-54-8" pos="punct" morph="none" start_char="7772" end_char="7772">"</TOKEN>
<TOKEN id="token-54-9" pos="word" morph="none" start_char="7774" end_char="7781">suffered</TOKEN>
<TOKEN id="token-54-10" pos="word" morph="none" start_char="7783" end_char="7783">a</TOKEN>
<TOKEN id="token-54-11" pos="word" morph="none" start_char="7785" end_char="7793">temporary</TOKEN>
<TOKEN id="token-54-12" pos="word" morph="none" start_char="7795" end_char="7800">outage</TOKEN>
<TOKEN id="token-54-13" pos="punct" morph="none" start_char="7801" end_char="7801">.</TOKEN>
</SEG>
<SEG id="segment-55" start_char="7803" end_char="7895">
<ORIGINAL_TEXT>A spokesperson for Harvard Medical School told ABC News they were investigating the incident.</ORIGINAL_TEXT>
<TOKEN id="token-55-0" pos="word" morph="none" start_char="7803" end_char="7803">A</TOKEN>
<TOKEN id="token-55-1" pos="word" morph="none" start_char="7805" end_char="7816">spokesperson</TOKEN>
<TOKEN id="token-55-2" pos="word" morph="none" start_char="7818" end_char="7820">for</TOKEN>
<TOKEN id="token-55-3" pos="word" morph="none" start_char="7822" end_char="7828">Harvard</TOKEN>
<TOKEN id="token-55-4" pos="word" morph="none" start_char="7830" end_char="7836">Medical</TOKEN>
<TOKEN id="token-55-5" pos="word" morph="none" start_char="7838" end_char="7843">School</TOKEN>
<TOKEN id="token-55-6" pos="word" morph="none" start_char="7845" end_char="7848">told</TOKEN>
<TOKEN id="token-55-7" pos="word" morph="none" start_char="7850" end_char="7852">ABC</TOKEN>
<TOKEN id="token-55-8" pos="word" morph="none" start_char="7854" end_char="7857">News</TOKEN>
<TOKEN id="token-55-9" pos="word" morph="none" start_char="7859" end_char="7862">they</TOKEN>
<TOKEN id="token-55-10" pos="word" morph="none" start_char="7864" end_char="7867">were</TOKEN>
<TOKEN id="token-55-11" pos="word" morph="none" start_char="7869" end_char="7881">investigating</TOKEN>
<TOKEN id="token-55-12" pos="word" morph="none" start_char="7883" end_char="7885">the</TOKEN>
<TOKEN id="token-55-13" pos="word" morph="none" start_char="7887" end_char="7894">incident</TOKEN>
<TOKEN id="token-55-14" pos="punct" morph="none" start_char="7895" end_char="7895">.</TOKEN>
</SEG>
<SEG id="segment-56" start_char="7898" end_char="8119">
<ORIGINAL_TEXT>In conducting the project, RS Metrics, an intelligence-analysis firm that analyzes satellite imagery for corporate clients, employed techniques designed to identify and monitor changes in the patterns of life and business.</ORIGINAL_TEXT>
<TOKEN id="token-56-0" pos="word" morph="none" start_char="7898" end_char="7899">In</TOKEN>
<TOKEN id="token-56-1" pos="word" morph="none" start_char="7901" end_char="7910">conducting</TOKEN>
<TOKEN id="token-56-2" pos="word" morph="none" start_char="7912" end_char="7914">the</TOKEN>
<TOKEN id="token-56-3" pos="word" morph="none" start_char="7916" end_char="7922">project</TOKEN>
<TOKEN id="token-56-4" pos="punct" morph="none" start_char="7923" end_char="7923">,</TOKEN>
<TOKEN id="token-56-5" pos="word" morph="none" start_char="7925" end_char="7926">RS</TOKEN>
<TOKEN id="token-56-6" pos="word" morph="none" start_char="7928" end_char="7934">Metrics</TOKEN>
<TOKEN id="token-56-7" pos="punct" morph="none" start_char="7935" end_char="7935">,</TOKEN>
<TOKEN id="token-56-8" pos="word" morph="none" start_char="7937" end_char="7938">an</TOKEN>
<TOKEN id="token-56-9" pos="unknown" morph="none" start_char="7940" end_char="7960">intelligence-analysis</TOKEN>
<TOKEN id="token-56-10" pos="word" morph="none" start_char="7962" end_char="7965">firm</TOKEN>
<TOKEN id="token-56-11" pos="word" morph="none" start_char="7967" end_char="7970">that</TOKEN>
<TOKEN id="token-56-12" pos="word" morph="none" start_char="7972" end_char="7979">analyzes</TOKEN>
<TOKEN id="token-56-13" pos="word" morph="none" start_char="7981" end_char="7989">satellite</TOKEN>
<TOKEN id="token-56-14" pos="word" morph="none" start_char="7991" end_char="7997">imagery</TOKEN>
<TOKEN id="token-56-15" pos="word" morph="none" start_char="7999" end_char="8001">for</TOKEN>
<TOKEN id="token-56-16" pos="word" morph="none" start_char="8003" end_char="8011">corporate</TOKEN>
<TOKEN id="token-56-17" pos="word" morph="none" start_char="8013" end_char="8019">clients</TOKEN>
<TOKEN id="token-56-18" pos="punct" morph="none" start_char="8020" end_char="8020">,</TOKEN>
<TOKEN id="token-56-19" pos="word" morph="none" start_char="8022" end_char="8029">employed</TOKEN>
<TOKEN id="token-56-20" pos="word" morph="none" start_char="8031" end_char="8040">techniques</TOKEN>
<TOKEN id="token-56-21" pos="word" morph="none" start_char="8042" end_char="8049">designed</TOKEN>
<TOKEN id="token-56-22" pos="word" morph="none" start_char="8051" end_char="8052">to</TOKEN>
<TOKEN id="token-56-23" pos="word" morph="none" start_char="8054" end_char="8061">identify</TOKEN>
<TOKEN id="token-56-24" pos="word" morph="none" start_char="8063" end_char="8065">and</TOKEN>
<TOKEN id="token-56-25" pos="word" morph="none" start_char="8067" end_char="8073">monitor</TOKEN>
<TOKEN id="token-56-26" pos="word" morph="none" start_char="8075" end_char="8081">changes</TOKEN>
<TOKEN id="token-56-27" pos="word" morph="none" start_char="8083" end_char="8084">in</TOKEN>
<TOKEN id="token-56-28" pos="word" morph="none" start_char="8086" end_char="8088">the</TOKEN>
<TOKEN id="token-56-29" pos="word" morph="none" start_char="8090" end_char="8097">patterns</TOKEN>
<TOKEN id="token-56-30" pos="word" morph="none" start_char="8099" end_char="8100">of</TOKEN>
<TOKEN id="token-56-31" pos="word" morph="none" start_char="8102" end_char="8105">life</TOKEN>
<TOKEN id="token-56-32" pos="word" morph="none" start_char="8107" end_char="8109">and</TOKEN>
<TOKEN id="token-56-33" pos="word" morph="none" start_char="8111" end_char="8118">business</TOKEN>
<TOKEN id="token-56-34" pos="punct" morph="none" start_char="8119" end_char="8119">.</TOKEN>
</SEG>
<SEG id="segment-57" start_char="8122" end_char="8392">
<ORIGINAL_TEXT>It’s similar to work done by analysts at the Central Intelligence Agency and the Defense Intelligence Agency, who pore over images each day to try to figure out what is happening on the ground – especially in places where governments restrict the flow of people and news.</ORIGINAL_TEXT>
<TOKEN id="token-57-0" pos="word" morph="none" start_char="8122" end_char="8125">It’s</TOKEN>
<TOKEN id="token-57-1" pos="word" morph="none" start_char="8127" end_char="8133">similar</TOKEN>
<TOKEN id="token-57-2" pos="word" morph="none" start_char="8135" end_char="8136">to</TOKEN>
<TOKEN id="token-57-3" pos="word" morph="none" start_char="8138" end_char="8141">work</TOKEN>
<TOKEN id="token-57-4" pos="word" morph="none" start_char="8143" end_char="8146">done</TOKEN>
<TOKEN id="token-57-5" pos="word" morph="none" start_char="8148" end_char="8149">by</TOKEN>
<TOKEN id="token-57-6" pos="word" morph="none" start_char="8151" end_char="8158">analysts</TOKEN>
<TOKEN id="token-57-7" pos="word" morph="none" start_char="8160" end_char="8161">at</TOKEN>
<TOKEN id="token-57-8" pos="word" morph="none" start_char="8163" end_char="8165">the</TOKEN>
<TOKEN id="token-57-9" pos="word" morph="none" start_char="8167" end_char="8173">Central</TOKEN>
<TOKEN id="token-57-10" pos="word" morph="none" start_char="8175" end_char="8186">Intelligence</TOKEN>
<TOKEN id="token-57-11" pos="word" morph="none" start_char="8188" end_char="8193">Agency</TOKEN>
<TOKEN id="token-57-12" pos="word" morph="none" start_char="8195" end_char="8197">and</TOKEN>
<TOKEN id="token-57-13" pos="word" morph="none" start_char="8199" end_char="8201">the</TOKEN>
<TOKEN id="token-57-14" pos="word" morph="none" start_char="8203" end_char="8209">Defense</TOKEN>
<TOKEN id="token-57-15" pos="word" morph="none" start_char="8211" end_char="8222">Intelligence</TOKEN>
<TOKEN id="token-57-16" pos="word" morph="none" start_char="8224" end_char="8229">Agency</TOKEN>
<TOKEN id="token-57-17" pos="punct" morph="none" start_char="8230" end_char="8230">,</TOKEN>
<TOKEN id="token-57-18" pos="word" morph="none" start_char="8232" end_char="8234">who</TOKEN>
<TOKEN id="token-57-19" pos="word" morph="none" start_char="8236" end_char="8239">pore</TOKEN>
<TOKEN id="token-57-20" pos="word" morph="none" start_char="8241" end_char="8244">over</TOKEN>
<TOKEN id="token-57-21" pos="word" morph="none" start_char="8246" end_char="8251">images</TOKEN>
<TOKEN id="token-57-22" pos="word" morph="none" start_char="8253" end_char="8256">each</TOKEN>
<TOKEN id="token-57-23" pos="word" morph="none" start_char="8258" end_char="8260">day</TOKEN>
<TOKEN id="token-57-24" pos="word" morph="none" start_char="8262" end_char="8263">to</TOKEN>
<TOKEN id="token-57-25" pos="word" morph="none" start_char="8265" end_char="8267">try</TOKEN>
<TOKEN id="token-57-26" pos="word" morph="none" start_char="8269" end_char="8270">to</TOKEN>
<TOKEN id="token-57-27" pos="word" morph="none" start_char="8272" end_char="8277">figure</TOKEN>
<TOKEN id="token-57-28" pos="word" morph="none" start_char="8279" end_char="8281">out</TOKEN>
<TOKEN id="token-57-29" pos="word" morph="none" start_char="8283" end_char="8286">what</TOKEN>
<TOKEN id="token-57-30" pos="word" morph="none" start_char="8288" end_char="8289">is</TOKEN>
<TOKEN id="token-57-31" pos="word" morph="none" start_char="8291" end_char="8299">happening</TOKEN>
<TOKEN id="token-57-32" pos="word" morph="none" start_char="8301" end_char="8302">on</TOKEN>
<TOKEN id="token-57-33" pos="word" morph="none" start_char="8304" end_char="8306">the</TOKEN>
<TOKEN id="token-57-34" pos="word" morph="none" start_char="8308" end_char="8313">ground</TOKEN>
<TOKEN id="token-57-35" pos="punct" morph="none" start_char="8315" end_char="8315">–</TOKEN>
<TOKEN id="token-57-36" pos="word" morph="none" start_char="8317" end_char="8326">especially</TOKEN>
<TOKEN id="token-57-37" pos="word" morph="none" start_char="8328" end_char="8329">in</TOKEN>
<TOKEN id="token-57-38" pos="word" morph="none" start_char="8331" end_char="8336">places</TOKEN>
<TOKEN id="token-57-39" pos="word" morph="none" start_char="8338" end_char="8342">where</TOKEN>
<TOKEN id="token-57-40" pos="word" morph="none" start_char="8344" end_char="8354">governments</TOKEN>
<TOKEN id="token-57-41" pos="word" morph="none" start_char="8356" end_char="8363">restrict</TOKEN>
<TOKEN id="token-57-42" pos="word" morph="none" start_char="8365" end_char="8367">the</TOKEN>
<TOKEN id="token-57-43" pos="word" morph="none" start_char="8369" end_char="8372">flow</TOKEN>
<TOKEN id="token-57-44" pos="word" morph="none" start_char="8374" end_char="8375">of</TOKEN>
<TOKEN id="token-57-45" pos="word" morph="none" start_char="8377" end_char="8382">people</TOKEN>
<TOKEN id="token-57-46" pos="word" morph="none" start_char="8384" end_char="8386">and</TOKEN>
<TOKEN id="token-57-47" pos="word" morph="none" start_char="8388" end_char="8391">news</TOKEN>
<TOKEN id="token-57-48" pos="punct" morph="none" start_char="8392" end_char="8392">.</TOKEN>
</SEG>
<SEG id="segment-58" start_char="8395" end_char="8613">
<ORIGINAL_TEXT>Diamond told ABC News the Wuhan region was clearly experiencing a widespread health problem in the months before China’s government acknowledged publicly that a contagion was coursing through the densely populated city.</ORIGINAL_TEXT>
<TOKEN id="token-58-0" pos="word" morph="none" start_char="8395" end_char="8401">Diamond</TOKEN>
<TOKEN id="token-58-1" pos="word" morph="none" start_char="8403" end_char="8406">told</TOKEN>
<TOKEN id="token-58-2" pos="word" morph="none" start_char="8408" end_char="8410">ABC</TOKEN>
<TOKEN id="token-58-3" pos="word" morph="none" start_char="8412" end_char="8415">News</TOKEN>
<TOKEN id="token-58-4" pos="word" morph="none" start_char="8417" end_char="8419">the</TOKEN>
<TOKEN id="token-58-5" pos="word" morph="none" start_char="8421" end_char="8425">Wuhan</TOKEN>
<TOKEN id="token-58-6" pos="word" morph="none" start_char="8427" end_char="8432">region</TOKEN>
<TOKEN id="token-58-7" pos="word" morph="none" start_char="8434" end_char="8436">was</TOKEN>
<TOKEN id="token-58-8" pos="word" morph="none" start_char="8438" end_char="8444">clearly</TOKEN>
<TOKEN id="token-58-9" pos="word" morph="none" start_char="8446" end_char="8457">experiencing</TOKEN>
<TOKEN id="token-58-10" pos="word" morph="none" start_char="8459" end_char="8459">a</TOKEN>
<TOKEN id="token-58-11" pos="word" morph="none" start_char="8461" end_char="8470">widespread</TOKEN>
<TOKEN id="token-58-12" pos="word" morph="none" start_char="8472" end_char="8477">health</TOKEN>
<TOKEN id="token-58-13" pos="word" morph="none" start_char="8479" end_char="8485">problem</TOKEN>
<TOKEN id="token-58-14" pos="word" morph="none" start_char="8487" end_char="8488">in</TOKEN>
<TOKEN id="token-58-15" pos="word" morph="none" start_char="8490" end_char="8492">the</TOKEN>
<TOKEN id="token-58-16" pos="word" morph="none" start_char="8494" end_char="8499">months</TOKEN>
<TOKEN id="token-58-17" pos="word" morph="none" start_char="8501" end_char="8506">before</TOKEN>
<TOKEN id="token-58-18" pos="word" morph="none" start_char="8508" end_char="8514">China’s</TOKEN>
<TOKEN id="token-58-19" pos="word" morph="none" start_char="8516" end_char="8525">government</TOKEN>
<TOKEN id="token-58-20" pos="word" morph="none" start_char="8527" end_char="8538">acknowledged</TOKEN>
<TOKEN id="token-58-21" pos="word" morph="none" start_char="8540" end_char="8547">publicly</TOKEN>
<TOKEN id="token-58-22" pos="word" morph="none" start_char="8549" end_char="8552">that</TOKEN>
<TOKEN id="token-58-23" pos="word" morph="none" start_char="8554" end_char="8554">a</TOKEN>
<TOKEN id="token-58-24" pos="word" morph="none" start_char="8556" end_char="8564">contagion</TOKEN>
<TOKEN id="token-58-25" pos="word" morph="none" start_char="8566" end_char="8568">was</TOKEN>
<TOKEN id="token-58-26" pos="word" morph="none" start_char="8570" end_char="8577">coursing</TOKEN>
<TOKEN id="token-58-27" pos="word" morph="none" start_char="8579" end_char="8585">through</TOKEN>
<TOKEN id="token-58-28" pos="word" morph="none" start_char="8587" end_char="8589">the</TOKEN>
<TOKEN id="token-58-29" pos="word" morph="none" start_char="8591" end_char="8597">densely</TOKEN>
<TOKEN id="token-58-30" pos="word" morph="none" start_char="8599" end_char="8607">populated</TOKEN>
<TOKEN id="token-58-31" pos="word" morph="none" start_char="8609" end_char="8612">city</TOKEN>
<TOKEN id="token-58-32" pos="punct" morph="none" start_char="8613" end_char="8613">.</TOKEN>
</SEG>
<SEG id="segment-59" start_char="8615" end_char="8757">
<ORIGINAL_TEXT>That announcement came on New Year’s Eve when the Wuhan Municipal Health Commission, China reported a "cluster" of pneumonia cases in its city.</ORIGINAL_TEXT>
<TOKEN id="token-59-0" pos="word" morph="none" start_char="8615" end_char="8618">That</TOKEN>
<TOKEN id="token-59-1" pos="word" morph="none" start_char="8620" end_char="8631">announcement</TOKEN>
<TOKEN id="token-59-2" pos="word" morph="none" start_char="8633" end_char="8636">came</TOKEN>
<TOKEN id="token-59-3" pos="word" morph="none" start_char="8638" end_char="8639">on</TOKEN>
<TOKEN id="token-59-4" pos="word" morph="none" start_char="8641" end_char="8643">New</TOKEN>
<TOKEN id="token-59-5" pos="word" morph="none" start_char="8645" end_char="8650">Year’s</TOKEN>
<TOKEN id="token-59-6" pos="word" morph="none" start_char="8652" end_char="8654">Eve</TOKEN>
<TOKEN id="token-59-7" pos="word" morph="none" start_char="8656" end_char="8659">when</TOKEN>
<TOKEN id="token-59-8" pos="word" morph="none" start_char="8661" end_char="8663">the</TOKEN>
<TOKEN id="token-59-9" pos="word" morph="none" start_char="8665" end_char="8669">Wuhan</TOKEN>
<TOKEN id="token-59-10" pos="word" morph="none" start_char="8671" end_char="8679">Municipal</TOKEN>
<TOKEN id="token-59-11" pos="word" morph="none" start_char="8681" end_char="8686">Health</TOKEN>
<TOKEN id="token-59-12" pos="word" morph="none" start_char="8688" end_char="8697">Commission</TOKEN>
<TOKEN id="token-59-13" pos="punct" morph="none" start_char="8698" end_char="8698">,</TOKEN>
<TOKEN id="token-59-14" pos="word" morph="none" start_char="8700" end_char="8704">China</TOKEN>
<TOKEN id="token-59-15" pos="word" morph="none" start_char="8706" end_char="8713">reported</TOKEN>
<TOKEN id="token-59-16" pos="word" morph="none" start_char="8715" end_char="8715">a</TOKEN>
<TOKEN id="token-59-17" pos="punct" morph="none" start_char="8717" end_char="8717">"</TOKEN>
<TOKEN id="token-59-18" pos="word" morph="none" start_char="8718" end_char="8724">cluster</TOKEN>
<TOKEN id="token-59-19" pos="punct" morph="none" start_char="8725" end_char="8725">"</TOKEN>
<TOKEN id="token-59-20" pos="word" morph="none" start_char="8727" end_char="8728">of</TOKEN>
<TOKEN id="token-59-21" pos="word" morph="none" start_char="8730" end_char="8738">pneumonia</TOKEN>
<TOKEN id="token-59-22" pos="word" morph="none" start_char="8740" end_char="8744">cases</TOKEN>
<TOKEN id="token-59-23" pos="word" morph="none" start_char="8746" end_char="8747">in</TOKEN>
<TOKEN id="token-59-24" pos="word" morph="none" start_char="8749" end_char="8751">its</TOKEN>
<TOKEN id="token-59-25" pos="word" morph="none" start_char="8753" end_char="8756">city</TOKEN>
<TOKEN id="token-59-26" pos="punct" morph="none" start_char="8757" end_char="8757">.</TOKEN>
</SEG>
<SEG id="segment-60" start_char="8760" end_char="8928">
<ORIGINAL_TEXT>"At all the larger hospitals in Wuhan, we measured the highest traffic we’ve seen in over two years during the September through December 2019 time frame," Diamond said.</ORIGINAL_TEXT>
<TOKEN id="token-60-0" pos="punct" morph="none" start_char="8760" end_char="8760">"</TOKEN>
<TOKEN id="token-60-1" pos="word" morph="none" start_char="8761" end_char="8762">At</TOKEN>
<TOKEN id="token-60-2" pos="word" morph="none" start_char="8764" end_char="8766">all</TOKEN>
<TOKEN id="token-60-3" pos="word" morph="none" start_char="8768" end_char="8770">the</TOKEN>
<TOKEN id="token-60-4" pos="word" morph="none" start_char="8772" end_char="8777">larger</TOKEN>
<TOKEN id="token-60-5" pos="word" morph="none" start_char="8779" end_char="8787">hospitals</TOKEN>
<TOKEN id="token-60-6" pos="word" morph="none" start_char="8789" end_char="8790">in</TOKEN>
<TOKEN id="token-60-7" pos="word" morph="none" start_char="8792" end_char="8796">Wuhan</TOKEN>
<TOKEN id="token-60-8" pos="punct" morph="none" start_char="8797" end_char="8797">,</TOKEN>
<TOKEN id="token-60-9" pos="word" morph="none" start_char="8799" end_char="8800">we</TOKEN>
<TOKEN id="token-60-10" pos="word" morph="none" start_char="8802" end_char="8809">measured</TOKEN>
<TOKEN id="token-60-11" pos="word" morph="none" start_char="8811" end_char="8813">the</TOKEN>
<TOKEN id="token-60-12" pos="word" morph="none" start_char="8815" end_char="8821">highest</TOKEN>
<TOKEN id="token-60-13" pos="word" morph="none" start_char="8823" end_char="8829">traffic</TOKEN>
<TOKEN id="token-60-14" pos="word" morph="none" start_char="8831" end_char="8835">we’ve</TOKEN>
<TOKEN id="token-60-15" pos="word" morph="none" start_char="8837" end_char="8840">seen</TOKEN>
<TOKEN id="token-60-16" pos="word" morph="none" start_char="8842" end_char="8843">in</TOKEN>
<TOKEN id="token-60-17" pos="word" morph="none" start_char="8845" end_char="8848">over</TOKEN>
<TOKEN id="token-60-18" pos="word" morph="none" start_char="8850" end_char="8852">two</TOKEN>
<TOKEN id="token-60-19" pos="word" morph="none" start_char="8854" end_char="8858">years</TOKEN>
<TOKEN id="token-60-20" pos="word" morph="none" start_char="8860" end_char="8865">during</TOKEN>
<TOKEN id="token-60-21" pos="word" morph="none" start_char="8867" end_char="8869">the</TOKEN>
<TOKEN id="token-60-22" pos="word" morph="none" start_char="8871" end_char="8879">September</TOKEN>
<TOKEN id="token-60-23" pos="word" morph="none" start_char="8881" end_char="8887">through</TOKEN>
<TOKEN id="token-60-24" pos="word" morph="none" start_char="8889" end_char="8896">December</TOKEN>
<TOKEN id="token-60-25" pos="word" morph="none" start_char="8898" end_char="8901">2019</TOKEN>
<TOKEN id="token-60-26" pos="word" morph="none" start_char="8903" end_char="8906">time</TOKEN>
<TOKEN id="token-60-27" pos="word" morph="none" start_char="8908" end_char="8912">frame</TOKEN>
<TOKEN id="token-60-28" pos="punct" morph="none" start_char="8913" end_char="8914">,"</TOKEN>
<TOKEN id="token-60-29" pos="word" morph="none" start_char="8916" end_char="8922">Diamond</TOKEN>
<TOKEN id="token-60-30" pos="word" morph="none" start_char="8924" end_char="8927">said</TOKEN>
<TOKEN id="token-60-31" pos="punct" morph="none" start_char="8928" end_char="8928">.</TOKEN>
</SEG>
<SEG id="segment-61" start_char="8930" end_char="9037">
<ORIGINAL_TEXT>"Our company is used to measuring tiny changes, like 2% to 3% growth in a Cabella’s or Wal-Mart parking lot.</ORIGINAL_TEXT>
<TOKEN id="token-61-0" pos="punct" morph="none" start_char="8930" end_char="8930">"</TOKEN>
<TOKEN id="token-61-1" pos="word" morph="none" start_char="8931" end_char="8933">Our</TOKEN>
<TOKEN id="token-61-2" pos="word" morph="none" start_char="8935" end_char="8941">company</TOKEN>
<TOKEN id="token-61-3" pos="word" morph="none" start_char="8943" end_char="8944">is</TOKEN>
<TOKEN id="token-61-4" pos="word" morph="none" start_char="8946" end_char="8949">used</TOKEN>
<TOKEN id="token-61-5" pos="word" morph="none" start_char="8951" end_char="8952">to</TOKEN>
<TOKEN id="token-61-6" pos="word" morph="none" start_char="8954" end_char="8962">measuring</TOKEN>
<TOKEN id="token-61-7" pos="word" morph="none" start_char="8964" end_char="8967">tiny</TOKEN>
<TOKEN id="token-61-8" pos="word" morph="none" start_char="8969" end_char="8975">changes</TOKEN>
<TOKEN id="token-61-9" pos="punct" morph="none" start_char="8976" end_char="8976">,</TOKEN>
<TOKEN id="token-61-10" pos="word" morph="none" start_char="8978" end_char="8981">like</TOKEN>
<TOKEN id="token-61-11" pos="word" morph="none" start_char="8983" end_char="8983">2</TOKEN>
<TOKEN id="token-61-12" pos="punct" morph="none" start_char="8984" end_char="8984">%</TOKEN>
<TOKEN id="token-61-13" pos="word" morph="none" start_char="8986" end_char="8987">to</TOKEN>
<TOKEN id="token-61-14" pos="word" morph="none" start_char="8989" end_char="8989">3</TOKEN>
<TOKEN id="token-61-15" pos="punct" morph="none" start_char="8990" end_char="8990">%</TOKEN>
<TOKEN id="token-61-16" pos="word" morph="none" start_char="8992" end_char="8997">growth</TOKEN>
<TOKEN id="token-61-17" pos="word" morph="none" start_char="8999" end_char="9000">in</TOKEN>
<TOKEN id="token-61-18" pos="word" morph="none" start_char="9002" end_char="9002">a</TOKEN>
<TOKEN id="token-61-19" pos="word" morph="none" start_char="9004" end_char="9012">Cabella’s</TOKEN>
<TOKEN id="token-61-20" pos="word" morph="none" start_char="9014" end_char="9015">or</TOKEN>
<TOKEN id="token-61-21" pos="unknown" morph="none" start_char="9017" end_char="9024">Wal-Mart</TOKEN>
<TOKEN id="token-61-22" pos="word" morph="none" start_char="9026" end_char="9032">parking</TOKEN>
<TOKEN id="token-61-23" pos="word" morph="none" start_char="9034" end_char="9036">lot</TOKEN>
<TOKEN id="token-61-24" pos="punct" morph="none" start_char="9037" end_char="9037">.</TOKEN>
</SEG>
<SEG id="segment-62" start_char="9039" end_char="9065">
<ORIGINAL_TEXT>That was not the case here.</ORIGINAL_TEXT>
<TOKEN id="token-62-0" pos="word" morph="none" start_char="9039" end_char="9042">That</TOKEN>
<TOKEN id="token-62-1" pos="word" morph="none" start_char="9044" end_char="9046">was</TOKEN>
<TOKEN id="token-62-2" pos="word" morph="none" start_char="9048" end_char="9050">not</TOKEN>
<TOKEN id="token-62-3" pos="word" morph="none" start_char="9052" end_char="9054">the</TOKEN>
<TOKEN id="token-62-4" pos="word" morph="none" start_char="9056" end_char="9059">case</TOKEN>
<TOKEN id="token-62-5" pos="word" morph="none" start_char="9061" end_char="9064">here</TOKEN>
<TOKEN id="token-62-6" pos="punct" morph="none" start_char="9065" end_char="9065">.</TOKEN>
</SEG>
<SEG id="segment-63" start_char="9067" end_char="9101">
<ORIGINAL_TEXT>Here, there is a very clear trend."</ORIGINAL_TEXT>
<TOKEN id="token-63-0" pos="word" morph="none" start_char="9067" end_char="9070">Here</TOKEN>
<TOKEN id="token-63-1" pos="punct" morph="none" start_char="9071" end_char="9071">,</TOKEN>
<TOKEN id="token-63-2" pos="word" morph="none" start_char="9073" end_char="9077">there</TOKEN>
<TOKEN id="token-63-3" pos="word" morph="none" start_char="9079" end_char="9080">is</TOKEN>
<TOKEN id="token-63-4" pos="word" morph="none" start_char="9082" end_char="9082">a</TOKEN>
<TOKEN id="token-63-5" pos="word" morph="none" start_char="9084" end_char="9087">very</TOKEN>
<TOKEN id="token-63-6" pos="word" morph="none" start_char="9089" end_char="9093">clear</TOKEN>
<TOKEN id="token-63-7" pos="word" morph="none" start_char="9095" end_char="9099">trend</TOKEN>
<TOKEN id="token-63-8" pos="punct" morph="none" start_char="9100" end_char="9101">."</TOKEN>
</SEG>
<SEG id="segment-64" start_char="9104" end_char="9418">
<ORIGINAL_TEXT>Former acting Homeland Security Undersecretary John Cohen, who oversaw DHS intelligence operations during the Obama administration, said the new research suggests that COVID-19, which has already killed more than 110,000 Americans, was likely brought to the U.S. by travelers from Wuhan long before it was detected.</ORIGINAL_TEXT>
<TOKEN id="token-64-0" pos="word" morph="none" start_char="9104" end_char="9109">Former</TOKEN>
<TOKEN id="token-64-1" pos="word" morph="none" start_char="9111" end_char="9116">acting</TOKEN>
<TOKEN id="token-64-2" pos="word" morph="none" start_char="9118" end_char="9125">Homeland</TOKEN>
<TOKEN id="token-64-3" pos="word" morph="none" start_char="9127" end_char="9134">Security</TOKEN>
<TOKEN id="token-64-4" pos="word" morph="none" start_char="9136" end_char="9149">Undersecretary</TOKEN>
<TOKEN id="token-64-5" pos="word" morph="none" start_char="9151" end_char="9154">John</TOKEN>
<TOKEN id="token-64-6" pos="word" morph="none" start_char="9156" end_char="9160">Cohen</TOKEN>
<TOKEN id="token-64-7" pos="punct" morph="none" start_char="9161" end_char="9161">,</TOKEN>
<TOKEN id="token-64-8" pos="word" morph="none" start_char="9163" end_char="9165">who</TOKEN>
<TOKEN id="token-64-9" pos="word" morph="none" start_char="9167" end_char="9173">oversaw</TOKEN>
<TOKEN id="token-64-10" pos="word" morph="none" start_char="9175" end_char="9177">DHS</TOKEN>
<TOKEN id="token-64-11" pos="word" morph="none" start_char="9179" end_char="9190">intelligence</TOKEN>
<TOKEN id="token-64-12" pos="word" morph="none" start_char="9192" end_char="9201">operations</TOKEN>
<TOKEN id="token-64-13" pos="word" morph="none" start_char="9203" end_char="9208">during</TOKEN>
<TOKEN id="token-64-14" pos="word" morph="none" start_char="9210" end_char="9212">the</TOKEN>
<TOKEN id="token-64-15" pos="word" morph="none" start_char="9214" end_char="9218">Obama</TOKEN>
<TOKEN id="token-64-16" pos="word" morph="none" start_char="9220" end_char="9233">administration</TOKEN>
<TOKEN id="token-64-17" pos="punct" morph="none" start_char="9234" end_char="9234">,</TOKEN>
<TOKEN id="token-64-18" pos="word" morph="none" start_char="9236" end_char="9239">said</TOKEN>
<TOKEN id="token-64-19" pos="word" morph="none" start_char="9241" end_char="9243">the</TOKEN>
<TOKEN id="token-64-20" pos="word" morph="none" start_char="9245" end_char="9247">new</TOKEN>
<TOKEN id="token-64-21" pos="word" morph="none" start_char="9249" end_char="9256">research</TOKEN>
<TOKEN id="token-64-22" pos="word" morph="none" start_char="9258" end_char="9265">suggests</TOKEN>
<TOKEN id="token-64-23" pos="word" morph="none" start_char="9267" end_char="9270">that</TOKEN>
<TOKEN id="token-64-24" pos="unknown" morph="none" start_char="9272" end_char="9279">COVID-19</TOKEN>
<TOKEN id="token-64-25" pos="punct" morph="none" start_char="9280" end_char="9280">,</TOKEN>
<TOKEN id="token-64-26" pos="word" morph="none" start_char="9282" end_char="9286">which</TOKEN>
<TOKEN id="token-64-27" pos="word" morph="none" start_char="9288" end_char="9290">has</TOKEN>
<TOKEN id="token-64-28" pos="word" morph="none" start_char="9292" end_char="9298">already</TOKEN>
<TOKEN id="token-64-29" pos="word" morph="none" start_char="9300" end_char="9305">killed</TOKEN>
<TOKEN id="token-64-30" pos="word" morph="none" start_char="9307" end_char="9310">more</TOKEN>
<TOKEN id="token-64-31" pos="word" morph="none" start_char="9312" end_char="9315">than</TOKEN>
<TOKEN id="token-64-32" pos="unknown" morph="none" start_char="9317" end_char="9323">110,000</TOKEN>
<TOKEN id="token-64-33" pos="word" morph="none" start_char="9325" end_char="9333">Americans</TOKEN>
<TOKEN id="token-64-34" pos="punct" morph="none" start_char="9334" end_char="9334">,</TOKEN>
<TOKEN id="token-64-35" pos="word" morph="none" start_char="9336" end_char="9338">was</TOKEN>
<TOKEN id="token-64-36" pos="word" morph="none" start_char="9340" end_char="9345">likely</TOKEN>
<TOKEN id="token-64-37" pos="word" morph="none" start_char="9347" end_char="9353">brought</TOKEN>
<TOKEN id="token-64-38" pos="word" morph="none" start_char="9355" end_char="9356">to</TOKEN>
<TOKEN id="token-64-39" pos="word" morph="none" start_char="9358" end_char="9360">the</TOKEN>
<TOKEN id="token-64-40" pos="unknown" morph="none" start_char="9362" end_char="9364">U.S</TOKEN>
<TOKEN id="token-64-41" pos="punct" morph="none" start_char="9365" end_char="9365">.</TOKEN>
<TOKEN id="token-64-42" pos="word" morph="none" start_char="9367" end_char="9368">by</TOKEN>
<TOKEN id="token-64-43" pos="word" morph="none" start_char="9370" end_char="9378">travelers</TOKEN>
<TOKEN id="token-64-44" pos="word" morph="none" start_char="9380" end_char="9383">from</TOKEN>
<TOKEN id="token-64-45" pos="word" morph="none" start_char="9385" end_char="9389">Wuhan</TOKEN>
<TOKEN id="token-64-46" pos="word" morph="none" start_char="9391" end_char="9394">long</TOKEN>
<TOKEN id="token-64-47" pos="word" morph="none" start_char="9396" end_char="9401">before</TOKEN>
<TOKEN id="token-64-48" pos="word" morph="none" start_char="9403" end_char="9404">it</TOKEN>
<TOKEN id="token-64-49" pos="word" morph="none" start_char="9406" end_char="9408">was</TOKEN>
<TOKEN id="token-64-50" pos="word" morph="none" start_char="9410" end_char="9417">detected</TOKEN>
<TOKEN id="token-64-51" pos="punct" morph="none" start_char="9418" end_char="9418">.</TOKEN>
</SEG>
<SEG id="segment-65" start_char="9421" end_char="9708">
<ORIGINAL_TEXT>"This study raises serious questions about whether the coronavirus was first introduced into the United States earlier than previously reported and whether measures announced in late January restricting travel from China were too little too late," said Cohen, now an ABC News contributor.</ORIGINAL_TEXT>
<TOKEN id="token-65-0" pos="punct" morph="none" start_char="9421" end_char="9421">"</TOKEN>
<TOKEN id="token-65-1" pos="word" morph="none" start_char="9422" end_char="9425">This</TOKEN>
<TOKEN id="token-65-2" pos="word" morph="none" start_char="9427" end_char="9431">study</TOKEN>
<TOKEN id="token-65-3" pos="word" morph="none" start_char="9433" end_char="9438">raises</TOKEN>
<TOKEN id="token-65-4" pos="word" morph="none" start_char="9440" end_char="9446">serious</TOKEN>
<TOKEN id="token-65-5" pos="word" morph="none" start_char="9448" end_char="9456">questions</TOKEN>
<TOKEN id="token-65-6" pos="word" morph="none" start_char="9458" end_char="9462">about</TOKEN>
<TOKEN id="token-65-7" pos="word" morph="none" start_char="9464" end_char="9470">whether</TOKEN>
<TOKEN id="token-65-8" pos="word" morph="none" start_char="9472" end_char="9474">the</TOKEN>
<TOKEN id="token-65-9" pos="word" morph="none" start_char="9476" end_char="9486">coronavirus</TOKEN>
<TOKEN id="token-65-10" pos="word" morph="none" start_char="9488" end_char="9490">was</TOKEN>
<TOKEN id="token-65-11" pos="word" morph="none" start_char="9492" end_char="9496">first</TOKEN>
<TOKEN id="token-65-12" pos="word" morph="none" start_char="9498" end_char="9507">introduced</TOKEN>
<TOKEN id="token-65-13" pos="word" morph="none" start_char="9509" end_char="9512">into</TOKEN>
<TOKEN id="token-65-14" pos="word" morph="none" start_char="9514" end_char="9516">the</TOKEN>
<TOKEN id="token-65-15" pos="word" morph="none" start_char="9518" end_char="9523">United</TOKEN>
<TOKEN id="token-65-16" pos="word" morph="none" start_char="9525" end_char="9530">States</TOKEN>
<TOKEN id="token-65-17" pos="word" morph="none" start_char="9532" end_char="9538">earlier</TOKEN>
<TOKEN id="token-65-18" pos="word" morph="none" start_char="9540" end_char="9543">than</TOKEN>
<TOKEN id="token-65-19" pos="word" morph="none" start_char="9545" end_char="9554">previously</TOKEN>
<TOKEN id="token-65-20" pos="word" morph="none" start_char="9556" end_char="9563">reported</TOKEN>
<TOKEN id="token-65-21" pos="word" morph="none" start_char="9565" end_char="9567">and</TOKEN>
<TOKEN id="token-65-22" pos="word" morph="none" start_char="9569" end_char="9575">whether</TOKEN>
<TOKEN id="token-65-23" pos="word" morph="none" start_char="9577" end_char="9584">measures</TOKEN>
<TOKEN id="token-65-24" pos="word" morph="none" start_char="9586" end_char="9594">announced</TOKEN>
<TOKEN id="token-65-25" pos="word" morph="none" start_char="9596" end_char="9597">in</TOKEN>
<TOKEN id="token-65-26" pos="word" morph="none" start_char="9599" end_char="9602">late</TOKEN>
<TOKEN id="token-65-27" pos="word" morph="none" start_char="9604" end_char="9610">January</TOKEN>
<TOKEN id="token-65-28" pos="word" morph="none" start_char="9612" end_char="9622">restricting</TOKEN>
<TOKEN id="token-65-29" pos="word" morph="none" start_char="9624" end_char="9629">travel</TOKEN>
<TOKEN id="token-65-30" pos="word" morph="none" start_char="9631" end_char="9634">from</TOKEN>
<TOKEN id="token-65-31" pos="word" morph="none" start_char="9636" end_char="9640">China</TOKEN>
<TOKEN id="token-65-32" pos="word" morph="none" start_char="9642" end_char="9645">were</TOKEN>
<TOKEN id="token-65-33" pos="word" morph="none" start_char="9647" end_char="9649">too</TOKEN>
<TOKEN id="token-65-34" pos="word" morph="none" start_char="9651" end_char="9656">little</TOKEN>
<TOKEN id="token-65-35" pos="word" morph="none" start_char="9658" end_char="9660">too</TOKEN>
<TOKEN id="token-65-36" pos="word" morph="none" start_char="9662" end_char="9665">late</TOKEN>
<TOKEN id="token-65-37" pos="punct" morph="none" start_char="9666" end_char="9667">,"</TOKEN>
<TOKEN id="token-65-38" pos="word" morph="none" start_char="9669" end_char="9672">said</TOKEN>
<TOKEN id="token-65-39" pos="word" morph="none" start_char="9674" end_char="9678">Cohen</TOKEN>
<TOKEN id="token-65-40" pos="punct" morph="none" start_char="9679" end_char="9679">,</TOKEN>
<TOKEN id="token-65-41" pos="word" morph="none" start_char="9681" end_char="9683">now</TOKEN>
<TOKEN id="token-65-42" pos="word" morph="none" start_char="9685" end_char="9686">an</TOKEN>
<TOKEN id="token-65-43" pos="word" morph="none" start_char="9688" end_char="9690">ABC</TOKEN>
<TOKEN id="token-65-44" pos="word" morph="none" start_char="9692" end_char="9695">News</TOKEN>
<TOKEN id="token-65-45" pos="word" morph="none" start_char="9697" end_char="9707">contributor</TOKEN>
<TOKEN id="token-65-46" pos="punct" morph="none" start_char="9708" end_char="9708">.</TOKEN>
</SEG>
<SEG id="segment-66" start_char="9711" end_char="9844">
<ORIGINAL_TEXT>Satellite images suggesting a change in life patterns in Wuhan were also a key factor in classified early U.S. intelligence reporting.</ORIGINAL_TEXT>
<TOKEN id="token-66-0" pos="word" morph="none" start_char="9711" end_char="9719">Satellite</TOKEN>
<TOKEN id="token-66-1" pos="word" morph="none" start_char="9721" end_char="9726">images</TOKEN>
<TOKEN id="token-66-2" pos="word" morph="none" start_char="9728" end_char="9737">suggesting</TOKEN>
<TOKEN id="token-66-3" pos="word" morph="none" start_char="9739" end_char="9739">a</TOKEN>
<TOKEN id="token-66-4" pos="word" morph="none" start_char="9741" end_char="9746">change</TOKEN>
<TOKEN id="token-66-5" pos="word" morph="none" start_char="9748" end_char="9749">in</TOKEN>
<TOKEN id="token-66-6" pos="word" morph="none" start_char="9751" end_char="9754">life</TOKEN>
<TOKEN id="token-66-7" pos="word" morph="none" start_char="9756" end_char="9763">patterns</TOKEN>
<TOKEN id="token-66-8" pos="word" morph="none" start_char="9765" end_char="9766">in</TOKEN>
<TOKEN id="token-66-9" pos="word" morph="none" start_char="9768" end_char="9772">Wuhan</TOKEN>
<TOKEN id="token-66-10" pos="word" morph="none" start_char="9774" end_char="9777">were</TOKEN>
<TOKEN id="token-66-11" pos="word" morph="none" start_char="9779" end_char="9782">also</TOKEN>
<TOKEN id="token-66-12" pos="word" morph="none" start_char="9784" end_char="9784">a</TOKEN>
<TOKEN id="token-66-13" pos="word" morph="none" start_char="9786" end_char="9788">key</TOKEN>
<TOKEN id="token-66-14" pos="word" morph="none" start_char="9790" end_char="9795">factor</TOKEN>
<TOKEN id="token-66-15" pos="word" morph="none" start_char="9797" end_char="9798">in</TOKEN>
<TOKEN id="token-66-16" pos="word" morph="none" start_char="9800" end_char="9809">classified</TOKEN>
<TOKEN id="token-66-17" pos="word" morph="none" start_char="9811" end_char="9815">early</TOKEN>
<TOKEN id="token-66-18" pos="unknown" morph="none" start_char="9817" end_char="9819">U.S</TOKEN>
<TOKEN id="token-66-19" pos="punct" morph="none" start_char="9820" end_char="9820">.</TOKEN>
<TOKEN id="token-66-20" pos="word" morph="none" start_char="9822" end_char="9833">intelligence</TOKEN>
<TOKEN id="token-66-21" pos="word" morph="none" start_char="9835" end_char="9843">reporting</TOKEN>
<TOKEN id="token-66-22" pos="punct" morph="none" start_char="9844" end_char="9844">.</TOKEN>
</SEG>
<SEG id="segment-67" start_char="9847" end_char="10088">
<ORIGINAL_TEXT>In April, ABC News reported that the National Center for Medical Intelligence (NCMI) received word in late November that a contagion was sweeping through Wuhan, changing the patterns of life and business and posing a threat to the population.</ORIGINAL_TEXT>
<TOKEN id="token-67-0" pos="word" morph="none" start_char="9847" end_char="9848">In</TOKEN>
<TOKEN id="token-67-1" pos="word" morph="none" start_char="9850" end_char="9854">April</TOKEN>
<TOKEN id="token-67-2" pos="punct" morph="none" start_char="9855" end_char="9855">,</TOKEN>
<TOKEN id="token-67-3" pos="word" morph="none" start_char="9857" end_char="9859">ABC</TOKEN>
<TOKEN id="token-67-4" pos="word" morph="none" start_char="9861" end_char="9864">News</TOKEN>
<TOKEN id="token-67-5" pos="word" morph="none" start_char="9866" end_char="9873">reported</TOKEN>
<TOKEN id="token-67-6" pos="word" morph="none" start_char="9875" end_char="9878">that</TOKEN>
<TOKEN id="token-67-7" pos="word" morph="none" start_char="9880" end_char="9882">the</TOKEN>
<TOKEN id="token-67-8" pos="word" morph="none" start_char="9884" end_char="9891">National</TOKEN>
<TOKEN id="token-67-9" pos="word" morph="none" start_char="9893" end_char="9898">Center</TOKEN>
<TOKEN id="token-67-10" pos="word" morph="none" start_char="9900" end_char="9902">for</TOKEN>
<TOKEN id="token-67-11" pos="word" morph="none" start_char="9904" end_char="9910">Medical</TOKEN>
<TOKEN id="token-67-12" pos="word" morph="none" start_char="9912" end_char="9923">Intelligence</TOKEN>
<TOKEN id="token-67-13" pos="punct" morph="none" start_char="9925" end_char="9925">(</TOKEN>
<TOKEN id="token-67-14" pos="word" morph="none" start_char="9926" end_char="9929">NCMI</TOKEN>
<TOKEN id="token-67-15" pos="punct" morph="none" start_char="9930" end_char="9930">)</TOKEN>
<TOKEN id="token-67-16" pos="word" morph="none" start_char="9932" end_char="9939">received</TOKEN>
<TOKEN id="token-67-17" pos="word" morph="none" start_char="9941" end_char="9944">word</TOKEN>
<TOKEN id="token-67-18" pos="word" morph="none" start_char="9946" end_char="9947">in</TOKEN>
<TOKEN id="token-67-19" pos="word" morph="none" start_char="9949" end_char="9952">late</TOKEN>
<TOKEN id="token-67-20" pos="word" morph="none" start_char="9954" end_char="9961">November</TOKEN>
<TOKEN id="token-67-21" pos="word" morph="none" start_char="9963" end_char="9966">that</TOKEN>
<TOKEN id="token-67-22" pos="word" morph="none" start_char="9968" end_char="9968">a</TOKEN>
<TOKEN id="token-67-23" pos="word" morph="none" start_char="9970" end_char="9978">contagion</TOKEN>
<TOKEN id="token-67-24" pos="word" morph="none" start_char="9980" end_char="9982">was</TOKEN>
<TOKEN id="token-67-25" pos="word" morph="none" start_char="9984" end_char="9991">sweeping</TOKEN>
<TOKEN id="token-67-26" pos="word" morph="none" start_char="9993" end_char="9999">through</TOKEN>
<TOKEN id="token-67-27" pos="word" morph="none" start_char="10001" end_char="10005">Wuhan</TOKEN>
<TOKEN id="token-67-28" pos="punct" morph="none" start_char="10006" end_char="10006">,</TOKEN>
<TOKEN id="token-67-29" pos="word" morph="none" start_char="10008" end_char="10015">changing</TOKEN>
<TOKEN id="token-67-30" pos="word" morph="none" start_char="10017" end_char="10019">the</TOKEN>
<TOKEN id="token-67-31" pos="word" morph="none" start_char="10021" end_char="10028">patterns</TOKEN>
<TOKEN id="token-67-32" pos="word" morph="none" start_char="10030" end_char="10031">of</TOKEN>
<TOKEN id="token-67-33" pos="word" morph="none" start_char="10033" end_char="10036">life</TOKEN>
<TOKEN id="token-67-34" pos="word" morph="none" start_char="10038" end_char="10040">and</TOKEN>
<TOKEN id="token-67-35" pos="word" morph="none" start_char="10042" end_char="10049">business</TOKEN>
<TOKEN id="token-67-36" pos="word" morph="none" start_char="10051" end_char="10053">and</TOKEN>
<TOKEN id="token-67-37" pos="word" morph="none" start_char="10055" end_char="10060">posing</TOKEN>
<TOKEN id="token-67-38" pos="word" morph="none" start_char="10062" end_char="10062">a</TOKEN>
<TOKEN id="token-67-39" pos="word" morph="none" start_char="10064" end_char="10069">threat</TOKEN>
<TOKEN id="token-67-40" pos="word" morph="none" start_char="10071" end_char="10072">to</TOKEN>
<TOKEN id="token-67-41" pos="word" morph="none" start_char="10074" end_char="10076">the</TOKEN>
<TOKEN id="token-67-42" pos="word" morph="none" start_char="10078" end_char="10087">population</TOKEN>
<TOKEN id="token-67-43" pos="punct" morph="none" start_char="10088" end_char="10088">.</TOKEN>
</SEG>
<SEG id="segment-68" start_char="10090" end_char="10317">
<ORIGINAL_TEXT>Sources familiar with the reports said NCMI, a component of the military’s Defense Intelligence Agency, based the analysis on wire and computer intercepts coupled with satellite images similar to those used by Brownstein’s team.</ORIGINAL_TEXT>
<TOKEN id="token-68-0" pos="word" morph="none" start_char="10090" end_char="10096">Sources</TOKEN>
<TOKEN id="token-68-1" pos="word" morph="none" start_char="10098" end_char="10105">familiar</TOKEN>
<TOKEN id="token-68-2" pos="word" morph="none" start_char="10107" end_char="10110">with</TOKEN>
<TOKEN id="token-68-3" pos="word" morph="none" start_char="10112" end_char="10114">the</TOKEN>
<TOKEN id="token-68-4" pos="word" morph="none" start_char="10116" end_char="10122">reports</TOKEN>
<TOKEN id="token-68-5" pos="word" morph="none" start_char="10124" end_char="10127">said</TOKEN>
<TOKEN id="token-68-6" pos="word" morph="none" start_char="10129" end_char="10132">NCMI</TOKEN>
<TOKEN id="token-68-7" pos="punct" morph="none" start_char="10133" end_char="10133">,</TOKEN>
<TOKEN id="token-68-8" pos="word" morph="none" start_char="10135" end_char="10135">a</TOKEN>
<TOKEN id="token-68-9" pos="word" morph="none" start_char="10137" end_char="10145">component</TOKEN>
<TOKEN id="token-68-10" pos="word" morph="none" start_char="10147" end_char="10148">of</TOKEN>
<TOKEN id="token-68-11" pos="word" morph="none" start_char="10150" end_char="10152">the</TOKEN>
<TOKEN id="token-68-12" pos="word" morph="none" start_char="10154" end_char="10163">military’s</TOKEN>
<TOKEN id="token-68-13" pos="word" morph="none" start_char="10165" end_char="10171">Defense</TOKEN>
<TOKEN id="token-68-14" pos="word" morph="none" start_char="10173" end_char="10184">Intelligence</TOKEN>
<TOKEN id="token-68-15" pos="word" morph="none" start_char="10186" end_char="10191">Agency</TOKEN>
<TOKEN id="token-68-16" pos="punct" morph="none" start_char="10192" end_char="10192">,</TOKEN>
<TOKEN id="token-68-17" pos="word" morph="none" start_char="10194" end_char="10198">based</TOKEN>
<TOKEN id="token-68-18" pos="word" morph="none" start_char="10200" end_char="10202">the</TOKEN>
<TOKEN id="token-68-19" pos="word" morph="none" start_char="10204" end_char="10211">analysis</TOKEN>
<TOKEN id="token-68-20" pos="word" morph="none" start_char="10213" end_char="10214">on</TOKEN>
<TOKEN id="token-68-21" pos="word" morph="none" start_char="10216" end_char="10219">wire</TOKEN>
<TOKEN id="token-68-22" pos="word" morph="none" start_char="10221" end_char="10223">and</TOKEN>
<TOKEN id="token-68-23" pos="word" morph="none" start_char="10225" end_char="10232">computer</TOKEN>
<TOKEN id="token-68-24" pos="word" morph="none" start_char="10234" end_char="10243">intercepts</TOKEN>
<TOKEN id="token-68-25" pos="word" morph="none" start_char="10245" end_char="10251">coupled</TOKEN>
<TOKEN id="token-68-26" pos="word" morph="none" start_char="10253" end_char="10256">with</TOKEN>
<TOKEN id="token-68-27" pos="word" morph="none" start_char="10258" end_char="10266">satellite</TOKEN>
<TOKEN id="token-68-28" pos="word" morph="none" start_char="10268" end_char="10273">images</TOKEN>
<TOKEN id="token-68-29" pos="word" morph="none" start_char="10275" end_char="10281">similar</TOKEN>
<TOKEN id="token-68-30" pos="word" morph="none" start_char="10283" end_char="10284">to</TOKEN>
<TOKEN id="token-68-31" pos="word" morph="none" start_char="10286" end_char="10290">those</TOKEN>
<TOKEN id="token-68-32" pos="word" morph="none" start_char="10292" end_char="10295">used</TOKEN>
<TOKEN id="token-68-33" pos="word" morph="none" start_char="10297" end_char="10298">by</TOKEN>
<TOKEN id="token-68-34" pos="word" morph="none" start_char="10300" end_char="10311">Brownstein’s</TOKEN>
<TOKEN id="token-68-35" pos="word" morph="none" start_char="10313" end_char="10316">team</TOKEN>
<TOKEN id="token-68-36" pos="punct" morph="none" start_char="10317" end_char="10317">.</TOKEN>
</SEG>
<SEG id="segment-69" start_char="10320" end_char="10460">
<ORIGINAL_TEXT>After that story was broadcast, the NCMI’s director issued a statement, denying that a formal "product/assessment" was generated in November.</ORIGINAL_TEXT>
<TOKEN id="token-69-0" pos="word" morph="none" start_char="10320" end_char="10324">After</TOKEN>
<TOKEN id="token-69-1" pos="word" morph="none" start_char="10326" end_char="10329">that</TOKEN>
<TOKEN id="token-69-2" pos="word" morph="none" start_char="10331" end_char="10335">story</TOKEN>
<TOKEN id="token-69-3" pos="word" morph="none" start_char="10337" end_char="10339">was</TOKEN>
<TOKEN id="token-69-4" pos="word" morph="none" start_char="10341" end_char="10349">broadcast</TOKEN>
<TOKEN id="token-69-5" pos="punct" morph="none" start_char="10350" end_char="10350">,</TOKEN>
<TOKEN id="token-69-6" pos="word" morph="none" start_char="10352" end_char="10354">the</TOKEN>
<TOKEN id="token-69-7" pos="word" morph="none" start_char="10356" end_char="10361">NCMI’s</TOKEN>
<TOKEN id="token-69-8" pos="word" morph="none" start_char="10363" end_char="10370">director</TOKEN>
<TOKEN id="token-69-9" pos="word" morph="none" start_char="10372" end_char="10377">issued</TOKEN>
<TOKEN id="token-69-10" pos="word" morph="none" start_char="10379" end_char="10379">a</TOKEN>
<TOKEN id="token-69-11" pos="word" morph="none" start_char="10381" end_char="10389">statement</TOKEN>
<TOKEN id="token-69-12" pos="punct" morph="none" start_char="10390" end_char="10390">,</TOKEN>
<TOKEN id="token-69-13" pos="word" morph="none" start_char="10392" end_char="10398">denying</TOKEN>
<TOKEN id="token-69-14" pos="word" morph="none" start_char="10400" end_char="10403">that</TOKEN>
<TOKEN id="token-69-15" pos="word" morph="none" start_char="10405" end_char="10405">a</TOKEN>
<TOKEN id="token-69-16" pos="word" morph="none" start_char="10407" end_char="10412">formal</TOKEN>
<TOKEN id="token-69-17" pos="punct" morph="none" start_char="10414" end_char="10414">"</TOKEN>
<TOKEN id="token-69-18" pos="unknown" morph="none" start_char="10415" end_char="10432">product/assessment</TOKEN>
<TOKEN id="token-69-19" pos="punct" morph="none" start_char="10433" end_char="10433">"</TOKEN>
<TOKEN id="token-69-20" pos="word" morph="none" start_char="10435" end_char="10437">was</TOKEN>
<TOKEN id="token-69-21" pos="word" morph="none" start_char="10439" end_char="10447">generated</TOKEN>
<TOKEN id="token-69-22" pos="word" morph="none" start_char="10449" end_char="10450">in</TOKEN>
<TOKEN id="token-69-23" pos="word" morph="none" start_char="10452" end_char="10459">November</TOKEN>
<TOKEN id="token-69-24" pos="punct" morph="none" start_char="10460" end_char="10460">.</TOKEN>
</SEG>
<SEG id="segment-70" start_char="10462" end_char="10524">
<ORIGINAL_TEXT>The statement did not address preliminary intelligence reports.</ORIGINAL_TEXT>
<TOKEN id="token-70-0" pos="word" morph="none" start_char="10462" end_char="10464">The</TOKEN>
<TOKEN id="token-70-1" pos="word" morph="none" start_char="10466" end_char="10474">statement</TOKEN>
<TOKEN id="token-70-2" pos="word" morph="none" start_char="10476" end_char="10478">did</TOKEN>
<TOKEN id="token-70-3" pos="word" morph="none" start_char="10480" end_char="10482">not</TOKEN>
<TOKEN id="token-70-4" pos="word" morph="none" start_char="10484" end_char="10490">address</TOKEN>
<TOKEN id="token-70-5" pos="word" morph="none" start_char="10492" end_char="10502">preliminary</TOKEN>
<TOKEN id="token-70-6" pos="word" morph="none" start_char="10504" end_char="10515">intelligence</TOKEN>
<TOKEN id="token-70-7" pos="word" morph="none" start_char="10517" end_char="10523">reports</TOKEN>
<TOKEN id="token-70-8" pos="punct" morph="none" start_char="10524" end_char="10524">.</TOKEN>
</SEG>
<SEG id="segment-71" start_char="10526" end_char="10669">
<ORIGINAL_TEXT>When contacted Friday with the results of the new Harvard study, the Pentagon’s chief spokesman, Jonathan Hoffman, said he had "nothing to add."</ORIGINAL_TEXT>
<TOKEN id="token-71-0" pos="word" morph="none" start_char="10526" end_char="10529">When</TOKEN>
<TOKEN id="token-71-1" pos="word" morph="none" start_char="10531" end_char="10539">contacted</TOKEN>
<TOKEN id="token-71-2" pos="word" morph="none" start_char="10541" end_char="10546">Friday</TOKEN>
<TOKEN id="token-71-3" pos="word" morph="none" start_char="10548" end_char="10551">with</TOKEN>
<TOKEN id="token-71-4" pos="word" morph="none" start_char="10553" end_char="10555">the</TOKEN>
<TOKEN id="token-71-5" pos="word" morph="none" start_char="10557" end_char="10563">results</TOKEN>
<TOKEN id="token-71-6" pos="word" morph="none" start_char="10565" end_char="10566">of</TOKEN>
<TOKEN id="token-71-7" pos="word" morph="none" start_char="10568" end_char="10570">the</TOKEN>
<TOKEN id="token-71-8" pos="word" morph="none" start_char="10572" end_char="10574">new</TOKEN>
<TOKEN id="token-71-9" pos="word" morph="none" start_char="10576" end_char="10582">Harvard</TOKEN>
<TOKEN id="token-71-10" pos="word" morph="none" start_char="10584" end_char="10588">study</TOKEN>
<TOKEN id="token-71-11" pos="punct" morph="none" start_char="10589" end_char="10589">,</TOKEN>
<TOKEN id="token-71-12" pos="word" morph="none" start_char="10591" end_char="10593">the</TOKEN>
<TOKEN id="token-71-13" pos="word" morph="none" start_char="10595" end_char="10604">Pentagon’s</TOKEN>
<TOKEN id="token-71-14" pos="word" morph="none" start_char="10606" end_char="10610">chief</TOKEN>
<TOKEN id="token-71-15" pos="word" morph="none" start_char="10612" end_char="10620">spokesman</TOKEN>
<TOKEN id="token-71-16" pos="punct" morph="none" start_char="10621" end_char="10621">,</TOKEN>
<TOKEN id="token-71-17" pos="word" morph="none" start_char="10623" end_char="10630">Jonathan</TOKEN>
<TOKEN id="token-71-18" pos="word" morph="none" start_char="10632" end_char="10638">Hoffman</TOKEN>
<TOKEN id="token-71-19" pos="punct" morph="none" start_char="10639" end_char="10639">,</TOKEN>
<TOKEN id="token-71-20" pos="word" morph="none" start_char="10641" end_char="10644">said</TOKEN>
<TOKEN id="token-71-21" pos="word" morph="none" start_char="10646" end_char="10647">he</TOKEN>
<TOKEN id="token-71-22" pos="word" morph="none" start_char="10649" end_char="10651">had</TOKEN>
<TOKEN id="token-71-23" pos="punct" morph="none" start_char="10653" end_char="10653">"</TOKEN>
<TOKEN id="token-71-24" pos="word" morph="none" start_char="10654" end_char="10660">nothing</TOKEN>
<TOKEN id="token-71-25" pos="word" morph="none" start_char="10662" end_char="10663">to</TOKEN>
<TOKEN id="token-71-26" pos="word" morph="none" start_char="10665" end_char="10667">add</TOKEN>
<TOKEN id="token-71-27" pos="punct" morph="none" start_char="10668" end_char="10669">."</TOKEN>
</SEG>
<SEG id="segment-72" start_char="10672" end_char="10743">
<ORIGINAL_TEXT>The Office of the Director of National Intelligence declined to comment.</ORIGINAL_TEXT>
<TOKEN id="token-72-0" pos="word" morph="none" start_char="10672" end_char="10674">The</TOKEN>
<TOKEN id="token-72-1" pos="word" morph="none" start_char="10676" end_char="10681">Office</TOKEN>
<TOKEN id="token-72-2" pos="word" morph="none" start_char="10683" end_char="10684">of</TOKEN>
<TOKEN id="token-72-3" pos="word" morph="none" start_char="10686" end_char="10688">the</TOKEN>
<TOKEN id="token-72-4" pos="word" morph="none" start_char="10690" end_char="10697">Director</TOKEN>
<TOKEN id="token-72-5" pos="word" morph="none" start_char="10699" end_char="10700">of</TOKEN>
<TOKEN id="token-72-6" pos="word" morph="none" start_char="10702" end_char="10709">National</TOKEN>
<TOKEN id="token-72-7" pos="word" morph="none" start_char="10711" end_char="10722">Intelligence</TOKEN>
<TOKEN id="token-72-8" pos="word" morph="none" start_char="10724" end_char="10731">declined</TOKEN>
<TOKEN id="token-72-9" pos="word" morph="none" start_char="10733" end_char="10734">to</TOKEN>
<TOKEN id="token-72-10" pos="word" morph="none" start_char="10736" end_char="10742">comment</TOKEN>
<TOKEN id="token-72-11" pos="punct" morph="none" start_char="10743" end_char="10743">.</TOKEN>
</SEG>
<SEG id="segment-73" start_char="10746" end_char="10954">
<ORIGINAL_TEXT>In response to questions about the new Harvard Medical study, the State Department Sunday again criticized the government in Beijing for withholding from the world community critical public health information.</ORIGINAL_TEXT>
<TOKEN id="token-73-0" pos="word" morph="none" start_char="10746" end_char="10747">In</TOKEN>
<TOKEN id="token-73-1" pos="word" morph="none" start_char="10749" end_char="10756">response</TOKEN>
<TOKEN id="token-73-2" pos="word" morph="none" start_char="10758" end_char="10759">to</TOKEN>
<TOKEN id="token-73-3" pos="word" morph="none" start_char="10761" end_char="10769">questions</TOKEN>
<TOKEN id="token-73-4" pos="word" morph="none" start_char="10771" end_char="10775">about</TOKEN>
<TOKEN id="token-73-5" pos="word" morph="none" start_char="10777" end_char="10779">the</TOKEN>
<TOKEN id="token-73-6" pos="word" morph="none" start_char="10781" end_char="10783">new</TOKEN>
<TOKEN id="token-73-7" pos="word" morph="none" start_char="10785" end_char="10791">Harvard</TOKEN>
<TOKEN id="token-73-8" pos="word" morph="none" start_char="10793" end_char="10799">Medical</TOKEN>
<TOKEN id="token-73-9" pos="word" morph="none" start_char="10801" end_char="10805">study</TOKEN>
<TOKEN id="token-73-10" pos="punct" morph="none" start_char="10806" end_char="10806">,</TOKEN>
<TOKEN id="token-73-11" pos="word" morph="none" start_char="10808" end_char="10810">the</TOKEN>
<TOKEN id="token-73-12" pos="word" morph="none" start_char="10812" end_char="10816">State</TOKEN>
<TOKEN id="token-73-13" pos="word" morph="none" start_char="10818" end_char="10827">Department</TOKEN>
<TOKEN id="token-73-14" pos="word" morph="none" start_char="10829" end_char="10834">Sunday</TOKEN>
<TOKEN id="token-73-15" pos="word" morph="none" start_char="10836" end_char="10840">again</TOKEN>
<TOKEN id="token-73-16" pos="word" morph="none" start_char="10842" end_char="10851">criticized</TOKEN>
<TOKEN id="token-73-17" pos="word" morph="none" start_char="10853" end_char="10855">the</TOKEN>
<TOKEN id="token-73-18" pos="word" morph="none" start_char="10857" end_char="10866">government</TOKEN>
<TOKEN id="token-73-19" pos="word" morph="none" start_char="10868" end_char="10869">in</TOKEN>
<TOKEN id="token-73-20" pos="word" morph="none" start_char="10871" end_char="10877">Beijing</TOKEN>
<TOKEN id="token-73-21" pos="word" morph="none" start_char="10879" end_char="10881">for</TOKEN>
<TOKEN id="token-73-22" pos="word" morph="none" start_char="10883" end_char="10893">withholding</TOKEN>
<TOKEN id="token-73-23" pos="word" morph="none" start_char="10895" end_char="10898">from</TOKEN>
<TOKEN id="token-73-24" pos="word" morph="none" start_char="10900" end_char="10902">the</TOKEN>
<TOKEN id="token-73-25" pos="word" morph="none" start_char="10904" end_char="10908">world</TOKEN>
<TOKEN id="token-73-26" pos="word" morph="none" start_char="10910" end_char="10918">community</TOKEN>
<TOKEN id="token-73-27" pos="word" morph="none" start_char="10920" end_char="10927">critical</TOKEN>
<TOKEN id="token-73-28" pos="word" morph="none" start_char="10929" end_char="10934">public</TOKEN>
<TOKEN id="token-73-29" pos="word" morph="none" start_char="10936" end_char="10941">health</TOKEN>
<TOKEN id="token-73-30" pos="word" morph="none" start_char="10943" end_char="10953">information</TOKEN>
<TOKEN id="token-73-31" pos="punct" morph="none" start_char="10954" end_char="10954">.</TOKEN>
</SEG>
<SEG id="segment-74" start_char="10957" end_char="11189">
<ORIGINAL_TEXT>"The Chinese government's cover up of initial reporting on the virus is just one more example of the challenges presented by the Chinese Communist Party's hostility toward transparency," a State Department spokesperson told ABC News.</ORIGINAL_TEXT>
<TOKEN id="token-74-0" pos="punct" morph="none" start_char="10957" end_char="10957">"</TOKEN>
<TOKEN id="token-74-1" pos="word" morph="none" start_char="10958" end_char="10960">The</TOKEN>
<TOKEN id="token-74-2" pos="word" morph="none" start_char="10962" end_char="10968">Chinese</TOKEN>
<TOKEN id="token-74-3" pos="word" morph="none" start_char="10970" end_char="10981">government's</TOKEN>
<TOKEN id="token-74-4" pos="word" morph="none" start_char="10983" end_char="10987">cover</TOKEN>
<TOKEN id="token-74-5" pos="word" morph="none" start_char="10989" end_char="10990">up</TOKEN>
<TOKEN id="token-74-6" pos="word" morph="none" start_char="10992" end_char="10993">of</TOKEN>
<TOKEN id="token-74-7" pos="word" morph="none" start_char="10995" end_char="11001">initial</TOKEN>
<TOKEN id="token-74-8" pos="word" morph="none" start_char="11003" end_char="11011">reporting</TOKEN>
<TOKEN id="token-74-9" pos="word" morph="none" start_char="11013" end_char="11014">on</TOKEN>
<TOKEN id="token-74-10" pos="word" morph="none" start_char="11016" end_char="11018">the</TOKEN>
<TOKEN id="token-74-11" pos="word" morph="none" start_char="11020" end_char="11024">virus</TOKEN>
<TOKEN id="token-74-12" pos="word" morph="none" start_char="11026" end_char="11027">is</TOKEN>
<TOKEN id="token-74-13" pos="word" morph="none" start_char="11029" end_char="11032">just</TOKEN>
<TOKEN id="token-74-14" pos="word" morph="none" start_char="11034" end_char="11036">one</TOKEN>
<TOKEN id="token-74-15" pos="word" morph="none" start_char="11038" end_char="11041">more</TOKEN>
<TOKEN id="token-74-16" pos="word" morph="none" start_char="11043" end_char="11049">example</TOKEN>
<TOKEN id="token-74-17" pos="word" morph="none" start_char="11051" end_char="11052">of</TOKEN>
<TOKEN id="token-74-18" pos="word" morph="none" start_char="11054" end_char="11056">the</TOKEN>
<TOKEN id="token-74-19" pos="word" morph="none" start_char="11058" end_char="11067">challenges</TOKEN>
<TOKEN id="token-74-20" pos="word" morph="none" start_char="11069" end_char="11077">presented</TOKEN>
<TOKEN id="token-74-21" pos="word" morph="none" start_char="11079" end_char="11080">by</TOKEN>
<TOKEN id="token-74-22" pos="word" morph="none" start_char="11082" end_char="11084">the</TOKEN>
<TOKEN id="token-74-23" pos="word" morph="none" start_char="11086" end_char="11092">Chinese</TOKEN>
<TOKEN id="token-74-24" pos="word" morph="none" start_char="11094" end_char="11102">Communist</TOKEN>
<TOKEN id="token-74-25" pos="word" morph="none" start_char="11104" end_char="11110">Party's</TOKEN>
<TOKEN id="token-74-26" pos="word" morph="none" start_char="11112" end_char="11120">hostility</TOKEN>
<TOKEN id="token-74-27" pos="word" morph="none" start_char="11122" end_char="11127">toward</TOKEN>
<TOKEN id="token-74-28" pos="word" morph="none" start_char="11129" end_char="11140">transparency</TOKEN>
<TOKEN id="token-74-29" pos="punct" morph="none" start_char="11141" end_char="11142">,"</TOKEN>
<TOKEN id="token-74-30" pos="word" morph="none" start_char="11144" end_char="11144">a</TOKEN>
<TOKEN id="token-74-31" pos="word" morph="none" start_char="11146" end_char="11150">State</TOKEN>
<TOKEN id="token-74-32" pos="word" morph="none" start_char="11152" end_char="11161">Department</TOKEN>
<TOKEN id="token-74-33" pos="word" morph="none" start_char="11163" end_char="11174">spokesperson</TOKEN>
<TOKEN id="token-74-34" pos="word" morph="none" start_char="11176" end_char="11179">told</TOKEN>
<TOKEN id="token-74-35" pos="word" morph="none" start_char="11181" end_char="11183">ABC</TOKEN>
<TOKEN id="token-74-36" pos="word" morph="none" start_char="11185" end_char="11188">News</TOKEN>
<TOKEN id="token-74-37" pos="punct" morph="none" start_char="11189" end_char="11189">.</TOKEN>
</SEG>
<SEG id="segment-75" start_char="11191" end_char="11326">
<ORIGINAL_TEXT>"The Chinese government has a responsibility to share information on the virus and support countries as the world responds to COVID-19."</ORIGINAL_TEXT>
<TOKEN id="token-75-0" pos="punct" morph="none" start_char="11191" end_char="11191">"</TOKEN>
<TOKEN id="token-75-1" pos="word" morph="none" start_char="11192" end_char="11194">The</TOKEN>
<TOKEN id="token-75-2" pos="word" morph="none" start_char="11196" end_char="11202">Chinese</TOKEN>
<TOKEN id="token-75-3" pos="word" morph="none" start_char="11204" end_char="11213">government</TOKEN>
<TOKEN id="token-75-4" pos="word" morph="none" start_char="11215" end_char="11217">has</TOKEN>
<TOKEN id="token-75-5" pos="word" morph="none" start_char="11219" end_char="11219">a</TOKEN>
<TOKEN id="token-75-6" pos="word" morph="none" start_char="11221" end_char="11234">responsibility</TOKEN>
<TOKEN id="token-75-7" pos="word" morph="none" start_char="11236" end_char="11237">to</TOKEN>
<TOKEN id="token-75-8" pos="word" morph="none" start_char="11239" end_char="11243">share</TOKEN>
<TOKEN id="token-75-9" pos="word" morph="none" start_char="11245" end_char="11255">information</TOKEN>
<TOKEN id="token-75-10" pos="word" morph="none" start_char="11257" end_char="11258">on</TOKEN>
<TOKEN id="token-75-11" pos="word" morph="none" start_char="11260" end_char="11262">the</TOKEN>
<TOKEN id="token-75-12" pos="word" morph="none" start_char="11264" end_char="11268">virus</TOKEN>
<TOKEN id="token-75-13" pos="word" morph="none" start_char="11270" end_char="11272">and</TOKEN>
<TOKEN id="token-75-14" pos="word" morph="none" start_char="11274" end_char="11280">support</TOKEN>
<TOKEN id="token-75-15" pos="word" morph="none" start_char="11282" end_char="11290">countries</TOKEN>
<TOKEN id="token-75-16" pos="word" morph="none" start_char="11292" end_char="11293">as</TOKEN>
<TOKEN id="token-75-17" pos="word" morph="none" start_char="11295" end_char="11297">the</TOKEN>
<TOKEN id="token-75-18" pos="word" morph="none" start_char="11299" end_char="11303">world</TOKEN>
<TOKEN id="token-75-19" pos="word" morph="none" start_char="11305" end_char="11312">responds</TOKEN>
<TOKEN id="token-75-20" pos="word" morph="none" start_char="11314" end_char="11315">to</TOKEN>
<TOKEN id="token-75-21" pos="unknown" morph="none" start_char="11317" end_char="11324">COVID-19</TOKEN>
<TOKEN id="token-75-22" pos="punct" morph="none" start_char="11325" end_char="11326">."</TOKEN>
</SEG>
<SEG id="segment-76" start_char="11329" end_char="11509">
<ORIGINAL_TEXT>In March, the Hong Kong-based South China Morning Post newspaper, citing Chinese government data, reported that the first case of COVID-19 could be traced back to November 17, 2019.</ORIGINAL_TEXT>
<TOKEN id="token-76-0" pos="word" morph="none" start_char="11329" end_char="11330">In</TOKEN>
<TOKEN id="token-76-1" pos="word" morph="none" start_char="11332" end_char="11336">March</TOKEN>
<TOKEN id="token-76-2" pos="punct" morph="none" start_char="11337" end_char="11337">,</TOKEN>
<TOKEN id="token-76-3" pos="word" morph="none" start_char="11339" end_char="11341">the</TOKEN>
<TOKEN id="token-76-4" pos="word" morph="none" start_char="11343" end_char="11346">Hong</TOKEN>
<TOKEN id="token-76-5" pos="unknown" morph="none" start_char="11348" end_char="11357">Kong-based</TOKEN>
<TOKEN id="token-76-6" pos="word" morph="none" start_char="11359" end_char="11363">South</TOKEN>
<TOKEN id="token-76-7" pos="word" morph="none" start_char="11365" end_char="11369">China</TOKEN>
<TOKEN id="token-76-8" pos="word" morph="none" start_char="11371" end_char="11377">Morning</TOKEN>
<TOKEN id="token-76-9" pos="word" morph="none" start_char="11379" end_char="11382">Post</TOKEN>
<TOKEN id="token-76-10" pos="word" morph="none" start_char="11384" end_char="11392">newspaper</TOKEN>
<TOKEN id="token-76-11" pos="punct" morph="none" start_char="11393" end_char="11393">,</TOKEN>
<TOKEN id="token-76-12" pos="word" morph="none" start_char="11395" end_char="11400">citing</TOKEN>
<TOKEN id="token-76-13" pos="word" morph="none" start_char="11402" end_char="11408">Chinese</TOKEN>
<TOKEN id="token-76-14" pos="word" morph="none" start_char="11410" end_char="11419">government</TOKEN>
<TOKEN id="token-76-15" pos="word" morph="none" start_char="11421" end_char="11424">data</TOKEN>
<TOKEN id="token-76-16" pos="punct" morph="none" start_char="11425" end_char="11425">,</TOKEN>
<TOKEN id="token-76-17" pos="word" morph="none" start_char="11427" end_char="11434">reported</TOKEN>
<TOKEN id="token-76-18" pos="word" morph="none" start_char="11436" end_char="11439">that</TOKEN>
<TOKEN id="token-76-19" pos="word" morph="none" start_char="11441" end_char="11443">the</TOKEN>
<TOKEN id="token-76-20" pos="word" morph="none" start_char="11445" end_char="11449">first</TOKEN>
<TOKEN id="token-76-21" pos="word" morph="none" start_char="11451" end_char="11454">case</TOKEN>
<TOKEN id="token-76-22" pos="word" morph="none" start_char="11456" end_char="11457">of</TOKEN>
<TOKEN id="token-76-23" pos="unknown" morph="none" start_char="11459" end_char="11466">COVID-19</TOKEN>
<TOKEN id="token-76-24" pos="word" morph="none" start_char="11468" end_char="11472">could</TOKEN>
<TOKEN id="token-76-25" pos="word" morph="none" start_char="11474" end_char="11475">be</TOKEN>
<TOKEN id="token-76-26" pos="word" morph="none" start_char="11477" end_char="11482">traced</TOKEN>
<TOKEN id="token-76-27" pos="word" morph="none" start_char="11484" end_char="11487">back</TOKEN>
<TOKEN id="token-76-28" pos="word" morph="none" start_char="11489" end_char="11490">to</TOKEN>
<TOKEN id="token-76-29" pos="word" morph="none" start_char="11492" end_char="11499">November</TOKEN>
<TOKEN id="token-76-30" pos="word" morph="none" start_char="11501" end_char="11502">17</TOKEN>
<TOKEN id="token-76-31" pos="punct" morph="none" start_char="11503" end_char="11503">,</TOKEN>
<TOKEN id="token-76-32" pos="word" morph="none" start_char="11505" end_char="11508">2019</TOKEN>
<TOKEN id="token-76-33" pos="punct" morph="none" start_char="11509" end_char="11509">.</TOKEN>
</SEG>
<SEG id="segment-77" start_char="11511" end_char="11667">
<ORIGINAL_TEXT>In recent days, Chinese health officials have told local media that the virus likely was spreading before they realized, though they have offered no details.</ORIGINAL_TEXT>
<TOKEN id="token-77-0" pos="word" morph="none" start_char="11511" end_char="11512">In</TOKEN>
<TOKEN id="token-77-1" pos="word" morph="none" start_char="11514" end_char="11519">recent</TOKEN>
<TOKEN id="token-77-2" pos="word" morph="none" start_char="11521" end_char="11524">days</TOKEN>
<TOKEN id="token-77-3" pos="punct" morph="none" start_char="11525" end_char="11525">,</TOKEN>
<TOKEN id="token-77-4" pos="word" morph="none" start_char="11527" end_char="11533">Chinese</TOKEN>
<TOKEN id="token-77-5" pos="word" morph="none" start_char="11535" end_char="11540">health</TOKEN>
<TOKEN id="token-77-6" pos="word" morph="none" start_char="11542" end_char="11550">officials</TOKEN>
<TOKEN id="token-77-7" pos="word" morph="none" start_char="11552" end_char="11555">have</TOKEN>
<TOKEN id="token-77-8" pos="word" morph="none" start_char="11557" end_char="11560">told</TOKEN>
<TOKEN id="token-77-9" pos="word" morph="none" start_char="11562" end_char="11566">local</TOKEN>
<TOKEN id="token-77-10" pos="word" morph="none" start_char="11568" end_char="11572">media</TOKEN>
<TOKEN id="token-77-11" pos="word" morph="none" start_char="11574" end_char="11577">that</TOKEN>
<TOKEN id="token-77-12" pos="word" morph="none" start_char="11579" end_char="11581">the</TOKEN>
<TOKEN id="token-77-13" pos="word" morph="none" start_char="11583" end_char="11587">virus</TOKEN>
<TOKEN id="token-77-14" pos="word" morph="none" start_char="11589" end_char="11594">likely</TOKEN>
<TOKEN id="token-77-15" pos="word" morph="none" start_char="11596" end_char="11598">was</TOKEN>
<TOKEN id="token-77-16" pos="word" morph="none" start_char="11600" end_char="11608">spreading</TOKEN>
<TOKEN id="token-77-17" pos="word" morph="none" start_char="11610" end_char="11615">before</TOKEN>
<TOKEN id="token-77-18" pos="word" morph="none" start_char="11617" end_char="11620">they</TOKEN>
<TOKEN id="token-77-19" pos="word" morph="none" start_char="11622" end_char="11629">realized</TOKEN>
<TOKEN id="token-77-20" pos="punct" morph="none" start_char="11630" end_char="11630">,</TOKEN>
<TOKEN id="token-77-21" pos="word" morph="none" start_char="11632" end_char="11637">though</TOKEN>
<TOKEN id="token-77-22" pos="word" morph="none" start_char="11639" end_char="11642">they</TOKEN>
<TOKEN id="token-77-23" pos="word" morph="none" start_char="11644" end_char="11647">have</TOKEN>
<TOKEN id="token-77-24" pos="word" morph="none" start_char="11649" end_char="11655">offered</TOKEN>
<TOKEN id="token-77-25" pos="word" morph="none" start_char="11657" end_char="11658">no</TOKEN>
<TOKEN id="token-77-26" pos="word" morph="none" start_char="11660" end_char="11666">details</TOKEN>
<TOKEN id="token-77-27" pos="punct" morph="none" start_char="11667" end_char="11667">.</TOKEN>
</SEG>
<SEG id="segment-78" start_char="11670" end_char="11827">
<ORIGINAL_TEXT>ABC News sought comment on the new study from the hospitals in Wuhan that were analyzed, the local public health agency and the Chinese embassy in Washington.</ORIGINAL_TEXT>
<TOKEN id="token-78-0" pos="word" morph="none" start_char="11670" end_char="11672">ABC</TOKEN>
<TOKEN id="token-78-1" pos="word" morph="none" start_char="11674" end_char="11677">News</TOKEN>
<TOKEN id="token-78-2" pos="word" morph="none" start_char="11679" end_char="11684">sought</TOKEN>
<TOKEN id="token-78-3" pos="word" morph="none" start_char="11686" end_char="11692">comment</TOKEN>
<TOKEN id="token-78-4" pos="word" morph="none" start_char="11694" end_char="11695">on</TOKEN>
<TOKEN id="token-78-5" pos="word" morph="none" start_char="11697" end_char="11699">the</TOKEN>
<TOKEN id="token-78-6" pos="word" morph="none" start_char="11701" end_char="11703">new</TOKEN>
<TOKEN id="token-78-7" pos="word" morph="none" start_char="11705" end_char="11709">study</TOKEN>
<TOKEN id="token-78-8" pos="word" morph="none" start_char="11711" end_char="11714">from</TOKEN>
<TOKEN id="token-78-9" pos="word" morph="none" start_char="11716" end_char="11718">the</TOKEN>
<TOKEN id="token-78-10" pos="word" morph="none" start_char="11720" end_char="11728">hospitals</TOKEN>
<TOKEN id="token-78-11" pos="word" morph="none" start_char="11730" end_char="11731">in</TOKEN>
<TOKEN id="token-78-12" pos="word" morph="none" start_char="11733" end_char="11737">Wuhan</TOKEN>
<TOKEN id="token-78-13" pos="word" morph="none" start_char="11739" end_char="11742">that</TOKEN>
<TOKEN id="token-78-14" pos="word" morph="none" start_char="11744" end_char="11747">were</TOKEN>
<TOKEN id="token-78-15" pos="word" morph="none" start_char="11749" end_char="11756">analyzed</TOKEN>
<TOKEN id="token-78-16" pos="punct" morph="none" start_char="11757" end_char="11757">,</TOKEN>
<TOKEN id="token-78-17" pos="word" morph="none" start_char="11759" end_char="11761">the</TOKEN>
<TOKEN id="token-78-18" pos="word" morph="none" start_char="11763" end_char="11767">local</TOKEN>
<TOKEN id="token-78-19" pos="word" morph="none" start_char="11769" end_char="11774">public</TOKEN>
<TOKEN id="token-78-20" pos="word" morph="none" start_char="11776" end_char="11781">health</TOKEN>
<TOKEN id="token-78-21" pos="word" morph="none" start_char="11783" end_char="11788">agency</TOKEN>
<TOKEN id="token-78-22" pos="word" morph="none" start_char="11790" end_char="11792">and</TOKEN>
<TOKEN id="token-78-23" pos="word" morph="none" start_char="11794" end_char="11796">the</TOKEN>
<TOKEN id="token-78-24" pos="word" morph="none" start_char="11798" end_char="11804">Chinese</TOKEN>
<TOKEN id="token-78-25" pos="word" morph="none" start_char="11806" end_char="11812">embassy</TOKEN>
<TOKEN id="token-78-26" pos="word" morph="none" start_char="11814" end_char="11815">in</TOKEN>
<TOKEN id="token-78-27" pos="word" morph="none" start_char="11817" end_char="11826">Washington</TOKEN>
<TOKEN id="token-78-28" pos="punct" morph="none" start_char="11827" end_char="11827">.</TOKEN>
</SEG>
<SEG id="segment-79" start_char="11829" end_char="11982">
<ORIGINAL_TEXT>The only response received by the network came from the Chinese embassy, where officials pointed to a white paper released Sunday the China State Council.</ORIGINAL_TEXT>
<TOKEN id="token-79-0" pos="word" morph="none" start_char="11829" end_char="11831">The</TOKEN>
<TOKEN id="token-79-1" pos="word" morph="none" start_char="11833" end_char="11836">only</TOKEN>
<TOKEN id="token-79-2" pos="word" morph="none" start_char="11838" end_char="11845">response</TOKEN>
<TOKEN id="token-79-3" pos="word" morph="none" start_char="11847" end_char="11854">received</TOKEN>
<TOKEN id="token-79-4" pos="word" morph="none" start_char="11856" end_char="11857">by</TOKEN>
<TOKEN id="token-79-5" pos="word" morph="none" start_char="11859" end_char="11861">the</TOKEN>
<TOKEN id="token-79-6" pos="word" morph="none" start_char="11863" end_char="11869">network</TOKEN>
<TOKEN id="token-79-7" pos="word" morph="none" start_char="11871" end_char="11874">came</TOKEN>
<TOKEN id="token-79-8" pos="word" morph="none" start_char="11876" end_char="11879">from</TOKEN>
<TOKEN id="token-79-9" pos="word" morph="none" start_char="11881" end_char="11883">the</TOKEN>
<TOKEN id="token-79-10" pos="word" morph="none" start_char="11885" end_char="11891">Chinese</TOKEN>
<TOKEN id="token-79-11" pos="word" morph="none" start_char="11893" end_char="11899">embassy</TOKEN>
<TOKEN id="token-79-12" pos="punct" morph="none" start_char="11900" end_char="11900">,</TOKEN>
<TOKEN id="token-79-13" pos="word" morph="none" start_char="11902" end_char="11906">where</TOKEN>
<TOKEN id="token-79-14" pos="word" morph="none" start_char="11908" end_char="11916">officials</TOKEN>
<TOKEN id="token-79-15" pos="word" morph="none" start_char="11918" end_char="11924">pointed</TOKEN>
<TOKEN id="token-79-16" pos="word" morph="none" start_char="11926" end_char="11927">to</TOKEN>
<TOKEN id="token-79-17" pos="word" morph="none" start_char="11929" end_char="11929">a</TOKEN>
<TOKEN id="token-79-18" pos="word" morph="none" start_char="11931" end_char="11935">white</TOKEN>
<TOKEN id="token-79-19" pos="word" morph="none" start_char="11937" end_char="11941">paper</TOKEN>
<TOKEN id="token-79-20" pos="word" morph="none" start_char="11943" end_char="11950">released</TOKEN>
<TOKEN id="token-79-21" pos="word" morph="none" start_char="11952" end_char="11957">Sunday</TOKEN>
<TOKEN id="token-79-22" pos="word" morph="none" start_char="11959" end_char="11961">the</TOKEN>
<TOKEN id="token-79-23" pos="word" morph="none" start_char="11963" end_char="11967">China</TOKEN>
<TOKEN id="token-79-24" pos="word" morph="none" start_char="11969" end_char="11973">State</TOKEN>
<TOKEN id="token-79-25" pos="word" morph="none" start_char="11975" end_char="11981">Council</TOKEN>
<TOKEN id="token-79-26" pos="punct" morph="none" start_char="11982" end_char="11982">.</TOKEN>
</SEG>
<SEG id="segment-80" start_char="11985" end_char="12060">
<ORIGINAL_TEXT>"The novel coronavirus is a previously unknown virus," the report documents.</ORIGINAL_TEXT>
<TOKEN id="token-80-0" pos="punct" morph="none" start_char="11985" end_char="11985">"</TOKEN>
<TOKEN id="token-80-1" pos="word" morph="none" start_char="11986" end_char="11988">The</TOKEN>
<TOKEN id="token-80-2" pos="word" morph="none" start_char="11990" end_char="11994">novel</TOKEN>
<TOKEN id="token-80-3" pos="word" morph="none" start_char="11996" end_char="12006">coronavirus</TOKEN>
<TOKEN id="token-80-4" pos="word" morph="none" start_char="12008" end_char="12009">is</TOKEN>
<TOKEN id="token-80-5" pos="word" morph="none" start_char="12011" end_char="12011">a</TOKEN>
<TOKEN id="token-80-6" pos="word" morph="none" start_char="12013" end_char="12022">previously</TOKEN>
<TOKEN id="token-80-7" pos="word" morph="none" start_char="12024" end_char="12030">unknown</TOKEN>
<TOKEN id="token-80-8" pos="word" morph="none" start_char="12032" end_char="12036">virus</TOKEN>
<TOKEN id="token-80-9" pos="punct" morph="none" start_char="12037" end_char="12038">,"</TOKEN>
<TOKEN id="token-80-10" pos="word" morph="none" start_char="12040" end_char="12042">the</TOKEN>
<TOKEN id="token-80-11" pos="word" morph="none" start_char="12044" end_char="12049">report</TOKEN>
<TOKEN id="token-80-12" pos="word" morph="none" start_char="12051" end_char="12059">documents</TOKEN>
<TOKEN id="token-80-13" pos="punct" morph="none" start_char="12060" end_char="12060">.</TOKEN>
</SEG>
<SEG id="segment-81" start_char="12062" end_char="12156">
<ORIGINAL_TEXT>"Determining its origin is a scientific issue that requires research by scientists and doctors.</ORIGINAL_TEXT>
<TOKEN id="token-81-0" pos="punct" morph="none" start_char="12062" end_char="12062">"</TOKEN>
<TOKEN id="token-81-1" pos="word" morph="none" start_char="12063" end_char="12073">Determining</TOKEN>
<TOKEN id="token-81-2" pos="word" morph="none" start_char="12075" end_char="12077">its</TOKEN>
<TOKEN id="token-81-3" pos="word" morph="none" start_char="12079" end_char="12084">origin</TOKEN>
<TOKEN id="token-81-4" pos="word" morph="none" start_char="12086" end_char="12087">is</TOKEN>
<TOKEN id="token-81-5" pos="word" morph="none" start_char="12089" end_char="12089">a</TOKEN>
<TOKEN id="token-81-6" pos="word" morph="none" start_char="12091" end_char="12100">scientific</TOKEN>
<TOKEN id="token-81-7" pos="word" morph="none" start_char="12102" end_char="12106">issue</TOKEN>
<TOKEN id="token-81-8" pos="word" morph="none" start_char="12108" end_char="12111">that</TOKEN>
<TOKEN id="token-81-9" pos="word" morph="none" start_char="12113" end_char="12120">requires</TOKEN>
<TOKEN id="token-81-10" pos="word" morph="none" start_char="12122" end_char="12129">research</TOKEN>
<TOKEN id="token-81-11" pos="word" morph="none" start_char="12131" end_char="12132">by</TOKEN>
<TOKEN id="token-81-12" pos="word" morph="none" start_char="12134" end_char="12143">scientists</TOKEN>
<TOKEN id="token-81-13" pos="word" morph="none" start_char="12145" end_char="12147">and</TOKEN>
<TOKEN id="token-81-14" pos="word" morph="none" start_char="12149" end_char="12155">doctors</TOKEN>
<TOKEN id="token-81-15" pos="punct" morph="none" start_char="12156" end_char="12156">.</TOKEN>
</SEG>
<SEG id="segment-82" start_char="12158" end_char="12209">
<ORIGINAL_TEXT>The conclusion must be based on facts and evidence."</ORIGINAL_TEXT>
<TOKEN id="token-82-0" pos="word" morph="none" start_char="12158" end_char="12160">The</TOKEN>
<TOKEN id="token-82-1" pos="word" morph="none" start_char="12162" end_char="12171">conclusion</TOKEN>
<TOKEN id="token-82-2" pos="word" morph="none" start_char="12173" end_char="12176">must</TOKEN>
<TOKEN id="token-82-3" pos="word" morph="none" start_char="12178" end_char="12179">be</TOKEN>
<TOKEN id="token-82-4" pos="word" morph="none" start_char="12181" end_char="12185">based</TOKEN>
<TOKEN id="token-82-5" pos="word" morph="none" start_char="12187" end_char="12188">on</TOKEN>
<TOKEN id="token-82-6" pos="word" morph="none" start_char="12190" end_char="12194">facts</TOKEN>
<TOKEN id="token-82-7" pos="word" morph="none" start_char="12196" end_char="12198">and</TOKEN>
<TOKEN id="token-82-8" pos="word" morph="none" start_char="12200" end_char="12207">evidence</TOKEN>
<TOKEN id="token-82-9" pos="punct" morph="none" start_char="12208" end_char="12209">."</TOKEN>
</SEG>
<SEG id="segment-83" start_char="12212" end_char="12408">
<ORIGINAL_TEXT>The council also defended the Chinese government’s response, writing, "China has also acted with a keen sense of responsibility to humanity, its people, posterity, and the international community."</ORIGINAL_TEXT>
<TOKEN id="token-83-0" pos="word" morph="none" start_char="12212" end_char="12214">The</TOKEN>
<TOKEN id="token-83-1" pos="word" morph="none" start_char="12216" end_char="12222">council</TOKEN>
<TOKEN id="token-83-2" pos="word" morph="none" start_char="12224" end_char="12227">also</TOKEN>
<TOKEN id="token-83-3" pos="word" morph="none" start_char="12229" end_char="12236">defended</TOKEN>
<TOKEN id="token-83-4" pos="word" morph="none" start_char="12238" end_char="12240">the</TOKEN>
<TOKEN id="token-83-5" pos="word" morph="none" start_char="12242" end_char="12248">Chinese</TOKEN>
<TOKEN id="token-83-6" pos="word" morph="none" start_char="12250" end_char="12261">government’s</TOKEN>
<TOKEN id="token-83-7" pos="word" morph="none" start_char="12263" end_char="12270">response</TOKEN>
<TOKEN id="token-83-8" pos="punct" morph="none" start_char="12271" end_char="12271">,</TOKEN>
<TOKEN id="token-83-9" pos="word" morph="none" start_char="12273" end_char="12279">writing</TOKEN>
<TOKEN id="token-83-10" pos="punct" morph="none" start_char="12280" end_char="12280">,</TOKEN>
<TOKEN id="token-83-11" pos="punct" morph="none" start_char="12282" end_char="12282">"</TOKEN>
<TOKEN id="token-83-12" pos="word" morph="none" start_char="12283" end_char="12287">China</TOKEN>
<TOKEN id="token-83-13" pos="word" morph="none" start_char="12289" end_char="12291">has</TOKEN>
<TOKEN id="token-83-14" pos="word" morph="none" start_char="12293" end_char="12296">also</TOKEN>
<TOKEN id="token-83-15" pos="word" morph="none" start_char="12298" end_char="12302">acted</TOKEN>
<TOKEN id="token-83-16" pos="word" morph="none" start_char="12304" end_char="12307">with</TOKEN>
<TOKEN id="token-83-17" pos="word" morph="none" start_char="12309" end_char="12309">a</TOKEN>
<TOKEN id="token-83-18" pos="word" morph="none" start_char="12311" end_char="12314">keen</TOKEN>
<TOKEN id="token-83-19" pos="word" morph="none" start_char="12316" end_char="12320">sense</TOKEN>
<TOKEN id="token-83-20" pos="word" morph="none" start_char="12322" end_char="12323">of</TOKEN>
<TOKEN id="token-83-21" pos="word" morph="none" start_char="12325" end_char="12338">responsibility</TOKEN>
<TOKEN id="token-83-22" pos="word" morph="none" start_char="12340" end_char="12341">to</TOKEN>
<TOKEN id="token-83-23" pos="word" morph="none" start_char="12343" end_char="12350">humanity</TOKEN>
<TOKEN id="token-83-24" pos="punct" morph="none" start_char="12351" end_char="12351">,</TOKEN>
<TOKEN id="token-83-25" pos="word" morph="none" start_char="12353" end_char="12355">its</TOKEN>
<TOKEN id="token-83-26" pos="word" morph="none" start_char="12357" end_char="12362">people</TOKEN>
<TOKEN id="token-83-27" pos="punct" morph="none" start_char="12363" end_char="12363">,</TOKEN>
<TOKEN id="token-83-28" pos="word" morph="none" start_char="12365" end_char="12373">posterity</TOKEN>
<TOKEN id="token-83-29" pos="punct" morph="none" start_char="12374" end_char="12374">,</TOKEN>
<TOKEN id="token-83-30" pos="word" morph="none" start_char="12376" end_char="12378">and</TOKEN>
<TOKEN id="token-83-31" pos="word" morph="none" start_char="12380" end_char="12382">the</TOKEN>
<TOKEN id="token-83-32" pos="word" morph="none" start_char="12384" end_char="12396">international</TOKEN>
<TOKEN id="token-83-33" pos="word" morph="none" start_char="12398" end_char="12406">community</TOKEN>
<TOKEN id="token-83-34" pos="punct" morph="none" start_char="12407" end_char="12408">."</TOKEN>
</SEG>
<SEG id="segment-84" start_char="12411" end_char="12644">
<ORIGINAL_TEXT>Tuesday a spokesperson for the Chinese Foreign Ministry told reporters she had not seen the Harvard study, but thought it was "ridiculous to come to this kind of conclusion based on superficial [observations], such as traffic volumes.</ORIGINAL_TEXT>
<TOKEN id="token-84-0" pos="word" morph="none" start_char="12411" end_char="12417">Tuesday</TOKEN>
<TOKEN id="token-84-1" pos="word" morph="none" start_char="12419" end_char="12419">a</TOKEN>
<TOKEN id="token-84-2" pos="word" morph="none" start_char="12421" end_char="12432">spokesperson</TOKEN>
<TOKEN id="token-84-3" pos="word" morph="none" start_char="12434" end_char="12436">for</TOKEN>
<TOKEN id="token-84-4" pos="word" morph="none" start_char="12438" end_char="12440">the</TOKEN>
<TOKEN id="token-84-5" pos="word" morph="none" start_char="12442" end_char="12448">Chinese</TOKEN>
<TOKEN id="token-84-6" pos="word" morph="none" start_char="12450" end_char="12456">Foreign</TOKEN>
<TOKEN id="token-84-7" pos="word" morph="none" start_char="12458" end_char="12465">Ministry</TOKEN>
<TOKEN id="token-84-8" pos="word" morph="none" start_char="12467" end_char="12470">told</TOKEN>
<TOKEN id="token-84-9" pos="word" morph="none" start_char="12472" end_char="12480">reporters</TOKEN>
<TOKEN id="token-84-10" pos="word" morph="none" start_char="12482" end_char="12484">she</TOKEN>
<TOKEN id="token-84-11" pos="word" morph="none" start_char="12486" end_char="12488">had</TOKEN>
<TOKEN id="token-84-12" pos="word" morph="none" start_char="12490" end_char="12492">not</TOKEN>
<TOKEN id="token-84-13" pos="word" morph="none" start_char="12494" end_char="12497">seen</TOKEN>
<TOKEN id="token-84-14" pos="word" morph="none" start_char="12499" end_char="12501">the</TOKEN>
<TOKEN id="token-84-15" pos="word" morph="none" start_char="12503" end_char="12509">Harvard</TOKEN>
<TOKEN id="token-84-16" pos="word" morph="none" start_char="12511" end_char="12515">study</TOKEN>
<TOKEN id="token-84-17" pos="punct" morph="none" start_char="12516" end_char="12516">,</TOKEN>
<TOKEN id="token-84-18" pos="word" morph="none" start_char="12518" end_char="12520">but</TOKEN>
<TOKEN id="token-84-19" pos="word" morph="none" start_char="12522" end_char="12528">thought</TOKEN>
<TOKEN id="token-84-20" pos="word" morph="none" start_char="12530" end_char="12531">it</TOKEN>
<TOKEN id="token-84-21" pos="word" morph="none" start_char="12533" end_char="12535">was</TOKEN>
<TOKEN id="token-84-22" pos="punct" morph="none" start_char="12537" end_char="12537">"</TOKEN>
<TOKEN id="token-84-23" pos="word" morph="none" start_char="12538" end_char="12547">ridiculous</TOKEN>
<TOKEN id="token-84-24" pos="word" morph="none" start_char="12549" end_char="12550">to</TOKEN>
<TOKEN id="token-84-25" pos="word" morph="none" start_char="12552" end_char="12555">come</TOKEN>
<TOKEN id="token-84-26" pos="word" morph="none" start_char="12557" end_char="12558">to</TOKEN>
<TOKEN id="token-84-27" pos="word" morph="none" start_char="12560" end_char="12563">this</TOKEN>
<TOKEN id="token-84-28" pos="word" morph="none" start_char="12565" end_char="12568">kind</TOKEN>
<TOKEN id="token-84-29" pos="word" morph="none" start_char="12570" end_char="12571">of</TOKEN>
<TOKEN id="token-84-30" pos="word" morph="none" start_char="12573" end_char="12582">conclusion</TOKEN>
<TOKEN id="token-84-31" pos="word" morph="none" start_char="12584" end_char="12588">based</TOKEN>
<TOKEN id="token-84-32" pos="word" morph="none" start_char="12590" end_char="12591">on</TOKEN>
<TOKEN id="token-84-33" pos="word" morph="none" start_char="12593" end_char="12603">superficial</TOKEN>
<TOKEN id="token-84-34" pos="punct" morph="none" start_char="12605" end_char="12605">[</TOKEN>
<TOKEN id="token-84-35" pos="word" morph="none" start_char="12606" end_char="12617">observations</TOKEN>
<TOKEN id="token-84-36" pos="punct" morph="none" start_char="12618" end_char="12619">],</TOKEN>
<TOKEN id="token-84-37" pos="word" morph="none" start_char="12621" end_char="12624">such</TOKEN>
<TOKEN id="token-84-38" pos="word" morph="none" start_char="12626" end_char="12627">as</TOKEN>
<TOKEN id="token-84-39" pos="word" morph="none" start_char="12629" end_char="12635">traffic</TOKEN>
<TOKEN id="token-84-40" pos="word" morph="none" start_char="12637" end_char="12643">volumes</TOKEN>
<TOKEN id="token-84-41" pos="punct" morph="none" start_char="12644" end_char="12644">.</TOKEN>
</SEG>
<SEG id="segment-85" start_char="12646" end_char="12674">
<ORIGINAL_TEXT>It is incredibly ridiculous."</ORIGINAL_TEXT>
<TOKEN id="token-85-0" pos="word" morph="none" start_char="12646" end_char="12647">It</TOKEN>
<TOKEN id="token-85-1" pos="word" morph="none" start_char="12649" end_char="12650">is</TOKEN>
<TOKEN id="token-85-2" pos="word" morph="none" start_char="12652" end_char="12661">incredibly</TOKEN>
<TOKEN id="token-85-3" pos="word" morph="none" start_char="12663" end_char="12672">ridiculous</TOKEN>
<TOKEN id="token-85-4" pos="punct" morph="none" start_char="12673" end_char="12674">."</TOKEN>
</SEG>
<SEG id="segment-86" start_char="12677" end_char="12891">
<ORIGINAL_TEXT>The spokesperson, Hua Chunying, said she was not a scientist, but found the conclusions "very far-fetched" and more generally urged the U.S. and China to work together on combating what she called false information.</ORIGINAL_TEXT>
<TOKEN id="token-86-0" pos="word" morph="none" start_char="12677" end_char="12679">The</TOKEN>
<TOKEN id="token-86-1" pos="word" morph="none" start_char="12681" end_char="12692">spokesperson</TOKEN>
<TOKEN id="token-86-2" pos="punct" morph="none" start_char="12693" end_char="12693">,</TOKEN>
<TOKEN id="token-86-3" pos="word" morph="none" start_char="12695" end_char="12697">Hua</TOKEN>
<TOKEN id="token-86-4" pos="word" morph="none" start_char="12699" end_char="12706">Chunying</TOKEN>
<TOKEN id="token-86-5" pos="punct" morph="none" start_char="12707" end_char="12707">,</TOKEN>
<TOKEN id="token-86-6" pos="word" morph="none" start_char="12709" end_char="12712">said</TOKEN>
<TOKEN id="token-86-7" pos="word" morph="none" start_char="12714" end_char="12716">she</TOKEN>
<TOKEN id="token-86-8" pos="word" morph="none" start_char="12718" end_char="12720">was</TOKEN>
<TOKEN id="token-86-9" pos="word" morph="none" start_char="12722" end_char="12724">not</TOKEN>
<TOKEN id="token-86-10" pos="word" morph="none" start_char="12726" end_char="12726">a</TOKEN>
<TOKEN id="token-86-11" pos="word" morph="none" start_char="12728" end_char="12736">scientist</TOKEN>
<TOKEN id="token-86-12" pos="punct" morph="none" start_char="12737" end_char="12737">,</TOKEN>
<TOKEN id="token-86-13" pos="word" morph="none" start_char="12739" end_char="12741">but</TOKEN>
<TOKEN id="token-86-14" pos="word" morph="none" start_char="12743" end_char="12747">found</TOKEN>
<TOKEN id="token-86-15" pos="word" morph="none" start_char="12749" end_char="12751">the</TOKEN>
<TOKEN id="token-86-16" pos="word" morph="none" start_char="12753" end_char="12763">conclusions</TOKEN>
<TOKEN id="token-86-17" pos="punct" morph="none" start_char="12765" end_char="12765">"</TOKEN>
<TOKEN id="token-86-18" pos="word" morph="none" start_char="12766" end_char="12769">very</TOKEN>
<TOKEN id="token-86-19" pos="unknown" morph="none" start_char="12771" end_char="12781">far-fetched</TOKEN>
<TOKEN id="token-86-20" pos="punct" morph="none" start_char="12782" end_char="12782">"</TOKEN>
<TOKEN id="token-86-21" pos="word" morph="none" start_char="12784" end_char="12786">and</TOKEN>
<TOKEN id="token-86-22" pos="word" morph="none" start_char="12788" end_char="12791">more</TOKEN>
<TOKEN id="token-86-23" pos="word" morph="none" start_char="12793" end_char="12801">generally</TOKEN>
<TOKEN id="token-86-24" pos="word" morph="none" start_char="12803" end_char="12807">urged</TOKEN>
<TOKEN id="token-86-25" pos="word" morph="none" start_char="12809" end_char="12811">the</TOKEN>
<TOKEN id="token-86-26" pos="unknown" morph="none" start_char="12813" end_char="12815">U.S</TOKEN>
<TOKEN id="token-86-27" pos="punct" morph="none" start_char="12816" end_char="12816">.</TOKEN>
<TOKEN id="token-86-28" pos="word" morph="none" start_char="12818" end_char="12820">and</TOKEN>
<TOKEN id="token-86-29" pos="word" morph="none" start_char="12822" end_char="12826">China</TOKEN>
<TOKEN id="token-86-30" pos="word" morph="none" start_char="12828" end_char="12829">to</TOKEN>
<TOKEN id="token-86-31" pos="word" morph="none" start_char="12831" end_char="12834">work</TOKEN>
<TOKEN id="token-86-32" pos="word" morph="none" start_char="12836" end_char="12843">together</TOKEN>
<TOKEN id="token-86-33" pos="word" morph="none" start_char="12845" end_char="12846">on</TOKEN>
<TOKEN id="token-86-34" pos="word" morph="none" start_char="12848" end_char="12856">combating</TOKEN>
<TOKEN id="token-86-35" pos="word" morph="none" start_char="12858" end_char="12861">what</TOKEN>
<TOKEN id="token-86-36" pos="word" morph="none" start_char="12863" end_char="12865">she</TOKEN>
<TOKEN id="token-86-37" pos="word" morph="none" start_char="12867" end_char="12872">called</TOKEN>
<TOKEN id="token-86-38" pos="word" morph="none" start_char="12874" end_char="12878">false</TOKEN>
<TOKEN id="token-86-39" pos="word" morph="none" start_char="12880" end_char="12890">information</TOKEN>
<TOKEN id="token-86-40" pos="punct" morph="none" start_char="12891" end_char="12891">.</TOKEN>
</SEG>
<SEG id="segment-87" start_char="12894" end_char="12960">
<ORIGINAL_TEXT>On the ground, internet searches for symptoms associated with COVID</ORIGINAL_TEXT>
<TOKEN id="token-87-0" pos="word" morph="none" start_char="12894" end_char="12895">On</TOKEN>
<TOKEN id="token-87-1" pos="word" morph="none" start_char="12897" end_char="12899">the</TOKEN>
<TOKEN id="token-87-2" pos="word" morph="none" start_char="12901" end_char="12906">ground</TOKEN>
<TOKEN id="token-87-3" pos="punct" morph="none" start_char="12907" end_char="12907">,</TOKEN>
<TOKEN id="token-87-4" pos="word" morph="none" start_char="12909" end_char="12916">internet</TOKEN>
<TOKEN id="token-87-5" pos="word" morph="none" start_char="12918" end_char="12925">searches</TOKEN>
<TOKEN id="token-87-6" pos="word" morph="none" start_char="12927" end_char="12929">for</TOKEN>
<TOKEN id="token-87-7" pos="word" morph="none" start_char="12931" end_char="12938">symptoms</TOKEN>
<TOKEN id="token-87-8" pos="word" morph="none" start_char="12940" end_char="12949">associated</TOKEN>
<TOKEN id="token-87-9" pos="word" morph="none" start_char="12951" end_char="12954">with</TOKEN>
<TOKEN id="token-87-10" pos="word" morph="none" start_char="12956" end_char="12960">COVID</TOKEN>
</SEG>
<SEG id="segment-88" start_char="12963" end_char="13104">
<ORIGINAL_TEXT>Brownstein said he and his researchers found the hospital-traffic data to be even more compelling after digging into internet search patterns.</ORIGINAL_TEXT>
<TOKEN id="token-88-0" pos="word" morph="none" start_char="12963" end_char="12972">Brownstein</TOKEN>
<TOKEN id="token-88-1" pos="word" morph="none" start_char="12974" end_char="12977">said</TOKEN>
<TOKEN id="token-88-2" pos="word" morph="none" start_char="12979" end_char="12980">he</TOKEN>
<TOKEN id="token-88-3" pos="word" morph="none" start_char="12982" end_char="12984">and</TOKEN>
<TOKEN id="token-88-4" pos="word" morph="none" start_char="12986" end_char="12988">his</TOKEN>
<TOKEN id="token-88-5" pos="word" morph="none" start_char="12990" end_char="13000">researchers</TOKEN>
<TOKEN id="token-88-6" pos="word" morph="none" start_char="13002" end_char="13006">found</TOKEN>
<TOKEN id="token-88-7" pos="word" morph="none" start_char="13008" end_char="13010">the</TOKEN>
<TOKEN id="token-88-8" pos="unknown" morph="none" start_char="13012" end_char="13027">hospital-traffic</TOKEN>
<TOKEN id="token-88-9" pos="word" morph="none" start_char="13029" end_char="13032">data</TOKEN>
<TOKEN id="token-88-10" pos="word" morph="none" start_char="13034" end_char="13035">to</TOKEN>
<TOKEN id="token-88-11" pos="word" morph="none" start_char="13037" end_char="13038">be</TOKEN>
<TOKEN id="token-88-12" pos="word" morph="none" start_char="13040" end_char="13043">even</TOKEN>
<TOKEN id="token-88-13" pos="word" morph="none" start_char="13045" end_char="13048">more</TOKEN>
<TOKEN id="token-88-14" pos="word" morph="none" start_char="13050" end_char="13059">compelling</TOKEN>
<TOKEN id="token-88-15" pos="word" morph="none" start_char="13061" end_char="13065">after</TOKEN>
<TOKEN id="token-88-16" pos="word" morph="none" start_char="13067" end_char="13073">digging</TOKEN>
<TOKEN id="token-88-17" pos="word" morph="none" start_char="13075" end_char="13078">into</TOKEN>
<TOKEN id="token-88-18" pos="word" morph="none" start_char="13080" end_char="13087">internet</TOKEN>
<TOKEN id="token-88-19" pos="word" morph="none" start_char="13089" end_char="13094">search</TOKEN>
<TOKEN id="token-88-20" pos="word" morph="none" start_char="13096" end_char="13103">patterns</TOKEN>
<TOKEN id="token-88-21" pos="punct" morph="none" start_char="13104" end_char="13104">.</TOKEN>
</SEG>
<SEG id="segment-89" start_char="13106" end_char="13300">
<ORIGINAL_TEXT>Around the time the hospital traffic was surging, there was a spike in online traffic in the Wuhan region among users asking China’s Baidu search engine for information on "cough" and "diarrhea."</ORIGINAL_TEXT>
<TOKEN id="token-89-0" pos="word" morph="none" start_char="13106" end_char="13111">Around</TOKEN>
<TOKEN id="token-89-1" pos="word" morph="none" start_char="13113" end_char="13115">the</TOKEN>
<TOKEN id="token-89-2" pos="word" morph="none" start_char="13117" end_char="13120">time</TOKEN>
<TOKEN id="token-89-3" pos="word" morph="none" start_char="13122" end_char="13124">the</TOKEN>
<TOKEN id="token-89-4" pos="word" morph="none" start_char="13126" end_char="13133">hospital</TOKEN>
<TOKEN id="token-89-5" pos="word" morph="none" start_char="13135" end_char="13141">traffic</TOKEN>
<TOKEN id="token-89-6" pos="word" morph="none" start_char="13143" end_char="13145">was</TOKEN>
<TOKEN id="token-89-7" pos="word" morph="none" start_char="13147" end_char="13153">surging</TOKEN>
<TOKEN id="token-89-8" pos="punct" morph="none" start_char="13154" end_char="13154">,</TOKEN>
<TOKEN id="token-89-9" pos="word" morph="none" start_char="13156" end_char="13160">there</TOKEN>
<TOKEN id="token-89-10" pos="word" morph="none" start_char="13162" end_char="13164">was</TOKEN>
<TOKEN id="token-89-11" pos="word" morph="none" start_char="13166" end_char="13166">a</TOKEN>
<TOKEN id="token-89-12" pos="word" morph="none" start_char="13168" end_char="13172">spike</TOKEN>
<TOKEN id="token-89-13" pos="word" morph="none" start_char="13174" end_char="13175">in</TOKEN>
<TOKEN id="token-89-14" pos="word" morph="none" start_char="13177" end_char="13182">online</TOKEN>
<TOKEN id="token-89-15" pos="word" morph="none" start_char="13184" end_char="13190">traffic</TOKEN>
<TOKEN id="token-89-16" pos="word" morph="none" start_char="13192" end_char="13193">in</TOKEN>
<TOKEN id="token-89-17" pos="word" morph="none" start_char="13195" end_char="13197">the</TOKEN>
<TOKEN id="token-89-18" pos="word" morph="none" start_char="13199" end_char="13203">Wuhan</TOKEN>
<TOKEN id="token-89-19" pos="word" morph="none" start_char="13205" end_char="13210">region</TOKEN>
<TOKEN id="token-89-20" pos="word" morph="none" start_char="13212" end_char="13216">among</TOKEN>
<TOKEN id="token-89-21" pos="word" morph="none" start_char="13218" end_char="13222">users</TOKEN>
<TOKEN id="token-89-22" pos="word" morph="none" start_char="13224" end_char="13229">asking</TOKEN>
<TOKEN id="token-89-23" pos="word" morph="none" start_char="13231" end_char="13237">China’s</TOKEN>
<TOKEN id="token-89-24" pos="word" morph="none" start_char="13239" end_char="13243">Baidu</TOKEN>
<TOKEN id="token-89-25" pos="word" morph="none" start_char="13245" end_char="13250">search</TOKEN>
<TOKEN id="token-89-26" pos="word" morph="none" start_char="13252" end_char="13257">engine</TOKEN>
<TOKEN id="token-89-27" pos="word" morph="none" start_char="13259" end_char="13261">for</TOKEN>
<TOKEN id="token-89-28" pos="word" morph="none" start_char="13263" end_char="13273">information</TOKEN>
<TOKEN id="token-89-29" pos="word" morph="none" start_char="13275" end_char="13276">on</TOKEN>
<TOKEN id="token-89-30" pos="punct" morph="none" start_char="13278" end_char="13278">"</TOKEN>
<TOKEN id="token-89-31" pos="word" morph="none" start_char="13279" end_char="13283">cough</TOKEN>
<TOKEN id="token-89-32" pos="punct" morph="none" start_char="13284" end_char="13284">"</TOKEN>
<TOKEN id="token-89-33" pos="word" morph="none" start_char="13286" end_char="13288">and</TOKEN>
<TOKEN id="token-89-34" pos="punct" morph="none" start_char="13290" end_char="13290">"</TOKEN>
<TOKEN id="token-89-35" pos="word" morph="none" start_char="13291" end_char="13298">diarrhea</TOKEN>
<TOKEN id="token-89-36" pos="punct" morph="none" start_char="13299" end_char="13300">."</TOKEN>
</SEG>
<SEG id="segment-90" start_char="13303" end_char="13549">
<ORIGINAL_TEXT>"While queries of the respiratory symptom ‘cough’ show seasonal fluctuations coinciding with yearly influenza seasons, ‘diarrhea’ is a more COVID-19-specific symptom and only shows an association with the current epidemic," according to the study.</ORIGINAL_TEXT>
<TOKEN id="token-90-0" pos="punct" morph="none" start_char="13303" end_char="13303">"</TOKEN>
<TOKEN id="token-90-1" pos="word" morph="none" start_char="13304" end_char="13308">While</TOKEN>
<TOKEN id="token-90-2" pos="word" morph="none" start_char="13310" end_char="13316">queries</TOKEN>
<TOKEN id="token-90-3" pos="word" morph="none" start_char="13318" end_char="13319">of</TOKEN>
<TOKEN id="token-90-4" pos="word" morph="none" start_char="13321" end_char="13323">the</TOKEN>
<TOKEN id="token-90-5" pos="word" morph="none" start_char="13325" end_char="13335">respiratory</TOKEN>
<TOKEN id="token-90-6" pos="word" morph="none" start_char="13337" end_char="13343">symptom</TOKEN>
<TOKEN id="token-90-7" pos="punct" morph="none" start_char="13345" end_char="13345">‘</TOKEN>
<TOKEN id="token-90-8" pos="word" morph="none" start_char="13346" end_char="13350">cough</TOKEN>
<TOKEN id="token-90-9" pos="punct" morph="none" start_char="13351" end_char="13351">’</TOKEN>
<TOKEN id="token-90-10" pos="word" morph="none" start_char="13353" end_char="13356">show</TOKEN>
<TOKEN id="token-90-11" pos="word" morph="none" start_char="13358" end_char="13365">seasonal</TOKEN>
<TOKEN id="token-90-12" pos="word" morph="none" start_char="13367" end_char="13378">fluctuations</TOKEN>
<TOKEN id="token-90-13" pos="word" morph="none" start_char="13380" end_char="13389">coinciding</TOKEN>
<TOKEN id="token-90-14" pos="word" morph="none" start_char="13391" end_char="13394">with</TOKEN>
<TOKEN id="token-90-15" pos="word" morph="none" start_char="13396" end_char="13401">yearly</TOKEN>
<TOKEN id="token-90-16" pos="word" morph="none" start_char="13403" end_char="13411">influenza</TOKEN>
<TOKEN id="token-90-17" pos="word" morph="none" start_char="13413" end_char="13419">seasons</TOKEN>
<TOKEN id="token-90-18" pos="punct" morph="none" start_char="13420" end_char="13420">,</TOKEN>
<TOKEN id="token-90-19" pos="punct" morph="none" start_char="13422" end_char="13422">‘</TOKEN>
<TOKEN id="token-90-20" pos="word" morph="none" start_char="13423" end_char="13430">diarrhea</TOKEN>
<TOKEN id="token-90-21" pos="punct" morph="none" start_char="13431" end_char="13431">’</TOKEN>
<TOKEN id="token-90-22" pos="word" morph="none" start_char="13433" end_char="13434">is</TOKEN>
<TOKEN id="token-90-23" pos="word" morph="none" start_char="13436" end_char="13436">a</TOKEN>
<TOKEN id="token-90-24" pos="word" morph="none" start_char="13438" end_char="13441">more</TOKEN>
<TOKEN id="token-90-25" pos="unknown" morph="none" start_char="13443" end_char="13459">COVID-19-specific</TOKEN>
<TOKEN id="token-90-26" pos="word" morph="none" start_char="13461" end_char="13467">symptom</TOKEN>
<TOKEN id="token-90-27" pos="word" morph="none" start_char="13469" end_char="13471">and</TOKEN>
<TOKEN id="token-90-28" pos="word" morph="none" start_char="13473" end_char="13476">only</TOKEN>
<TOKEN id="token-90-29" pos="word" morph="none" start_char="13478" end_char="13482">shows</TOKEN>
<TOKEN id="token-90-30" pos="word" morph="none" start_char="13484" end_char="13485">an</TOKEN>
<TOKEN id="token-90-31" pos="word" morph="none" start_char="13487" end_char="13497">association</TOKEN>
<TOKEN id="token-90-32" pos="word" morph="none" start_char="13499" end_char="13502">with</TOKEN>
<TOKEN id="token-90-33" pos="word" morph="none" start_char="13504" end_char="13506">the</TOKEN>
<TOKEN id="token-90-34" pos="word" morph="none" start_char="13508" end_char="13514">current</TOKEN>
<TOKEN id="token-90-35" pos="word" morph="none" start_char="13516" end_char="13523">epidemic</TOKEN>
<TOKEN id="token-90-36" pos="punct" morph="none" start_char="13524" end_char="13525">,"</TOKEN>
<TOKEN id="token-90-37" pos="word" morph="none" start_char="13527" end_char="13535">according</TOKEN>
<TOKEN id="token-90-38" pos="word" morph="none" start_char="13537" end_char="13538">to</TOKEN>
<TOKEN id="token-90-39" pos="word" morph="none" start_char="13540" end_char="13542">the</TOKEN>
<TOKEN id="token-90-40" pos="word" morph="none" start_char="13544" end_char="13548">study</TOKEN>
<TOKEN id="token-90-41" pos="punct" morph="none" start_char="13549" end_char="13549">.</TOKEN>
</SEG>
<SEG id="segment-91" start_char="13551" end_char="13647">
<ORIGINAL_TEXT>"The increase of both signals precede the documented start of the COVID-19 pandemic in December."</ORIGINAL_TEXT>
<TOKEN id="token-91-0" pos="punct" morph="none" start_char="13551" end_char="13551">"</TOKEN>
<TOKEN id="token-91-1" pos="word" morph="none" start_char="13552" end_char="13554">The</TOKEN>
<TOKEN id="token-91-2" pos="word" morph="none" start_char="13556" end_char="13563">increase</TOKEN>
<TOKEN id="token-91-3" pos="word" morph="none" start_char="13565" end_char="13566">of</TOKEN>
<TOKEN id="token-91-4" pos="word" morph="none" start_char="13568" end_char="13571">both</TOKEN>
<TOKEN id="token-91-5" pos="word" morph="none" start_char="13573" end_char="13579">signals</TOKEN>
<TOKEN id="token-91-6" pos="word" morph="none" start_char="13581" end_char="13587">precede</TOKEN>
<TOKEN id="token-91-7" pos="word" morph="none" start_char="13589" end_char="13591">the</TOKEN>
<TOKEN id="token-91-8" pos="word" morph="none" start_char="13593" end_char="13602">documented</TOKEN>
<TOKEN id="token-91-9" pos="word" morph="none" start_char="13604" end_char="13608">start</TOKEN>
<TOKEN id="token-91-10" pos="word" morph="none" start_char="13610" end_char="13611">of</TOKEN>
<TOKEN id="token-91-11" pos="word" morph="none" start_char="13613" end_char="13615">the</TOKEN>
<TOKEN id="token-91-12" pos="unknown" morph="none" start_char="13617" end_char="13624">COVID-19</TOKEN>
<TOKEN id="token-91-13" pos="word" morph="none" start_char="13626" end_char="13633">pandemic</TOKEN>
<TOKEN id="token-91-14" pos="word" morph="none" start_char="13635" end_char="13636">in</TOKEN>
<TOKEN id="token-91-15" pos="word" morph="none" start_char="13638" end_char="13645">December</TOKEN>
<TOKEN id="token-91-16" pos="punct" morph="none" start_char="13646" end_char="13647">."</TOKEN>
</SEG>
<SEG id="segment-92" start_char="13650" end_char="13796">
<ORIGINAL_TEXT>"We've done previous studies where we could show that what people search for online is an indicator of disease in the population," Brownstein said.</ORIGINAL_TEXT>
<TOKEN id="token-92-0" pos="punct" morph="none" start_char="13650" end_char="13650">"</TOKEN>
<TOKEN id="token-92-1" pos="word" morph="none" start_char="13651" end_char="13655">We've</TOKEN>
<TOKEN id="token-92-2" pos="word" morph="none" start_char="13657" end_char="13660">done</TOKEN>
<TOKEN id="token-92-3" pos="word" morph="none" start_char="13662" end_char="13669">previous</TOKEN>
<TOKEN id="token-92-4" pos="word" morph="none" start_char="13671" end_char="13677">studies</TOKEN>
<TOKEN id="token-92-5" pos="word" morph="none" start_char="13679" end_char="13683">where</TOKEN>
<TOKEN id="token-92-6" pos="word" morph="none" start_char="13685" end_char="13686">we</TOKEN>
<TOKEN id="token-92-7" pos="word" morph="none" start_char="13688" end_char="13692">could</TOKEN>
<TOKEN id="token-92-8" pos="word" morph="none" start_char="13694" end_char="13697">show</TOKEN>
<TOKEN id="token-92-9" pos="word" morph="none" start_char="13699" end_char="13702">that</TOKEN>
<TOKEN id="token-92-10" pos="word" morph="none" start_char="13704" end_char="13707">what</TOKEN>
<TOKEN id="token-92-11" pos="word" morph="none" start_char="13709" end_char="13714">people</TOKEN>
<TOKEN id="token-92-12" pos="word" morph="none" start_char="13716" end_char="13721">search</TOKEN>
<TOKEN id="token-92-13" pos="word" morph="none" start_char="13723" end_char="13725">for</TOKEN>
<TOKEN id="token-92-14" pos="word" morph="none" start_char="13727" end_char="13732">online</TOKEN>
<TOKEN id="token-92-15" pos="word" morph="none" start_char="13734" end_char="13735">is</TOKEN>
<TOKEN id="token-92-16" pos="word" morph="none" start_char="13737" end_char="13738">an</TOKEN>
<TOKEN id="token-92-17" pos="word" morph="none" start_char="13740" end_char="13748">indicator</TOKEN>
<TOKEN id="token-92-18" pos="word" morph="none" start_char="13750" end_char="13751">of</TOKEN>
<TOKEN id="token-92-19" pos="word" morph="none" start_char="13753" end_char="13759">disease</TOKEN>
<TOKEN id="token-92-20" pos="word" morph="none" start_char="13761" end_char="13762">in</TOKEN>
<TOKEN id="token-92-21" pos="word" morph="none" start_char="13764" end_char="13766">the</TOKEN>
<TOKEN id="token-92-22" pos="word" morph="none" start_char="13768" end_char="13777">population</TOKEN>
<TOKEN id="token-92-23" pos="punct" morph="none" start_char="13778" end_char="13779">,"</TOKEN>
<TOKEN id="token-92-24" pos="word" morph="none" start_char="13781" end_char="13790">Brownstein</TOKEN>
<TOKEN id="token-92-25" pos="word" morph="none" start_char="13792" end_char="13795">said</TOKEN>
<TOKEN id="token-92-26" pos="punct" morph="none" start_char="13796" end_char="13796">.</TOKEN>
</SEG>
<SEG id="segment-93" start_char="13798" end_char="13905">
<ORIGINAL_TEXT>"And we actually saw people searching for symptoms that might be related to COVID: diarrheal disease, cough.</ORIGINAL_TEXT>
<TOKEN id="token-93-0" pos="punct" morph="none" start_char="13798" end_char="13798">"</TOKEN>
<TOKEN id="token-93-1" pos="word" morph="none" start_char="13799" end_char="13801">And</TOKEN>
<TOKEN id="token-93-2" pos="word" morph="none" start_char="13803" end_char="13804">we</TOKEN>
<TOKEN id="token-93-3" pos="word" morph="none" start_char="13806" end_char="13813">actually</TOKEN>
<TOKEN id="token-93-4" pos="word" morph="none" start_char="13815" end_char="13817">saw</TOKEN>
<TOKEN id="token-93-5" pos="word" morph="none" start_char="13819" end_char="13824">people</TOKEN>
<TOKEN id="token-93-6" pos="word" morph="none" start_char="13826" end_char="13834">searching</TOKEN>
<TOKEN id="token-93-7" pos="word" morph="none" start_char="13836" end_char="13838">for</TOKEN>
<TOKEN id="token-93-8" pos="word" morph="none" start_char="13840" end_char="13847">symptoms</TOKEN>
<TOKEN id="token-93-9" pos="word" morph="none" start_char="13849" end_char="13852">that</TOKEN>
<TOKEN id="token-93-10" pos="word" morph="none" start_char="13854" end_char="13858">might</TOKEN>
<TOKEN id="token-93-11" pos="word" morph="none" start_char="13860" end_char="13861">be</TOKEN>
<TOKEN id="token-93-12" pos="word" morph="none" start_char="13863" end_char="13869">related</TOKEN>
<TOKEN id="token-93-13" pos="word" morph="none" start_char="13871" end_char="13872">to</TOKEN>
<TOKEN id="token-93-14" pos="word" morph="none" start_char="13874" end_char="13878">COVID</TOKEN>
<TOKEN id="token-93-15" pos="punct" morph="none" start_char="13879" end_char="13879">:</TOKEN>
<TOKEN id="token-93-16" pos="word" morph="none" start_char="13881" end_char="13889">diarrheal</TOKEN>
<TOKEN id="token-93-17" pos="word" morph="none" start_char="13891" end_char="13897">disease</TOKEN>
<TOKEN id="token-93-18" pos="punct" morph="none" start_char="13898" end_char="13898">,</TOKEN>
<TOKEN id="token-93-19" pos="word" morph="none" start_char="13900" end_char="13904">cough</TOKEN>
<TOKEN id="token-93-20" pos="punct" morph="none" start_char="13905" end_char="13905">.</TOKEN>
</SEG>
<SEG id="segment-94" start_char="13907" end_char="13953">
<ORIGINAL_TEXT>That was even starting as early as late summer.</ORIGINAL_TEXT>
<TOKEN id="token-94-0" pos="word" morph="none" start_char="13907" end_char="13910">That</TOKEN>
<TOKEN id="token-94-1" pos="word" morph="none" start_char="13912" end_char="13914">was</TOKEN>
<TOKEN id="token-94-2" pos="word" morph="none" start_char="13916" end_char="13919">even</TOKEN>
<TOKEN id="token-94-3" pos="word" morph="none" start_char="13921" end_char="13928">starting</TOKEN>
<TOKEN id="token-94-4" pos="word" morph="none" start_char="13930" end_char="13931">as</TOKEN>
<TOKEN id="token-94-5" pos="word" morph="none" start_char="13933" end_char="13937">early</TOKEN>
<TOKEN id="token-94-6" pos="word" morph="none" start_char="13939" end_char="13940">as</TOKEN>
<TOKEN id="token-94-7" pos="word" morph="none" start_char="13942" end_char="13945">late</TOKEN>
<TOKEN id="token-94-8" pos="word" morph="none" start_char="13947" end_char="13952">summer</TOKEN>
<TOKEN id="token-94-9" pos="punct" morph="none" start_char="13953" end_char="13953">.</TOKEN>
</SEG>
<SEG id="segment-95" start_char="13956" end_char="14098">
<ORIGINAL_TEXT>"Now, we can't confirm 100% what the virus was that was causing this illness and what was causing this business in hospitals," Brownstein said.</ORIGINAL_TEXT>
<TOKEN id="token-95-0" pos="punct" morph="none" start_char="13956" end_char="13956">"</TOKEN>
<TOKEN id="token-95-1" pos="word" morph="none" start_char="13957" end_char="13959">Now</TOKEN>
<TOKEN id="token-95-2" pos="punct" morph="none" start_char="13960" end_char="13960">,</TOKEN>
<TOKEN id="token-95-3" pos="word" morph="none" start_char="13962" end_char="13963">we</TOKEN>
<TOKEN id="token-95-4" pos="word" morph="none" start_char="13965" end_char="13969">can't</TOKEN>
<TOKEN id="token-95-5" pos="word" morph="none" start_char="13971" end_char="13977">confirm</TOKEN>
<TOKEN id="token-95-6" pos="word" morph="none" start_char="13979" end_char="13981">100</TOKEN>
<TOKEN id="token-95-7" pos="punct" morph="none" start_char="13982" end_char="13982">%</TOKEN>
<TOKEN id="token-95-8" pos="word" morph="none" start_char="13984" end_char="13987">what</TOKEN>
<TOKEN id="token-95-9" pos="word" morph="none" start_char="13989" end_char="13991">the</TOKEN>
<TOKEN id="token-95-10" pos="word" morph="none" start_char="13993" end_char="13997">virus</TOKEN>
<TOKEN id="token-95-11" pos="word" morph="none" start_char="13999" end_char="14001">was</TOKEN>
<TOKEN id="token-95-12" pos="word" morph="none" start_char="14003" end_char="14006">that</TOKEN>
<TOKEN id="token-95-13" pos="word" morph="none" start_char="14008" end_char="14010">was</TOKEN>
<TOKEN id="token-95-14" pos="word" morph="none" start_char="14012" end_char="14018">causing</TOKEN>
<TOKEN id="token-95-15" pos="word" morph="none" start_char="14020" end_char="14023">this</TOKEN>
<TOKEN id="token-95-16" pos="word" morph="none" start_char="14025" end_char="14031">illness</TOKEN>
<TOKEN id="token-95-17" pos="word" morph="none" start_char="14033" end_char="14035">and</TOKEN>
<TOKEN id="token-95-18" pos="word" morph="none" start_char="14037" end_char="14040">what</TOKEN>
<TOKEN id="token-95-19" pos="word" morph="none" start_char="14042" end_char="14044">was</TOKEN>
<TOKEN id="token-95-20" pos="word" morph="none" start_char="14046" end_char="14052">causing</TOKEN>
<TOKEN id="token-95-21" pos="word" morph="none" start_char="14054" end_char="14057">this</TOKEN>
<TOKEN id="token-95-22" pos="word" morph="none" start_char="14059" end_char="14066">business</TOKEN>
<TOKEN id="token-95-23" pos="word" morph="none" start_char="14068" end_char="14069">in</TOKEN>
<TOKEN id="token-95-24" pos="word" morph="none" start_char="14071" end_char="14079">hospitals</TOKEN>
<TOKEN id="token-95-25" pos="punct" morph="none" start_char="14080" end_char="14081">,"</TOKEN>
<TOKEN id="token-95-26" pos="word" morph="none" start_char="14083" end_char="14092">Brownstein</TOKEN>
<TOKEN id="token-95-27" pos="word" morph="none" start_char="14094" end_char="14097">said</TOKEN>
<TOKEN id="token-95-28" pos="punct" morph="none" start_char="14098" end_char="14098">.</TOKEN>
</SEG>
<SEG id="segment-96" start_char="14100" end_char="14197">
<ORIGINAL_TEXT>"But something was going on that looked very different than any other time that we had looked at."</ORIGINAL_TEXT>
<TOKEN id="token-96-0" pos="punct" morph="none" start_char="14100" end_char="14100">"</TOKEN>
<TOKEN id="token-96-1" pos="word" morph="none" start_char="14101" end_char="14103">But</TOKEN>
<TOKEN id="token-96-2" pos="word" morph="none" start_char="14105" end_char="14113">something</TOKEN>
<TOKEN id="token-96-3" pos="word" morph="none" start_char="14115" end_char="14117">was</TOKEN>
<TOKEN id="token-96-4" pos="word" morph="none" start_char="14119" end_char="14123">going</TOKEN>
<TOKEN id="token-96-5" pos="word" morph="none" start_char="14125" end_char="14126">on</TOKEN>
<TOKEN id="token-96-6" pos="word" morph="none" start_char="14128" end_char="14131">that</TOKEN>
<TOKEN id="token-96-7" pos="word" morph="none" start_char="14133" end_char="14138">looked</TOKEN>
<TOKEN id="token-96-8" pos="word" morph="none" start_char="14140" end_char="14143">very</TOKEN>
<TOKEN id="token-96-9" pos="word" morph="none" start_char="14145" end_char="14153">different</TOKEN>
<TOKEN id="token-96-10" pos="word" morph="none" start_char="14155" end_char="14158">than</TOKEN>
<TOKEN id="token-96-11" pos="word" morph="none" start_char="14160" end_char="14162">any</TOKEN>
<TOKEN id="token-96-12" pos="word" morph="none" start_char="14164" end_char="14168">other</TOKEN>
<TOKEN id="token-96-13" pos="word" morph="none" start_char="14170" end_char="14173">time</TOKEN>
<TOKEN id="token-96-14" pos="word" morph="none" start_char="14175" end_char="14178">that</TOKEN>
<TOKEN id="token-96-15" pos="word" morph="none" start_char="14180" end_char="14181">we</TOKEN>
<TOKEN id="token-96-16" pos="word" morph="none" start_char="14183" end_char="14185">had</TOKEN>
<TOKEN id="token-96-17" pos="word" morph="none" start_char="14187" end_char="14192">looked</TOKEN>
<TOKEN id="token-96-18" pos="word" morph="none" start_char="14194" end_char="14195">at</TOKEN>
<TOKEN id="token-96-19" pos="punct" morph="none" start_char="14196" end_char="14197">."</TOKEN>
</SEG>
<SEG id="segment-97" start_char="14200" end_char="14368">
<ORIGINAL_TEXT>Brownstein and his research team used satellite imagery in 2015 to investigate how health care systems could predict outbreaks of influenza-like illnesses as they occur.</ORIGINAL_TEXT>
<TOKEN id="token-97-0" pos="word" morph="none" start_char="14200" end_char="14209">Brownstein</TOKEN>
<TOKEN id="token-97-1" pos="word" morph="none" start_char="14211" end_char="14213">and</TOKEN>
<TOKEN id="token-97-2" pos="word" morph="none" start_char="14215" end_char="14217">his</TOKEN>
<TOKEN id="token-97-3" pos="word" morph="none" start_char="14219" end_char="14226">research</TOKEN>
<TOKEN id="token-97-4" pos="word" morph="none" start_char="14228" end_char="14231">team</TOKEN>
<TOKEN id="token-97-5" pos="word" morph="none" start_char="14233" end_char="14236">used</TOKEN>
<TOKEN id="token-97-6" pos="word" morph="none" start_char="14238" end_char="14246">satellite</TOKEN>
<TOKEN id="token-97-7" pos="word" morph="none" start_char="14248" end_char="14254">imagery</TOKEN>
<TOKEN id="token-97-8" pos="word" morph="none" start_char="14256" end_char="14257">in</TOKEN>
<TOKEN id="token-97-9" pos="word" morph="none" start_char="14259" end_char="14262">2015</TOKEN>
<TOKEN id="token-97-10" pos="word" morph="none" start_char="14264" end_char="14265">to</TOKEN>
<TOKEN id="token-97-11" pos="word" morph="none" start_char="14267" end_char="14277">investigate</TOKEN>
<TOKEN id="token-97-12" pos="word" morph="none" start_char="14279" end_char="14281">how</TOKEN>
<TOKEN id="token-97-13" pos="word" morph="none" start_char="14283" end_char="14288">health</TOKEN>
<TOKEN id="token-97-14" pos="word" morph="none" start_char="14290" end_char="14293">care</TOKEN>
<TOKEN id="token-97-15" pos="word" morph="none" start_char="14295" end_char="14301">systems</TOKEN>
<TOKEN id="token-97-16" pos="word" morph="none" start_char="14303" end_char="14307">could</TOKEN>
<TOKEN id="token-97-17" pos="word" morph="none" start_char="14309" end_char="14315">predict</TOKEN>
<TOKEN id="token-97-18" pos="word" morph="none" start_char="14317" end_char="14325">outbreaks</TOKEN>
<TOKEN id="token-97-19" pos="word" morph="none" start_char="14327" end_char="14328">of</TOKEN>
<TOKEN id="token-97-20" pos="unknown" morph="none" start_char="14330" end_char="14343">influenza-like</TOKEN>
<TOKEN id="token-97-21" pos="word" morph="none" start_char="14345" end_char="14353">illnesses</TOKEN>
<TOKEN id="token-97-22" pos="word" morph="none" start_char="14355" end_char="14356">as</TOKEN>
<TOKEN id="token-97-23" pos="word" morph="none" start_char="14358" end_char="14361">they</TOKEN>
<TOKEN id="token-97-24" pos="word" morph="none" start_char="14363" end_char="14367">occur</TOKEN>
<TOKEN id="token-97-25" pos="punct" morph="none" start_char="14368" end_char="14368">.</TOKEN>
</SEG>
<SEG id="segment-98" start_char="14371" end_char="14645">
<ORIGINAL_TEXT>"We previously validated this method of indirectly measuring disease activity by monitoring hospital parking lot usage in Chile, Argentina and Mexico," said researcher Elaine Nsoesie, a global health professor at Boston University who worked with Brownstein on both projects.</ORIGINAL_TEXT>
<TOKEN id="token-98-0" pos="punct" morph="none" start_char="14371" end_char="14371">"</TOKEN>
<TOKEN id="token-98-1" pos="word" morph="none" start_char="14372" end_char="14373">We</TOKEN>
<TOKEN id="token-98-2" pos="word" morph="none" start_char="14375" end_char="14384">previously</TOKEN>
<TOKEN id="token-98-3" pos="word" morph="none" start_char="14386" end_char="14394">validated</TOKEN>
<TOKEN id="token-98-4" pos="word" morph="none" start_char="14396" end_char="14399">this</TOKEN>
<TOKEN id="token-98-5" pos="word" morph="none" start_char="14401" end_char="14406">method</TOKEN>
<TOKEN id="token-98-6" pos="word" morph="none" start_char="14408" end_char="14409">of</TOKEN>
<TOKEN id="token-98-7" pos="word" morph="none" start_char="14411" end_char="14420">indirectly</TOKEN>
<TOKEN id="token-98-8" pos="word" morph="none" start_char="14422" end_char="14430">measuring</TOKEN>
<TOKEN id="token-98-9" pos="word" morph="none" start_char="14432" end_char="14438">disease</TOKEN>
<TOKEN id="token-98-10" pos="word" morph="none" start_char="14440" end_char="14447">activity</TOKEN>
<TOKEN id="token-98-11" pos="word" morph="none" start_char="14449" end_char="14450">by</TOKEN>
<TOKEN id="token-98-12" pos="word" morph="none" start_char="14452" end_char="14461">monitoring</TOKEN>
<TOKEN id="token-98-13" pos="word" morph="none" start_char="14463" end_char="14470">hospital</TOKEN>
<TOKEN id="token-98-14" pos="word" morph="none" start_char="14472" end_char="14478">parking</TOKEN>
<TOKEN id="token-98-15" pos="word" morph="none" start_char="14480" end_char="14482">lot</TOKEN>
<TOKEN id="token-98-16" pos="word" morph="none" start_char="14484" end_char="14488">usage</TOKEN>
<TOKEN id="token-98-17" pos="word" morph="none" start_char="14490" end_char="14491">in</TOKEN>
<TOKEN id="token-98-18" pos="word" morph="none" start_char="14493" end_char="14497">Chile</TOKEN>
<TOKEN id="token-98-19" pos="punct" morph="none" start_char="14498" end_char="14498">,</TOKEN>
<TOKEN id="token-98-20" pos="word" morph="none" start_char="14500" end_char="14508">Argentina</TOKEN>
<TOKEN id="token-98-21" pos="word" morph="none" start_char="14510" end_char="14512">and</TOKEN>
<TOKEN id="token-98-22" pos="word" morph="none" start_char="14514" end_char="14519">Mexico</TOKEN>
<TOKEN id="token-98-23" pos="punct" morph="none" start_char="14520" end_char="14521">,"</TOKEN>
<TOKEN id="token-98-24" pos="word" morph="none" start_char="14523" end_char="14526">said</TOKEN>
<TOKEN id="token-98-25" pos="word" morph="none" start_char="14528" end_char="14537">researcher</TOKEN>
<TOKEN id="token-98-26" pos="word" morph="none" start_char="14539" end_char="14544">Elaine</TOKEN>
<TOKEN id="token-98-27" pos="word" morph="none" start_char="14546" end_char="14552">Nsoesie</TOKEN>
<TOKEN id="token-98-28" pos="punct" morph="none" start_char="14553" end_char="14553">,</TOKEN>
<TOKEN id="token-98-29" pos="word" morph="none" start_char="14555" end_char="14555">a</TOKEN>
<TOKEN id="token-98-30" pos="word" morph="none" start_char="14557" end_char="14562">global</TOKEN>
<TOKEN id="token-98-31" pos="word" morph="none" start_char="14564" end_char="14569">health</TOKEN>
<TOKEN id="token-98-32" pos="word" morph="none" start_char="14571" end_char="14579">professor</TOKEN>
<TOKEN id="token-98-33" pos="word" morph="none" start_char="14581" end_char="14582">at</TOKEN>
<TOKEN id="token-98-34" pos="word" morph="none" start_char="14584" end_char="14589">Boston</TOKEN>
<TOKEN id="token-98-35" pos="word" morph="none" start_char="14591" end_char="14600">University</TOKEN>
<TOKEN id="token-98-36" pos="word" morph="none" start_char="14602" end_char="14604">who</TOKEN>
<TOKEN id="token-98-37" pos="word" morph="none" start_char="14606" end_char="14611">worked</TOKEN>
<TOKEN id="token-98-38" pos="word" morph="none" start_char="14613" end_char="14616">with</TOKEN>
<TOKEN id="token-98-39" pos="word" morph="none" start_char="14618" end_char="14627">Brownstein</TOKEN>
<TOKEN id="token-98-40" pos="word" morph="none" start_char="14629" end_char="14630">on</TOKEN>
<TOKEN id="token-98-41" pos="word" morph="none" start_char="14632" end_char="14635">both</TOKEN>
<TOKEN id="token-98-42" pos="word" morph="none" start_char="14637" end_char="14644">projects</TOKEN>
<TOKEN id="token-98-43" pos="punct" morph="none" start_char="14645" end_char="14645">.</TOKEN>
</SEG>
<SEG id="segment-99" start_char="14647" end_char="14743">
<ORIGINAL_TEXT>"Using the data, we were able to forecast trends in influenza-like illnesses over several years."</ORIGINAL_TEXT>
<TOKEN id="token-99-0" pos="punct" morph="none" start_char="14647" end_char="14647">"</TOKEN>
<TOKEN id="token-99-1" pos="word" morph="none" start_char="14648" end_char="14652">Using</TOKEN>
<TOKEN id="token-99-2" pos="word" morph="none" start_char="14654" end_char="14656">the</TOKEN>
<TOKEN id="token-99-3" pos="word" morph="none" start_char="14658" end_char="14661">data</TOKEN>
<TOKEN id="token-99-4" pos="punct" morph="none" start_char="14662" end_char="14662">,</TOKEN>
<TOKEN id="token-99-5" pos="word" morph="none" start_char="14664" end_char="14665">we</TOKEN>
<TOKEN id="token-99-6" pos="word" morph="none" start_char="14667" end_char="14670">were</TOKEN>
<TOKEN id="token-99-7" pos="word" morph="none" start_char="14672" end_char="14675">able</TOKEN>
<TOKEN id="token-99-8" pos="word" morph="none" start_char="14677" end_char="14678">to</TOKEN>
<TOKEN id="token-99-9" pos="word" morph="none" start_char="14680" end_char="14687">forecast</TOKEN>
<TOKEN id="token-99-10" pos="word" morph="none" start_char="14689" end_char="14694">trends</TOKEN>
<TOKEN id="token-99-11" pos="word" morph="none" start_char="14696" end_char="14697">in</TOKEN>
<TOKEN id="token-99-12" pos="unknown" morph="none" start_char="14699" end_char="14712">influenza-like</TOKEN>
<TOKEN id="token-99-13" pos="word" morph="none" start_char="14714" end_char="14722">illnesses</TOKEN>
<TOKEN id="token-99-14" pos="word" morph="none" start_char="14724" end_char="14727">over</TOKEN>
<TOKEN id="token-99-15" pos="word" morph="none" start_char="14729" end_char="14735">several</TOKEN>
<TOKEN id="token-99-16" pos="word" morph="none" start_char="14737" end_char="14741">years</TOKEN>
<TOKEN id="token-99-17" pos="punct" morph="none" start_char="14742" end_char="14743">."</TOKEN>
</SEG>
<SEG id="segment-100" start_char="14746" end_char="14876">
<ORIGINAL_TEXT>For that study, the scientists reviewed nearly 3,000 satellite images from 2010 to 2013, again, measuring car traffic at hospitals.</ORIGINAL_TEXT>
<TOKEN id="token-100-0" pos="word" morph="none" start_char="14746" end_char="14748">For</TOKEN>
<TOKEN id="token-100-1" pos="word" morph="none" start_char="14750" end_char="14753">that</TOKEN>
<TOKEN id="token-100-2" pos="word" morph="none" start_char="14755" end_char="14759">study</TOKEN>
<TOKEN id="token-100-3" pos="punct" morph="none" start_char="14760" end_char="14760">,</TOKEN>
<TOKEN id="token-100-4" pos="word" morph="none" start_char="14762" end_char="14764">the</TOKEN>
<TOKEN id="token-100-5" pos="word" morph="none" start_char="14766" end_char="14775">scientists</TOKEN>
<TOKEN id="token-100-6" pos="word" morph="none" start_char="14777" end_char="14784">reviewed</TOKEN>
<TOKEN id="token-100-7" pos="word" morph="none" start_char="14786" end_char="14791">nearly</TOKEN>
<TOKEN id="token-100-8" pos="unknown" morph="none" start_char="14793" end_char="14797">3,000</TOKEN>
<TOKEN id="token-100-9" pos="word" morph="none" start_char="14799" end_char="14807">satellite</TOKEN>
<TOKEN id="token-100-10" pos="word" morph="none" start_char="14809" end_char="14814">images</TOKEN>
<TOKEN id="token-100-11" pos="word" morph="none" start_char="14816" end_char="14819">from</TOKEN>
<TOKEN id="token-100-12" pos="word" morph="none" start_char="14821" end_char="14824">2010</TOKEN>
<TOKEN id="token-100-13" pos="word" morph="none" start_char="14826" end_char="14827">to</TOKEN>
<TOKEN id="token-100-14" pos="word" morph="none" start_char="14829" end_char="14832">2013</TOKEN>
<TOKEN id="token-100-15" pos="punct" morph="none" start_char="14833" end_char="14833">,</TOKEN>
<TOKEN id="token-100-16" pos="word" morph="none" start_char="14835" end_char="14839">again</TOKEN>
<TOKEN id="token-100-17" pos="punct" morph="none" start_char="14840" end_char="14840">,</TOKEN>
<TOKEN id="token-100-18" pos="word" morph="none" start_char="14842" end_char="14850">measuring</TOKEN>
<TOKEN id="token-100-19" pos="word" morph="none" start_char="14852" end_char="14854">car</TOKEN>
<TOKEN id="token-100-20" pos="word" morph="none" start_char="14856" end_char="14862">traffic</TOKEN>
<TOKEN id="token-100-21" pos="word" morph="none" start_char="14864" end_char="14865">at</TOKEN>
<TOKEN id="token-100-22" pos="word" morph="none" start_char="14867" end_char="14875">hospitals</TOKEN>
<TOKEN id="token-100-23" pos="punct" morph="none" start_char="14876" end_char="14876">.</TOKEN>
</SEG>
<SEG id="segment-101" start_char="14878" end_char="15091">
<ORIGINAL_TEXT>They concluded that traffic spikes coincide with an outbreak of influenza-like illness, so public health officials could use parking-lot data to help them prepare for something that could strain medical facilities.</ORIGINAL_TEXT>
<TOKEN id="token-101-0" pos="word" morph="none" start_char="14878" end_char="14881">They</TOKEN>
<TOKEN id="token-101-1" pos="word" morph="none" start_char="14883" end_char="14891">concluded</TOKEN>
<TOKEN id="token-101-2" pos="word" morph="none" start_char="14893" end_char="14896">that</TOKEN>
<TOKEN id="token-101-3" pos="word" morph="none" start_char="14898" end_char="14904">traffic</TOKEN>
<TOKEN id="token-101-4" pos="word" morph="none" start_char="14906" end_char="14911">spikes</TOKEN>
<TOKEN id="token-101-5" pos="word" morph="none" start_char="14913" end_char="14920">coincide</TOKEN>
<TOKEN id="token-101-6" pos="word" morph="none" start_char="14922" end_char="14925">with</TOKEN>
<TOKEN id="token-101-7" pos="word" morph="none" start_char="14927" end_char="14928">an</TOKEN>
<TOKEN id="token-101-8" pos="word" morph="none" start_char="14930" end_char="14937">outbreak</TOKEN>
<TOKEN id="token-101-9" pos="word" morph="none" start_char="14939" end_char="14940">of</TOKEN>
<TOKEN id="token-101-10" pos="unknown" morph="none" start_char="14942" end_char="14955">influenza-like</TOKEN>
<TOKEN id="token-101-11" pos="word" morph="none" start_char="14957" end_char="14963">illness</TOKEN>
<TOKEN id="token-101-12" pos="punct" morph="none" start_char="14964" end_char="14964">,</TOKEN>
<TOKEN id="token-101-13" pos="word" morph="none" start_char="14966" end_char="14967">so</TOKEN>
<TOKEN id="token-101-14" pos="word" morph="none" start_char="14969" end_char="14974">public</TOKEN>
<TOKEN id="token-101-15" pos="word" morph="none" start_char="14976" end_char="14981">health</TOKEN>
<TOKEN id="token-101-16" pos="word" morph="none" start_char="14983" end_char="14991">officials</TOKEN>
<TOKEN id="token-101-17" pos="word" morph="none" start_char="14993" end_char="14997">could</TOKEN>
<TOKEN id="token-101-18" pos="word" morph="none" start_char="14999" end_char="15001">use</TOKEN>
<TOKEN id="token-101-19" pos="unknown" morph="none" start_char="15003" end_char="15013">parking-lot</TOKEN>
<TOKEN id="token-101-20" pos="word" morph="none" start_char="15015" end_char="15018">data</TOKEN>
<TOKEN id="token-101-21" pos="word" morph="none" start_char="15020" end_char="15021">to</TOKEN>
<TOKEN id="token-101-22" pos="word" morph="none" start_char="15023" end_char="15026">help</TOKEN>
<TOKEN id="token-101-23" pos="word" morph="none" start_char="15028" end_char="15031">them</TOKEN>
<TOKEN id="token-101-24" pos="word" morph="none" start_char="15033" end_char="15039">prepare</TOKEN>
<TOKEN id="token-101-25" pos="word" morph="none" start_char="15041" end_char="15043">for</TOKEN>
<TOKEN id="token-101-26" pos="word" morph="none" start_char="15045" end_char="15053">something</TOKEN>
<TOKEN id="token-101-27" pos="word" morph="none" start_char="15055" end_char="15058">that</TOKEN>
<TOKEN id="token-101-28" pos="word" morph="none" start_char="15060" end_char="15064">could</TOKEN>
<TOKEN id="token-101-29" pos="word" morph="none" start_char="15066" end_char="15071">strain</TOKEN>
<TOKEN id="token-101-30" pos="word" morph="none" start_char="15073" end_char="15079">medical</TOKEN>
<TOKEN id="token-101-31" pos="word" morph="none" start_char="15081" end_char="15090">facilities</TOKEN>
<TOKEN id="token-101-32" pos="punct" morph="none" start_char="15091" end_char="15091">.</TOKEN>
</SEG>
<SEG id="segment-102" start_char="15094" end_char="15320">
<ORIGINAL_TEXT>"We are in need of new and innovative methods for predicting disease," said epidemiology professor Anne Rimoin, the director of the Center for Global and Immigrant Health at UCLA, who was not connected with the research effort.</ORIGINAL_TEXT>
<TOKEN id="token-102-0" pos="punct" morph="none" start_char="15094" end_char="15094">"</TOKEN>
<TOKEN id="token-102-1" pos="word" morph="none" start_char="15095" end_char="15096">We</TOKEN>
<TOKEN id="token-102-2" pos="word" morph="none" start_char="15098" end_char="15100">are</TOKEN>
<TOKEN id="token-102-3" pos="word" morph="none" start_char="15102" end_char="15103">in</TOKEN>
<TOKEN id="token-102-4" pos="word" morph="none" start_char="15105" end_char="15108">need</TOKEN>
<TOKEN id="token-102-5" pos="word" morph="none" start_char="15110" end_char="15111">of</TOKEN>
<TOKEN id="token-102-6" pos="word" morph="none" start_char="15113" end_char="15115">new</TOKEN>
<TOKEN id="token-102-7" pos="word" morph="none" start_char="15117" end_char="15119">and</TOKEN>
<TOKEN id="token-102-8" pos="word" morph="none" start_char="15121" end_char="15130">innovative</TOKEN>
<TOKEN id="token-102-9" pos="word" morph="none" start_char="15132" end_char="15138">methods</TOKEN>
<TOKEN id="token-102-10" pos="word" morph="none" start_char="15140" end_char="15142">for</TOKEN>
<TOKEN id="token-102-11" pos="word" morph="none" start_char="15144" end_char="15153">predicting</TOKEN>
<TOKEN id="token-102-12" pos="word" morph="none" start_char="15155" end_char="15161">disease</TOKEN>
<TOKEN id="token-102-13" pos="punct" morph="none" start_char="15162" end_char="15163">,"</TOKEN>
<TOKEN id="token-102-14" pos="word" morph="none" start_char="15165" end_char="15168">said</TOKEN>
<TOKEN id="token-102-15" pos="word" morph="none" start_char="15170" end_char="15181">epidemiology</TOKEN>
<TOKEN id="token-102-16" pos="word" morph="none" start_char="15183" end_char="15191">professor</TOKEN>
<TOKEN id="token-102-17" pos="word" morph="none" start_char="15193" end_char="15196">Anne</TOKEN>
<TOKEN id="token-102-18" pos="word" morph="none" start_char="15198" end_char="15203">Rimoin</TOKEN>
<TOKEN id="token-102-19" pos="punct" morph="none" start_char="15204" end_char="15204">,</TOKEN>
<TOKEN id="token-102-20" pos="word" morph="none" start_char="15206" end_char="15208">the</TOKEN>
<TOKEN id="token-102-21" pos="word" morph="none" start_char="15210" end_char="15217">director</TOKEN>
<TOKEN id="token-102-22" pos="word" morph="none" start_char="15219" end_char="15220">of</TOKEN>
<TOKEN id="token-102-23" pos="word" morph="none" start_char="15222" end_char="15224">the</TOKEN>
<TOKEN id="token-102-24" pos="word" morph="none" start_char="15226" end_char="15231">Center</TOKEN>
<TOKEN id="token-102-25" pos="word" morph="none" start_char="15233" end_char="15235">for</TOKEN>
<TOKEN id="token-102-26" pos="word" morph="none" start_char="15237" end_char="15242">Global</TOKEN>
<TOKEN id="token-102-27" pos="word" morph="none" start_char="15244" end_char="15246">and</TOKEN>
<TOKEN id="token-102-28" pos="word" morph="none" start_char="15248" end_char="15256">Immigrant</TOKEN>
<TOKEN id="token-102-29" pos="word" morph="none" start_char="15258" end_char="15263">Health</TOKEN>
<TOKEN id="token-102-30" pos="word" morph="none" start_char="15265" end_char="15266">at</TOKEN>
<TOKEN id="token-102-31" pos="word" morph="none" start_char="15268" end_char="15271">UCLA</TOKEN>
<TOKEN id="token-102-32" pos="punct" morph="none" start_char="15272" end_char="15272">,</TOKEN>
<TOKEN id="token-102-33" pos="word" morph="none" start_char="15274" end_char="15276">who</TOKEN>
<TOKEN id="token-102-34" pos="word" morph="none" start_char="15278" end_char="15280">was</TOKEN>
<TOKEN id="token-102-35" pos="word" morph="none" start_char="15282" end_char="15284">not</TOKEN>
<TOKEN id="token-102-36" pos="word" morph="none" start_char="15286" end_char="15294">connected</TOKEN>
<TOKEN id="token-102-37" pos="word" morph="none" start_char="15296" end_char="15299">with</TOKEN>
<TOKEN id="token-102-38" pos="word" morph="none" start_char="15301" end_char="15303">the</TOKEN>
<TOKEN id="token-102-39" pos="word" morph="none" start_char="15305" end_char="15312">research</TOKEN>
<TOKEN id="token-102-40" pos="word" morph="none" start_char="15314" end_char="15319">effort</TOKEN>
<TOKEN id="token-102-41" pos="punct" morph="none" start_char="15320" end_char="15320">.</TOKEN>
</SEG>
<SEG id="segment-103" start_char="15322" end_char="15474">
<ORIGINAL_TEXT>"In this specific case, data on events such as increases in hospital traffic could serve as early indicators of social disruption resulting from disease.</ORIGINAL_TEXT>
<TOKEN id="token-103-0" pos="punct" morph="none" start_char="15322" end_char="15322">"</TOKEN>
<TOKEN id="token-103-1" pos="word" morph="none" start_char="15323" end_char="15324">In</TOKEN>
<TOKEN id="token-103-2" pos="word" morph="none" start_char="15326" end_char="15329">this</TOKEN>
<TOKEN id="token-103-3" pos="word" morph="none" start_char="15331" end_char="15338">specific</TOKEN>
<TOKEN id="token-103-4" pos="word" morph="none" start_char="15340" end_char="15343">case</TOKEN>
<TOKEN id="token-103-5" pos="punct" morph="none" start_char="15344" end_char="15344">,</TOKEN>
<TOKEN id="token-103-6" pos="word" morph="none" start_char="15346" end_char="15349">data</TOKEN>
<TOKEN id="token-103-7" pos="word" morph="none" start_char="15351" end_char="15352">on</TOKEN>
<TOKEN id="token-103-8" pos="word" morph="none" start_char="15354" end_char="15359">events</TOKEN>
<TOKEN id="token-103-9" pos="word" morph="none" start_char="15361" end_char="15364">such</TOKEN>
<TOKEN id="token-103-10" pos="word" morph="none" start_char="15366" end_char="15367">as</TOKEN>
<TOKEN id="token-103-11" pos="word" morph="none" start_char="15369" end_char="15377">increases</TOKEN>
<TOKEN id="token-103-12" pos="word" morph="none" start_char="15379" end_char="15380">in</TOKEN>
<TOKEN id="token-103-13" pos="word" morph="none" start_char="15382" end_char="15389">hospital</TOKEN>
<TOKEN id="token-103-14" pos="word" morph="none" start_char="15391" end_char="15397">traffic</TOKEN>
<TOKEN id="token-103-15" pos="word" morph="none" start_char="15399" end_char="15403">could</TOKEN>
<TOKEN id="token-103-16" pos="word" morph="none" start_char="15405" end_char="15409">serve</TOKEN>
<TOKEN id="token-103-17" pos="word" morph="none" start_char="15411" end_char="15412">as</TOKEN>
<TOKEN id="token-103-18" pos="word" morph="none" start_char="15414" end_char="15418">early</TOKEN>
<TOKEN id="token-103-19" pos="word" morph="none" start_char="15420" end_char="15429">indicators</TOKEN>
<TOKEN id="token-103-20" pos="word" morph="none" start_char="15431" end_char="15432">of</TOKEN>
<TOKEN id="token-103-21" pos="word" morph="none" start_char="15434" end_char="15439">social</TOKEN>
<TOKEN id="token-103-22" pos="word" morph="none" start_char="15441" end_char="15450">disruption</TOKEN>
<TOKEN id="token-103-23" pos="word" morph="none" start_char="15452" end_char="15460">resulting</TOKEN>
<TOKEN id="token-103-24" pos="word" morph="none" start_char="15462" end_char="15465">from</TOKEN>
<TOKEN id="token-103-25" pos="word" morph="none" start_char="15467" end_char="15473">disease</TOKEN>
<TOKEN id="token-103-26" pos="punct" morph="none" start_char="15474" end_char="15474">.</TOKEN>
</SEG>
<SEG id="segment-104" start_char="15476" end_char="15606">
<ORIGINAL_TEXT>High-resolution satellite imagery can be extremely useful for understanding disease spread and implementation of control measures."</ORIGINAL_TEXT>
<TOKEN id="token-104-0" pos="unknown" morph="none" start_char="15476" end_char="15490">High-resolution</TOKEN>
<TOKEN id="token-104-1" pos="word" morph="none" start_char="15492" end_char="15500">satellite</TOKEN>
<TOKEN id="token-104-2" pos="word" morph="none" start_char="15502" end_char="15508">imagery</TOKEN>
<TOKEN id="token-104-3" pos="word" morph="none" start_char="15510" end_char="15512">can</TOKEN>
<TOKEN id="token-104-4" pos="word" morph="none" start_char="15514" end_char="15515">be</TOKEN>
<TOKEN id="token-104-5" pos="word" morph="none" start_char="15517" end_char="15525">extremely</TOKEN>
<TOKEN id="token-104-6" pos="word" morph="none" start_char="15527" end_char="15532">useful</TOKEN>
<TOKEN id="token-104-7" pos="word" morph="none" start_char="15534" end_char="15536">for</TOKEN>
<TOKEN id="token-104-8" pos="word" morph="none" start_char="15538" end_char="15550">understanding</TOKEN>
<TOKEN id="token-104-9" pos="word" morph="none" start_char="15552" end_char="15558">disease</TOKEN>
<TOKEN id="token-104-10" pos="word" morph="none" start_char="15560" end_char="15565">spread</TOKEN>
<TOKEN id="token-104-11" pos="word" morph="none" start_char="15567" end_char="15569">and</TOKEN>
<TOKEN id="token-104-12" pos="word" morph="none" start_char="15571" end_char="15584">implementation</TOKEN>
<TOKEN id="token-104-13" pos="word" morph="none" start_char="15586" end_char="15587">of</TOKEN>
<TOKEN id="token-104-14" pos="word" morph="none" start_char="15589" end_char="15595">control</TOKEN>
<TOKEN id="token-104-15" pos="word" morph="none" start_char="15597" end_char="15604">measures</TOKEN>
<TOKEN id="token-104-16" pos="punct" morph="none" start_char="15605" end_char="15606">."</TOKEN>
</SEG>
<SEG id="segment-105" start_char="15609" end_char="15699">
<ORIGINAL_TEXT>Karson Yiu, Conor Finnegan, Luis Martinez and James Gordon Meek contributed to this report.</ORIGINAL_TEXT>
<TOKEN id="token-105-0" pos="word" morph="none" start_char="15609" end_char="15614">Karson</TOKEN>
<TOKEN id="token-105-1" pos="word" morph="none" start_char="15616" end_char="15618">Yiu</TOKEN>
<TOKEN id="token-105-2" pos="punct" morph="none" start_char="15619" end_char="15619">,</TOKEN>
<TOKEN id="token-105-3" pos="word" morph="none" start_char="15621" end_char="15625">Conor</TOKEN>
<TOKEN id="token-105-4" pos="word" morph="none" start_char="15627" end_char="15634">Finnegan</TOKEN>
<TOKEN id="token-105-5" pos="punct" morph="none" start_char="15635" end_char="15635">,</TOKEN>
<TOKEN id="token-105-6" pos="word" morph="none" start_char="15637" end_char="15640">Luis</TOKEN>
<TOKEN id="token-105-7" pos="word" morph="none" start_char="15642" end_char="15649">Martinez</TOKEN>
<TOKEN id="token-105-8" pos="word" morph="none" start_char="15651" end_char="15653">and</TOKEN>
<TOKEN id="token-105-9" pos="word" morph="none" start_char="15655" end_char="15659">James</TOKEN>
<TOKEN id="token-105-10" pos="word" morph="none" start_char="15661" end_char="15666">Gordon</TOKEN>
<TOKEN id="token-105-11" pos="word" morph="none" start_char="15668" end_char="15671">Meek</TOKEN>
<TOKEN id="token-105-12" pos="word" morph="none" start_char="15673" end_char="15683">contributed</TOKEN>
<TOKEN id="token-105-13" pos="word" morph="none" start_char="15685" end_char="15686">to</TOKEN>
<TOKEN id="token-105-14" pos="word" morph="none" start_char="15688" end_char="15691">this</TOKEN>
<TOKEN id="token-105-15" pos="word" morph="none" start_char="15693" end_char="15698">report</TOKEN>
<TOKEN id="token-105-16" pos="punct" morph="none" start_char="15699" end_char="15699">.</TOKEN>
</SEG>
<SEG id="segment-106" start_char="15701" end_char="15786">
<ORIGINAL_TEXT>This report was updated Tuesday to include comments from the Chinese Foreign Ministry.</ORIGINAL_TEXT>
<TOKEN id="token-106-0" pos="word" morph="none" start_char="15701" end_char="15704">This</TOKEN>
<TOKEN id="token-106-1" pos="word" morph="none" start_char="15706" end_char="15711">report</TOKEN>
<TOKEN id="token-106-2" pos="word" morph="none" start_char="15713" end_char="15715">was</TOKEN>
<TOKEN id="token-106-3" pos="word" morph="none" start_char="15717" end_char="15723">updated</TOKEN>
<TOKEN id="token-106-4" pos="word" morph="none" start_char="15725" end_char="15731">Tuesday</TOKEN>
<TOKEN id="token-106-5" pos="word" morph="none" start_char="15733" end_char="15734">to</TOKEN>
<TOKEN id="token-106-6" pos="word" morph="none" start_char="15736" end_char="15742">include</TOKEN>
<TOKEN id="token-106-7" pos="word" morph="none" start_char="15744" end_char="15751">comments</TOKEN>
<TOKEN id="token-106-8" pos="word" morph="none" start_char="15753" end_char="15756">from</TOKEN>
<TOKEN id="token-106-9" pos="word" morph="none" start_char="15758" end_char="15760">the</TOKEN>
<TOKEN id="token-106-10" pos="word" morph="none" start_char="15762" end_char="15768">Chinese</TOKEN>
<TOKEN id="token-106-11" pos="word" morph="none" start_char="15770" end_char="15776">Foreign</TOKEN>
<TOKEN id="token-106-12" pos="word" morph="none" start_char="15778" end_char="15785">Ministry</TOKEN>
<TOKEN id="token-106-13" pos="punct" morph="none" start_char="15786" end_char="15786">.</TOKEN>
</SEG>
<SEG id="segment-107" start_char="15789" end_char="15897">
<ORIGINAL_TEXT>This report was featured in the Tuesday, June 9, 2020, episode of "Start Here," ABC News’ daily news podcast.</ORIGINAL_TEXT>
<TOKEN id="token-107-0" pos="word" morph="none" start_char="15789" end_char="15792">This</TOKEN>
<TOKEN id="token-107-1" pos="word" morph="none" start_char="15794" end_char="15799">report</TOKEN>
<TOKEN id="token-107-2" pos="word" morph="none" start_char="15801" end_char="15803">was</TOKEN>
<TOKEN id="token-107-3" pos="word" morph="none" start_char="15805" end_char="15812">featured</TOKEN>
<TOKEN id="token-107-4" pos="word" morph="none" start_char="15814" end_char="15815">in</TOKEN>
<TOKEN id="token-107-5" pos="word" morph="none" start_char="15817" end_char="15819">the</TOKEN>
<TOKEN id="token-107-6" pos="word" morph="none" start_char="15821" end_char="15827">Tuesday</TOKEN>
<TOKEN id="token-107-7" pos="punct" morph="none" start_char="15828" end_char="15828">,</TOKEN>
<TOKEN id="token-107-8" pos="word" morph="none" start_char="15830" end_char="15833">June</TOKEN>
<TOKEN id="token-107-9" pos="word" morph="none" start_char="15835" end_char="15835">9</TOKEN>
<TOKEN id="token-107-10" pos="punct" morph="none" start_char="15836" end_char="15836">,</TOKEN>
<TOKEN id="token-107-11" pos="word" morph="none" start_char="15838" end_char="15841">2020</TOKEN>
<TOKEN id="token-107-12" pos="punct" morph="none" start_char="15842" end_char="15842">,</TOKEN>
<TOKEN id="token-107-13" pos="word" morph="none" start_char="15844" end_char="15850">episode</TOKEN>
<TOKEN id="token-107-14" pos="word" morph="none" start_char="15852" end_char="15853">of</TOKEN>
<TOKEN id="token-107-15" pos="punct" morph="none" start_char="15855" end_char="15855">"</TOKEN>
<TOKEN id="token-107-16" pos="word" morph="none" start_char="15856" end_char="15860">Start</TOKEN>
<TOKEN id="token-107-17" pos="word" morph="none" start_char="15862" end_char="15865">Here</TOKEN>
<TOKEN id="token-107-18" pos="punct" morph="none" start_char="15866" end_char="15867">,"</TOKEN>
<TOKEN id="token-107-19" pos="word" morph="none" start_char="15869" end_char="15871">ABC</TOKEN>
<TOKEN id="token-107-20" pos="word" morph="none" start_char="15873" end_char="15876">News</TOKEN>
<TOKEN id="token-107-21" pos="punct" morph="none" start_char="15877" end_char="15877">’</TOKEN>
<TOKEN id="token-107-22" pos="word" morph="none" start_char="15879" end_char="15883">daily</TOKEN>
<TOKEN id="token-107-23" pos="word" morph="none" start_char="15885" end_char="15888">news</TOKEN>
<TOKEN id="token-107-24" pos="word" morph="none" start_char="15890" end_char="15896">podcast</TOKEN>
<TOKEN id="token-107-25" pos="punct" morph="none" start_char="15897" end_char="15897">.</TOKEN>
</SEG>
<SEG id="segment-108" start_char="15900" end_char="15981">
<ORIGINAL_TEXT>"Start Here" offers a straightforward look at the day's top stories in 20 minutes.</ORIGINAL_TEXT>
<TOKEN id="token-108-0" pos="punct" morph="none" start_char="15900" end_char="15900">"</TOKEN>
<TOKEN id="token-108-1" pos="word" morph="none" start_char="15901" end_char="15905">Start</TOKEN>
<TOKEN id="token-108-2" pos="word" morph="none" start_char="15907" end_char="15910">Here</TOKEN>
<TOKEN id="token-108-3" pos="punct" morph="none" start_char="15911" end_char="15911">"</TOKEN>
<TOKEN id="token-108-4" pos="word" morph="none" start_char="15913" end_char="15918">offers</TOKEN>
<TOKEN id="token-108-5" pos="word" morph="none" start_char="15920" end_char="15920">a</TOKEN>
<TOKEN id="token-108-6" pos="word" morph="none" start_char="15922" end_char="15936">straightforward</TOKEN>
<TOKEN id="token-108-7" pos="word" morph="none" start_char="15938" end_char="15941">look</TOKEN>
<TOKEN id="token-108-8" pos="word" morph="none" start_char="15943" end_char="15944">at</TOKEN>
<TOKEN id="token-108-9" pos="word" morph="none" start_char="15946" end_char="15948">the</TOKEN>
<TOKEN id="token-108-10" pos="word" morph="none" start_char="15950" end_char="15954">day's</TOKEN>
<TOKEN id="token-108-11" pos="word" morph="none" start_char="15956" end_char="15958">top</TOKEN>
<TOKEN id="token-108-12" pos="word" morph="none" start_char="15960" end_char="15966">stories</TOKEN>
<TOKEN id="token-108-13" pos="word" morph="none" start_char="15968" end_char="15969">in</TOKEN>
<TOKEN id="token-108-14" pos="word" morph="none" start_char="15971" end_char="15972">20</TOKEN>
<TOKEN id="token-108-15" pos="word" morph="none" start_char="15974" end_char="15980">minutes</TOKEN>
<TOKEN id="token-108-16" pos="punct" morph="none" start_char="15981" end_char="15981">.</TOKEN>
</SEG>
<SEG id="segment-109" start_char="15983" end_char="16108">
<ORIGINAL_TEXT>Listen for free every weekday on Apple Podcasts, Google Podcasts, Spotify, the ABC News app or wherever you get your podcasts.</ORIGINAL_TEXT>
<TOKEN id="token-109-0" pos="word" morph="none" start_char="15983" end_char="15988">Listen</TOKEN>
<TOKEN id="token-109-1" pos="word" morph="none" start_char="15990" end_char="15992">for</TOKEN>
<TOKEN id="token-109-2" pos="word" morph="none" start_char="15994" end_char="15997">free</TOKEN>
<TOKEN id="token-109-3" pos="word" morph="none" start_char="15999" end_char="16003">every</TOKEN>
<TOKEN id="token-109-4" pos="word" morph="none" start_char="16005" end_char="16011">weekday</TOKEN>
<TOKEN id="token-109-5" pos="word" morph="none" start_char="16013" end_char="16014">on</TOKEN>
<TOKEN id="token-109-6" pos="word" morph="none" start_char="16016" end_char="16020">Apple</TOKEN>
<TOKEN id="token-109-7" pos="word" morph="none" start_char="16022" end_char="16029">Podcasts</TOKEN>
<TOKEN id="token-109-8" pos="punct" morph="none" start_char="16030" end_char="16030">,</TOKEN>
<TOKEN id="token-109-9" pos="word" morph="none" start_char="16032" end_char="16037">Google</TOKEN>
<TOKEN id="token-109-10" pos="word" morph="none" start_char="16039" end_char="16046">Podcasts</TOKEN>
<TOKEN id="token-109-11" pos="punct" morph="none" start_char="16047" end_char="16047">,</TOKEN>
<TOKEN id="token-109-12" pos="word" morph="none" start_char="16049" end_char="16055">Spotify</TOKEN>
<TOKEN id="token-109-13" pos="punct" morph="none" start_char="16056" end_char="16056">,</TOKEN>
<TOKEN id="token-109-14" pos="word" morph="none" start_char="16058" end_char="16060">the</TOKEN>
<TOKEN id="token-109-15" pos="word" morph="none" start_char="16062" end_char="16064">ABC</TOKEN>
<TOKEN id="token-109-16" pos="word" morph="none" start_char="16066" end_char="16069">News</TOKEN>
<TOKEN id="token-109-17" pos="word" morph="none" start_char="16071" end_char="16073">app</TOKEN>
<TOKEN id="token-109-18" pos="word" morph="none" start_char="16075" end_char="16076">or</TOKEN>
<TOKEN id="token-109-19" pos="word" morph="none" start_char="16078" end_char="16085">wherever</TOKEN>
<TOKEN id="token-109-20" pos="word" morph="none" start_char="16087" end_char="16089">you</TOKEN>
<TOKEN id="token-109-21" pos="word" morph="none" start_char="16091" end_char="16093">get</TOKEN>
<TOKEN id="token-109-22" pos="word" morph="none" start_char="16095" end_char="16098">your</TOKEN>
<TOKEN id="token-109-23" pos="word" morph="none" start_char="16100" end_char="16107">podcasts</TOKEN>
<TOKEN id="token-109-24" pos="punct" morph="none" start_char="16108" end_char="16108">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
