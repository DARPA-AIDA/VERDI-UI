<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="ukr">
<DOC id="L0C049PCB" lang="ukr" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="13709" raw_text_md5="b2c129ede33b0de828b98af0360c64c5">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="68">
<ORIGINAL_TEXT>Baseless Conspiracy Theories Claim New Coronavirus Was Bioengineered</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="8">Baseless</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="10" end_char="19">Conspiracy</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="21" end_char="28">Theories</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="30" end_char="34">Claim</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="36" end_char="38">New</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="40" end_char="50">Coronavirus</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="52" end_char="54">Was</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="56" end_char="68">Bioengineered</TOKEN>
</SEG>
<SEG id="segment-1" start_char="72" end_char="81">
<ORIGINAL_TEXT>Quick Take</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="72" end_char="76">Quick</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="78" end_char="81">Take</TOKEN>
</SEG>
<SEG id="segment-2" start_char="85" end_char="219">
<ORIGINAL_TEXT>Several online stories inaccurately claim that the new coronavirus contains HIV "insertions" and shows signs of being created in a lab.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="85" end_char="91">Several</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="93" end_char="98">online</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="100" end_char="106">stories</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="108" end_char="119">inaccurately</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="121" end_char="125">claim</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="127" end_char="130">that</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="132" end_char="134">the</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="136" end_char="138">new</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="140" end_char="150">coronavirus</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="152" end_char="159">contains</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="161" end_char="163">HIV</TOKEN>
<TOKEN id="token-2-11" pos="punct" morph="none" start_char="165" end_char="165">"</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="166" end_char="175">insertions</TOKEN>
<TOKEN id="token-2-13" pos="punct" morph="none" start_char="176" end_char="176">"</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="178" end_char="180">and</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="182" end_char="186">shows</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="188" end_char="192">signs</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="194" end_char="195">of</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="197" end_char="201">being</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="203" end_char="209">created</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="211" end_char="212">in</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="214" end_char="214">a</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="216" end_char="218">lab</TOKEN>
<TOKEN id="token-2-23" pos="punct" morph="none" start_char="219" end_char="219">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="221" end_char="327">
<ORIGINAL_TEXT>But there is no evidence that the new virus was bioengineered, and every indication it came from an animal.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="221" end_char="223">But</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="225" end_char="229">there</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="231" end_char="232">is</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="234" end_char="235">no</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="237" end_char="244">evidence</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="246" end_char="249">that</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="251" end_char="253">the</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="255" end_char="257">new</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="259" end_char="263">virus</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="265" end_char="267">was</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="269" end_char="281">bioengineered</TOKEN>
<TOKEN id="token-3-11" pos="punct" morph="none" start_char="282" end_char="282">,</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="284" end_char="286">and</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="288" end_char="292">every</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="294" end_char="303">indication</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="305" end_char="306">it</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="308" end_char="311">came</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="313" end_char="316">from</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="318" end_char="319">an</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="321" end_char="326">animal</TOKEN>
<TOKEN id="token-3-20" pos="punct" morph="none" start_char="327" end_char="327">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="330" end_char="339">
<ORIGINAL_TEXT>Full Story</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="330" end_char="333">Full</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="335" end_char="339">Story</TOKEN>
</SEG>
<SEG id="segment-5" start_char="343" end_char="576">
<ORIGINAL_TEXT>The latest conspiracy theories about the new coronavirus, which first led to an outbreak in Wuhan, China in late 2019, allege that the virus was man-made, rather than the natural result of people coming into contact with wild animals.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="343" end_char="345">The</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="347" end_char="352">latest</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="354" end_char="363">conspiracy</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="365" end_char="372">theories</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="374" end_char="378">about</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="380" end_char="382">the</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="384" end_char="386">new</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="388" end_char="398">coronavirus</TOKEN>
<TOKEN id="token-5-8" pos="punct" morph="none" start_char="399" end_char="399">,</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="401" end_char="405">which</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="407" end_char="411">first</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="413" end_char="415">led</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="417" end_char="418">to</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="420" end_char="421">an</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="423" end_char="430">outbreak</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="432" end_char="433">in</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="435" end_char="439">Wuhan</TOKEN>
<TOKEN id="token-5-17" pos="punct" morph="none" start_char="440" end_char="440">,</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="442" end_char="446">China</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="448" end_char="449">in</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="451" end_char="454">late</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="456" end_char="459">2019</TOKEN>
<TOKEN id="token-5-22" pos="punct" morph="none" start_char="460" end_char="460">,</TOKEN>
<TOKEN id="token-5-23" pos="word" morph="none" start_char="462" end_char="467">allege</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="469" end_char="472">that</TOKEN>
<TOKEN id="token-5-25" pos="word" morph="none" start_char="474" end_char="476">the</TOKEN>
<TOKEN id="token-5-26" pos="word" morph="none" start_char="478" end_char="482">virus</TOKEN>
<TOKEN id="token-5-27" pos="word" morph="none" start_char="484" end_char="486">was</TOKEN>
<TOKEN id="token-5-28" pos="unknown" morph="none" start_char="488" end_char="495">man-made</TOKEN>
<TOKEN id="token-5-29" pos="punct" morph="none" start_char="496" end_char="496">,</TOKEN>
<TOKEN id="token-5-30" pos="word" morph="none" start_char="498" end_char="503">rather</TOKEN>
<TOKEN id="token-5-31" pos="word" morph="none" start_char="505" end_char="508">than</TOKEN>
<TOKEN id="token-5-32" pos="word" morph="none" start_char="510" end_char="512">the</TOKEN>
<TOKEN id="token-5-33" pos="word" morph="none" start_char="514" end_char="520">natural</TOKEN>
<TOKEN id="token-5-34" pos="word" morph="none" start_char="522" end_char="527">result</TOKEN>
<TOKEN id="token-5-35" pos="word" morph="none" start_char="529" end_char="530">of</TOKEN>
<TOKEN id="token-5-36" pos="word" morph="none" start_char="532" end_char="537">people</TOKEN>
<TOKEN id="token-5-37" pos="word" morph="none" start_char="539" end_char="544">coming</TOKEN>
<TOKEN id="token-5-38" pos="word" morph="none" start_char="546" end_char="549">into</TOKEN>
<TOKEN id="token-5-39" pos="word" morph="none" start_char="551" end_char="557">contact</TOKEN>
<TOKEN id="token-5-40" pos="word" morph="none" start_char="559" end_char="562">with</TOKEN>
<TOKEN id="token-5-41" pos="word" morph="none" start_char="564" end_char="567">wild</TOKEN>
<TOKEN id="token-5-42" pos="word" morph="none" start_char="569" end_char="575">animals</TOKEN>
<TOKEN id="token-5-43" pos="punct" morph="none" start_char="576" end_char="576">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="579" end_char="713">
<ORIGINAL_TEXT>We’ve seen similar claims before, but this time many claims are being fueled by an unpublished — and highly dubious — scientific paper.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="579" end_char="583">We’ve</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="585" end_char="588">seen</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="590" end_char="596">similar</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="598" end_char="603">claims</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="605" end_char="610">before</TOKEN>
<TOKEN id="token-6-5" pos="punct" morph="none" start_char="611" end_char="611">,</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="613" end_char="615">but</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="617" end_char="620">this</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="622" end_char="625">time</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="627" end_char="630">many</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="632" end_char="637">claims</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="639" end_char="641">are</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="643" end_char="647">being</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="649" end_char="654">fueled</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="656" end_char="657">by</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="659" end_char="660">an</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="662" end_char="672">unpublished</TOKEN>
<TOKEN id="token-6-17" pos="punct" morph="none" start_char="674" end_char="674">—</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="676" end_char="678">and</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="680" end_char="685">highly</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="687" end_char="693">dubious</TOKEN>
<TOKEN id="token-6-21" pos="punct" morph="none" start_char="695" end_char="695">—</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="697" end_char="706">scientific</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="708" end_char="712">paper</TOKEN>
<TOKEN id="token-6-24" pos="punct" morph="none" start_char="713" end_char="713">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="715" end_char="838">
<ORIGINAL_TEXT>By delving into the genetic or protein sequences of the virus, many of these stories have an aura of scientific credibility.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="715" end_char="716">By</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="718" end_char="724">delving</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="726" end_char="729">into</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="731" end_char="733">the</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="735" end_char="741">genetic</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="743" end_char="744">or</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="746" end_char="752">protein</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="754" end_char="762">sequences</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="764" end_char="765">of</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="767" end_char="769">the</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="771" end_char="775">virus</TOKEN>
<TOKEN id="token-7-11" pos="punct" morph="none" start_char="776" end_char="776">,</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="778" end_char="781">many</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="783" end_char="784">of</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="786" end_char="790">these</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="792" end_char="798">stories</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="800" end_char="803">have</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="805" end_char="806">an</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="808" end_char="811">aura</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="813" end_char="814">of</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="816" end_char="825">scientific</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="827" end_char="837">credibility</TOKEN>
<TOKEN id="token-7-22" pos="punct" morph="none" start_char="838" end_char="838">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="840" end_char="895">
<ORIGINAL_TEXT>But scientists who study viruses say they are incorrect.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="840" end_char="842">But</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="844" end_char="853">scientists</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="855" end_char="857">who</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="859" end_char="863">study</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="865" end_char="871">viruses</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="873" end_char="875">say</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="877" end_char="880">they</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="882" end_char="884">are</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="886" end_char="894">incorrect</TOKEN>
<TOKEN id="token-8-9" pos="punct" morph="none" start_char="895" end_char="895">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="898" end_char="1134">
<ORIGINAL_TEXT>One set of stories, subsequently shared on Facebook, inaccurately asserts a link between the new coronavirus, also known as 2019 novel coronavirus, or 2019-nCoV, and HIV, largely based on an unpublished manuscript by scientists in India.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="898" end_char="900">One</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="902" end_char="904">set</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="906" end_char="907">of</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="909" end_char="915">stories</TOKEN>
<TOKEN id="token-9-4" pos="punct" morph="none" start_char="916" end_char="916">,</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="918" end_char="929">subsequently</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="931" end_char="936">shared</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="938" end_char="939">on</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="941" end_char="948">Facebook</TOKEN>
<TOKEN id="token-9-9" pos="punct" morph="none" start_char="949" end_char="949">,</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="951" end_char="962">inaccurately</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="964" end_char="970">asserts</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="972" end_char="972">a</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="974" end_char="977">link</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="979" end_char="985">between</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="987" end_char="989">the</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="991" end_char="993">new</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="995" end_char="1005">coronavirus</TOKEN>
<TOKEN id="token-9-18" pos="punct" morph="none" start_char="1006" end_char="1006">,</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1008" end_char="1011">also</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1013" end_char="1017">known</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1019" end_char="1020">as</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1022" end_char="1025">2019</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="1027" end_char="1031">novel</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1033" end_char="1043">coronavirus</TOKEN>
<TOKEN id="token-9-25" pos="punct" morph="none" start_char="1044" end_char="1044">,</TOKEN>
<TOKEN id="token-9-26" pos="word" morph="none" start_char="1046" end_char="1047">or</TOKEN>
<TOKEN id="token-9-27" pos="unknown" morph="none" start_char="1049" end_char="1057">2019-nCoV</TOKEN>
<TOKEN id="token-9-28" pos="punct" morph="none" start_char="1058" end_char="1058">,</TOKEN>
<TOKEN id="token-9-29" pos="word" morph="none" start_char="1060" end_char="1062">and</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1064" end_char="1066">HIV</TOKEN>
<TOKEN id="token-9-31" pos="punct" morph="none" start_char="1067" end_char="1067">,</TOKEN>
<TOKEN id="token-9-32" pos="word" morph="none" start_char="1069" end_char="1075">largely</TOKEN>
<TOKEN id="token-9-33" pos="word" morph="none" start_char="1077" end_char="1081">based</TOKEN>
<TOKEN id="token-9-34" pos="word" morph="none" start_char="1083" end_char="1084">on</TOKEN>
<TOKEN id="token-9-35" pos="word" morph="none" start_char="1086" end_char="1087">an</TOKEN>
<TOKEN id="token-9-36" pos="word" morph="none" start_char="1089" end_char="1099">unpublished</TOKEN>
<TOKEN id="token-9-37" pos="word" morph="none" start_char="1101" end_char="1110">manuscript</TOKEN>
<TOKEN id="token-9-38" pos="word" morph="none" start_char="1112" end_char="1113">by</TOKEN>
<TOKEN id="token-9-39" pos="word" morph="none" start_char="1115" end_char="1124">scientists</TOKEN>
<TOKEN id="token-9-40" pos="word" morph="none" start_char="1126" end_char="1127">in</TOKEN>
<TOKEN id="token-9-41" pos="word" morph="none" start_char="1129" end_char="1133">India</TOKEN>
<TOKEN id="token-9-42" pos="punct" morph="none" start_char="1134" end_char="1134">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1137" end_char="1230">
<ORIGINAL_TEXT>The paper, which was posted on the preprint website bioRxiv (pronounced "bio-archive") on Jan.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1137" end_char="1139">The</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1141" end_char="1145">paper</TOKEN>
<TOKEN id="token-10-2" pos="punct" morph="none" start_char="1146" end_char="1146">,</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1148" end_char="1152">which</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1154" end_char="1156">was</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1158" end_char="1163">posted</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1165" end_char="1166">on</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1168" end_char="1170">the</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1172" end_char="1179">preprint</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1181" end_char="1187">website</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1189" end_char="1195">bioRxiv</TOKEN>
<TOKEN id="token-10-11" pos="punct" morph="none" start_char="1197" end_char="1197">(</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1198" end_char="1207">pronounced</TOKEN>
<TOKEN id="token-10-13" pos="punct" morph="none" start_char="1209" end_char="1209">"</TOKEN>
<TOKEN id="token-10-14" pos="unknown" morph="none" start_char="1210" end_char="1220">bio-archive</TOKEN>
<TOKEN id="token-10-15" pos="punct" morph="none" start_char="1221" end_char="1222">")</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1224" end_char="1225">on</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1227" end_char="1229">Jan</TOKEN>
<TOKEN id="token-10-18" pos="punct" morph="none" start_char="1230" end_char="1230">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1232" end_char="1357">
<ORIGINAL_TEXT>31, claimed to have identified very short "insertions" in the virus’ protein sequence that had an "uncanny similarity" to HIV.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1232" end_char="1233">31</TOKEN>
<TOKEN id="token-11-1" pos="punct" morph="none" start_char="1234" end_char="1234">,</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1236" end_char="1242">claimed</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1244" end_char="1245">to</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1247" end_char="1250">have</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1252" end_char="1261">identified</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1263" end_char="1266">very</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1268" end_char="1272">short</TOKEN>
<TOKEN id="token-11-8" pos="punct" morph="none" start_char="1274" end_char="1274">"</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1275" end_char="1284">insertions</TOKEN>
<TOKEN id="token-11-10" pos="punct" morph="none" start_char="1285" end_char="1285">"</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1287" end_char="1288">in</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1290" end_char="1292">the</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1294" end_char="1298">virus</TOKEN>
<TOKEN id="token-11-14" pos="punct" morph="none" start_char="1299" end_char="1299">’</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1301" end_char="1307">protein</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1309" end_char="1316">sequence</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1318" end_char="1321">that</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1323" end_char="1325">had</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1327" end_char="1328">an</TOKEN>
<TOKEN id="token-11-20" pos="punct" morph="none" start_char="1330" end_char="1330">"</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1331" end_char="1337">uncanny</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1339" end_char="1348">similarity</TOKEN>
<TOKEN id="token-11-23" pos="punct" morph="none" start_char="1349" end_char="1349">"</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1351" end_char="1352">to</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1354" end_char="1356">HIV</TOKEN>
<TOKEN id="token-11-26" pos="punct" morph="none" start_char="1357" end_char="1357">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1359" end_char="1576">
<ORIGINAL_TEXT>Numerous scientists, however, almost immediately pointed out flaws in the analysis, noting that the sequences are so short, they match a bevy of other organisms — and there’s no reason to conclude they derive from HIV.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1359" end_char="1366">Numerous</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1368" end_char="1377">scientists</TOKEN>
<TOKEN id="token-12-2" pos="punct" morph="none" start_char="1378" end_char="1378">,</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1380" end_char="1386">however</TOKEN>
<TOKEN id="token-12-4" pos="punct" morph="none" start_char="1387" end_char="1387">,</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1389" end_char="1394">almost</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1396" end_char="1406">immediately</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1408" end_char="1414">pointed</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1416" end_char="1418">out</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1420" end_char="1424">flaws</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1426" end_char="1427">in</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1429" end_char="1431">the</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1433" end_char="1440">analysis</TOKEN>
<TOKEN id="token-12-13" pos="punct" morph="none" start_char="1441" end_char="1441">,</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1443" end_char="1448">noting</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1450" end_char="1453">that</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1455" end_char="1457">the</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1459" end_char="1467">sequences</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="1469" end_char="1471">are</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="1473" end_char="1474">so</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1476" end_char="1480">short</TOKEN>
<TOKEN id="token-12-21" pos="punct" morph="none" start_char="1481" end_char="1481">,</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1483" end_char="1486">they</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1488" end_char="1492">match</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1494" end_char="1494">a</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1496" end_char="1499">bevy</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="1501" end_char="1502">of</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="1504" end_char="1508">other</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="1510" end_char="1518">organisms</TOKEN>
<TOKEN id="token-12-29" pos="punct" morph="none" start_char="1520" end_char="1520">—</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="1522" end_char="1524">and</TOKEN>
<TOKEN id="token-12-31" pos="word" morph="none" start_char="1526" end_char="1532">there’s</TOKEN>
<TOKEN id="token-12-32" pos="word" morph="none" start_char="1534" end_char="1535">no</TOKEN>
<TOKEN id="token-12-33" pos="word" morph="none" start_char="1537" end_char="1542">reason</TOKEN>
<TOKEN id="token-12-34" pos="word" morph="none" start_char="1544" end_char="1545">to</TOKEN>
<TOKEN id="token-12-35" pos="word" morph="none" start_char="1547" end_char="1554">conclude</TOKEN>
<TOKEN id="token-12-36" pos="word" morph="none" start_char="1556" end_char="1559">they</TOKEN>
<TOKEN id="token-12-37" pos="word" morph="none" start_char="1561" end_char="1566">derive</TOKEN>
<TOKEN id="token-12-38" pos="word" morph="none" start_char="1568" end_char="1571">from</TOKEN>
<TOKEN id="token-12-39" pos="word" morph="none" start_char="1573" end_char="1575">HIV</TOKEN>
<TOKEN id="token-12-40" pos="punct" morph="none" start_char="1576" end_char="1576">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1578" end_char="1763">
<ORIGINAL_TEXT>The paper was voluntarily withdrawn by its authors just two days later, with one saying, "It was not our intention to feed into the conspiracy theories and no such claims are made here."</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1578" end_char="1580">The</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1582" end_char="1586">paper</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1588" end_char="1590">was</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1592" end_char="1602">voluntarily</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1604" end_char="1612">withdrawn</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1614" end_char="1615">by</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1617" end_char="1619">its</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1621" end_char="1627">authors</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1629" end_char="1632">just</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1634" end_char="1636">two</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1638" end_char="1641">days</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1643" end_char="1647">later</TOKEN>
<TOKEN id="token-13-12" pos="punct" morph="none" start_char="1648" end_char="1648">,</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="1650" end_char="1653">with</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="1655" end_char="1657">one</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="1659" end_char="1664">saying</TOKEN>
<TOKEN id="token-13-16" pos="punct" morph="none" start_char="1665" end_char="1665">,</TOKEN>
<TOKEN id="token-13-17" pos="punct" morph="none" start_char="1667" end_char="1667">"</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="1668" end_char="1669">It</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="1671" end_char="1673">was</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="1675" end_char="1677">not</TOKEN>
<TOKEN id="token-13-21" pos="word" morph="none" start_char="1679" end_char="1681">our</TOKEN>
<TOKEN id="token-13-22" pos="word" morph="none" start_char="1683" end_char="1691">intention</TOKEN>
<TOKEN id="token-13-23" pos="word" morph="none" start_char="1693" end_char="1694">to</TOKEN>
<TOKEN id="token-13-24" pos="word" morph="none" start_char="1696" end_char="1699">feed</TOKEN>
<TOKEN id="token-13-25" pos="word" morph="none" start_char="1701" end_char="1704">into</TOKEN>
<TOKEN id="token-13-26" pos="word" morph="none" start_char="1706" end_char="1708">the</TOKEN>
<TOKEN id="token-13-27" pos="word" morph="none" start_char="1710" end_char="1719">conspiracy</TOKEN>
<TOKEN id="token-13-28" pos="word" morph="none" start_char="1721" end_char="1728">theories</TOKEN>
<TOKEN id="token-13-29" pos="word" morph="none" start_char="1730" end_char="1732">and</TOKEN>
<TOKEN id="token-13-30" pos="word" morph="none" start_char="1734" end_char="1735">no</TOKEN>
<TOKEN id="token-13-31" pos="word" morph="none" start_char="1737" end_char="1740">such</TOKEN>
<TOKEN id="token-13-32" pos="word" morph="none" start_char="1742" end_char="1747">claims</TOKEN>
<TOKEN id="token-13-33" pos="word" morph="none" start_char="1749" end_char="1751">are</TOKEN>
<TOKEN id="token-13-34" pos="word" morph="none" start_char="1753" end_char="1756">made</TOKEN>
<TOKEN id="token-13-35" pos="word" morph="none" start_char="1758" end_char="1761">here</TOKEN>
<TOKEN id="token-13-36" pos="punct" morph="none" start_char="1762" end_char="1763">."</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1766" end_char="1934">
<ORIGINAL_TEXT>But the speedy withdrawal wasn’t fast enough to prevent some websites from picking up the story and concluding that the new coronavirus had been crafted in a laboratory.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1766" end_char="1768">But</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1770" end_char="1772">the</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1774" end_char="1779">speedy</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1781" end_char="1790">withdrawal</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1792" end_char="1797">wasn’t</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1799" end_char="1802">fast</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1804" end_char="1809">enough</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1811" end_char="1812">to</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1814" end_char="1820">prevent</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1822" end_char="1825">some</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1827" end_char="1834">websites</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1836" end_char="1839">from</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="1841" end_char="1847">picking</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="1849" end_char="1850">up</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="1852" end_char="1854">the</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="1856" end_char="1860">story</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="1862" end_char="1864">and</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="1866" end_char="1875">concluding</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="1877" end_char="1880">that</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="1882" end_char="1884">the</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="1886" end_char="1888">new</TOKEN>
<TOKEN id="token-14-21" pos="word" morph="none" start_char="1890" end_char="1900">coronavirus</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="1902" end_char="1904">had</TOKEN>
<TOKEN id="token-14-23" pos="word" morph="none" start_char="1906" end_char="1909">been</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="1911" end_char="1917">crafted</TOKEN>
<TOKEN id="token-14-25" pos="word" morph="none" start_char="1919" end_char="1920">in</TOKEN>
<TOKEN id="token-14-26" pos="word" morph="none" start_char="1922" end_char="1922">a</TOKEN>
<TOKEN id="token-14-27" pos="word" morph="none" start_char="1924" end_char="1933">laboratory</TOKEN>
<TOKEN id="token-14-28" pos="punct" morph="none" start_char="1934" end_char="1934">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1937" end_char="2186">
<ORIGINAL_TEXT>A ZeroHedge article with the headline, "Coronavirus Contains ‘HIV Insertions,’ Stoking Fears Over Artificially Created Bioweapon," pounced on some of the language in the preprint to argue that the scientists were saying the virus might be "man-made."</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1937" end_char="1937">A</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1939" end_char="1947">ZeroHedge</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1949" end_char="1955">article</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="1957" end_char="1960">with</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1962" end_char="1964">the</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1966" end_char="1973">headline</TOKEN>
<TOKEN id="token-15-6" pos="punct" morph="none" start_char="1974" end_char="1974">,</TOKEN>
<TOKEN id="token-15-7" pos="punct" morph="none" start_char="1976" end_char="1976">"</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1977" end_char="1987">Coronavirus</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="1989" end_char="1996">Contains</TOKEN>
<TOKEN id="token-15-10" pos="punct" morph="none" start_char="1998" end_char="1998">‘</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="1999" end_char="2001">HIV</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="2003" end_char="2012">Insertions</TOKEN>
<TOKEN id="token-15-13" pos="punct" morph="none" start_char="2013" end_char="2014">,’</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="2016" end_char="2022">Stoking</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="2024" end_char="2028">Fears</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="2030" end_char="2033">Over</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="2035" end_char="2046">Artificially</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="2048" end_char="2054">Created</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="2056" end_char="2064">Bioweapon</TOKEN>
<TOKEN id="token-15-20" pos="punct" morph="none" start_char="2065" end_char="2066">,"</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="2068" end_char="2074">pounced</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="2076" end_char="2077">on</TOKEN>
<TOKEN id="token-15-23" pos="word" morph="none" start_char="2079" end_char="2082">some</TOKEN>
<TOKEN id="token-15-24" pos="word" morph="none" start_char="2084" end_char="2085">of</TOKEN>
<TOKEN id="token-15-25" pos="word" morph="none" start_char="2087" end_char="2089">the</TOKEN>
<TOKEN id="token-15-26" pos="word" morph="none" start_char="2091" end_char="2098">language</TOKEN>
<TOKEN id="token-15-27" pos="word" morph="none" start_char="2100" end_char="2101">in</TOKEN>
<TOKEN id="token-15-28" pos="word" morph="none" start_char="2103" end_char="2105">the</TOKEN>
<TOKEN id="token-15-29" pos="word" morph="none" start_char="2107" end_char="2114">preprint</TOKEN>
<TOKEN id="token-15-30" pos="word" morph="none" start_char="2116" end_char="2117">to</TOKEN>
<TOKEN id="token-15-31" pos="word" morph="none" start_char="2119" end_char="2123">argue</TOKEN>
<TOKEN id="token-15-32" pos="word" morph="none" start_char="2125" end_char="2128">that</TOKEN>
<TOKEN id="token-15-33" pos="word" morph="none" start_char="2130" end_char="2132">the</TOKEN>
<TOKEN id="token-15-34" pos="word" morph="none" start_char="2134" end_char="2143">scientists</TOKEN>
<TOKEN id="token-15-35" pos="word" morph="none" start_char="2145" end_char="2148">were</TOKEN>
<TOKEN id="token-15-36" pos="word" morph="none" start_char="2150" end_char="2155">saying</TOKEN>
<TOKEN id="token-15-37" pos="word" morph="none" start_char="2157" end_char="2159">the</TOKEN>
<TOKEN id="token-15-38" pos="word" morph="none" start_char="2161" end_char="2165">virus</TOKEN>
<TOKEN id="token-15-39" pos="word" morph="none" start_char="2167" end_char="2171">might</TOKEN>
<TOKEN id="token-15-40" pos="word" morph="none" start_char="2173" end_char="2174">be</TOKEN>
<TOKEN id="token-15-41" pos="punct" morph="none" start_char="2176" end_char="2176">"</TOKEN>
<TOKEN id="token-15-42" pos="unknown" morph="none" start_char="2177" end_char="2184">man-made</TOKEN>
<TOKEN id="token-15-43" pos="punct" morph="none" start_char="2185" end_char="2186">."</TOKEN>
</SEG>
<SEG id="segment-16" start_char="2188" end_char="2420">
<ORIGINAL_TEXT>The story also cited tweets from a visiting scientist at Harvard who had commented on the preprint and stated that the scientist’s tweets suggested that the virus "might have been genetically engineered for the purposes of a weapon."</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="2188" end_char="2190">The</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="2192" end_char="2196">story</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="2198" end_char="2201">also</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="2203" end_char="2207">cited</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="2209" end_char="2214">tweets</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="2216" end_char="2219">from</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="2221" end_char="2221">a</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="2223" end_char="2230">visiting</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="2232" end_char="2240">scientist</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="2242" end_char="2243">at</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="2245" end_char="2251">Harvard</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="2253" end_char="2255">who</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="2257" end_char="2259">had</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2261" end_char="2269">commented</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="2271" end_char="2272">on</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="2274" end_char="2276">the</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="2278" end_char="2285">preprint</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="2287" end_char="2289">and</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="2291" end_char="2296">stated</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="2298" end_char="2301">that</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="2303" end_char="2305">the</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="2307" end_char="2317">scientist’s</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="2319" end_char="2324">tweets</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="2326" end_char="2334">suggested</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="2336" end_char="2339">that</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="2341" end_char="2343">the</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="2345" end_char="2349">virus</TOKEN>
<TOKEN id="token-16-27" pos="punct" morph="none" start_char="2351" end_char="2351">"</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="2352" end_char="2356">might</TOKEN>
<TOKEN id="token-16-29" pos="word" morph="none" start_char="2358" end_char="2361">have</TOKEN>
<TOKEN id="token-16-30" pos="word" morph="none" start_char="2363" end_char="2366">been</TOKEN>
<TOKEN id="token-16-31" pos="word" morph="none" start_char="2368" end_char="2378">genetically</TOKEN>
<TOKEN id="token-16-32" pos="word" morph="none" start_char="2380" end_char="2389">engineered</TOKEN>
<TOKEN id="token-16-33" pos="word" morph="none" start_char="2391" end_char="2393">for</TOKEN>
<TOKEN id="token-16-34" pos="word" morph="none" start_char="2395" end_char="2397">the</TOKEN>
<TOKEN id="token-16-35" pos="word" morph="none" start_char="2399" end_char="2406">purposes</TOKEN>
<TOKEN id="token-16-36" pos="word" morph="none" start_char="2408" end_char="2409">of</TOKEN>
<TOKEN id="token-16-37" pos="word" morph="none" start_char="2411" end_char="2411">a</TOKEN>
<TOKEN id="token-16-38" pos="word" morph="none" start_char="2413" end_char="2418">weapon</TOKEN>
<TOKEN id="token-16-39" pos="punct" morph="none" start_char="2419" end_char="2420">."</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2423" end_char="2620">
<ORIGINAL_TEXT>ZeroHedge is a website that we’ve written about before, including for spreading the false idea that the new coronavirus was stolen from a lab in Canada and then weaponized by the Chinese government.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2423" end_char="2431">ZeroHedge</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2433" end_char="2434">is</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2436" end_char="2436">a</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2438" end_char="2444">website</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2446" end_char="2449">that</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2451" end_char="2455">we’ve</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2457" end_char="2463">written</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="2465" end_char="2469">about</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="2471" end_char="2476">before</TOKEN>
<TOKEN id="token-17-9" pos="punct" morph="none" start_char="2477" end_char="2477">,</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="2479" end_char="2487">including</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="2489" end_char="2491">for</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="2493" end_char="2501">spreading</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="2503" end_char="2505">the</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="2507" end_char="2511">false</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="2513" end_char="2516">idea</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="2518" end_char="2521">that</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="2523" end_char="2525">the</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="2527" end_char="2529">new</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="2531" end_char="2541">coronavirus</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="2543" end_char="2545">was</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="2547" end_char="2552">stolen</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="2554" end_char="2557">from</TOKEN>
<TOKEN id="token-17-23" pos="word" morph="none" start_char="2559" end_char="2559">a</TOKEN>
<TOKEN id="token-17-24" pos="word" morph="none" start_char="2561" end_char="2563">lab</TOKEN>
<TOKEN id="token-17-25" pos="word" morph="none" start_char="2565" end_char="2566">in</TOKEN>
<TOKEN id="token-17-26" pos="word" morph="none" start_char="2568" end_char="2573">Canada</TOKEN>
<TOKEN id="token-17-27" pos="word" morph="none" start_char="2575" end_char="2577">and</TOKEN>
<TOKEN id="token-17-28" pos="word" morph="none" start_char="2579" end_char="2582">then</TOKEN>
<TOKEN id="token-17-29" pos="word" morph="none" start_char="2584" end_char="2593">weaponized</TOKEN>
<TOKEN id="token-17-30" pos="word" morph="none" start_char="2595" end_char="2596">by</TOKEN>
<TOKEN id="token-17-31" pos="word" morph="none" start_char="2598" end_char="2600">the</TOKEN>
<TOKEN id="token-17-32" pos="word" morph="none" start_char="2602" end_char="2608">Chinese</TOKEN>
<TOKEN id="token-17-33" pos="word" morph="none" start_char="2610" end_char="2619">government</TOKEN>
<TOKEN id="token-17-34" pos="punct" morph="none" start_char="2620" end_char="2620">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2623" end_char="2931">
<ORIGINAL_TEXT>Originally published the same day of the preprint, the ZeroHedge article was updated the following day to include tweets from the Harvard visiting scientist, who by then had seen some of the criticisms of the preprint and was now advocating for additional studies to be done before jumping to any conclusions.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2623" end_char="2632">Originally</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2634" end_char="2642">published</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="2644" end_char="2646">the</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="2648" end_char="2651">same</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2653" end_char="2655">day</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="2657" end_char="2658">of</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2660" end_char="2662">the</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2664" end_char="2671">preprint</TOKEN>
<TOKEN id="token-18-8" pos="punct" morph="none" start_char="2672" end_char="2672">,</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2674" end_char="2676">the</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2678" end_char="2686">ZeroHedge</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="2688" end_char="2694">article</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="2696" end_char="2698">was</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="2700" end_char="2706">updated</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="2708" end_char="2710">the</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="2712" end_char="2720">following</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="2722" end_char="2724">day</TOKEN>
<TOKEN id="token-18-17" pos="word" morph="none" start_char="2726" end_char="2727">to</TOKEN>
<TOKEN id="token-18-18" pos="word" morph="none" start_char="2729" end_char="2735">include</TOKEN>
<TOKEN id="token-18-19" pos="word" morph="none" start_char="2737" end_char="2742">tweets</TOKEN>
<TOKEN id="token-18-20" pos="word" morph="none" start_char="2744" end_char="2747">from</TOKEN>
<TOKEN id="token-18-21" pos="word" morph="none" start_char="2749" end_char="2751">the</TOKEN>
<TOKEN id="token-18-22" pos="word" morph="none" start_char="2753" end_char="2759">Harvard</TOKEN>
<TOKEN id="token-18-23" pos="word" morph="none" start_char="2761" end_char="2768">visiting</TOKEN>
<TOKEN id="token-18-24" pos="word" morph="none" start_char="2770" end_char="2778">scientist</TOKEN>
<TOKEN id="token-18-25" pos="punct" morph="none" start_char="2779" end_char="2779">,</TOKEN>
<TOKEN id="token-18-26" pos="word" morph="none" start_char="2781" end_char="2783">who</TOKEN>
<TOKEN id="token-18-27" pos="word" morph="none" start_char="2785" end_char="2786">by</TOKEN>
<TOKEN id="token-18-28" pos="word" morph="none" start_char="2788" end_char="2791">then</TOKEN>
<TOKEN id="token-18-29" pos="word" morph="none" start_char="2793" end_char="2795">had</TOKEN>
<TOKEN id="token-18-30" pos="word" morph="none" start_char="2797" end_char="2800">seen</TOKEN>
<TOKEN id="token-18-31" pos="word" morph="none" start_char="2802" end_char="2805">some</TOKEN>
<TOKEN id="token-18-32" pos="word" morph="none" start_char="2807" end_char="2808">of</TOKEN>
<TOKEN id="token-18-33" pos="word" morph="none" start_char="2810" end_char="2812">the</TOKEN>
<TOKEN id="token-18-34" pos="word" morph="none" start_char="2814" end_char="2823">criticisms</TOKEN>
<TOKEN id="token-18-35" pos="word" morph="none" start_char="2825" end_char="2826">of</TOKEN>
<TOKEN id="token-18-36" pos="word" morph="none" start_char="2828" end_char="2830">the</TOKEN>
<TOKEN id="token-18-37" pos="word" morph="none" start_char="2832" end_char="2839">preprint</TOKEN>
<TOKEN id="token-18-38" pos="word" morph="none" start_char="2841" end_char="2843">and</TOKEN>
<TOKEN id="token-18-39" pos="word" morph="none" start_char="2845" end_char="2847">was</TOKEN>
<TOKEN id="token-18-40" pos="word" morph="none" start_char="2849" end_char="2851">now</TOKEN>
<TOKEN id="token-18-41" pos="word" morph="none" start_char="2853" end_char="2862">advocating</TOKEN>
<TOKEN id="token-18-42" pos="word" morph="none" start_char="2864" end_char="2866">for</TOKEN>
<TOKEN id="token-18-43" pos="word" morph="none" start_char="2868" end_char="2877">additional</TOKEN>
<TOKEN id="token-18-44" pos="word" morph="none" start_char="2879" end_char="2885">studies</TOKEN>
<TOKEN id="token-18-45" pos="word" morph="none" start_char="2887" end_char="2888">to</TOKEN>
<TOKEN id="token-18-46" pos="word" morph="none" start_char="2890" end_char="2891">be</TOKEN>
<TOKEN id="token-18-47" pos="word" morph="none" start_char="2893" end_char="2896">done</TOKEN>
<TOKEN id="token-18-48" pos="word" morph="none" start_char="2898" end_char="2903">before</TOKEN>
<TOKEN id="token-18-49" pos="word" morph="none" start_char="2905" end_char="2911">jumping</TOKEN>
<TOKEN id="token-18-50" pos="word" morph="none" start_char="2913" end_char="2914">to</TOKEN>
<TOKEN id="token-18-51" pos="word" morph="none" start_char="2916" end_char="2918">any</TOKEN>
<TOKEN id="token-18-52" pos="word" morph="none" start_char="2920" end_char="2930">conclusions</TOKEN>
<TOKEN id="token-18-53" pos="punct" morph="none" start_char="2931" end_char="2931">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2933" end_char="3089">
<ORIGINAL_TEXT>The Harvard scientist, it should be said, is an epidemiologist, health economist and nutritionist, and does not have expertise in virology or bioinformatics.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2933" end_char="2935">The</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2937" end_char="2943">Harvard</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2945" end_char="2953">scientist</TOKEN>
<TOKEN id="token-19-3" pos="punct" morph="none" start_char="2954" end_char="2954">,</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2956" end_char="2957">it</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="2959" end_char="2964">should</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2966" end_char="2967">be</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="2969" end_char="2972">said</TOKEN>
<TOKEN id="token-19-8" pos="punct" morph="none" start_char="2973" end_char="2973">,</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="2975" end_char="2976">is</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="2978" end_char="2979">an</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="2981" end_char="2994">epidemiologist</TOKEN>
<TOKEN id="token-19-12" pos="punct" morph="none" start_char="2995" end_char="2995">,</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="2997" end_char="3002">health</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="3004" end_char="3012">economist</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="3014" end_char="3016">and</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="3018" end_char="3029">nutritionist</TOKEN>
<TOKEN id="token-19-17" pos="punct" morph="none" start_char="3030" end_char="3030">,</TOKEN>
<TOKEN id="token-19-18" pos="word" morph="none" start_char="3032" end_char="3034">and</TOKEN>
<TOKEN id="token-19-19" pos="word" morph="none" start_char="3036" end_char="3039">does</TOKEN>
<TOKEN id="token-19-20" pos="word" morph="none" start_char="3041" end_char="3043">not</TOKEN>
<TOKEN id="token-19-21" pos="word" morph="none" start_char="3045" end_char="3048">have</TOKEN>
<TOKEN id="token-19-22" pos="word" morph="none" start_char="3050" end_char="3058">expertise</TOKEN>
<TOKEN id="token-19-23" pos="word" morph="none" start_char="3060" end_char="3061">in</TOKEN>
<TOKEN id="token-19-24" pos="word" morph="none" start_char="3063" end_char="3070">virology</TOKEN>
<TOKEN id="token-19-25" pos="word" morph="none" start_char="3072" end_char="3073">or</TOKEN>
<TOKEN id="token-19-26" pos="word" morph="none" start_char="3075" end_char="3088">bioinformatics</TOKEN>
<TOKEN id="token-19-27" pos="punct" morph="none" start_char="3089" end_char="3089">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="3091" end_char="3142">
<ORIGINAL_TEXT>The bulk of the article, however, remains unchanged.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="3091" end_char="3093">The</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="3095" end_char="3098">bulk</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="3100" end_char="3101">of</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="3103" end_char="3105">the</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="3107" end_char="3113">article</TOKEN>
<TOKEN id="token-20-5" pos="punct" morph="none" start_char="3114" end_char="3114">,</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="3116" end_char="3122">however</TOKEN>
<TOKEN id="token-20-7" pos="punct" morph="none" start_char="3123" end_char="3123">,</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="3125" end_char="3131">remains</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="3133" end_char="3141">unchanged</TOKEN>
<TOKEN id="token-20-10" pos="punct" morph="none" start_char="3142" end_char="3142">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="3145" end_char="3298">
<ORIGINAL_TEXT>Well after the preprint was withdrawn, a website that traffics in vaccine misinformation, Health Impact News, also highlighted the invalid HIV connection.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="3145" end_char="3148">Well</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="3150" end_char="3154">after</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="3156" end_char="3158">the</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="3160" end_char="3167">preprint</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="3169" end_char="3171">was</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="3173" end_char="3181">withdrawn</TOKEN>
<TOKEN id="token-21-6" pos="punct" morph="none" start_char="3182" end_char="3182">,</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="3184" end_char="3184">a</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="3186" end_char="3192">website</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="3194" end_char="3197">that</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="3199" end_char="3206">traffics</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="3208" end_char="3209">in</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="3211" end_char="3217">vaccine</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="3219" end_char="3232">misinformation</TOKEN>
<TOKEN id="token-21-14" pos="punct" morph="none" start_char="3233" end_char="3233">,</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="3235" end_char="3240">Health</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="3242" end_char="3247">Impact</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="3249" end_char="3252">News</TOKEN>
<TOKEN id="token-21-18" pos="punct" morph="none" start_char="3253" end_char="3253">,</TOKEN>
<TOKEN id="token-21-19" pos="word" morph="none" start_char="3255" end_char="3258">also</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="3260" end_char="3270">highlighted</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="3272" end_char="3274">the</TOKEN>
<TOKEN id="token-21-22" pos="word" morph="none" start_char="3276" end_char="3282">invalid</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="3284" end_char="3286">HIV</TOKEN>
<TOKEN id="token-21-24" pos="word" morph="none" start_char="3288" end_char="3297">connection</TOKEN>
<TOKEN id="token-21-25" pos="punct" morph="none" start_char="3298" end_char="3298">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="3301" end_char="3576">
<ORIGINAL_TEXT>Separately, a blogger posted a different bogus analysis — also making the rounds on Facebook — that posits a portion of the new coronavirus genome is similar to part of a viral vector that was used in previous research on the severe acute respiratory syndrome, or SARS, virus.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="3301" end_char="3310">Separately</TOKEN>
<TOKEN id="token-22-1" pos="punct" morph="none" start_char="3311" end_char="3311">,</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="3313" end_char="3313">a</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="3315" end_char="3321">blogger</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="3323" end_char="3328">posted</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="3330" end_char="3330">a</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="3332" end_char="3340">different</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="3342" end_char="3346">bogus</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="3348" end_char="3355">analysis</TOKEN>
<TOKEN id="token-22-9" pos="punct" morph="none" start_char="3357" end_char="3357">—</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="3359" end_char="3362">also</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="3364" end_char="3369">making</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="3371" end_char="3373">the</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="3375" end_char="3380">rounds</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="3382" end_char="3383">on</TOKEN>
<TOKEN id="token-22-15" pos="word" morph="none" start_char="3385" end_char="3392">Facebook</TOKEN>
<TOKEN id="token-22-16" pos="punct" morph="none" start_char="3394" end_char="3394">—</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="3396" end_char="3399">that</TOKEN>
<TOKEN id="token-22-18" pos="word" morph="none" start_char="3401" end_char="3406">posits</TOKEN>
<TOKEN id="token-22-19" pos="word" morph="none" start_char="3408" end_char="3408">a</TOKEN>
<TOKEN id="token-22-20" pos="word" morph="none" start_char="3410" end_char="3416">portion</TOKEN>
<TOKEN id="token-22-21" pos="word" morph="none" start_char="3418" end_char="3419">of</TOKEN>
<TOKEN id="token-22-22" pos="word" morph="none" start_char="3421" end_char="3423">the</TOKEN>
<TOKEN id="token-22-23" pos="word" morph="none" start_char="3425" end_char="3427">new</TOKEN>
<TOKEN id="token-22-24" pos="word" morph="none" start_char="3429" end_char="3439">coronavirus</TOKEN>
<TOKEN id="token-22-25" pos="word" morph="none" start_char="3441" end_char="3446">genome</TOKEN>
<TOKEN id="token-22-26" pos="word" morph="none" start_char="3448" end_char="3449">is</TOKEN>
<TOKEN id="token-22-27" pos="word" morph="none" start_char="3451" end_char="3457">similar</TOKEN>
<TOKEN id="token-22-28" pos="word" morph="none" start_char="3459" end_char="3460">to</TOKEN>
<TOKEN id="token-22-29" pos="word" morph="none" start_char="3462" end_char="3465">part</TOKEN>
<TOKEN id="token-22-30" pos="word" morph="none" start_char="3467" end_char="3468">of</TOKEN>
<TOKEN id="token-22-31" pos="word" morph="none" start_char="3470" end_char="3470">a</TOKEN>
<TOKEN id="token-22-32" pos="word" morph="none" start_char="3472" end_char="3476">viral</TOKEN>
<TOKEN id="token-22-33" pos="word" morph="none" start_char="3478" end_char="3483">vector</TOKEN>
<TOKEN id="token-22-34" pos="word" morph="none" start_char="3485" end_char="3488">that</TOKEN>
<TOKEN id="token-22-35" pos="word" morph="none" start_char="3490" end_char="3492">was</TOKEN>
<TOKEN id="token-22-36" pos="word" morph="none" start_char="3494" end_char="3497">used</TOKEN>
<TOKEN id="token-22-37" pos="word" morph="none" start_char="3499" end_char="3500">in</TOKEN>
<TOKEN id="token-22-38" pos="word" morph="none" start_char="3502" end_char="3509">previous</TOKEN>
<TOKEN id="token-22-39" pos="word" morph="none" start_char="3511" end_char="3518">research</TOKEN>
<TOKEN id="token-22-40" pos="word" morph="none" start_char="3520" end_char="3521">on</TOKEN>
<TOKEN id="token-22-41" pos="word" morph="none" start_char="3523" end_char="3525">the</TOKEN>
<TOKEN id="token-22-42" pos="word" morph="none" start_char="3527" end_char="3532">severe</TOKEN>
<TOKEN id="token-22-43" pos="word" morph="none" start_char="3534" end_char="3538">acute</TOKEN>
<TOKEN id="token-22-44" pos="word" morph="none" start_char="3540" end_char="3550">respiratory</TOKEN>
<TOKEN id="token-22-45" pos="word" morph="none" start_char="3552" end_char="3559">syndrome</TOKEN>
<TOKEN id="token-22-46" pos="punct" morph="none" start_char="3560" end_char="3560">,</TOKEN>
<TOKEN id="token-22-47" pos="word" morph="none" start_char="3562" end_char="3563">or</TOKEN>
<TOKEN id="token-22-48" pos="word" morph="none" start_char="3565" end_char="3568">SARS</TOKEN>
<TOKEN id="token-22-49" pos="punct" morph="none" start_char="3569" end_char="3569">,</TOKEN>
<TOKEN id="token-22-50" pos="word" morph="none" start_char="3571" end_char="3575">virus</TOKEN>
<TOKEN id="token-22-51" pos="punct" morph="none" start_char="3576" end_char="3576">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="3578" end_char="3687">
<ORIGINAL_TEXT>Based on this, the author argues that the new virus could have leaked from a Chinese lab working on a vaccine.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="3578" end_char="3582">Based</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="3584" end_char="3585">on</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="3587" end_char="3590">this</TOKEN>
<TOKEN id="token-23-3" pos="punct" morph="none" start_char="3591" end_char="3591">,</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="3593" end_char="3595">the</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="3597" end_char="3602">author</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="3604" end_char="3609">argues</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="3611" end_char="3614">that</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="3616" end_char="3618">the</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="3620" end_char="3622">new</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="3624" end_char="3628">virus</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="3630" end_char="3634">could</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="3636" end_char="3639">have</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="3641" end_char="3646">leaked</TOKEN>
<TOKEN id="token-23-14" pos="word" morph="none" start_char="3648" end_char="3651">from</TOKEN>
<TOKEN id="token-23-15" pos="word" morph="none" start_char="3653" end_char="3653">a</TOKEN>
<TOKEN id="token-23-16" pos="word" morph="none" start_char="3655" end_char="3661">Chinese</TOKEN>
<TOKEN id="token-23-17" pos="word" morph="none" start_char="3663" end_char="3665">lab</TOKEN>
<TOKEN id="token-23-18" pos="word" morph="none" start_char="3667" end_char="3673">working</TOKEN>
<TOKEN id="token-23-19" pos="word" morph="none" start_char="3675" end_char="3676">on</TOKEN>
<TOKEN id="token-23-20" pos="word" morph="none" start_char="3678" end_char="3678">a</TOKEN>
<TOKEN id="token-23-21" pos="word" morph="none" start_char="3680" end_char="3686">vaccine</TOKEN>
<TOKEN id="token-23-22" pos="punct" morph="none" start_char="3687" end_char="3687">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="3689" end_char="3779">
<ORIGINAL_TEXT>The SARS virus caused a global outbreak in 2003 and is similar but distinct from 2019-nCoV.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="3689" end_char="3691">The</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="3693" end_char="3696">SARS</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="3698" end_char="3702">virus</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="3704" end_char="3709">caused</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="3711" end_char="3711">a</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="3713" end_char="3718">global</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="3720" end_char="3727">outbreak</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="3729" end_char="3730">in</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="3732" end_char="3735">2003</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="3737" end_char="3739">and</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="3741" end_char="3742">is</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="3744" end_char="3750">similar</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="3752" end_char="3754">but</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="3756" end_char="3763">distinct</TOKEN>
<TOKEN id="token-24-14" pos="word" morph="none" start_char="3765" end_char="3768">from</TOKEN>
<TOKEN id="token-24-15" pos="unknown" morph="none" start_char="3770" end_char="3778">2019-nCoV</TOKEN>
<TOKEN id="token-24-16" pos="punct" morph="none" start_char="3779" end_char="3779">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="3782" end_char="3960">
<ORIGINAL_TEXT>Alex Jones, the conspiracy theorist behind InfoWars and the false idea that the Sandy Hook school shooting in 2012 was a hoax, also waded into the coronavirus misinformation pool.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="3782" end_char="3785">Alex</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="3787" end_char="3791">Jones</TOKEN>
<TOKEN id="token-25-2" pos="punct" morph="none" start_char="3792" end_char="3792">,</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="3794" end_char="3796">the</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="3798" end_char="3807">conspiracy</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="3809" end_char="3816">theorist</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="3818" end_char="3823">behind</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="3825" end_char="3832">InfoWars</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="3834" end_char="3836">and</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="3838" end_char="3840">the</TOKEN>
<TOKEN id="token-25-10" pos="word" morph="none" start_char="3842" end_char="3846">false</TOKEN>
<TOKEN id="token-25-11" pos="word" morph="none" start_char="3848" end_char="3851">idea</TOKEN>
<TOKEN id="token-25-12" pos="word" morph="none" start_char="3853" end_char="3856">that</TOKEN>
<TOKEN id="token-25-13" pos="word" morph="none" start_char="3858" end_char="3860">the</TOKEN>
<TOKEN id="token-25-14" pos="word" morph="none" start_char="3862" end_char="3866">Sandy</TOKEN>
<TOKEN id="token-25-15" pos="word" morph="none" start_char="3868" end_char="3871">Hook</TOKEN>
<TOKEN id="token-25-16" pos="word" morph="none" start_char="3873" end_char="3878">school</TOKEN>
<TOKEN id="token-25-17" pos="word" morph="none" start_char="3880" end_char="3887">shooting</TOKEN>
<TOKEN id="token-25-18" pos="word" morph="none" start_char="3889" end_char="3890">in</TOKEN>
<TOKEN id="token-25-19" pos="word" morph="none" start_char="3892" end_char="3895">2012</TOKEN>
<TOKEN id="token-25-20" pos="word" morph="none" start_char="3897" end_char="3899">was</TOKEN>
<TOKEN id="token-25-21" pos="word" morph="none" start_char="3901" end_char="3901">a</TOKEN>
<TOKEN id="token-25-22" pos="word" morph="none" start_char="3903" end_char="3906">hoax</TOKEN>
<TOKEN id="token-25-23" pos="punct" morph="none" start_char="3907" end_char="3907">,</TOKEN>
<TOKEN id="token-25-24" pos="word" morph="none" start_char="3909" end_char="3912">also</TOKEN>
<TOKEN id="token-25-25" pos="word" morph="none" start_char="3914" end_char="3918">waded</TOKEN>
<TOKEN id="token-25-26" pos="word" morph="none" start_char="3920" end_char="3923">into</TOKEN>
<TOKEN id="token-25-27" pos="word" morph="none" start_char="3925" end_char="3927">the</TOKEN>
<TOKEN id="token-25-28" pos="word" morph="none" start_char="3929" end_char="3939">coronavirus</TOKEN>
<TOKEN id="token-25-29" pos="word" morph="none" start_char="3941" end_char="3954">misinformation</TOKEN>
<TOKEN id="token-25-30" pos="word" morph="none" start_char="3956" end_char="3959">pool</TOKEN>
<TOKEN id="token-25-31" pos="punct" morph="none" start_char="3960" end_char="3960">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="3962" end_char="4115">
<ORIGINAL_TEXT>Multiple episodes of his talk show address both of these groundless theories and claim there is evidence that "proves" the new coronavirus was "man-made."</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="3962" end_char="3969">Multiple</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="3971" end_char="3978">episodes</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="3980" end_char="3981">of</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="3983" end_char="3985">his</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="3987" end_char="3990">talk</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="3992" end_char="3995">show</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="3997" end_char="4003">address</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="4005" end_char="4008">both</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="4010" end_char="4011">of</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="4013" end_char="4017">these</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="4019" end_char="4028">groundless</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="4030" end_char="4037">theories</TOKEN>
<TOKEN id="token-26-12" pos="word" morph="none" start_char="4039" end_char="4041">and</TOKEN>
<TOKEN id="token-26-13" pos="word" morph="none" start_char="4043" end_char="4047">claim</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="4049" end_char="4053">there</TOKEN>
<TOKEN id="token-26-15" pos="word" morph="none" start_char="4055" end_char="4056">is</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="4058" end_char="4065">evidence</TOKEN>
<TOKEN id="token-26-17" pos="word" morph="none" start_char="4067" end_char="4070">that</TOKEN>
<TOKEN id="token-26-18" pos="punct" morph="none" start_char="4072" end_char="4072">"</TOKEN>
<TOKEN id="token-26-19" pos="word" morph="none" start_char="4073" end_char="4078">proves</TOKEN>
<TOKEN id="token-26-20" pos="punct" morph="none" start_char="4079" end_char="4079">"</TOKEN>
<TOKEN id="token-26-21" pos="word" morph="none" start_char="4081" end_char="4083">the</TOKEN>
<TOKEN id="token-26-22" pos="word" morph="none" start_char="4085" end_char="4087">new</TOKEN>
<TOKEN id="token-26-23" pos="word" morph="none" start_char="4089" end_char="4099">coronavirus</TOKEN>
<TOKEN id="token-26-24" pos="word" morph="none" start_char="4101" end_char="4103">was</TOKEN>
<TOKEN id="token-26-25" pos="punct" morph="none" start_char="4105" end_char="4105">"</TOKEN>
<TOKEN id="token-26-26" pos="unknown" morph="none" start_char="4106" end_char="4113">man-made</TOKEN>
<TOKEN id="token-26-27" pos="punct" morph="none" start_char="4114" end_char="4115">."</TOKEN>
</SEG>
<SEG id="segment-27" start_char="4118" end_char="4204">
<ORIGINAL_TEXT>Scientists with expertise in viral genomics, however, say that no such evidence exists.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="4118" end_char="4127">Scientists</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="4129" end_char="4132">with</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="4134" end_char="4142">expertise</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="4144" end_char="4145">in</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="4147" end_char="4151">viral</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="4153" end_char="4160">genomics</TOKEN>
<TOKEN id="token-27-6" pos="punct" morph="none" start_char="4161" end_char="4161">,</TOKEN>
<TOKEN id="token-27-7" pos="word" morph="none" start_char="4163" end_char="4169">however</TOKEN>
<TOKEN id="token-27-8" pos="punct" morph="none" start_char="4170" end_char="4170">,</TOKEN>
<TOKEN id="token-27-9" pos="word" morph="none" start_char="4172" end_char="4174">say</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="4176" end_char="4179">that</TOKEN>
<TOKEN id="token-27-11" pos="word" morph="none" start_char="4181" end_char="4182">no</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="4184" end_char="4187">such</TOKEN>
<TOKEN id="token-27-13" pos="word" morph="none" start_char="4189" end_char="4196">evidence</TOKEN>
<TOKEN id="token-27-14" pos="word" morph="none" start_char="4198" end_char="4203">exists</TOKEN>
<TOKEN id="token-27-15" pos="punct" morph="none" start_char="4204" end_char="4204">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="4206" end_char="4393">
<ORIGINAL_TEXT>Kristian Andersen, the director of infectious disease genomics at the Scripps Research Translational Institute, told us in an email that in both cases, the analyses are "completely wrong."</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="4206" end_char="4213">Kristian</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="4215" end_char="4222">Andersen</TOKEN>
<TOKEN id="token-28-2" pos="punct" morph="none" start_char="4223" end_char="4223">,</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="4225" end_char="4227">the</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="4229" end_char="4236">director</TOKEN>
<TOKEN id="token-28-5" pos="word" morph="none" start_char="4238" end_char="4239">of</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="4241" end_char="4250">infectious</TOKEN>
<TOKEN id="token-28-7" pos="word" morph="none" start_char="4252" end_char="4258">disease</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="4260" end_char="4267">genomics</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="4269" end_char="4270">at</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="4272" end_char="4274">the</TOKEN>
<TOKEN id="token-28-11" pos="word" morph="none" start_char="4276" end_char="4282">Scripps</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="4284" end_char="4291">Research</TOKEN>
<TOKEN id="token-28-13" pos="word" morph="none" start_char="4293" end_char="4305">Translational</TOKEN>
<TOKEN id="token-28-14" pos="word" morph="none" start_char="4307" end_char="4315">Institute</TOKEN>
<TOKEN id="token-28-15" pos="punct" morph="none" start_char="4316" end_char="4316">,</TOKEN>
<TOKEN id="token-28-16" pos="word" morph="none" start_char="4318" end_char="4321">told</TOKEN>
<TOKEN id="token-28-17" pos="word" morph="none" start_char="4323" end_char="4324">us</TOKEN>
<TOKEN id="token-28-18" pos="word" morph="none" start_char="4326" end_char="4327">in</TOKEN>
<TOKEN id="token-28-19" pos="word" morph="none" start_char="4329" end_char="4330">an</TOKEN>
<TOKEN id="token-28-20" pos="word" morph="none" start_char="4332" end_char="4336">email</TOKEN>
<TOKEN id="token-28-21" pos="word" morph="none" start_char="4338" end_char="4341">that</TOKEN>
<TOKEN id="token-28-22" pos="word" morph="none" start_char="4343" end_char="4344">in</TOKEN>
<TOKEN id="token-28-23" pos="word" morph="none" start_char="4346" end_char="4349">both</TOKEN>
<TOKEN id="token-28-24" pos="word" morph="none" start_char="4351" end_char="4355">cases</TOKEN>
<TOKEN id="token-28-25" pos="punct" morph="none" start_char="4356" end_char="4356">,</TOKEN>
<TOKEN id="token-28-26" pos="word" morph="none" start_char="4358" end_char="4360">the</TOKEN>
<TOKEN id="token-28-27" pos="word" morph="none" start_char="4362" end_char="4369">analyses</TOKEN>
<TOKEN id="token-28-28" pos="word" morph="none" start_char="4371" end_char="4373">are</TOKEN>
<TOKEN id="token-28-29" pos="punct" morph="none" start_char="4375" end_char="4375">"</TOKEN>
<TOKEN id="token-28-30" pos="word" morph="none" start_char="4376" end_char="4385">completely</TOKEN>
<TOKEN id="token-28-31" pos="word" morph="none" start_char="4387" end_char="4391">wrong</TOKEN>
<TOKEN id="token-28-32" pos="punct" morph="none" start_char="4392" end_char="4393">."</TOKEN>
</SEG>
<SEG id="segment-29" start_char="4396" end_char="4523">
<ORIGINAL_TEXT>The HIV study, he said, was a "misunderstanding of how to perform these types of analyses" that also cherry-picked its findings.</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="4396" end_char="4398">The</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="4400" end_char="4402">HIV</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="4404" end_char="4408">study</TOKEN>
<TOKEN id="token-29-3" pos="punct" morph="none" start_char="4409" end_char="4409">,</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="4411" end_char="4412">he</TOKEN>
<TOKEN id="token-29-5" pos="word" morph="none" start_char="4414" end_char="4417">said</TOKEN>
<TOKEN id="token-29-6" pos="punct" morph="none" start_char="4418" end_char="4418">,</TOKEN>
<TOKEN id="token-29-7" pos="word" morph="none" start_char="4420" end_char="4422">was</TOKEN>
<TOKEN id="token-29-8" pos="word" morph="none" start_char="4424" end_char="4424">a</TOKEN>
<TOKEN id="token-29-9" pos="punct" morph="none" start_char="4426" end_char="4426">"</TOKEN>
<TOKEN id="token-29-10" pos="word" morph="none" start_char="4427" end_char="4442">misunderstanding</TOKEN>
<TOKEN id="token-29-11" pos="word" morph="none" start_char="4444" end_char="4445">of</TOKEN>
<TOKEN id="token-29-12" pos="word" morph="none" start_char="4447" end_char="4449">how</TOKEN>
<TOKEN id="token-29-13" pos="word" morph="none" start_char="4451" end_char="4452">to</TOKEN>
<TOKEN id="token-29-14" pos="word" morph="none" start_char="4454" end_char="4460">perform</TOKEN>
<TOKEN id="token-29-15" pos="word" morph="none" start_char="4462" end_char="4466">these</TOKEN>
<TOKEN id="token-29-16" pos="word" morph="none" start_char="4468" end_char="4472">types</TOKEN>
<TOKEN id="token-29-17" pos="word" morph="none" start_char="4474" end_char="4475">of</TOKEN>
<TOKEN id="token-29-18" pos="word" morph="none" start_char="4477" end_char="4484">analyses</TOKEN>
<TOKEN id="token-29-19" pos="punct" morph="none" start_char="4485" end_char="4485">"</TOKEN>
<TOKEN id="token-29-20" pos="word" morph="none" start_char="4487" end_char="4490">that</TOKEN>
<TOKEN id="token-29-21" pos="word" morph="none" start_char="4492" end_char="4495">also</TOKEN>
<TOKEN id="token-29-22" pos="unknown" morph="none" start_char="4497" end_char="4509">cherry-picked</TOKEN>
<TOKEN id="token-29-23" pos="word" morph="none" start_char="4511" end_char="4513">its</TOKEN>
<TOKEN id="token-29-24" pos="word" morph="none" start_char="4515" end_char="4522">findings</TOKEN>
<TOKEN id="token-29-25" pos="punct" morph="none" start_char="4523" end_char="4523">.</TOKEN>
</SEG>
<SEG id="segment-30" start_char="4525" end_char="4693">
<ORIGINAL_TEXT>The short proteins the Indian scientists found to be similar to HIV are not from HIV at all, Andersen said, but are the result of the natural evolution of coronaviruses.</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="4525" end_char="4527">The</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="4529" end_char="4533">short</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="4535" end_char="4542">proteins</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="4544" end_char="4546">the</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="4548" end_char="4553">Indian</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="4555" end_char="4564">scientists</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="4566" end_char="4570">found</TOKEN>
<TOKEN id="token-30-7" pos="word" morph="none" start_char="4572" end_char="4573">to</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="4575" end_char="4576">be</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="4578" end_char="4584">similar</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="4586" end_char="4587">to</TOKEN>
<TOKEN id="token-30-11" pos="word" morph="none" start_char="4589" end_char="4591">HIV</TOKEN>
<TOKEN id="token-30-12" pos="word" morph="none" start_char="4593" end_char="4595">are</TOKEN>
<TOKEN id="token-30-13" pos="word" morph="none" start_char="4597" end_char="4599">not</TOKEN>
<TOKEN id="token-30-14" pos="word" morph="none" start_char="4601" end_char="4604">from</TOKEN>
<TOKEN id="token-30-15" pos="word" morph="none" start_char="4606" end_char="4608">HIV</TOKEN>
<TOKEN id="token-30-16" pos="word" morph="none" start_char="4610" end_char="4611">at</TOKEN>
<TOKEN id="token-30-17" pos="word" morph="none" start_char="4613" end_char="4615">all</TOKEN>
<TOKEN id="token-30-18" pos="punct" morph="none" start_char="4616" end_char="4616">,</TOKEN>
<TOKEN id="token-30-19" pos="word" morph="none" start_char="4618" end_char="4625">Andersen</TOKEN>
<TOKEN id="token-30-20" pos="word" morph="none" start_char="4627" end_char="4630">said</TOKEN>
<TOKEN id="token-30-21" pos="punct" morph="none" start_char="4631" end_char="4631">,</TOKEN>
<TOKEN id="token-30-22" pos="word" morph="none" start_char="4633" end_char="4635">but</TOKEN>
<TOKEN id="token-30-23" pos="word" morph="none" start_char="4637" end_char="4639">are</TOKEN>
<TOKEN id="token-30-24" pos="word" morph="none" start_char="4641" end_char="4643">the</TOKEN>
<TOKEN id="token-30-25" pos="word" morph="none" start_char="4645" end_char="4650">result</TOKEN>
<TOKEN id="token-30-26" pos="word" morph="none" start_char="4652" end_char="4653">of</TOKEN>
<TOKEN id="token-30-27" pos="word" morph="none" start_char="4655" end_char="4657">the</TOKEN>
<TOKEN id="token-30-28" pos="word" morph="none" start_char="4659" end_char="4665">natural</TOKEN>
<TOKEN id="token-30-29" pos="word" morph="none" start_char="4667" end_char="4675">evolution</TOKEN>
<TOKEN id="token-30-30" pos="word" morph="none" start_char="4677" end_char="4678">of</TOKEN>
<TOKEN id="token-30-31" pos="word" morph="none" start_char="4680" end_char="4692">coronaviruses</TOKEN>
<TOKEN id="token-30-32" pos="punct" morph="none" start_char="4693" end_char="4693">.</TOKEN>
</SEG>
<SEG id="segment-31" start_char="4695" end_char="4914">
<ORIGINAL_TEXT>"Had the authors compared nCoV to related bat viruses (and not just SARS as they did)," he wrote, "they would have realized that the peptides are also present in the bat viruses — and most certainly don’t come from HIV."</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="punct" morph="none" start_char="4695" end_char="4695">"</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="4696" end_char="4698">Had</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="4700" end_char="4702">the</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="4704" end_char="4710">authors</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="4712" end_char="4719">compared</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="4721" end_char="4724">nCoV</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="4726" end_char="4727">to</TOKEN>
<TOKEN id="token-31-7" pos="word" morph="none" start_char="4729" end_char="4735">related</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="4737" end_char="4739">bat</TOKEN>
<TOKEN id="token-31-9" pos="word" morph="none" start_char="4741" end_char="4747">viruses</TOKEN>
<TOKEN id="token-31-10" pos="punct" morph="none" start_char="4749" end_char="4749">(</TOKEN>
<TOKEN id="token-31-11" pos="word" morph="none" start_char="4750" end_char="4752">and</TOKEN>
<TOKEN id="token-31-12" pos="word" morph="none" start_char="4754" end_char="4756">not</TOKEN>
<TOKEN id="token-31-13" pos="word" morph="none" start_char="4758" end_char="4761">just</TOKEN>
<TOKEN id="token-31-14" pos="word" morph="none" start_char="4763" end_char="4766">SARS</TOKEN>
<TOKEN id="token-31-15" pos="word" morph="none" start_char="4768" end_char="4769">as</TOKEN>
<TOKEN id="token-31-16" pos="word" morph="none" start_char="4771" end_char="4774">they</TOKEN>
<TOKEN id="token-31-17" pos="word" morph="none" start_char="4776" end_char="4778">did</TOKEN>
<TOKEN id="token-31-18" pos="punct" morph="none" start_char="4779" end_char="4781">),"</TOKEN>
<TOKEN id="token-31-19" pos="word" morph="none" start_char="4783" end_char="4784">he</TOKEN>
<TOKEN id="token-31-20" pos="word" morph="none" start_char="4786" end_char="4790">wrote</TOKEN>
<TOKEN id="token-31-21" pos="punct" morph="none" start_char="4791" end_char="4791">,</TOKEN>
<TOKEN id="token-31-22" pos="punct" morph="none" start_char="4793" end_char="4793">"</TOKEN>
<TOKEN id="token-31-23" pos="word" morph="none" start_char="4794" end_char="4797">they</TOKEN>
<TOKEN id="token-31-24" pos="word" morph="none" start_char="4799" end_char="4803">would</TOKEN>
<TOKEN id="token-31-25" pos="word" morph="none" start_char="4805" end_char="4808">have</TOKEN>
<TOKEN id="token-31-26" pos="word" morph="none" start_char="4810" end_char="4817">realized</TOKEN>
<TOKEN id="token-31-27" pos="word" morph="none" start_char="4819" end_char="4822">that</TOKEN>
<TOKEN id="token-31-28" pos="word" morph="none" start_char="4824" end_char="4826">the</TOKEN>
<TOKEN id="token-31-29" pos="word" morph="none" start_char="4828" end_char="4835">peptides</TOKEN>
<TOKEN id="token-31-30" pos="word" morph="none" start_char="4837" end_char="4839">are</TOKEN>
<TOKEN id="token-31-31" pos="word" morph="none" start_char="4841" end_char="4844">also</TOKEN>
<TOKEN id="token-31-32" pos="word" morph="none" start_char="4846" end_char="4852">present</TOKEN>
<TOKEN id="token-31-33" pos="word" morph="none" start_char="4854" end_char="4855">in</TOKEN>
<TOKEN id="token-31-34" pos="word" morph="none" start_char="4857" end_char="4859">the</TOKEN>
<TOKEN id="token-31-35" pos="word" morph="none" start_char="4861" end_char="4863">bat</TOKEN>
<TOKEN id="token-31-36" pos="word" morph="none" start_char="4865" end_char="4871">viruses</TOKEN>
<TOKEN id="token-31-37" pos="punct" morph="none" start_char="4873" end_char="4873">—</TOKEN>
<TOKEN id="token-31-38" pos="word" morph="none" start_char="4875" end_char="4877">and</TOKEN>
<TOKEN id="token-31-39" pos="word" morph="none" start_char="4879" end_char="4882">most</TOKEN>
<TOKEN id="token-31-40" pos="word" morph="none" start_char="4884" end_char="4892">certainly</TOKEN>
<TOKEN id="token-31-41" pos="word" morph="none" start_char="4894" end_char="4898">don’t</TOKEN>
<TOKEN id="token-31-42" pos="word" morph="none" start_char="4900" end_char="4903">come</TOKEN>
<TOKEN id="token-31-43" pos="word" morph="none" start_char="4905" end_char="4908">from</TOKEN>
<TOKEN id="token-31-44" pos="word" morph="none" start_char="4910" end_char="4912">HIV</TOKEN>
<TOKEN id="token-31-45" pos="punct" morph="none" start_char="4913" end_char="4914">."</TOKEN>
</SEG>
<SEG id="segment-32" start_char="4917" end_char="5170">
<ORIGINAL_TEXT>Indeed, other experts have noted the same shortcomings, including Trevor Bedford, a computational biologist at the Fred Hutchinson Cancer Research Center in Seattle, who performed the proper sequence alignments and shared the results in a Twitter thread.</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="4917" end_char="4922">Indeed</TOKEN>
<TOKEN id="token-32-1" pos="punct" morph="none" start_char="4923" end_char="4923">,</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="4925" end_char="4929">other</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="4931" end_char="4937">experts</TOKEN>
<TOKEN id="token-32-4" pos="word" morph="none" start_char="4939" end_char="4942">have</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="4944" end_char="4948">noted</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="4950" end_char="4952">the</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="4954" end_char="4957">same</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="4959" end_char="4970">shortcomings</TOKEN>
<TOKEN id="token-32-9" pos="punct" morph="none" start_char="4971" end_char="4971">,</TOKEN>
<TOKEN id="token-32-10" pos="word" morph="none" start_char="4973" end_char="4981">including</TOKEN>
<TOKEN id="token-32-11" pos="word" morph="none" start_char="4983" end_char="4988">Trevor</TOKEN>
<TOKEN id="token-32-12" pos="word" morph="none" start_char="4990" end_char="4996">Bedford</TOKEN>
<TOKEN id="token-32-13" pos="punct" morph="none" start_char="4997" end_char="4997">,</TOKEN>
<TOKEN id="token-32-14" pos="word" morph="none" start_char="4999" end_char="4999">a</TOKEN>
<TOKEN id="token-32-15" pos="word" morph="none" start_char="5001" end_char="5013">computational</TOKEN>
<TOKEN id="token-32-16" pos="word" morph="none" start_char="5015" end_char="5023">biologist</TOKEN>
<TOKEN id="token-32-17" pos="word" morph="none" start_char="5025" end_char="5026">at</TOKEN>
<TOKEN id="token-32-18" pos="word" morph="none" start_char="5028" end_char="5030">the</TOKEN>
<TOKEN id="token-32-19" pos="word" morph="none" start_char="5032" end_char="5035">Fred</TOKEN>
<TOKEN id="token-32-20" pos="word" morph="none" start_char="5037" end_char="5046">Hutchinson</TOKEN>
<TOKEN id="token-32-21" pos="word" morph="none" start_char="5048" end_char="5053">Cancer</TOKEN>
<TOKEN id="token-32-22" pos="word" morph="none" start_char="5055" end_char="5062">Research</TOKEN>
<TOKEN id="token-32-23" pos="word" morph="none" start_char="5064" end_char="5069">Center</TOKEN>
<TOKEN id="token-32-24" pos="word" morph="none" start_char="5071" end_char="5072">in</TOKEN>
<TOKEN id="token-32-25" pos="word" morph="none" start_char="5074" end_char="5080">Seattle</TOKEN>
<TOKEN id="token-32-26" pos="punct" morph="none" start_char="5081" end_char="5081">,</TOKEN>
<TOKEN id="token-32-27" pos="word" morph="none" start_char="5083" end_char="5085">who</TOKEN>
<TOKEN id="token-32-28" pos="word" morph="none" start_char="5087" end_char="5095">performed</TOKEN>
<TOKEN id="token-32-29" pos="word" morph="none" start_char="5097" end_char="5099">the</TOKEN>
<TOKEN id="token-32-30" pos="word" morph="none" start_char="5101" end_char="5106">proper</TOKEN>
<TOKEN id="token-32-31" pos="word" morph="none" start_char="5108" end_char="5115">sequence</TOKEN>
<TOKEN id="token-32-32" pos="word" morph="none" start_char="5117" end_char="5126">alignments</TOKEN>
<TOKEN id="token-32-33" pos="word" morph="none" start_char="5128" end_char="5130">and</TOKEN>
<TOKEN id="token-32-34" pos="word" morph="none" start_char="5132" end_char="5137">shared</TOKEN>
<TOKEN id="token-32-35" pos="word" morph="none" start_char="5139" end_char="5141">the</TOKEN>
<TOKEN id="token-32-36" pos="word" morph="none" start_char="5143" end_char="5149">results</TOKEN>
<TOKEN id="token-32-37" pos="word" morph="none" start_char="5151" end_char="5152">in</TOKEN>
<TOKEN id="token-32-38" pos="word" morph="none" start_char="5154" end_char="5154">a</TOKEN>
<TOKEN id="token-32-39" pos="word" morph="none" start_char="5156" end_char="5162">Twitter</TOKEN>
<TOKEN id="token-32-40" pos="word" morph="none" start_char="5164" end_char="5169">thread</TOKEN>
<TOKEN id="token-32-41" pos="punct" morph="none" start_char="5170" end_char="5170">.</TOKEN>
</SEG>
<SEG id="segment-33" start_char="5172" end_char="5323">
<ORIGINAL_TEXT>He found that all of the so-called "insertions" appear in a bat virus identified from a cave in Yunnan, China — or were artifacts of improper alignment.</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="word" morph="none" start_char="5172" end_char="5173">He</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="5175" end_char="5179">found</TOKEN>
<TOKEN id="token-33-2" pos="word" morph="none" start_char="5181" end_char="5184">that</TOKEN>
<TOKEN id="token-33-3" pos="word" morph="none" start_char="5186" end_char="5188">all</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="5190" end_char="5191">of</TOKEN>
<TOKEN id="token-33-5" pos="word" morph="none" start_char="5193" end_char="5195">the</TOKEN>
<TOKEN id="token-33-6" pos="unknown" morph="none" start_char="5197" end_char="5205">so-called</TOKEN>
<TOKEN id="token-33-7" pos="punct" morph="none" start_char="5207" end_char="5207">"</TOKEN>
<TOKEN id="token-33-8" pos="word" morph="none" start_char="5208" end_char="5217">insertions</TOKEN>
<TOKEN id="token-33-9" pos="punct" morph="none" start_char="5218" end_char="5218">"</TOKEN>
<TOKEN id="token-33-10" pos="word" morph="none" start_char="5220" end_char="5225">appear</TOKEN>
<TOKEN id="token-33-11" pos="word" morph="none" start_char="5227" end_char="5228">in</TOKEN>
<TOKEN id="token-33-12" pos="word" morph="none" start_char="5230" end_char="5230">a</TOKEN>
<TOKEN id="token-33-13" pos="word" morph="none" start_char="5232" end_char="5234">bat</TOKEN>
<TOKEN id="token-33-14" pos="word" morph="none" start_char="5236" end_char="5240">virus</TOKEN>
<TOKEN id="token-33-15" pos="word" morph="none" start_char="5242" end_char="5251">identified</TOKEN>
<TOKEN id="token-33-16" pos="word" morph="none" start_char="5253" end_char="5256">from</TOKEN>
<TOKEN id="token-33-17" pos="word" morph="none" start_char="5258" end_char="5258">a</TOKEN>
<TOKEN id="token-33-18" pos="word" morph="none" start_char="5260" end_char="5263">cave</TOKEN>
<TOKEN id="token-33-19" pos="word" morph="none" start_char="5265" end_char="5266">in</TOKEN>
<TOKEN id="token-33-20" pos="word" morph="none" start_char="5268" end_char="5273">Yunnan</TOKEN>
<TOKEN id="token-33-21" pos="punct" morph="none" start_char="5274" end_char="5274">,</TOKEN>
<TOKEN id="token-33-22" pos="word" morph="none" start_char="5276" end_char="5280">China</TOKEN>
<TOKEN id="token-33-23" pos="punct" morph="none" start_char="5282" end_char="5282">—</TOKEN>
<TOKEN id="token-33-24" pos="word" morph="none" start_char="5284" end_char="5285">or</TOKEN>
<TOKEN id="token-33-25" pos="word" morph="none" start_char="5287" end_char="5290">were</TOKEN>
<TOKEN id="token-33-26" pos="word" morph="none" start_char="5292" end_char="5300">artifacts</TOKEN>
<TOKEN id="token-33-27" pos="word" morph="none" start_char="5302" end_char="5303">of</TOKEN>
<TOKEN id="token-33-28" pos="word" morph="none" start_char="5305" end_char="5312">improper</TOKEN>
<TOKEN id="token-33-29" pos="word" morph="none" start_char="5314" end_char="5322">alignment</TOKEN>
<TOKEN id="token-33-30" pos="punct" morph="none" start_char="5323" end_char="5323">.</TOKEN>
</SEG>
<SEG id="segment-34" start_char="5326" end_char="5537">
<ORIGINAL_TEXT>Only one "insertion" is not fully shared with the bat virus, Bedford explained, and "in no way suggests engineering" since it is consistent with the types of insertions and deletions that happen in coronaviruses.</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="5326" end_char="5329">Only</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="5331" end_char="5333">one</TOKEN>
<TOKEN id="token-34-2" pos="punct" morph="none" start_char="5335" end_char="5335">"</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="5336" end_char="5344">insertion</TOKEN>
<TOKEN id="token-34-4" pos="punct" morph="none" start_char="5345" end_char="5345">"</TOKEN>
<TOKEN id="token-34-5" pos="word" morph="none" start_char="5347" end_char="5348">is</TOKEN>
<TOKEN id="token-34-6" pos="word" morph="none" start_char="5350" end_char="5352">not</TOKEN>
<TOKEN id="token-34-7" pos="word" morph="none" start_char="5354" end_char="5358">fully</TOKEN>
<TOKEN id="token-34-8" pos="word" morph="none" start_char="5360" end_char="5365">shared</TOKEN>
<TOKEN id="token-34-9" pos="word" morph="none" start_char="5367" end_char="5370">with</TOKEN>
<TOKEN id="token-34-10" pos="word" morph="none" start_char="5372" end_char="5374">the</TOKEN>
<TOKEN id="token-34-11" pos="word" morph="none" start_char="5376" end_char="5378">bat</TOKEN>
<TOKEN id="token-34-12" pos="word" morph="none" start_char="5380" end_char="5384">virus</TOKEN>
<TOKEN id="token-34-13" pos="punct" morph="none" start_char="5385" end_char="5385">,</TOKEN>
<TOKEN id="token-34-14" pos="word" morph="none" start_char="5387" end_char="5393">Bedford</TOKEN>
<TOKEN id="token-34-15" pos="word" morph="none" start_char="5395" end_char="5403">explained</TOKEN>
<TOKEN id="token-34-16" pos="punct" morph="none" start_char="5404" end_char="5404">,</TOKEN>
<TOKEN id="token-34-17" pos="word" morph="none" start_char="5406" end_char="5408">and</TOKEN>
<TOKEN id="token-34-18" pos="punct" morph="none" start_char="5410" end_char="5410">"</TOKEN>
<TOKEN id="token-34-19" pos="word" morph="none" start_char="5411" end_char="5412">in</TOKEN>
<TOKEN id="token-34-20" pos="word" morph="none" start_char="5414" end_char="5415">no</TOKEN>
<TOKEN id="token-34-21" pos="word" morph="none" start_char="5417" end_char="5419">way</TOKEN>
<TOKEN id="token-34-22" pos="word" morph="none" start_char="5421" end_char="5428">suggests</TOKEN>
<TOKEN id="token-34-23" pos="word" morph="none" start_char="5430" end_char="5440">engineering</TOKEN>
<TOKEN id="token-34-24" pos="punct" morph="none" start_char="5441" end_char="5441">"</TOKEN>
<TOKEN id="token-34-25" pos="word" morph="none" start_char="5443" end_char="5447">since</TOKEN>
<TOKEN id="token-34-26" pos="word" morph="none" start_char="5449" end_char="5450">it</TOKEN>
<TOKEN id="token-34-27" pos="word" morph="none" start_char="5452" end_char="5453">is</TOKEN>
<TOKEN id="token-34-28" pos="word" morph="none" start_char="5455" end_char="5464">consistent</TOKEN>
<TOKEN id="token-34-29" pos="word" morph="none" start_char="5466" end_char="5469">with</TOKEN>
<TOKEN id="token-34-30" pos="word" morph="none" start_char="5471" end_char="5473">the</TOKEN>
<TOKEN id="token-34-31" pos="word" morph="none" start_char="5475" end_char="5479">types</TOKEN>
<TOKEN id="token-34-32" pos="word" morph="none" start_char="5481" end_char="5482">of</TOKEN>
<TOKEN id="token-34-33" pos="word" morph="none" start_char="5484" end_char="5493">insertions</TOKEN>
<TOKEN id="token-34-34" pos="word" morph="none" start_char="5495" end_char="5497">and</TOKEN>
<TOKEN id="token-34-35" pos="word" morph="none" start_char="5499" end_char="5507">deletions</TOKEN>
<TOKEN id="token-34-36" pos="word" morph="none" start_char="5509" end_char="5512">that</TOKEN>
<TOKEN id="token-34-37" pos="word" morph="none" start_char="5514" end_char="5519">happen</TOKEN>
<TOKEN id="token-34-38" pos="word" morph="none" start_char="5521" end_char="5522">in</TOKEN>
<TOKEN id="token-34-39" pos="word" morph="none" start_char="5524" end_char="5536">coronaviruses</TOKEN>
<TOKEN id="token-34-40" pos="punct" morph="none" start_char="5537" end_char="5537">.</TOKEN>
</SEG>
<SEG id="segment-35" start_char="5539" end_char="5654">
<ORIGINAL_TEXT>"There is absolutely no evidence for either (1) sequence insertions or (2) their relationship to HIV," he concluded.</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="punct" morph="none" start_char="5539" end_char="5539">"</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="5540" end_char="5544">There</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="5546" end_char="5547">is</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="5549" end_char="5558">absolutely</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="5560" end_char="5561">no</TOKEN>
<TOKEN id="token-35-5" pos="word" morph="none" start_char="5563" end_char="5570">evidence</TOKEN>
<TOKEN id="token-35-6" pos="word" morph="none" start_char="5572" end_char="5574">for</TOKEN>
<TOKEN id="token-35-7" pos="word" morph="none" start_char="5576" end_char="5581">either</TOKEN>
<TOKEN id="token-35-8" pos="punct" morph="none" start_char="5583" end_char="5583">(</TOKEN>
<TOKEN id="token-35-9" pos="word" morph="none" start_char="5584" end_char="5584">1</TOKEN>
<TOKEN id="token-35-10" pos="punct" morph="none" start_char="5585" end_char="5585">)</TOKEN>
<TOKEN id="token-35-11" pos="word" morph="none" start_char="5587" end_char="5594">sequence</TOKEN>
<TOKEN id="token-35-12" pos="word" morph="none" start_char="5596" end_char="5605">insertions</TOKEN>
<TOKEN id="token-35-13" pos="word" morph="none" start_char="5607" end_char="5608">or</TOKEN>
<TOKEN id="token-35-14" pos="punct" morph="none" start_char="5610" end_char="5610">(</TOKEN>
<TOKEN id="token-35-15" pos="word" morph="none" start_char="5611" end_char="5611">2</TOKEN>
<TOKEN id="token-35-16" pos="punct" morph="none" start_char="5612" end_char="5612">)</TOKEN>
<TOKEN id="token-35-17" pos="word" morph="none" start_char="5614" end_char="5618">their</TOKEN>
<TOKEN id="token-35-18" pos="word" morph="none" start_char="5620" end_char="5631">relationship</TOKEN>
<TOKEN id="token-35-19" pos="word" morph="none" start_char="5633" end_char="5634">to</TOKEN>
<TOKEN id="token-35-20" pos="word" morph="none" start_char="5636" end_char="5638">HIV</TOKEN>
<TOKEN id="token-35-21" pos="punct" morph="none" start_char="5639" end_char="5640">,"</TOKEN>
<TOKEN id="token-35-22" pos="word" morph="none" start_char="5642" end_char="5643">he</TOKEN>
<TOKEN id="token-35-23" pos="word" morph="none" start_char="5645" end_char="5653">concluded</TOKEN>
<TOKEN id="token-35-24" pos="punct" morph="none" start_char="5654" end_char="5654">.</TOKEN>
</SEG>
<SEG id="segment-36" start_char="5657" end_char="5811">
<ORIGINAL_TEXT>The blogger’s contention that the new coronavirus may have been engineered using a SARS viral vector, Andersen said, is "just as absurd" as the HIV theory.</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="5657" end_char="5659">The</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="5661" end_char="5669">blogger’s</TOKEN>
<TOKEN id="token-36-2" pos="word" morph="none" start_char="5671" end_char="5680">contention</TOKEN>
<TOKEN id="token-36-3" pos="word" morph="none" start_char="5682" end_char="5685">that</TOKEN>
<TOKEN id="token-36-4" pos="word" morph="none" start_char="5687" end_char="5689">the</TOKEN>
<TOKEN id="token-36-5" pos="word" morph="none" start_char="5691" end_char="5693">new</TOKEN>
<TOKEN id="token-36-6" pos="word" morph="none" start_char="5695" end_char="5705">coronavirus</TOKEN>
<TOKEN id="token-36-7" pos="word" morph="none" start_char="5707" end_char="5709">may</TOKEN>
<TOKEN id="token-36-8" pos="word" morph="none" start_char="5711" end_char="5714">have</TOKEN>
<TOKEN id="token-36-9" pos="word" morph="none" start_char="5716" end_char="5719">been</TOKEN>
<TOKEN id="token-36-10" pos="word" morph="none" start_char="5721" end_char="5730">engineered</TOKEN>
<TOKEN id="token-36-11" pos="word" morph="none" start_char="5732" end_char="5736">using</TOKEN>
<TOKEN id="token-36-12" pos="word" morph="none" start_char="5738" end_char="5738">a</TOKEN>
<TOKEN id="token-36-13" pos="word" morph="none" start_char="5740" end_char="5743">SARS</TOKEN>
<TOKEN id="token-36-14" pos="word" morph="none" start_char="5745" end_char="5749">viral</TOKEN>
<TOKEN id="token-36-15" pos="word" morph="none" start_char="5751" end_char="5756">vector</TOKEN>
<TOKEN id="token-36-16" pos="punct" morph="none" start_char="5757" end_char="5757">,</TOKEN>
<TOKEN id="token-36-17" pos="word" morph="none" start_char="5759" end_char="5766">Andersen</TOKEN>
<TOKEN id="token-36-18" pos="word" morph="none" start_char="5768" end_char="5771">said</TOKEN>
<TOKEN id="token-36-19" pos="punct" morph="none" start_char="5772" end_char="5772">,</TOKEN>
<TOKEN id="token-36-20" pos="word" morph="none" start_char="5774" end_char="5775">is</TOKEN>
<TOKEN id="token-36-21" pos="punct" morph="none" start_char="5777" end_char="5777">"</TOKEN>
<TOKEN id="token-36-22" pos="word" morph="none" start_char="5778" end_char="5781">just</TOKEN>
<TOKEN id="token-36-23" pos="word" morph="none" start_char="5783" end_char="5784">as</TOKEN>
<TOKEN id="token-36-24" pos="word" morph="none" start_char="5786" end_char="5791">absurd</TOKEN>
<TOKEN id="token-36-25" pos="punct" morph="none" start_char="5792" end_char="5792">"</TOKEN>
<TOKEN id="token-36-26" pos="word" morph="none" start_char="5794" end_char="5795">as</TOKEN>
<TOKEN id="token-36-27" pos="word" morph="none" start_char="5797" end_char="5799">the</TOKEN>
<TOKEN id="token-36-28" pos="word" morph="none" start_char="5801" end_char="5803">HIV</TOKEN>
<TOKEN id="token-36-29" pos="word" morph="none" start_char="5805" end_char="5810">theory</TOKEN>
<TOKEN id="token-36-30" pos="punct" morph="none" start_char="5811" end_char="5811">.</TOKEN>
</SEG>
<SEG id="segment-37" start_char="5813" end_char="5924">
<ORIGINAL_TEXT>The vector, he said, was used to understand coronaviruses and develop vaccines — but is different from 2019nCoV.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="5813" end_char="5815">The</TOKEN>
<TOKEN id="token-37-1" pos="word" morph="none" start_char="5817" end_char="5822">vector</TOKEN>
<TOKEN id="token-37-2" pos="punct" morph="none" start_char="5823" end_char="5823">,</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="5825" end_char="5826">he</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="5828" end_char="5831">said</TOKEN>
<TOKEN id="token-37-5" pos="punct" morph="none" start_char="5832" end_char="5832">,</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="5834" end_char="5836">was</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="5838" end_char="5841">used</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="5843" end_char="5844">to</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="5846" end_char="5855">understand</TOKEN>
<TOKEN id="token-37-10" pos="word" morph="none" start_char="5857" end_char="5869">coronaviruses</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="5871" end_char="5873">and</TOKEN>
<TOKEN id="token-37-12" pos="word" morph="none" start_char="5875" end_char="5881">develop</TOKEN>
<TOKEN id="token-37-13" pos="word" morph="none" start_char="5883" end_char="5890">vaccines</TOKEN>
<TOKEN id="token-37-14" pos="punct" morph="none" start_char="5892" end_char="5892">—</TOKEN>
<TOKEN id="token-37-15" pos="word" morph="none" start_char="5894" end_char="5896">but</TOKEN>
<TOKEN id="token-37-16" pos="word" morph="none" start_char="5898" end_char="5899">is</TOKEN>
<TOKEN id="token-37-17" pos="word" morph="none" start_char="5901" end_char="5909">different</TOKEN>
<TOKEN id="token-37-18" pos="word" morph="none" start_char="5911" end_char="5914">from</TOKEN>
<TOKEN id="token-37-19" pos="word" morph="none" start_char="5916" end_char="5923">2019nCoV</TOKEN>
<TOKEN id="token-37-20" pos="punct" morph="none" start_char="5924" end_char="5924">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="5927" end_char="6054">
<ORIGINAL_TEXT>"While they’re similar (like worms and people are similar) there is absolutely no way that nCoV is in any way related," he said.</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="punct" morph="none" start_char="5927" end_char="5927">"</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="5928" end_char="5932">While</TOKEN>
<TOKEN id="token-38-2" pos="word" morph="none" start_char="5934" end_char="5940">they’re</TOKEN>
<TOKEN id="token-38-3" pos="word" morph="none" start_char="5942" end_char="5948">similar</TOKEN>
<TOKEN id="token-38-4" pos="punct" morph="none" start_char="5950" end_char="5950">(</TOKEN>
<TOKEN id="token-38-5" pos="word" morph="none" start_char="5951" end_char="5954">like</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="5956" end_char="5960">worms</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="5962" end_char="5964">and</TOKEN>
<TOKEN id="token-38-8" pos="word" morph="none" start_char="5966" end_char="5971">people</TOKEN>
<TOKEN id="token-38-9" pos="word" morph="none" start_char="5973" end_char="5975">are</TOKEN>
<TOKEN id="token-38-10" pos="word" morph="none" start_char="5977" end_char="5983">similar</TOKEN>
<TOKEN id="token-38-11" pos="punct" morph="none" start_char="5984" end_char="5984">)</TOKEN>
<TOKEN id="token-38-12" pos="word" morph="none" start_char="5986" end_char="5990">there</TOKEN>
<TOKEN id="token-38-13" pos="word" morph="none" start_char="5992" end_char="5993">is</TOKEN>
<TOKEN id="token-38-14" pos="word" morph="none" start_char="5995" end_char="6004">absolutely</TOKEN>
<TOKEN id="token-38-15" pos="word" morph="none" start_char="6006" end_char="6007">no</TOKEN>
<TOKEN id="token-38-16" pos="word" morph="none" start_char="6009" end_char="6011">way</TOKEN>
<TOKEN id="token-38-17" pos="word" morph="none" start_char="6013" end_char="6016">that</TOKEN>
<TOKEN id="token-38-18" pos="word" morph="none" start_char="6018" end_char="6021">nCoV</TOKEN>
<TOKEN id="token-38-19" pos="word" morph="none" start_char="6023" end_char="6024">is</TOKEN>
<TOKEN id="token-38-20" pos="word" morph="none" start_char="6026" end_char="6027">in</TOKEN>
<TOKEN id="token-38-21" pos="word" morph="none" start_char="6029" end_char="6031">any</TOKEN>
<TOKEN id="token-38-22" pos="word" morph="none" start_char="6033" end_char="6035">way</TOKEN>
<TOKEN id="token-38-23" pos="word" morph="none" start_char="6037" end_char="6043">related</TOKEN>
<TOKEN id="token-38-24" pos="punct" morph="none" start_char="6044" end_char="6045">,"</TOKEN>
<TOKEN id="token-38-25" pos="word" morph="none" start_char="6047" end_char="6048">he</TOKEN>
<TOKEN id="token-38-26" pos="word" morph="none" start_char="6050" end_char="6053">said</TOKEN>
<TOKEN id="token-38-27" pos="punct" morph="none" start_char="6054" end_char="6054">.</TOKEN>
</SEG>
<SEG id="segment-39" start_char="6056" end_char="6208">
<ORIGINAL_TEXT>"If one were to look at the two genomes side by side, it’s very easy to show that they’re obviously not the same — or that one somehow led to the other."</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="punct" morph="none" start_char="6056" end_char="6056">"</TOKEN>
<TOKEN id="token-39-1" pos="word" morph="none" start_char="6057" end_char="6058">If</TOKEN>
<TOKEN id="token-39-2" pos="word" morph="none" start_char="6060" end_char="6062">one</TOKEN>
<TOKEN id="token-39-3" pos="word" morph="none" start_char="6064" end_char="6067">were</TOKEN>
<TOKEN id="token-39-4" pos="word" morph="none" start_char="6069" end_char="6070">to</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="6072" end_char="6075">look</TOKEN>
<TOKEN id="token-39-6" pos="word" morph="none" start_char="6077" end_char="6078">at</TOKEN>
<TOKEN id="token-39-7" pos="word" morph="none" start_char="6080" end_char="6082">the</TOKEN>
<TOKEN id="token-39-8" pos="word" morph="none" start_char="6084" end_char="6086">two</TOKEN>
<TOKEN id="token-39-9" pos="word" morph="none" start_char="6088" end_char="6094">genomes</TOKEN>
<TOKEN id="token-39-10" pos="word" morph="none" start_char="6096" end_char="6099">side</TOKEN>
<TOKEN id="token-39-11" pos="word" morph="none" start_char="6101" end_char="6102">by</TOKEN>
<TOKEN id="token-39-12" pos="word" morph="none" start_char="6104" end_char="6107">side</TOKEN>
<TOKEN id="token-39-13" pos="punct" morph="none" start_char="6108" end_char="6108">,</TOKEN>
<TOKEN id="token-39-14" pos="word" morph="none" start_char="6110" end_char="6113">it’s</TOKEN>
<TOKEN id="token-39-15" pos="word" morph="none" start_char="6115" end_char="6118">very</TOKEN>
<TOKEN id="token-39-16" pos="word" morph="none" start_char="6120" end_char="6123">easy</TOKEN>
<TOKEN id="token-39-17" pos="word" morph="none" start_char="6125" end_char="6126">to</TOKEN>
<TOKEN id="token-39-18" pos="word" morph="none" start_char="6128" end_char="6131">show</TOKEN>
<TOKEN id="token-39-19" pos="word" morph="none" start_char="6133" end_char="6136">that</TOKEN>
<TOKEN id="token-39-20" pos="word" morph="none" start_char="6138" end_char="6144">they’re</TOKEN>
<TOKEN id="token-39-21" pos="word" morph="none" start_char="6146" end_char="6154">obviously</TOKEN>
<TOKEN id="token-39-22" pos="word" morph="none" start_char="6156" end_char="6158">not</TOKEN>
<TOKEN id="token-39-23" pos="word" morph="none" start_char="6160" end_char="6162">the</TOKEN>
<TOKEN id="token-39-24" pos="word" morph="none" start_char="6164" end_char="6167">same</TOKEN>
<TOKEN id="token-39-25" pos="punct" morph="none" start_char="6169" end_char="6169">—</TOKEN>
<TOKEN id="token-39-26" pos="word" morph="none" start_char="6171" end_char="6172">or</TOKEN>
<TOKEN id="token-39-27" pos="word" morph="none" start_char="6174" end_char="6177">that</TOKEN>
<TOKEN id="token-39-28" pos="word" morph="none" start_char="6179" end_char="6181">one</TOKEN>
<TOKEN id="token-39-29" pos="word" morph="none" start_char="6183" end_char="6189">somehow</TOKEN>
<TOKEN id="token-39-30" pos="word" morph="none" start_char="6191" end_char="6193">led</TOKEN>
<TOKEN id="token-39-31" pos="word" morph="none" start_char="6195" end_char="6196">to</TOKEN>
<TOKEN id="token-39-32" pos="word" morph="none" start_char="6198" end_char="6200">the</TOKEN>
<TOKEN id="token-39-33" pos="word" morph="none" start_char="6202" end_char="6206">other</TOKEN>
<TOKEN id="token-39-34" pos="punct" morph="none" start_char="6207" end_char="6208">."</TOKEN>
</SEG>
<SEG id="segment-40" start_char="6211" end_char="6219">
<ORIGINAL_TEXT>HIV Drugs</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="word" morph="none" start_char="6211" end_char="6213">HIV</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="6215" end_char="6219">Drugs</TOKEN>
</SEG>
<SEG id="segment-41" start_char="6223" end_char="6473">
<ORIGINAL_TEXT>As we’ve just established, there’s no connection between HIV and the new coronavirus, but the fact that some countries are using HIV drugs to treat the new coronavirus is included in many of the social media posts to lend credence to the bogus theory.</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="6223" end_char="6224">As</TOKEN>
<TOKEN id="token-41-1" pos="word" morph="none" start_char="6226" end_char="6230">we’ve</TOKEN>
<TOKEN id="token-41-2" pos="word" morph="none" start_char="6232" end_char="6235">just</TOKEN>
<TOKEN id="token-41-3" pos="word" morph="none" start_char="6237" end_char="6247">established</TOKEN>
<TOKEN id="token-41-4" pos="punct" morph="none" start_char="6248" end_char="6248">,</TOKEN>
<TOKEN id="token-41-5" pos="word" morph="none" start_char="6250" end_char="6256">there’s</TOKEN>
<TOKEN id="token-41-6" pos="word" morph="none" start_char="6258" end_char="6259">no</TOKEN>
<TOKEN id="token-41-7" pos="word" morph="none" start_char="6261" end_char="6270">connection</TOKEN>
<TOKEN id="token-41-8" pos="word" morph="none" start_char="6272" end_char="6278">between</TOKEN>
<TOKEN id="token-41-9" pos="word" morph="none" start_char="6280" end_char="6282">HIV</TOKEN>
<TOKEN id="token-41-10" pos="word" morph="none" start_char="6284" end_char="6286">and</TOKEN>
<TOKEN id="token-41-11" pos="word" morph="none" start_char="6288" end_char="6290">the</TOKEN>
<TOKEN id="token-41-12" pos="word" morph="none" start_char="6292" end_char="6294">new</TOKEN>
<TOKEN id="token-41-13" pos="word" morph="none" start_char="6296" end_char="6306">coronavirus</TOKEN>
<TOKEN id="token-41-14" pos="punct" morph="none" start_char="6307" end_char="6307">,</TOKEN>
<TOKEN id="token-41-15" pos="word" morph="none" start_char="6309" end_char="6311">but</TOKEN>
<TOKEN id="token-41-16" pos="word" morph="none" start_char="6313" end_char="6315">the</TOKEN>
<TOKEN id="token-41-17" pos="word" morph="none" start_char="6317" end_char="6320">fact</TOKEN>
<TOKEN id="token-41-18" pos="word" morph="none" start_char="6322" end_char="6325">that</TOKEN>
<TOKEN id="token-41-19" pos="word" morph="none" start_char="6327" end_char="6330">some</TOKEN>
<TOKEN id="token-41-20" pos="word" morph="none" start_char="6332" end_char="6340">countries</TOKEN>
<TOKEN id="token-41-21" pos="word" morph="none" start_char="6342" end_char="6344">are</TOKEN>
<TOKEN id="token-41-22" pos="word" morph="none" start_char="6346" end_char="6350">using</TOKEN>
<TOKEN id="token-41-23" pos="word" morph="none" start_char="6352" end_char="6354">HIV</TOKEN>
<TOKEN id="token-41-24" pos="word" morph="none" start_char="6356" end_char="6360">drugs</TOKEN>
<TOKEN id="token-41-25" pos="word" morph="none" start_char="6362" end_char="6363">to</TOKEN>
<TOKEN id="token-41-26" pos="word" morph="none" start_char="6365" end_char="6369">treat</TOKEN>
<TOKEN id="token-41-27" pos="word" morph="none" start_char="6371" end_char="6373">the</TOKEN>
<TOKEN id="token-41-28" pos="word" morph="none" start_char="6375" end_char="6377">new</TOKEN>
<TOKEN id="token-41-29" pos="word" morph="none" start_char="6379" end_char="6389">coronavirus</TOKEN>
<TOKEN id="token-41-30" pos="word" morph="none" start_char="6391" end_char="6392">is</TOKEN>
<TOKEN id="token-41-31" pos="word" morph="none" start_char="6394" end_char="6401">included</TOKEN>
<TOKEN id="token-41-32" pos="word" morph="none" start_char="6403" end_char="6404">in</TOKEN>
<TOKEN id="token-41-33" pos="word" morph="none" start_char="6406" end_char="6409">many</TOKEN>
<TOKEN id="token-41-34" pos="word" morph="none" start_char="6411" end_char="6412">of</TOKEN>
<TOKEN id="token-41-35" pos="word" morph="none" start_char="6414" end_char="6416">the</TOKEN>
<TOKEN id="token-41-36" pos="word" morph="none" start_char="6418" end_char="6423">social</TOKEN>
<TOKEN id="token-41-37" pos="word" morph="none" start_char="6425" end_char="6429">media</TOKEN>
<TOKEN id="token-41-38" pos="word" morph="none" start_char="6431" end_char="6435">posts</TOKEN>
<TOKEN id="token-41-39" pos="word" morph="none" start_char="6437" end_char="6438">to</TOKEN>
<TOKEN id="token-41-40" pos="word" morph="none" start_char="6440" end_char="6443">lend</TOKEN>
<TOKEN id="token-41-41" pos="word" morph="none" start_char="6445" end_char="6452">credence</TOKEN>
<TOKEN id="token-41-42" pos="word" morph="none" start_char="6454" end_char="6455">to</TOKEN>
<TOKEN id="token-41-43" pos="word" morph="none" start_char="6457" end_char="6459">the</TOKEN>
<TOKEN id="token-41-44" pos="word" morph="none" start_char="6461" end_char="6465">bogus</TOKEN>
<TOKEN id="token-41-45" pos="word" morph="none" start_char="6467" end_char="6472">theory</TOKEN>
<TOKEN id="token-41-46" pos="punct" morph="none" start_char="6473" end_char="6473">.</TOKEN>
</SEG>
<SEG id="segment-42" start_char="6476" end_char="6666">
<ORIGINAL_TEXT>One Facebook post says, "Ask yourself why they have been treating with HIV drugs from the start," and the ZeroHedge story proclaims, "The virus even responds to treatment by HIV medications."</ORIGINAL_TEXT>
<TOKEN id="token-42-0" pos="word" morph="none" start_char="6476" end_char="6478">One</TOKEN>
<TOKEN id="token-42-1" pos="word" morph="none" start_char="6480" end_char="6487">Facebook</TOKEN>
<TOKEN id="token-42-2" pos="word" morph="none" start_char="6489" end_char="6492">post</TOKEN>
<TOKEN id="token-42-3" pos="word" morph="none" start_char="6494" end_char="6497">says</TOKEN>
<TOKEN id="token-42-4" pos="punct" morph="none" start_char="6498" end_char="6498">,</TOKEN>
<TOKEN id="token-42-5" pos="punct" morph="none" start_char="6500" end_char="6500">"</TOKEN>
<TOKEN id="token-42-6" pos="word" morph="none" start_char="6501" end_char="6503">Ask</TOKEN>
<TOKEN id="token-42-7" pos="word" morph="none" start_char="6505" end_char="6512">yourself</TOKEN>
<TOKEN id="token-42-8" pos="word" morph="none" start_char="6514" end_char="6516">why</TOKEN>
<TOKEN id="token-42-9" pos="word" morph="none" start_char="6518" end_char="6521">they</TOKEN>
<TOKEN id="token-42-10" pos="word" morph="none" start_char="6523" end_char="6526">have</TOKEN>
<TOKEN id="token-42-11" pos="word" morph="none" start_char="6528" end_char="6531">been</TOKEN>
<TOKEN id="token-42-12" pos="word" morph="none" start_char="6533" end_char="6540">treating</TOKEN>
<TOKEN id="token-42-13" pos="word" morph="none" start_char="6542" end_char="6545">with</TOKEN>
<TOKEN id="token-42-14" pos="word" morph="none" start_char="6547" end_char="6549">HIV</TOKEN>
<TOKEN id="token-42-15" pos="word" morph="none" start_char="6551" end_char="6555">drugs</TOKEN>
<TOKEN id="token-42-16" pos="word" morph="none" start_char="6557" end_char="6560">from</TOKEN>
<TOKEN id="token-42-17" pos="word" morph="none" start_char="6562" end_char="6564">the</TOKEN>
<TOKEN id="token-42-18" pos="word" morph="none" start_char="6566" end_char="6570">start</TOKEN>
<TOKEN id="token-42-19" pos="punct" morph="none" start_char="6571" end_char="6572">,"</TOKEN>
<TOKEN id="token-42-20" pos="word" morph="none" start_char="6574" end_char="6576">and</TOKEN>
<TOKEN id="token-42-21" pos="word" morph="none" start_char="6578" end_char="6580">the</TOKEN>
<TOKEN id="token-42-22" pos="word" morph="none" start_char="6582" end_char="6590">ZeroHedge</TOKEN>
<TOKEN id="token-42-23" pos="word" morph="none" start_char="6592" end_char="6596">story</TOKEN>
<TOKEN id="token-42-24" pos="word" morph="none" start_char="6598" end_char="6606">proclaims</TOKEN>
<TOKEN id="token-42-25" pos="punct" morph="none" start_char="6607" end_char="6607">,</TOKEN>
<TOKEN id="token-42-26" pos="punct" morph="none" start_char="6609" end_char="6609">"</TOKEN>
<TOKEN id="token-42-27" pos="word" morph="none" start_char="6610" end_char="6612">The</TOKEN>
<TOKEN id="token-42-28" pos="word" morph="none" start_char="6614" end_char="6618">virus</TOKEN>
<TOKEN id="token-42-29" pos="word" morph="none" start_char="6620" end_char="6623">even</TOKEN>
<TOKEN id="token-42-30" pos="word" morph="none" start_char="6625" end_char="6632">responds</TOKEN>
<TOKEN id="token-42-31" pos="word" morph="none" start_char="6634" end_char="6635">to</TOKEN>
<TOKEN id="token-42-32" pos="word" morph="none" start_char="6637" end_char="6645">treatment</TOKEN>
<TOKEN id="token-42-33" pos="word" morph="none" start_char="6647" end_char="6648">by</TOKEN>
<TOKEN id="token-42-34" pos="word" morph="none" start_char="6650" end_char="6652">HIV</TOKEN>
<TOKEN id="token-42-35" pos="word" morph="none" start_char="6654" end_char="6664">medications</TOKEN>
<TOKEN id="token-42-36" pos="punct" morph="none" start_char="6665" end_char="6666">."</TOKEN>
</SEG>
<SEG id="segment-43" start_char="6669" end_char="6782">
<ORIGINAL_TEXT>In fact, it’s not yet clear if the virus does respond to HIV drugs — but the rationale to try it is pretty simple.</ORIGINAL_TEXT>
<TOKEN id="token-43-0" pos="word" morph="none" start_char="6669" end_char="6670">In</TOKEN>
<TOKEN id="token-43-1" pos="word" morph="none" start_char="6672" end_char="6675">fact</TOKEN>
<TOKEN id="token-43-2" pos="punct" morph="none" start_char="6676" end_char="6676">,</TOKEN>
<TOKEN id="token-43-3" pos="word" morph="none" start_char="6678" end_char="6681">it’s</TOKEN>
<TOKEN id="token-43-4" pos="word" morph="none" start_char="6683" end_char="6685">not</TOKEN>
<TOKEN id="token-43-5" pos="word" morph="none" start_char="6687" end_char="6689">yet</TOKEN>
<TOKEN id="token-43-6" pos="word" morph="none" start_char="6691" end_char="6695">clear</TOKEN>
<TOKEN id="token-43-7" pos="word" morph="none" start_char="6697" end_char="6698">if</TOKEN>
<TOKEN id="token-43-8" pos="word" morph="none" start_char="6700" end_char="6702">the</TOKEN>
<TOKEN id="token-43-9" pos="word" morph="none" start_char="6704" end_char="6708">virus</TOKEN>
<TOKEN id="token-43-10" pos="word" morph="none" start_char="6710" end_char="6713">does</TOKEN>
<TOKEN id="token-43-11" pos="word" morph="none" start_char="6715" end_char="6721">respond</TOKEN>
<TOKEN id="token-43-12" pos="word" morph="none" start_char="6723" end_char="6724">to</TOKEN>
<TOKEN id="token-43-13" pos="word" morph="none" start_char="6726" end_char="6728">HIV</TOKEN>
<TOKEN id="token-43-14" pos="word" morph="none" start_char="6730" end_char="6734">drugs</TOKEN>
<TOKEN id="token-43-15" pos="punct" morph="none" start_char="6736" end_char="6736">—</TOKEN>
<TOKEN id="token-43-16" pos="word" morph="none" start_char="6738" end_char="6740">but</TOKEN>
<TOKEN id="token-43-17" pos="word" morph="none" start_char="6742" end_char="6744">the</TOKEN>
<TOKEN id="token-43-18" pos="word" morph="none" start_char="6746" end_char="6754">rationale</TOKEN>
<TOKEN id="token-43-19" pos="word" morph="none" start_char="6756" end_char="6757">to</TOKEN>
<TOKEN id="token-43-20" pos="word" morph="none" start_char="6759" end_char="6761">try</TOKEN>
<TOKEN id="token-43-21" pos="word" morph="none" start_char="6763" end_char="6764">it</TOKEN>
<TOKEN id="token-43-22" pos="word" morph="none" start_char="6766" end_char="6767">is</TOKEN>
<TOKEN id="token-43-23" pos="word" morph="none" start_char="6769" end_char="6774">pretty</TOKEN>
<TOKEN id="token-43-24" pos="word" morph="none" start_char="6776" end_char="6781">simple</TOKEN>
<TOKEN id="token-43-25" pos="punct" morph="none" start_char="6782" end_char="6782">.</TOKEN>
</SEG>
<SEG id="segment-44" start_char="6784" end_char="7041">
<ORIGINAL_TEXT>Timothy Sheahan, a virologist at the University of North Carolina at Chapel Hill, told us in a phone interview that there aren’t that many FDA-approved antiviral drugs, so when a new virus emerges, doctors just give patients "whatever they think might help."</ORIGINAL_TEXT>
<TOKEN id="token-44-0" pos="word" morph="none" start_char="6784" end_char="6790">Timothy</TOKEN>
<TOKEN id="token-44-1" pos="word" morph="none" start_char="6792" end_char="6798">Sheahan</TOKEN>
<TOKEN id="token-44-2" pos="punct" morph="none" start_char="6799" end_char="6799">,</TOKEN>
<TOKEN id="token-44-3" pos="word" morph="none" start_char="6801" end_char="6801">a</TOKEN>
<TOKEN id="token-44-4" pos="word" morph="none" start_char="6803" end_char="6812">virologist</TOKEN>
<TOKEN id="token-44-5" pos="word" morph="none" start_char="6814" end_char="6815">at</TOKEN>
<TOKEN id="token-44-6" pos="word" morph="none" start_char="6817" end_char="6819">the</TOKEN>
<TOKEN id="token-44-7" pos="word" morph="none" start_char="6821" end_char="6830">University</TOKEN>
<TOKEN id="token-44-8" pos="word" morph="none" start_char="6832" end_char="6833">of</TOKEN>
<TOKEN id="token-44-9" pos="word" morph="none" start_char="6835" end_char="6839">North</TOKEN>
<TOKEN id="token-44-10" pos="word" morph="none" start_char="6841" end_char="6848">Carolina</TOKEN>
<TOKEN id="token-44-11" pos="word" morph="none" start_char="6850" end_char="6851">at</TOKEN>
<TOKEN id="token-44-12" pos="word" morph="none" start_char="6853" end_char="6858">Chapel</TOKEN>
<TOKEN id="token-44-13" pos="word" morph="none" start_char="6860" end_char="6863">Hill</TOKEN>
<TOKEN id="token-44-14" pos="punct" morph="none" start_char="6864" end_char="6864">,</TOKEN>
<TOKEN id="token-44-15" pos="word" morph="none" start_char="6866" end_char="6869">told</TOKEN>
<TOKEN id="token-44-16" pos="word" morph="none" start_char="6871" end_char="6872">us</TOKEN>
<TOKEN id="token-44-17" pos="word" morph="none" start_char="6874" end_char="6875">in</TOKEN>
<TOKEN id="token-44-18" pos="word" morph="none" start_char="6877" end_char="6877">a</TOKEN>
<TOKEN id="token-44-19" pos="word" morph="none" start_char="6879" end_char="6883">phone</TOKEN>
<TOKEN id="token-44-20" pos="word" morph="none" start_char="6885" end_char="6893">interview</TOKEN>
<TOKEN id="token-44-21" pos="word" morph="none" start_char="6895" end_char="6898">that</TOKEN>
<TOKEN id="token-44-22" pos="word" morph="none" start_char="6900" end_char="6904">there</TOKEN>
<TOKEN id="token-44-23" pos="word" morph="none" start_char="6906" end_char="6911">aren’t</TOKEN>
<TOKEN id="token-44-24" pos="word" morph="none" start_char="6913" end_char="6916">that</TOKEN>
<TOKEN id="token-44-25" pos="word" morph="none" start_char="6918" end_char="6921">many</TOKEN>
<TOKEN id="token-44-26" pos="unknown" morph="none" start_char="6923" end_char="6934">FDA-approved</TOKEN>
<TOKEN id="token-44-27" pos="word" morph="none" start_char="6936" end_char="6944">antiviral</TOKEN>
<TOKEN id="token-44-28" pos="word" morph="none" start_char="6946" end_char="6950">drugs</TOKEN>
<TOKEN id="token-44-29" pos="punct" morph="none" start_char="6951" end_char="6951">,</TOKEN>
<TOKEN id="token-44-30" pos="word" morph="none" start_char="6953" end_char="6954">so</TOKEN>
<TOKEN id="token-44-31" pos="word" morph="none" start_char="6956" end_char="6959">when</TOKEN>
<TOKEN id="token-44-32" pos="word" morph="none" start_char="6961" end_char="6961">a</TOKEN>
<TOKEN id="token-44-33" pos="word" morph="none" start_char="6963" end_char="6965">new</TOKEN>
<TOKEN id="token-44-34" pos="word" morph="none" start_char="6967" end_char="6971">virus</TOKEN>
<TOKEN id="token-44-35" pos="word" morph="none" start_char="6973" end_char="6979">emerges</TOKEN>
<TOKEN id="token-44-36" pos="punct" morph="none" start_char="6980" end_char="6980">,</TOKEN>
<TOKEN id="token-44-37" pos="word" morph="none" start_char="6982" end_char="6988">doctors</TOKEN>
<TOKEN id="token-44-38" pos="word" morph="none" start_char="6990" end_char="6993">just</TOKEN>
<TOKEN id="token-44-39" pos="word" morph="none" start_char="6995" end_char="6998">give</TOKEN>
<TOKEN id="token-44-40" pos="word" morph="none" start_char="7000" end_char="7007">patients</TOKEN>
<TOKEN id="token-44-41" pos="punct" morph="none" start_char="7009" end_char="7009">"</TOKEN>
<TOKEN id="token-44-42" pos="word" morph="none" start_char="7010" end_char="7017">whatever</TOKEN>
<TOKEN id="token-44-43" pos="word" morph="none" start_char="7019" end_char="7022">they</TOKEN>
<TOKEN id="token-44-44" pos="word" morph="none" start_char="7024" end_char="7028">think</TOKEN>
<TOKEN id="token-44-45" pos="word" morph="none" start_char="7030" end_char="7034">might</TOKEN>
<TOKEN id="token-44-46" pos="word" morph="none" start_char="7036" end_char="7039">help</TOKEN>
<TOKEN id="token-44-47" pos="punct" morph="none" start_char="7040" end_char="7041">."</TOKEN>
</SEG>
<SEG id="segment-45" start_char="7044" end_char="7132">
<ORIGINAL_TEXT>Many existing antivirals, he said, are HIV medications, so it’s natural to turn to those.</ORIGINAL_TEXT>
<TOKEN id="token-45-0" pos="word" morph="none" start_char="7044" end_char="7047">Many</TOKEN>
<TOKEN id="token-45-1" pos="word" morph="none" start_char="7049" end_char="7056">existing</TOKEN>
<TOKEN id="token-45-2" pos="word" morph="none" start_char="7058" end_char="7067">antivirals</TOKEN>
<TOKEN id="token-45-3" pos="punct" morph="none" start_char="7068" end_char="7068">,</TOKEN>
<TOKEN id="token-45-4" pos="word" morph="none" start_char="7070" end_char="7071">he</TOKEN>
<TOKEN id="token-45-5" pos="word" morph="none" start_char="7073" end_char="7076">said</TOKEN>
<TOKEN id="token-45-6" pos="punct" morph="none" start_char="7077" end_char="7077">,</TOKEN>
<TOKEN id="token-45-7" pos="word" morph="none" start_char="7079" end_char="7081">are</TOKEN>
<TOKEN id="token-45-8" pos="word" morph="none" start_char="7083" end_char="7085">HIV</TOKEN>
<TOKEN id="token-45-9" pos="word" morph="none" start_char="7087" end_char="7097">medications</TOKEN>
<TOKEN id="token-45-10" pos="punct" morph="none" start_char="7098" end_char="7098">,</TOKEN>
<TOKEN id="token-45-11" pos="word" morph="none" start_char="7100" end_char="7101">so</TOKEN>
<TOKEN id="token-45-12" pos="word" morph="none" start_char="7103" end_char="7106">it’s</TOKEN>
<TOKEN id="token-45-13" pos="word" morph="none" start_char="7108" end_char="7114">natural</TOKEN>
<TOKEN id="token-45-14" pos="word" morph="none" start_char="7116" end_char="7117">to</TOKEN>
<TOKEN id="token-45-15" pos="word" morph="none" start_char="7119" end_char="7122">turn</TOKEN>
<TOKEN id="token-45-16" pos="word" morph="none" start_char="7124" end_char="7125">to</TOKEN>
<TOKEN id="token-45-17" pos="word" morph="none" start_char="7127" end_char="7131">those</TOKEN>
<TOKEN id="token-45-18" pos="punct" morph="none" start_char="7132" end_char="7132">.</TOKEN>
</SEG>
<SEG id="segment-46" start_char="7134" end_char="7214">
<ORIGINAL_TEXT>And there is some precedent for HIV drugs possibly working against coronaviruses.</ORIGINAL_TEXT>
<TOKEN id="token-46-0" pos="word" morph="none" start_char="7134" end_char="7136">And</TOKEN>
<TOKEN id="token-46-1" pos="word" morph="none" start_char="7138" end_char="7142">there</TOKEN>
<TOKEN id="token-46-2" pos="word" morph="none" start_char="7144" end_char="7145">is</TOKEN>
<TOKEN id="token-46-3" pos="word" morph="none" start_char="7147" end_char="7150">some</TOKEN>
<TOKEN id="token-46-4" pos="word" morph="none" start_char="7152" end_char="7160">precedent</TOKEN>
<TOKEN id="token-46-5" pos="word" morph="none" start_char="7162" end_char="7164">for</TOKEN>
<TOKEN id="token-46-6" pos="word" morph="none" start_char="7166" end_char="7168">HIV</TOKEN>
<TOKEN id="token-46-7" pos="word" morph="none" start_char="7170" end_char="7174">drugs</TOKEN>
<TOKEN id="token-46-8" pos="word" morph="none" start_char="7176" end_char="7183">possibly</TOKEN>
<TOKEN id="token-46-9" pos="word" morph="none" start_char="7185" end_char="7191">working</TOKEN>
<TOKEN id="token-46-10" pos="word" morph="none" start_char="7193" end_char="7199">against</TOKEN>
<TOKEN id="token-46-11" pos="word" morph="none" start_char="7201" end_char="7213">coronaviruses</TOKEN>
<TOKEN id="token-46-12" pos="punct" morph="none" start_char="7214" end_char="7214">.</TOKEN>
</SEG>
<SEG id="segment-47" start_char="7217" end_char="7406">
<ORIGINAL_TEXT>During the SARS outbreak, for example, scientists performed a drug screen and identified the HIV drug cocktail of lopinavir and ritonavir as having potential antiviral activity against SARS.</ORIGINAL_TEXT>
<TOKEN id="token-47-0" pos="word" morph="none" start_char="7217" end_char="7222">During</TOKEN>
<TOKEN id="token-47-1" pos="word" morph="none" start_char="7224" end_char="7226">the</TOKEN>
<TOKEN id="token-47-2" pos="word" morph="none" start_char="7228" end_char="7231">SARS</TOKEN>
<TOKEN id="token-47-3" pos="word" morph="none" start_char="7233" end_char="7240">outbreak</TOKEN>
<TOKEN id="token-47-4" pos="punct" morph="none" start_char="7241" end_char="7241">,</TOKEN>
<TOKEN id="token-47-5" pos="word" morph="none" start_char="7243" end_char="7245">for</TOKEN>
<TOKEN id="token-47-6" pos="word" morph="none" start_char="7247" end_char="7253">example</TOKEN>
<TOKEN id="token-47-7" pos="punct" morph="none" start_char="7254" end_char="7254">,</TOKEN>
<TOKEN id="token-47-8" pos="word" morph="none" start_char="7256" end_char="7265">scientists</TOKEN>
<TOKEN id="token-47-9" pos="word" morph="none" start_char="7267" end_char="7275">performed</TOKEN>
<TOKEN id="token-47-10" pos="word" morph="none" start_char="7277" end_char="7277">a</TOKEN>
<TOKEN id="token-47-11" pos="word" morph="none" start_char="7279" end_char="7282">drug</TOKEN>
<TOKEN id="token-47-12" pos="word" morph="none" start_char="7284" end_char="7289">screen</TOKEN>
<TOKEN id="token-47-13" pos="word" morph="none" start_char="7291" end_char="7293">and</TOKEN>
<TOKEN id="token-47-14" pos="word" morph="none" start_char="7295" end_char="7304">identified</TOKEN>
<TOKEN id="token-47-15" pos="word" morph="none" start_char="7306" end_char="7308">the</TOKEN>
<TOKEN id="token-47-16" pos="word" morph="none" start_char="7310" end_char="7312">HIV</TOKEN>
<TOKEN id="token-47-17" pos="word" morph="none" start_char="7314" end_char="7317">drug</TOKEN>
<TOKEN id="token-47-18" pos="word" morph="none" start_char="7319" end_char="7326">cocktail</TOKEN>
<TOKEN id="token-47-19" pos="word" morph="none" start_char="7328" end_char="7329">of</TOKEN>
<TOKEN id="token-47-20" pos="word" morph="none" start_char="7331" end_char="7339">lopinavir</TOKEN>
<TOKEN id="token-47-21" pos="word" morph="none" start_char="7341" end_char="7343">and</TOKEN>
<TOKEN id="token-47-22" pos="word" morph="none" start_char="7345" end_char="7353">ritonavir</TOKEN>
<TOKEN id="token-47-23" pos="word" morph="none" start_char="7355" end_char="7356">as</TOKEN>
<TOKEN id="token-47-24" pos="word" morph="none" start_char="7358" end_char="7363">having</TOKEN>
<TOKEN id="token-47-25" pos="word" morph="none" start_char="7365" end_char="7373">potential</TOKEN>
<TOKEN id="token-47-26" pos="word" morph="none" start_char="7375" end_char="7383">antiviral</TOKEN>
<TOKEN id="token-47-27" pos="word" morph="none" start_char="7385" end_char="7392">activity</TOKEN>
<TOKEN id="token-47-28" pos="word" morph="none" start_char="7394" end_char="7400">against</TOKEN>
<TOKEN id="token-47-29" pos="word" morph="none" start_char="7402" end_char="7405">SARS</TOKEN>
<TOKEN id="token-47-30" pos="punct" morph="none" start_char="7406" end_char="7406">.</TOKEN>
</SEG>
<SEG id="segment-48" start_char="7408" end_char="7598">
<ORIGINAL_TEXT>That drug combo was also associated with better outcomes among a small group of SARS patients, although it was never tested in a clinical trial, so it’s hard to say if it was truly effective.</ORIGINAL_TEXT>
<TOKEN id="token-48-0" pos="word" morph="none" start_char="7408" end_char="7411">That</TOKEN>
<TOKEN id="token-48-1" pos="word" morph="none" start_char="7413" end_char="7416">drug</TOKEN>
<TOKEN id="token-48-2" pos="word" morph="none" start_char="7418" end_char="7422">combo</TOKEN>
<TOKEN id="token-48-3" pos="word" morph="none" start_char="7424" end_char="7426">was</TOKEN>
<TOKEN id="token-48-4" pos="word" morph="none" start_char="7428" end_char="7431">also</TOKEN>
<TOKEN id="token-48-5" pos="word" morph="none" start_char="7433" end_char="7442">associated</TOKEN>
<TOKEN id="token-48-6" pos="word" morph="none" start_char="7444" end_char="7447">with</TOKEN>
<TOKEN id="token-48-7" pos="word" morph="none" start_char="7449" end_char="7454">better</TOKEN>
<TOKEN id="token-48-8" pos="word" morph="none" start_char="7456" end_char="7463">outcomes</TOKEN>
<TOKEN id="token-48-9" pos="word" morph="none" start_char="7465" end_char="7469">among</TOKEN>
<TOKEN id="token-48-10" pos="word" morph="none" start_char="7471" end_char="7471">a</TOKEN>
<TOKEN id="token-48-11" pos="word" morph="none" start_char="7473" end_char="7477">small</TOKEN>
<TOKEN id="token-48-12" pos="word" morph="none" start_char="7479" end_char="7483">group</TOKEN>
<TOKEN id="token-48-13" pos="word" morph="none" start_char="7485" end_char="7486">of</TOKEN>
<TOKEN id="token-48-14" pos="word" morph="none" start_char="7488" end_char="7491">SARS</TOKEN>
<TOKEN id="token-48-15" pos="word" morph="none" start_char="7493" end_char="7500">patients</TOKEN>
<TOKEN id="token-48-16" pos="punct" morph="none" start_char="7501" end_char="7501">,</TOKEN>
<TOKEN id="token-48-17" pos="word" morph="none" start_char="7503" end_char="7510">although</TOKEN>
<TOKEN id="token-48-18" pos="word" morph="none" start_char="7512" end_char="7513">it</TOKEN>
<TOKEN id="token-48-19" pos="word" morph="none" start_char="7515" end_char="7517">was</TOKEN>
<TOKEN id="token-48-20" pos="word" morph="none" start_char="7519" end_char="7523">never</TOKEN>
<TOKEN id="token-48-21" pos="word" morph="none" start_char="7525" end_char="7530">tested</TOKEN>
<TOKEN id="token-48-22" pos="word" morph="none" start_char="7532" end_char="7533">in</TOKEN>
<TOKEN id="token-48-23" pos="word" morph="none" start_char="7535" end_char="7535">a</TOKEN>
<TOKEN id="token-48-24" pos="word" morph="none" start_char="7537" end_char="7544">clinical</TOKEN>
<TOKEN id="token-48-25" pos="word" morph="none" start_char="7546" end_char="7550">trial</TOKEN>
<TOKEN id="token-48-26" pos="punct" morph="none" start_char="7551" end_char="7551">,</TOKEN>
<TOKEN id="token-48-27" pos="word" morph="none" start_char="7553" end_char="7554">so</TOKEN>
<TOKEN id="token-48-28" pos="word" morph="none" start_char="7556" end_char="7559">it’s</TOKEN>
<TOKEN id="token-48-29" pos="word" morph="none" start_char="7561" end_char="7564">hard</TOKEN>
<TOKEN id="token-48-30" pos="word" morph="none" start_char="7566" end_char="7567">to</TOKEN>
<TOKEN id="token-48-31" pos="word" morph="none" start_char="7569" end_char="7571">say</TOKEN>
<TOKEN id="token-48-32" pos="word" morph="none" start_char="7573" end_char="7574">if</TOKEN>
<TOKEN id="token-48-33" pos="word" morph="none" start_char="7576" end_char="7577">it</TOKEN>
<TOKEN id="token-48-34" pos="word" morph="none" start_char="7579" end_char="7581">was</TOKEN>
<TOKEN id="token-48-35" pos="word" morph="none" start_char="7583" end_char="7587">truly</TOKEN>
<TOKEN id="token-48-36" pos="word" morph="none" start_char="7589" end_char="7597">effective</TOKEN>
<TOKEN id="token-48-37" pos="punct" morph="none" start_char="7598" end_char="7598">.</TOKEN>
</SEG>
<SEG id="segment-49" start_char="7600" end_char="7760">
<ORIGINAL_TEXT>It is also currently being tested in a clinical trial in Saudi Arabia against another disease caused by a coronavirus, Middle East respiratory syndrome, or MERS.</ORIGINAL_TEXT>
<TOKEN id="token-49-0" pos="word" morph="none" start_char="7600" end_char="7601">It</TOKEN>
<TOKEN id="token-49-1" pos="word" morph="none" start_char="7603" end_char="7604">is</TOKEN>
<TOKEN id="token-49-2" pos="word" morph="none" start_char="7606" end_char="7609">also</TOKEN>
<TOKEN id="token-49-3" pos="word" morph="none" start_char="7611" end_char="7619">currently</TOKEN>
<TOKEN id="token-49-4" pos="word" morph="none" start_char="7621" end_char="7625">being</TOKEN>
<TOKEN id="token-49-5" pos="word" morph="none" start_char="7627" end_char="7632">tested</TOKEN>
<TOKEN id="token-49-6" pos="word" morph="none" start_char="7634" end_char="7635">in</TOKEN>
<TOKEN id="token-49-7" pos="word" morph="none" start_char="7637" end_char="7637">a</TOKEN>
<TOKEN id="token-49-8" pos="word" morph="none" start_char="7639" end_char="7646">clinical</TOKEN>
<TOKEN id="token-49-9" pos="word" morph="none" start_char="7648" end_char="7652">trial</TOKEN>
<TOKEN id="token-49-10" pos="word" morph="none" start_char="7654" end_char="7655">in</TOKEN>
<TOKEN id="token-49-11" pos="word" morph="none" start_char="7657" end_char="7661">Saudi</TOKEN>
<TOKEN id="token-49-12" pos="word" morph="none" start_char="7663" end_char="7668">Arabia</TOKEN>
<TOKEN id="token-49-13" pos="word" morph="none" start_char="7670" end_char="7676">against</TOKEN>
<TOKEN id="token-49-14" pos="word" morph="none" start_char="7678" end_char="7684">another</TOKEN>
<TOKEN id="token-49-15" pos="word" morph="none" start_char="7686" end_char="7692">disease</TOKEN>
<TOKEN id="token-49-16" pos="word" morph="none" start_char="7694" end_char="7699">caused</TOKEN>
<TOKEN id="token-49-17" pos="word" morph="none" start_char="7701" end_char="7702">by</TOKEN>
<TOKEN id="token-49-18" pos="word" morph="none" start_char="7704" end_char="7704">a</TOKEN>
<TOKEN id="token-49-19" pos="word" morph="none" start_char="7706" end_char="7716">coronavirus</TOKEN>
<TOKEN id="token-49-20" pos="punct" morph="none" start_char="7717" end_char="7717">,</TOKEN>
<TOKEN id="token-49-21" pos="word" morph="none" start_char="7719" end_char="7724">Middle</TOKEN>
<TOKEN id="token-49-22" pos="word" morph="none" start_char="7726" end_char="7729">East</TOKEN>
<TOKEN id="token-49-23" pos="word" morph="none" start_char="7731" end_char="7741">respiratory</TOKEN>
<TOKEN id="token-49-24" pos="word" morph="none" start_char="7743" end_char="7750">syndrome</TOKEN>
<TOKEN id="token-49-25" pos="punct" morph="none" start_char="7751" end_char="7751">,</TOKEN>
<TOKEN id="token-49-26" pos="word" morph="none" start_char="7753" end_char="7754">or</TOKEN>
<TOKEN id="token-49-27" pos="word" morph="none" start_char="7756" end_char="7759">MERS</TOKEN>
<TOKEN id="token-49-28" pos="punct" morph="none" start_char="7760" end_char="7760">.</TOKEN>
</SEG>
<SEG id="segment-50" start_char="7763" end_char="7853">
<ORIGINAL_TEXT>Sheahan, however, is skeptical that HIV drugs will be very effective against the new virus.</ORIGINAL_TEXT>
<TOKEN id="token-50-0" pos="word" morph="none" start_char="7763" end_char="7769">Sheahan</TOKEN>
<TOKEN id="token-50-1" pos="punct" morph="none" start_char="7770" end_char="7770">,</TOKEN>
<TOKEN id="token-50-2" pos="word" morph="none" start_char="7772" end_char="7778">however</TOKEN>
<TOKEN id="token-50-3" pos="punct" morph="none" start_char="7779" end_char="7779">,</TOKEN>
<TOKEN id="token-50-4" pos="word" morph="none" start_char="7781" end_char="7782">is</TOKEN>
<TOKEN id="token-50-5" pos="word" morph="none" start_char="7784" end_char="7792">skeptical</TOKEN>
<TOKEN id="token-50-6" pos="word" morph="none" start_char="7794" end_char="7797">that</TOKEN>
<TOKEN id="token-50-7" pos="word" morph="none" start_char="7799" end_char="7801">HIV</TOKEN>
<TOKEN id="token-50-8" pos="word" morph="none" start_char="7803" end_char="7807">drugs</TOKEN>
<TOKEN id="token-50-9" pos="word" morph="none" start_char="7809" end_char="7812">will</TOKEN>
<TOKEN id="token-50-10" pos="word" morph="none" start_char="7814" end_char="7815">be</TOKEN>
<TOKEN id="token-50-11" pos="word" morph="none" start_char="7817" end_char="7820">very</TOKEN>
<TOKEN id="token-50-12" pos="word" morph="none" start_char="7822" end_char="7830">effective</TOKEN>
<TOKEN id="token-50-13" pos="word" morph="none" start_char="7832" end_char="7838">against</TOKEN>
<TOKEN id="token-50-14" pos="word" morph="none" start_char="7840" end_char="7842">the</TOKEN>
<TOKEN id="token-50-15" pos="word" morph="none" start_char="7844" end_char="7846">new</TOKEN>
<TOKEN id="token-50-16" pos="word" morph="none" start_char="7848" end_char="7852">virus</TOKEN>
<TOKEN id="token-50-17" pos="punct" morph="none" start_char="7853" end_char="7853">.</TOKEN>
</SEG>
<SEG id="segment-51" start_char="7855" end_char="7973">
<ORIGINAL_TEXT>The levels of the drug that are likely required to diminish viral replication, he said, "are not achievable" in people.</ORIGINAL_TEXT>
<TOKEN id="token-51-0" pos="word" morph="none" start_char="7855" end_char="7857">The</TOKEN>
<TOKEN id="token-51-1" pos="word" morph="none" start_char="7859" end_char="7864">levels</TOKEN>
<TOKEN id="token-51-2" pos="word" morph="none" start_char="7866" end_char="7867">of</TOKEN>
<TOKEN id="token-51-3" pos="word" morph="none" start_char="7869" end_char="7871">the</TOKEN>
<TOKEN id="token-51-4" pos="word" morph="none" start_char="7873" end_char="7876">drug</TOKEN>
<TOKEN id="token-51-5" pos="word" morph="none" start_char="7878" end_char="7881">that</TOKEN>
<TOKEN id="token-51-6" pos="word" morph="none" start_char="7883" end_char="7885">are</TOKEN>
<TOKEN id="token-51-7" pos="word" morph="none" start_char="7887" end_char="7892">likely</TOKEN>
<TOKEN id="token-51-8" pos="word" morph="none" start_char="7894" end_char="7901">required</TOKEN>
<TOKEN id="token-51-9" pos="word" morph="none" start_char="7903" end_char="7904">to</TOKEN>
<TOKEN id="token-51-10" pos="word" morph="none" start_char="7906" end_char="7913">diminish</TOKEN>
<TOKEN id="token-51-11" pos="word" morph="none" start_char="7915" end_char="7919">viral</TOKEN>
<TOKEN id="token-51-12" pos="word" morph="none" start_char="7921" end_char="7931">replication</TOKEN>
<TOKEN id="token-51-13" pos="punct" morph="none" start_char="7932" end_char="7932">,</TOKEN>
<TOKEN id="token-51-14" pos="word" morph="none" start_char="7934" end_char="7935">he</TOKEN>
<TOKEN id="token-51-15" pos="word" morph="none" start_char="7937" end_char="7940">said</TOKEN>
<TOKEN id="token-51-16" pos="punct" morph="none" start_char="7941" end_char="7941">,</TOKEN>
<TOKEN id="token-51-17" pos="punct" morph="none" start_char="7943" end_char="7943">"</TOKEN>
<TOKEN id="token-51-18" pos="word" morph="none" start_char="7944" end_char="7946">are</TOKEN>
<TOKEN id="token-51-19" pos="word" morph="none" start_char="7948" end_char="7950">not</TOKEN>
<TOKEN id="token-51-20" pos="word" morph="none" start_char="7952" end_char="7961">achievable</TOKEN>
<TOKEN id="token-51-21" pos="punct" morph="none" start_char="7962" end_char="7962">"</TOKEN>
<TOKEN id="token-51-22" pos="word" morph="none" start_char="7964" end_char="7965">in</TOKEN>
<TOKEN id="token-51-23" pos="word" morph="none" start_char="7967" end_char="7972">people</TOKEN>
<TOKEN id="token-51-24" pos="punct" morph="none" start_char="7973" end_char="7973">.</TOKEN>
</SEG>
<SEG id="segment-52" start_char="7975" end_char="8153">
<ORIGINAL_TEXT>And in his experiments against the MERS virus in cell culture and in mice, he found lopinavir and ritonavir offered little improvement in severe lung disease or viral replication.</ORIGINAL_TEXT>
<TOKEN id="token-52-0" pos="word" morph="none" start_char="7975" end_char="7977">And</TOKEN>
<TOKEN id="token-52-1" pos="word" morph="none" start_char="7979" end_char="7980">in</TOKEN>
<TOKEN id="token-52-2" pos="word" morph="none" start_char="7982" end_char="7984">his</TOKEN>
<TOKEN id="token-52-3" pos="word" morph="none" start_char="7986" end_char="7996">experiments</TOKEN>
<TOKEN id="token-52-4" pos="word" morph="none" start_char="7998" end_char="8004">against</TOKEN>
<TOKEN id="token-52-5" pos="word" morph="none" start_char="8006" end_char="8008">the</TOKEN>
<TOKEN id="token-52-6" pos="word" morph="none" start_char="8010" end_char="8013">MERS</TOKEN>
<TOKEN id="token-52-7" pos="word" morph="none" start_char="8015" end_char="8019">virus</TOKEN>
<TOKEN id="token-52-8" pos="word" morph="none" start_char="8021" end_char="8022">in</TOKEN>
<TOKEN id="token-52-9" pos="word" morph="none" start_char="8024" end_char="8027">cell</TOKEN>
<TOKEN id="token-52-10" pos="word" morph="none" start_char="8029" end_char="8035">culture</TOKEN>
<TOKEN id="token-52-11" pos="word" morph="none" start_char="8037" end_char="8039">and</TOKEN>
<TOKEN id="token-52-12" pos="word" morph="none" start_char="8041" end_char="8042">in</TOKEN>
<TOKEN id="token-52-13" pos="word" morph="none" start_char="8044" end_char="8047">mice</TOKEN>
<TOKEN id="token-52-14" pos="punct" morph="none" start_char="8048" end_char="8048">,</TOKEN>
<TOKEN id="token-52-15" pos="word" morph="none" start_char="8050" end_char="8051">he</TOKEN>
<TOKEN id="token-52-16" pos="word" morph="none" start_char="8053" end_char="8057">found</TOKEN>
<TOKEN id="token-52-17" pos="word" morph="none" start_char="8059" end_char="8067">lopinavir</TOKEN>
<TOKEN id="token-52-18" pos="word" morph="none" start_char="8069" end_char="8071">and</TOKEN>
<TOKEN id="token-52-19" pos="word" morph="none" start_char="8073" end_char="8081">ritonavir</TOKEN>
<TOKEN id="token-52-20" pos="word" morph="none" start_char="8083" end_char="8089">offered</TOKEN>
<TOKEN id="token-52-21" pos="word" morph="none" start_char="8091" end_char="8096">little</TOKEN>
<TOKEN id="token-52-22" pos="word" morph="none" start_char="8098" end_char="8108">improvement</TOKEN>
<TOKEN id="token-52-23" pos="word" morph="none" start_char="8110" end_char="8111">in</TOKEN>
<TOKEN id="token-52-24" pos="word" morph="none" start_char="8113" end_char="8118">severe</TOKEN>
<TOKEN id="token-52-25" pos="word" morph="none" start_char="8120" end_char="8123">lung</TOKEN>
<TOKEN id="token-52-26" pos="word" morph="none" start_char="8125" end_char="8131">disease</TOKEN>
<TOKEN id="token-52-27" pos="word" morph="none" start_char="8133" end_char="8134">or</TOKEN>
<TOKEN id="token-52-28" pos="word" morph="none" start_char="8136" end_char="8140">viral</TOKEN>
<TOKEN id="token-52-29" pos="word" morph="none" start_char="8142" end_char="8152">replication</TOKEN>
<TOKEN id="token-52-30" pos="punct" morph="none" start_char="8153" end_char="8153">.</TOKEN>
</SEG>
<SEG id="segment-53" start_char="8156" end_char="8181">
<ORIGINAL_TEXT>No Signs of Bioengineering</ORIGINAL_TEXT>
<TOKEN id="token-53-0" pos="word" morph="none" start_char="8156" end_char="8157">No</TOKEN>
<TOKEN id="token-53-1" pos="word" morph="none" start_char="8159" end_char="8163">Signs</TOKEN>
<TOKEN id="token-53-2" pos="word" morph="none" start_char="8165" end_char="8166">of</TOKEN>
<TOKEN id="token-53-3" pos="word" morph="none" start_char="8168" end_char="8181">Bioengineering</TOKEN>
</SEG>
<SEG id="segment-54" start_char="8185" end_char="8281">
<ORIGINAL_TEXT>As for the general notion that the virus has been bioengineered, there’s no evidence that’s true.</ORIGINAL_TEXT>
<TOKEN id="token-54-0" pos="word" morph="none" start_char="8185" end_char="8186">As</TOKEN>
<TOKEN id="token-54-1" pos="word" morph="none" start_char="8188" end_char="8190">for</TOKEN>
<TOKEN id="token-54-2" pos="word" morph="none" start_char="8192" end_char="8194">the</TOKEN>
<TOKEN id="token-54-3" pos="word" morph="none" start_char="8196" end_char="8202">general</TOKEN>
<TOKEN id="token-54-4" pos="word" morph="none" start_char="8204" end_char="8209">notion</TOKEN>
<TOKEN id="token-54-5" pos="word" morph="none" start_char="8211" end_char="8214">that</TOKEN>
<TOKEN id="token-54-6" pos="word" morph="none" start_char="8216" end_char="8218">the</TOKEN>
<TOKEN id="token-54-7" pos="word" morph="none" start_char="8220" end_char="8224">virus</TOKEN>
<TOKEN id="token-54-8" pos="word" morph="none" start_char="8226" end_char="8228">has</TOKEN>
<TOKEN id="token-54-9" pos="word" morph="none" start_char="8230" end_char="8233">been</TOKEN>
<TOKEN id="token-54-10" pos="word" morph="none" start_char="8235" end_char="8247">bioengineered</TOKEN>
<TOKEN id="token-54-11" pos="punct" morph="none" start_char="8248" end_char="8248">,</TOKEN>
<TOKEN id="token-54-12" pos="word" morph="none" start_char="8250" end_char="8256">there’s</TOKEN>
<TOKEN id="token-54-13" pos="word" morph="none" start_char="8258" end_char="8259">no</TOKEN>
<TOKEN id="token-54-14" pos="word" morph="none" start_char="8261" end_char="8268">evidence</TOKEN>
<TOKEN id="token-54-15" pos="word" morph="none" start_char="8270" end_char="8275">that’s</TOKEN>
<TOKEN id="token-54-16" pos="word" morph="none" start_char="8277" end_char="8280">true</TOKEN>
<TOKEN id="token-54-17" pos="punct" morph="none" start_char="8281" end_char="8281">.</TOKEN>
</SEG>
<SEG id="segment-55" start_char="8283" end_char="8389">
<ORIGINAL_TEXT>On the contrary, as we’ve explained before, all lines of evidence point to the virus coming from an animal.</ORIGINAL_TEXT>
<TOKEN id="token-55-0" pos="word" morph="none" start_char="8283" end_char="8284">On</TOKEN>
<TOKEN id="token-55-1" pos="word" morph="none" start_char="8286" end_char="8288">the</TOKEN>
<TOKEN id="token-55-2" pos="word" morph="none" start_char="8290" end_char="8297">contrary</TOKEN>
<TOKEN id="token-55-3" pos="punct" morph="none" start_char="8298" end_char="8298">,</TOKEN>
<TOKEN id="token-55-4" pos="word" morph="none" start_char="8300" end_char="8301">as</TOKEN>
<TOKEN id="token-55-5" pos="word" morph="none" start_char="8303" end_char="8307">we’ve</TOKEN>
<TOKEN id="token-55-6" pos="word" morph="none" start_char="8309" end_char="8317">explained</TOKEN>
<TOKEN id="token-55-7" pos="word" morph="none" start_char="8319" end_char="8324">before</TOKEN>
<TOKEN id="token-55-8" pos="punct" morph="none" start_char="8325" end_char="8325">,</TOKEN>
<TOKEN id="token-55-9" pos="word" morph="none" start_char="8327" end_char="8329">all</TOKEN>
<TOKEN id="token-55-10" pos="word" morph="none" start_char="8331" end_char="8335">lines</TOKEN>
<TOKEN id="token-55-11" pos="word" morph="none" start_char="8337" end_char="8338">of</TOKEN>
<TOKEN id="token-55-12" pos="word" morph="none" start_char="8340" end_char="8347">evidence</TOKEN>
<TOKEN id="token-55-13" pos="word" morph="none" start_char="8349" end_char="8353">point</TOKEN>
<TOKEN id="token-55-14" pos="word" morph="none" start_char="8355" end_char="8356">to</TOKEN>
<TOKEN id="token-55-15" pos="word" morph="none" start_char="8358" end_char="8360">the</TOKEN>
<TOKEN id="token-55-16" pos="word" morph="none" start_char="8362" end_char="8366">virus</TOKEN>
<TOKEN id="token-55-17" pos="word" morph="none" start_char="8368" end_char="8373">coming</TOKEN>
<TOKEN id="token-55-18" pos="word" morph="none" start_char="8375" end_char="8378">from</TOKEN>
<TOKEN id="token-55-19" pos="word" morph="none" start_char="8380" end_char="8381">an</TOKEN>
<TOKEN id="token-55-20" pos="word" morph="none" start_char="8383" end_char="8388">animal</TOKEN>
<TOKEN id="token-55-21" pos="punct" morph="none" start_char="8389" end_char="8389">.</TOKEN>
</SEG>
<SEG id="segment-56" start_char="8391" end_char="8622">
<ORIGINAL_TEXT>That’s consistent with what scientists have learned about the ecology of coronaviruses in the last 20 years, Sheahan said, including SARS and MERS — and it fits with the fact that the virus shares 96% of its genome with a bat virus.</ORIGINAL_TEXT>
<TOKEN id="token-56-0" pos="word" morph="none" start_char="8391" end_char="8396">That’s</TOKEN>
<TOKEN id="token-56-1" pos="word" morph="none" start_char="8398" end_char="8407">consistent</TOKEN>
<TOKEN id="token-56-2" pos="word" morph="none" start_char="8409" end_char="8412">with</TOKEN>
<TOKEN id="token-56-3" pos="word" morph="none" start_char="8414" end_char="8417">what</TOKEN>
<TOKEN id="token-56-4" pos="word" morph="none" start_char="8419" end_char="8428">scientists</TOKEN>
<TOKEN id="token-56-5" pos="word" morph="none" start_char="8430" end_char="8433">have</TOKEN>
<TOKEN id="token-56-6" pos="word" morph="none" start_char="8435" end_char="8441">learned</TOKEN>
<TOKEN id="token-56-7" pos="word" morph="none" start_char="8443" end_char="8447">about</TOKEN>
<TOKEN id="token-56-8" pos="word" morph="none" start_char="8449" end_char="8451">the</TOKEN>
<TOKEN id="token-56-9" pos="word" morph="none" start_char="8453" end_char="8459">ecology</TOKEN>
<TOKEN id="token-56-10" pos="word" morph="none" start_char="8461" end_char="8462">of</TOKEN>
<TOKEN id="token-56-11" pos="word" morph="none" start_char="8464" end_char="8476">coronaviruses</TOKEN>
<TOKEN id="token-56-12" pos="word" morph="none" start_char="8478" end_char="8479">in</TOKEN>
<TOKEN id="token-56-13" pos="word" morph="none" start_char="8481" end_char="8483">the</TOKEN>
<TOKEN id="token-56-14" pos="word" morph="none" start_char="8485" end_char="8488">last</TOKEN>
<TOKEN id="token-56-15" pos="word" morph="none" start_char="8490" end_char="8491">20</TOKEN>
<TOKEN id="token-56-16" pos="word" morph="none" start_char="8493" end_char="8497">years</TOKEN>
<TOKEN id="token-56-17" pos="punct" morph="none" start_char="8498" end_char="8498">,</TOKEN>
<TOKEN id="token-56-18" pos="word" morph="none" start_char="8500" end_char="8506">Sheahan</TOKEN>
<TOKEN id="token-56-19" pos="word" morph="none" start_char="8508" end_char="8511">said</TOKEN>
<TOKEN id="token-56-20" pos="punct" morph="none" start_char="8512" end_char="8512">,</TOKEN>
<TOKEN id="token-56-21" pos="word" morph="none" start_char="8514" end_char="8522">including</TOKEN>
<TOKEN id="token-56-22" pos="word" morph="none" start_char="8524" end_char="8527">SARS</TOKEN>
<TOKEN id="token-56-23" pos="word" morph="none" start_char="8529" end_char="8531">and</TOKEN>
<TOKEN id="token-56-24" pos="word" morph="none" start_char="8533" end_char="8536">MERS</TOKEN>
<TOKEN id="token-56-25" pos="punct" morph="none" start_char="8538" end_char="8538">—</TOKEN>
<TOKEN id="token-56-26" pos="word" morph="none" start_char="8540" end_char="8542">and</TOKEN>
<TOKEN id="token-56-27" pos="word" morph="none" start_char="8544" end_char="8545">it</TOKEN>
<TOKEN id="token-56-28" pos="word" morph="none" start_char="8547" end_char="8550">fits</TOKEN>
<TOKEN id="token-56-29" pos="word" morph="none" start_char="8552" end_char="8555">with</TOKEN>
<TOKEN id="token-56-30" pos="word" morph="none" start_char="8557" end_char="8559">the</TOKEN>
<TOKEN id="token-56-31" pos="word" morph="none" start_char="8561" end_char="8564">fact</TOKEN>
<TOKEN id="token-56-32" pos="word" morph="none" start_char="8566" end_char="8569">that</TOKEN>
<TOKEN id="token-56-33" pos="word" morph="none" start_char="8571" end_char="8573">the</TOKEN>
<TOKEN id="token-56-34" pos="word" morph="none" start_char="8575" end_char="8579">virus</TOKEN>
<TOKEN id="token-56-35" pos="word" morph="none" start_char="8581" end_char="8586">shares</TOKEN>
<TOKEN id="token-56-36" pos="word" morph="none" start_char="8588" end_char="8589">96</TOKEN>
<TOKEN id="token-56-37" pos="punct" morph="none" start_char="8590" end_char="8590">%</TOKEN>
<TOKEN id="token-56-38" pos="word" morph="none" start_char="8592" end_char="8593">of</TOKEN>
<TOKEN id="token-56-39" pos="word" morph="none" start_char="8595" end_char="8597">its</TOKEN>
<TOKEN id="token-56-40" pos="word" morph="none" start_char="8599" end_char="8604">genome</TOKEN>
<TOKEN id="token-56-41" pos="word" morph="none" start_char="8606" end_char="8609">with</TOKEN>
<TOKEN id="token-56-42" pos="word" morph="none" start_char="8611" end_char="8611">a</TOKEN>
<TOKEN id="token-56-43" pos="word" morph="none" start_char="8613" end_char="8615">bat</TOKEN>
<TOKEN id="token-56-44" pos="word" morph="none" start_char="8617" end_char="8621">virus</TOKEN>
<TOKEN id="token-56-45" pos="punct" morph="none" start_char="8622" end_char="8622">.</TOKEN>
</SEG>
<SEG id="segment-57" start_char="8625" end_char="8719">
<ORIGINAL_TEXT>"The genetic data is pointing to this virus coming from a bat reservoir," he said, "not a lab."</ORIGINAL_TEXT>
<TOKEN id="token-57-0" pos="punct" morph="none" start_char="8625" end_char="8625">"</TOKEN>
<TOKEN id="token-57-1" pos="word" morph="none" start_char="8626" end_char="8628">The</TOKEN>
<TOKEN id="token-57-2" pos="word" morph="none" start_char="8630" end_char="8636">genetic</TOKEN>
<TOKEN id="token-57-3" pos="word" morph="none" start_char="8638" end_char="8641">data</TOKEN>
<TOKEN id="token-57-4" pos="word" morph="none" start_char="8643" end_char="8644">is</TOKEN>
<TOKEN id="token-57-5" pos="word" morph="none" start_char="8646" end_char="8653">pointing</TOKEN>
<TOKEN id="token-57-6" pos="word" morph="none" start_char="8655" end_char="8656">to</TOKEN>
<TOKEN id="token-57-7" pos="word" morph="none" start_char="8658" end_char="8661">this</TOKEN>
<TOKEN id="token-57-8" pos="word" morph="none" start_char="8663" end_char="8667">virus</TOKEN>
<TOKEN id="token-57-9" pos="word" morph="none" start_char="8669" end_char="8674">coming</TOKEN>
<TOKEN id="token-57-10" pos="word" morph="none" start_char="8676" end_char="8679">from</TOKEN>
<TOKEN id="token-57-11" pos="word" morph="none" start_char="8681" end_char="8681">a</TOKEN>
<TOKEN id="token-57-12" pos="word" morph="none" start_char="8683" end_char="8685">bat</TOKEN>
<TOKEN id="token-57-13" pos="word" morph="none" start_char="8687" end_char="8695">reservoir</TOKEN>
<TOKEN id="token-57-14" pos="punct" morph="none" start_char="8696" end_char="8697">,"</TOKEN>
<TOKEN id="token-57-15" pos="word" morph="none" start_char="8699" end_char="8700">he</TOKEN>
<TOKEN id="token-57-16" pos="word" morph="none" start_char="8702" end_char="8705">said</TOKEN>
<TOKEN id="token-57-17" pos="punct" morph="none" start_char="8706" end_char="8706">,</TOKEN>
<TOKEN id="token-57-18" pos="punct" morph="none" start_char="8708" end_char="8708">"</TOKEN>
<TOKEN id="token-57-19" pos="word" morph="none" start_char="8709" end_char="8711">not</TOKEN>
<TOKEN id="token-57-20" pos="word" morph="none" start_char="8713" end_char="8713">a</TOKEN>
<TOKEN id="token-57-21" pos="word" morph="none" start_char="8715" end_char="8717">lab</TOKEN>
<TOKEN id="token-57-22" pos="punct" morph="none" start_char="8718" end_char="8719">."</TOKEN>
</SEG>
<SEG id="segment-58" start_char="8722" end_char="8865">
<ORIGINAL_TEXT>And not only are there no HIV "insertions" in the virus, but by looking at the virus’ genome, scientists also see zero signs of human tampering.</ORIGINAL_TEXT>
<TOKEN id="token-58-0" pos="word" morph="none" start_char="8722" end_char="8724">And</TOKEN>
<TOKEN id="token-58-1" pos="word" morph="none" start_char="8726" end_char="8728">not</TOKEN>
<TOKEN id="token-58-2" pos="word" morph="none" start_char="8730" end_char="8733">only</TOKEN>
<TOKEN id="token-58-3" pos="word" morph="none" start_char="8735" end_char="8737">are</TOKEN>
<TOKEN id="token-58-4" pos="word" morph="none" start_char="8739" end_char="8743">there</TOKEN>
<TOKEN id="token-58-5" pos="word" morph="none" start_char="8745" end_char="8746">no</TOKEN>
<TOKEN id="token-58-6" pos="word" morph="none" start_char="8748" end_char="8750">HIV</TOKEN>
<TOKEN id="token-58-7" pos="punct" morph="none" start_char="8752" end_char="8752">"</TOKEN>
<TOKEN id="token-58-8" pos="word" morph="none" start_char="8753" end_char="8762">insertions</TOKEN>
<TOKEN id="token-58-9" pos="punct" morph="none" start_char="8763" end_char="8763">"</TOKEN>
<TOKEN id="token-58-10" pos="word" morph="none" start_char="8765" end_char="8766">in</TOKEN>
<TOKEN id="token-58-11" pos="word" morph="none" start_char="8768" end_char="8770">the</TOKEN>
<TOKEN id="token-58-12" pos="word" morph="none" start_char="8772" end_char="8776">virus</TOKEN>
<TOKEN id="token-58-13" pos="punct" morph="none" start_char="8777" end_char="8777">,</TOKEN>
<TOKEN id="token-58-14" pos="word" morph="none" start_char="8779" end_char="8781">but</TOKEN>
<TOKEN id="token-58-15" pos="word" morph="none" start_char="8783" end_char="8784">by</TOKEN>
<TOKEN id="token-58-16" pos="word" morph="none" start_char="8786" end_char="8792">looking</TOKEN>
<TOKEN id="token-58-17" pos="word" morph="none" start_char="8794" end_char="8795">at</TOKEN>
<TOKEN id="token-58-18" pos="word" morph="none" start_char="8797" end_char="8799">the</TOKEN>
<TOKEN id="token-58-19" pos="word" morph="none" start_char="8801" end_char="8805">virus</TOKEN>
<TOKEN id="token-58-20" pos="punct" morph="none" start_char="8806" end_char="8806">’</TOKEN>
<TOKEN id="token-58-21" pos="word" morph="none" start_char="8808" end_char="8813">genome</TOKEN>
<TOKEN id="token-58-22" pos="punct" morph="none" start_char="8814" end_char="8814">,</TOKEN>
<TOKEN id="token-58-23" pos="word" morph="none" start_char="8816" end_char="8825">scientists</TOKEN>
<TOKEN id="token-58-24" pos="word" morph="none" start_char="8827" end_char="8830">also</TOKEN>
<TOKEN id="token-58-25" pos="word" morph="none" start_char="8832" end_char="8834">see</TOKEN>
<TOKEN id="token-58-26" pos="word" morph="none" start_char="8836" end_char="8839">zero</TOKEN>
<TOKEN id="token-58-27" pos="word" morph="none" start_char="8841" end_char="8845">signs</TOKEN>
<TOKEN id="token-58-28" pos="word" morph="none" start_char="8847" end_char="8848">of</TOKEN>
<TOKEN id="token-58-29" pos="word" morph="none" start_char="8850" end_char="8854">human</TOKEN>
<TOKEN id="token-58-30" pos="word" morph="none" start_char="8856" end_char="8864">tampering</TOKEN>
<TOKEN id="token-58-31" pos="punct" morph="none" start_char="8865" end_char="8865">.</TOKEN>
</SEG>
<SEG id="segment-59" start_char="8868" end_char="9092">
<ORIGINAL_TEXT>Bedford, the Fred Hutchinson computational biologist, pointed out on Twitter that the virus’ genetic differences to its most recent common ancestor are "consistent with differences expected to arise during natural evolution."</ORIGINAL_TEXT>
<TOKEN id="token-59-0" pos="word" morph="none" start_char="8868" end_char="8874">Bedford</TOKEN>
<TOKEN id="token-59-1" pos="punct" morph="none" start_char="8875" end_char="8875">,</TOKEN>
<TOKEN id="token-59-2" pos="word" morph="none" start_char="8877" end_char="8879">the</TOKEN>
<TOKEN id="token-59-3" pos="word" morph="none" start_char="8881" end_char="8884">Fred</TOKEN>
<TOKEN id="token-59-4" pos="word" morph="none" start_char="8886" end_char="8895">Hutchinson</TOKEN>
<TOKEN id="token-59-5" pos="word" morph="none" start_char="8897" end_char="8909">computational</TOKEN>
<TOKEN id="token-59-6" pos="word" morph="none" start_char="8911" end_char="8919">biologist</TOKEN>
<TOKEN id="token-59-7" pos="punct" morph="none" start_char="8920" end_char="8920">,</TOKEN>
<TOKEN id="token-59-8" pos="word" morph="none" start_char="8922" end_char="8928">pointed</TOKEN>
<TOKEN id="token-59-9" pos="word" morph="none" start_char="8930" end_char="8932">out</TOKEN>
<TOKEN id="token-59-10" pos="word" morph="none" start_char="8934" end_char="8935">on</TOKEN>
<TOKEN id="token-59-11" pos="word" morph="none" start_char="8937" end_char="8943">Twitter</TOKEN>
<TOKEN id="token-59-12" pos="word" morph="none" start_char="8945" end_char="8948">that</TOKEN>
<TOKEN id="token-59-13" pos="word" morph="none" start_char="8950" end_char="8952">the</TOKEN>
<TOKEN id="token-59-14" pos="word" morph="none" start_char="8954" end_char="8958">virus</TOKEN>
<TOKEN id="token-59-15" pos="punct" morph="none" start_char="8959" end_char="8959">’</TOKEN>
<TOKEN id="token-59-16" pos="word" morph="none" start_char="8961" end_char="8967">genetic</TOKEN>
<TOKEN id="token-59-17" pos="word" morph="none" start_char="8969" end_char="8979">differences</TOKEN>
<TOKEN id="token-59-18" pos="word" morph="none" start_char="8981" end_char="8982">to</TOKEN>
<TOKEN id="token-59-19" pos="word" morph="none" start_char="8984" end_char="8986">its</TOKEN>
<TOKEN id="token-59-20" pos="word" morph="none" start_char="8988" end_char="8991">most</TOKEN>
<TOKEN id="token-59-21" pos="word" morph="none" start_char="8993" end_char="8998">recent</TOKEN>
<TOKEN id="token-59-22" pos="word" morph="none" start_char="9000" end_char="9005">common</TOKEN>
<TOKEN id="token-59-23" pos="word" morph="none" start_char="9007" end_char="9014">ancestor</TOKEN>
<TOKEN id="token-59-24" pos="word" morph="none" start_char="9016" end_char="9018">are</TOKEN>
<TOKEN id="token-59-25" pos="punct" morph="none" start_char="9020" end_char="9020">"</TOKEN>
<TOKEN id="token-59-26" pos="word" morph="none" start_char="9021" end_char="9030">consistent</TOKEN>
<TOKEN id="token-59-27" pos="word" morph="none" start_char="9032" end_char="9035">with</TOKEN>
<TOKEN id="token-59-28" pos="word" morph="none" start_char="9037" end_char="9047">differences</TOKEN>
<TOKEN id="token-59-29" pos="word" morph="none" start_char="9049" end_char="9056">expected</TOKEN>
<TOKEN id="token-59-30" pos="word" morph="none" start_char="9058" end_char="9059">to</TOKEN>
<TOKEN id="token-59-31" pos="word" morph="none" start_char="9061" end_char="9065">arise</TOKEN>
<TOKEN id="token-59-32" pos="word" morph="none" start_char="9067" end_char="9072">during</TOKEN>
<TOKEN id="token-59-33" pos="word" morph="none" start_char="9074" end_char="9080">natural</TOKEN>
<TOKEN id="token-59-34" pos="word" morph="none" start_char="9082" end_char="9090">evolution</TOKEN>
<TOKEN id="token-59-35" pos="punct" morph="none" start_char="9091" end_char="9092">."</TOKEN>
</SEG>
<SEG id="segment-60" start_char="9095" end_char="9249">
<ORIGINAL_TEXT>An engineered virus, he explained, would likely have a "distorted" amino acid to nucleotide ratio, and also have changes focused in on a "subset of genes."</ORIGINAL_TEXT>
<TOKEN id="token-60-0" pos="word" morph="none" start_char="9095" end_char="9096">An</TOKEN>
<TOKEN id="token-60-1" pos="word" morph="none" start_char="9098" end_char="9107">engineered</TOKEN>
<TOKEN id="token-60-2" pos="word" morph="none" start_char="9109" end_char="9113">virus</TOKEN>
<TOKEN id="token-60-3" pos="punct" morph="none" start_char="9114" end_char="9114">,</TOKEN>
<TOKEN id="token-60-4" pos="word" morph="none" start_char="9116" end_char="9117">he</TOKEN>
<TOKEN id="token-60-5" pos="word" morph="none" start_char="9119" end_char="9127">explained</TOKEN>
<TOKEN id="token-60-6" pos="punct" morph="none" start_char="9128" end_char="9128">,</TOKEN>
<TOKEN id="token-60-7" pos="word" morph="none" start_char="9130" end_char="9134">would</TOKEN>
<TOKEN id="token-60-8" pos="word" morph="none" start_char="9136" end_char="9141">likely</TOKEN>
<TOKEN id="token-60-9" pos="word" morph="none" start_char="9143" end_char="9146">have</TOKEN>
<TOKEN id="token-60-10" pos="word" morph="none" start_char="9148" end_char="9148">a</TOKEN>
<TOKEN id="token-60-11" pos="punct" morph="none" start_char="9150" end_char="9150">"</TOKEN>
<TOKEN id="token-60-12" pos="word" morph="none" start_char="9151" end_char="9159">distorted</TOKEN>
<TOKEN id="token-60-13" pos="punct" morph="none" start_char="9160" end_char="9160">"</TOKEN>
<TOKEN id="token-60-14" pos="word" morph="none" start_char="9162" end_char="9166">amino</TOKEN>
<TOKEN id="token-60-15" pos="word" morph="none" start_char="9168" end_char="9171">acid</TOKEN>
<TOKEN id="token-60-16" pos="word" morph="none" start_char="9173" end_char="9174">to</TOKEN>
<TOKEN id="token-60-17" pos="word" morph="none" start_char="9176" end_char="9185">nucleotide</TOKEN>
<TOKEN id="token-60-18" pos="word" morph="none" start_char="9187" end_char="9191">ratio</TOKEN>
<TOKEN id="token-60-19" pos="punct" morph="none" start_char="9192" end_char="9192">,</TOKEN>
<TOKEN id="token-60-20" pos="word" morph="none" start_char="9194" end_char="9196">and</TOKEN>
<TOKEN id="token-60-21" pos="word" morph="none" start_char="9198" end_char="9201">also</TOKEN>
<TOKEN id="token-60-22" pos="word" morph="none" start_char="9203" end_char="9206">have</TOKEN>
<TOKEN id="token-60-23" pos="word" morph="none" start_char="9208" end_char="9214">changes</TOKEN>
<TOKEN id="token-60-24" pos="word" morph="none" start_char="9216" end_char="9222">focused</TOKEN>
<TOKEN id="token-60-25" pos="word" morph="none" start_char="9224" end_char="9225">in</TOKEN>
<TOKEN id="token-60-26" pos="word" morph="none" start_char="9227" end_char="9228">on</TOKEN>
<TOKEN id="token-60-27" pos="word" morph="none" start_char="9230" end_char="9230">a</TOKEN>
<TOKEN id="token-60-28" pos="punct" morph="none" start_char="9232" end_char="9232">"</TOKEN>
<TOKEN id="token-60-29" pos="word" morph="none" start_char="9233" end_char="9238">subset</TOKEN>
<TOKEN id="token-60-30" pos="word" morph="none" start_char="9240" end_char="9241">of</TOKEN>
<TOKEN id="token-60-31" pos="word" morph="none" start_char="9243" end_char="9247">genes</TOKEN>
<TOKEN id="token-60-32" pos="punct" morph="none" start_char="9248" end_char="9249">."</TOKEN>
</SEG>
<SEG id="segment-61" start_char="9251" end_char="9410">
<ORIGINAL_TEXT>In other words, when engineering occurs, it’s usually to bring about a meaningful change to the virus — but there’s no evidence of that in the 2019-nCoV genome.</ORIGINAL_TEXT>
<TOKEN id="token-61-0" pos="word" morph="none" start_char="9251" end_char="9252">In</TOKEN>
<TOKEN id="token-61-1" pos="word" morph="none" start_char="9254" end_char="9258">other</TOKEN>
<TOKEN id="token-61-2" pos="word" morph="none" start_char="9260" end_char="9264">words</TOKEN>
<TOKEN id="token-61-3" pos="punct" morph="none" start_char="9265" end_char="9265">,</TOKEN>
<TOKEN id="token-61-4" pos="word" morph="none" start_char="9267" end_char="9270">when</TOKEN>
<TOKEN id="token-61-5" pos="word" morph="none" start_char="9272" end_char="9282">engineering</TOKEN>
<TOKEN id="token-61-6" pos="word" morph="none" start_char="9284" end_char="9289">occurs</TOKEN>
<TOKEN id="token-61-7" pos="punct" morph="none" start_char="9290" end_char="9290">,</TOKEN>
<TOKEN id="token-61-8" pos="word" morph="none" start_char="9292" end_char="9295">it’s</TOKEN>
<TOKEN id="token-61-9" pos="word" morph="none" start_char="9297" end_char="9303">usually</TOKEN>
<TOKEN id="token-61-10" pos="word" morph="none" start_char="9305" end_char="9306">to</TOKEN>
<TOKEN id="token-61-11" pos="word" morph="none" start_char="9308" end_char="9312">bring</TOKEN>
<TOKEN id="token-61-12" pos="word" morph="none" start_char="9314" end_char="9318">about</TOKEN>
<TOKEN id="token-61-13" pos="word" morph="none" start_char="9320" end_char="9320">a</TOKEN>
<TOKEN id="token-61-14" pos="word" morph="none" start_char="9322" end_char="9331">meaningful</TOKEN>
<TOKEN id="token-61-15" pos="word" morph="none" start_char="9333" end_char="9338">change</TOKEN>
<TOKEN id="token-61-16" pos="word" morph="none" start_char="9340" end_char="9341">to</TOKEN>
<TOKEN id="token-61-17" pos="word" morph="none" start_char="9343" end_char="9345">the</TOKEN>
<TOKEN id="token-61-18" pos="word" morph="none" start_char="9347" end_char="9351">virus</TOKEN>
<TOKEN id="token-61-19" pos="punct" morph="none" start_char="9353" end_char="9353">—</TOKEN>
<TOKEN id="token-61-20" pos="word" morph="none" start_char="9355" end_char="9357">but</TOKEN>
<TOKEN id="token-61-21" pos="word" morph="none" start_char="9359" end_char="9365">there’s</TOKEN>
<TOKEN id="token-61-22" pos="word" morph="none" start_char="9367" end_char="9368">no</TOKEN>
<TOKEN id="token-61-23" pos="word" morph="none" start_char="9370" end_char="9377">evidence</TOKEN>
<TOKEN id="token-61-24" pos="word" morph="none" start_char="9379" end_char="9380">of</TOKEN>
<TOKEN id="token-61-25" pos="word" morph="none" start_char="9382" end_char="9385">that</TOKEN>
<TOKEN id="token-61-26" pos="word" morph="none" start_char="9387" end_char="9388">in</TOKEN>
<TOKEN id="token-61-27" pos="word" morph="none" start_char="9390" end_char="9392">the</TOKEN>
<TOKEN id="token-61-28" pos="unknown" morph="none" start_char="9394" end_char="9402">2019-nCoV</TOKEN>
<TOKEN id="token-61-29" pos="word" morph="none" start_char="9404" end_char="9409">genome</TOKEN>
<TOKEN id="token-61-30" pos="punct" morph="none" start_char="9410" end_char="9410">.</TOKEN>
</SEG>
<SEG id="segment-62" start_char="9413" end_char="9522">
<ORIGINAL_TEXT>Typically, scientists change nucleotides in a targeted way to create changes in the amino acids they code for.</ORIGINAL_TEXT>
<TOKEN id="token-62-0" pos="word" morph="none" start_char="9413" end_char="9421">Typically</TOKEN>
<TOKEN id="token-62-1" pos="punct" morph="none" start_char="9422" end_char="9422">,</TOKEN>
<TOKEN id="token-62-2" pos="word" morph="none" start_char="9424" end_char="9433">scientists</TOKEN>
<TOKEN id="token-62-3" pos="word" morph="none" start_char="9435" end_char="9440">change</TOKEN>
<TOKEN id="token-62-4" pos="word" morph="none" start_char="9442" end_char="9452">nucleotides</TOKEN>
<TOKEN id="token-62-5" pos="word" morph="none" start_char="9454" end_char="9455">in</TOKEN>
<TOKEN id="token-62-6" pos="word" morph="none" start_char="9457" end_char="9457">a</TOKEN>
<TOKEN id="token-62-7" pos="word" morph="none" start_char="9459" end_char="9466">targeted</TOKEN>
<TOKEN id="token-62-8" pos="word" morph="none" start_char="9468" end_char="9470">way</TOKEN>
<TOKEN id="token-62-9" pos="word" morph="none" start_char="9472" end_char="9473">to</TOKEN>
<TOKEN id="token-62-10" pos="word" morph="none" start_char="9475" end_char="9480">create</TOKEN>
<TOKEN id="token-62-11" pos="word" morph="none" start_char="9482" end_char="9488">changes</TOKEN>
<TOKEN id="token-62-12" pos="word" morph="none" start_char="9490" end_char="9491">in</TOKEN>
<TOKEN id="token-62-13" pos="word" morph="none" start_char="9493" end_char="9495">the</TOKEN>
<TOKEN id="token-62-14" pos="word" morph="none" start_char="9497" end_char="9501">amino</TOKEN>
<TOKEN id="token-62-15" pos="word" morph="none" start_char="9503" end_char="9507">acids</TOKEN>
<TOKEN id="token-62-16" pos="word" morph="none" start_char="9509" end_char="9512">they</TOKEN>
<TOKEN id="token-62-17" pos="word" morph="none" start_char="9514" end_char="9517">code</TOKEN>
<TOKEN id="token-62-18" pos="word" morph="none" start_char="9519" end_char="9521">for</TOKEN>
<TOKEN id="token-62-19" pos="punct" morph="none" start_char="9522" end_char="9522">.</TOKEN>
</SEG>
<SEG id="segment-63" start_char="9524" end_char="9635">
<ORIGINAL_TEXT>Since amino acids are the building blocks of proteins, that’s the way to change the proteins the virus produces.</ORIGINAL_TEXT>
<TOKEN id="token-63-0" pos="word" morph="none" start_char="9524" end_char="9528">Since</TOKEN>
<TOKEN id="token-63-1" pos="word" morph="none" start_char="9530" end_char="9534">amino</TOKEN>
<TOKEN id="token-63-2" pos="word" morph="none" start_char="9536" end_char="9540">acids</TOKEN>
<TOKEN id="token-63-3" pos="word" morph="none" start_char="9542" end_char="9544">are</TOKEN>
<TOKEN id="token-63-4" pos="word" morph="none" start_char="9546" end_char="9548">the</TOKEN>
<TOKEN id="token-63-5" pos="word" morph="none" start_char="9550" end_char="9557">building</TOKEN>
<TOKEN id="token-63-6" pos="word" morph="none" start_char="9559" end_char="9564">blocks</TOKEN>
<TOKEN id="token-63-7" pos="word" morph="none" start_char="9566" end_char="9567">of</TOKEN>
<TOKEN id="token-63-8" pos="word" morph="none" start_char="9569" end_char="9576">proteins</TOKEN>
<TOKEN id="token-63-9" pos="punct" morph="none" start_char="9577" end_char="9577">,</TOKEN>
<TOKEN id="token-63-10" pos="word" morph="none" start_char="9579" end_char="9584">that’s</TOKEN>
<TOKEN id="token-63-11" pos="word" morph="none" start_char="9586" end_char="9588">the</TOKEN>
<TOKEN id="token-63-12" pos="word" morph="none" start_char="9590" end_char="9592">way</TOKEN>
<TOKEN id="token-63-13" pos="word" morph="none" start_char="9594" end_char="9595">to</TOKEN>
<TOKEN id="token-63-14" pos="word" morph="none" start_char="9597" end_char="9602">change</TOKEN>
<TOKEN id="token-63-15" pos="word" morph="none" start_char="9604" end_char="9606">the</TOKEN>
<TOKEN id="token-63-16" pos="word" morph="none" start_char="9608" end_char="9615">proteins</TOKEN>
<TOKEN id="token-63-17" pos="word" morph="none" start_char="9617" end_char="9619">the</TOKEN>
<TOKEN id="token-63-18" pos="word" morph="none" start_char="9621" end_char="9625">virus</TOKEN>
<TOKEN id="token-63-19" pos="word" morph="none" start_char="9627" end_char="9634">produces</TOKEN>
<TOKEN id="token-63-20" pos="punct" morph="none" start_char="9635" end_char="9635">.</TOKEN>
</SEG>
<SEG id="segment-64" start_char="9638" end_char="9820">
<ORIGINAL_TEXT>But as Bedford said, out of all the nucleotide changes, relatively few — around 14% — alter the corresponding amino acid, or about what you would expect in a naturally evolving virus.</ORIGINAL_TEXT>
<TOKEN id="token-64-0" pos="word" morph="none" start_char="9638" end_char="9640">But</TOKEN>
<TOKEN id="token-64-1" pos="word" morph="none" start_char="9642" end_char="9643">as</TOKEN>
<TOKEN id="token-64-2" pos="word" morph="none" start_char="9645" end_char="9651">Bedford</TOKEN>
<TOKEN id="token-64-3" pos="word" morph="none" start_char="9653" end_char="9656">said</TOKEN>
<TOKEN id="token-64-4" pos="punct" morph="none" start_char="9657" end_char="9657">,</TOKEN>
<TOKEN id="token-64-5" pos="word" morph="none" start_char="9659" end_char="9661">out</TOKEN>
<TOKEN id="token-64-6" pos="word" morph="none" start_char="9663" end_char="9664">of</TOKEN>
<TOKEN id="token-64-7" pos="word" morph="none" start_char="9666" end_char="9668">all</TOKEN>
<TOKEN id="token-64-8" pos="word" morph="none" start_char="9670" end_char="9672">the</TOKEN>
<TOKEN id="token-64-9" pos="word" morph="none" start_char="9674" end_char="9683">nucleotide</TOKEN>
<TOKEN id="token-64-10" pos="word" morph="none" start_char="9685" end_char="9691">changes</TOKEN>
<TOKEN id="token-64-11" pos="punct" morph="none" start_char="9692" end_char="9692">,</TOKEN>
<TOKEN id="token-64-12" pos="word" morph="none" start_char="9694" end_char="9703">relatively</TOKEN>
<TOKEN id="token-64-13" pos="word" morph="none" start_char="9705" end_char="9707">few</TOKEN>
<TOKEN id="token-64-14" pos="punct" morph="none" start_char="9709" end_char="9709">—</TOKEN>
<TOKEN id="token-64-15" pos="word" morph="none" start_char="9711" end_char="9716">around</TOKEN>
<TOKEN id="token-64-16" pos="word" morph="none" start_char="9718" end_char="9719">14</TOKEN>
<TOKEN id="token-64-17" pos="punct" morph="none" start_char="9720" end_char="9720">%</TOKEN>
<TOKEN id="token-64-18" pos="punct" morph="none" start_char="9722" end_char="9722">—</TOKEN>
<TOKEN id="token-64-19" pos="word" morph="none" start_char="9724" end_char="9728">alter</TOKEN>
<TOKEN id="token-64-20" pos="word" morph="none" start_char="9730" end_char="9732">the</TOKEN>
<TOKEN id="token-64-21" pos="word" morph="none" start_char="9734" end_char="9746">corresponding</TOKEN>
<TOKEN id="token-64-22" pos="word" morph="none" start_char="9748" end_char="9752">amino</TOKEN>
<TOKEN id="token-64-23" pos="word" morph="none" start_char="9754" end_char="9757">acid</TOKEN>
<TOKEN id="token-64-24" pos="punct" morph="none" start_char="9758" end_char="9758">,</TOKEN>
<TOKEN id="token-64-25" pos="word" morph="none" start_char="9760" end_char="9761">or</TOKEN>
<TOKEN id="token-64-26" pos="word" morph="none" start_char="9763" end_char="9767">about</TOKEN>
<TOKEN id="token-64-27" pos="word" morph="none" start_char="9769" end_char="9772">what</TOKEN>
<TOKEN id="token-64-28" pos="word" morph="none" start_char="9774" end_char="9776">you</TOKEN>
<TOKEN id="token-64-29" pos="word" morph="none" start_char="9778" end_char="9782">would</TOKEN>
<TOKEN id="token-64-30" pos="word" morph="none" start_char="9784" end_char="9789">expect</TOKEN>
<TOKEN id="token-64-31" pos="word" morph="none" start_char="9791" end_char="9792">in</TOKEN>
<TOKEN id="token-64-32" pos="word" morph="none" start_char="9794" end_char="9794">a</TOKEN>
<TOKEN id="token-64-33" pos="word" morph="none" start_char="9796" end_char="9804">naturally</TOKEN>
<TOKEN id="token-64-34" pos="word" morph="none" start_char="9806" end_char="9813">evolving</TOKEN>
<TOKEN id="token-64-35" pos="word" morph="none" start_char="9815" end_char="9819">virus</TOKEN>
<TOKEN id="token-64-36" pos="punct" morph="none" start_char="9820" end_char="9820">.</TOKEN>
</SEG>
<SEG id="segment-65" start_char="9822" end_char="9916">
<ORIGINAL_TEXT>This ratio also matches that of the bat virus that’s found to be the most similar to 2019-nCoV.</ORIGINAL_TEXT>
<TOKEN id="token-65-0" pos="word" morph="none" start_char="9822" end_char="9825">This</TOKEN>
<TOKEN id="token-65-1" pos="word" morph="none" start_char="9827" end_char="9831">ratio</TOKEN>
<TOKEN id="token-65-2" pos="word" morph="none" start_char="9833" end_char="9836">also</TOKEN>
<TOKEN id="token-65-3" pos="word" morph="none" start_char="9838" end_char="9844">matches</TOKEN>
<TOKEN id="token-65-4" pos="word" morph="none" start_char="9846" end_char="9849">that</TOKEN>
<TOKEN id="token-65-5" pos="word" morph="none" start_char="9851" end_char="9852">of</TOKEN>
<TOKEN id="token-65-6" pos="word" morph="none" start_char="9854" end_char="9856">the</TOKEN>
<TOKEN id="token-65-7" pos="word" morph="none" start_char="9858" end_char="9860">bat</TOKEN>
<TOKEN id="token-65-8" pos="word" morph="none" start_char="9862" end_char="9866">virus</TOKEN>
<TOKEN id="token-65-9" pos="word" morph="none" start_char="9868" end_char="9873">that’s</TOKEN>
<TOKEN id="token-65-10" pos="word" morph="none" start_char="9875" end_char="9879">found</TOKEN>
<TOKEN id="token-65-11" pos="word" morph="none" start_char="9881" end_char="9882">to</TOKEN>
<TOKEN id="token-65-12" pos="word" morph="none" start_char="9884" end_char="9885">be</TOKEN>
<TOKEN id="token-65-13" pos="word" morph="none" start_char="9887" end_char="9889">the</TOKEN>
<TOKEN id="token-65-14" pos="word" morph="none" start_char="9891" end_char="9894">most</TOKEN>
<TOKEN id="token-65-15" pos="word" morph="none" start_char="9896" end_char="9902">similar</TOKEN>
<TOKEN id="token-65-16" pos="word" morph="none" start_char="9904" end_char="9905">to</TOKEN>
<TOKEN id="token-65-17" pos="unknown" morph="none" start_char="9907" end_char="9915">2019-nCoV</TOKEN>
<TOKEN id="token-65-18" pos="punct" morph="none" start_char="9916" end_char="9916">.</TOKEN>
</SEG>
<SEG id="segment-66" start_char="9919" end_char="10081">
<ORIGINAL_TEXT>Further, when comparing the amino acid changes that do exist, the number of changes in the respective genes in both 2019-nCoV and the bat virus are highly similar.</ORIGINAL_TEXT>
<TOKEN id="token-66-0" pos="word" morph="none" start_char="9919" end_char="9925">Further</TOKEN>
<TOKEN id="token-66-1" pos="punct" morph="none" start_char="9926" end_char="9926">,</TOKEN>
<TOKEN id="token-66-2" pos="word" morph="none" start_char="9928" end_char="9931">when</TOKEN>
<TOKEN id="token-66-3" pos="word" morph="none" start_char="9933" end_char="9941">comparing</TOKEN>
<TOKEN id="token-66-4" pos="word" morph="none" start_char="9943" end_char="9945">the</TOKEN>
<TOKEN id="token-66-5" pos="word" morph="none" start_char="9947" end_char="9951">amino</TOKEN>
<TOKEN id="token-66-6" pos="word" morph="none" start_char="9953" end_char="9956">acid</TOKEN>
<TOKEN id="token-66-7" pos="word" morph="none" start_char="9958" end_char="9964">changes</TOKEN>
<TOKEN id="token-66-8" pos="word" morph="none" start_char="9966" end_char="9969">that</TOKEN>
<TOKEN id="token-66-9" pos="word" morph="none" start_char="9971" end_char="9972">do</TOKEN>
<TOKEN id="token-66-10" pos="word" morph="none" start_char="9974" end_char="9978">exist</TOKEN>
<TOKEN id="token-66-11" pos="punct" morph="none" start_char="9979" end_char="9979">,</TOKEN>
<TOKEN id="token-66-12" pos="word" morph="none" start_char="9981" end_char="9983">the</TOKEN>
<TOKEN id="token-66-13" pos="word" morph="none" start_char="9985" end_char="9990">number</TOKEN>
<TOKEN id="token-66-14" pos="word" morph="none" start_char="9992" end_char="9993">of</TOKEN>
<TOKEN id="token-66-15" pos="word" morph="none" start_char="9995" end_char="10001">changes</TOKEN>
<TOKEN id="token-66-16" pos="word" morph="none" start_char="10003" end_char="10004">in</TOKEN>
<TOKEN id="token-66-17" pos="word" morph="none" start_char="10006" end_char="10008">the</TOKEN>
<TOKEN id="token-66-18" pos="word" morph="none" start_char="10010" end_char="10019">respective</TOKEN>
<TOKEN id="token-66-19" pos="word" morph="none" start_char="10021" end_char="10025">genes</TOKEN>
<TOKEN id="token-66-20" pos="word" morph="none" start_char="10027" end_char="10028">in</TOKEN>
<TOKEN id="token-66-21" pos="word" morph="none" start_char="10030" end_char="10033">both</TOKEN>
<TOKEN id="token-66-22" pos="unknown" morph="none" start_char="10035" end_char="10043">2019-nCoV</TOKEN>
<TOKEN id="token-66-23" pos="word" morph="none" start_char="10045" end_char="10047">and</TOKEN>
<TOKEN id="token-66-24" pos="word" morph="none" start_char="10049" end_char="10051">the</TOKEN>
<TOKEN id="token-66-25" pos="word" morph="none" start_char="10053" end_char="10055">bat</TOKEN>
<TOKEN id="token-66-26" pos="word" morph="none" start_char="10057" end_char="10061">virus</TOKEN>
<TOKEN id="token-66-27" pos="word" morph="none" start_char="10063" end_char="10065">are</TOKEN>
<TOKEN id="token-66-28" pos="word" morph="none" start_char="10067" end_char="10072">highly</TOKEN>
<TOKEN id="token-66-29" pos="word" morph="none" start_char="10074" end_char="10080">similar</TOKEN>
<TOKEN id="token-66-30" pos="punct" morph="none" start_char="10081" end_char="10081">.</TOKEN>
</SEG>
<SEG id="segment-67" start_char="10083" end_char="10221">
<ORIGINAL_TEXT>Again, if the virus had been engineered, one might expect many of the changes to cluster in one or two genes, but that’s not the case here.</ORIGINAL_TEXT>
<TOKEN id="token-67-0" pos="word" morph="none" start_char="10083" end_char="10087">Again</TOKEN>
<TOKEN id="token-67-1" pos="punct" morph="none" start_char="10088" end_char="10088">,</TOKEN>
<TOKEN id="token-67-2" pos="word" morph="none" start_char="10090" end_char="10091">if</TOKEN>
<TOKEN id="token-67-3" pos="word" morph="none" start_char="10093" end_char="10095">the</TOKEN>
<TOKEN id="token-67-4" pos="word" morph="none" start_char="10097" end_char="10101">virus</TOKEN>
<TOKEN id="token-67-5" pos="word" morph="none" start_char="10103" end_char="10105">had</TOKEN>
<TOKEN id="token-67-6" pos="word" morph="none" start_char="10107" end_char="10110">been</TOKEN>
<TOKEN id="token-67-7" pos="word" morph="none" start_char="10112" end_char="10121">engineered</TOKEN>
<TOKEN id="token-67-8" pos="punct" morph="none" start_char="10122" end_char="10122">,</TOKEN>
<TOKEN id="token-67-9" pos="word" morph="none" start_char="10124" end_char="10126">one</TOKEN>
<TOKEN id="token-67-10" pos="word" morph="none" start_char="10128" end_char="10132">might</TOKEN>
<TOKEN id="token-67-11" pos="word" morph="none" start_char="10134" end_char="10139">expect</TOKEN>
<TOKEN id="token-67-12" pos="word" morph="none" start_char="10141" end_char="10144">many</TOKEN>
<TOKEN id="token-67-13" pos="word" morph="none" start_char="10146" end_char="10147">of</TOKEN>
<TOKEN id="token-67-14" pos="word" morph="none" start_char="10149" end_char="10151">the</TOKEN>
<TOKEN id="token-67-15" pos="word" morph="none" start_char="10153" end_char="10159">changes</TOKEN>
<TOKEN id="token-67-16" pos="word" morph="none" start_char="10161" end_char="10162">to</TOKEN>
<TOKEN id="token-67-17" pos="word" morph="none" start_char="10164" end_char="10170">cluster</TOKEN>
<TOKEN id="token-67-18" pos="word" morph="none" start_char="10172" end_char="10173">in</TOKEN>
<TOKEN id="token-67-19" pos="word" morph="none" start_char="10175" end_char="10177">one</TOKEN>
<TOKEN id="token-67-20" pos="word" morph="none" start_char="10179" end_char="10180">or</TOKEN>
<TOKEN id="token-67-21" pos="word" morph="none" start_char="10182" end_char="10184">two</TOKEN>
<TOKEN id="token-67-22" pos="word" morph="none" start_char="10186" end_char="10190">genes</TOKEN>
<TOKEN id="token-67-23" pos="punct" morph="none" start_char="10191" end_char="10191">,</TOKEN>
<TOKEN id="token-67-24" pos="word" morph="none" start_char="10193" end_char="10195">but</TOKEN>
<TOKEN id="token-67-25" pos="word" morph="none" start_char="10197" end_char="10202">that’s</TOKEN>
<TOKEN id="token-67-26" pos="word" morph="none" start_char="10204" end_char="10206">not</TOKEN>
<TOKEN id="token-67-27" pos="word" morph="none" start_char="10208" end_char="10210">the</TOKEN>
<TOKEN id="token-67-28" pos="word" morph="none" start_char="10212" end_char="10215">case</TOKEN>
<TOKEN id="token-67-29" pos="word" morph="none" start_char="10217" end_char="10220">here</TOKEN>
<TOKEN id="token-67-30" pos="punct" morph="none" start_char="10221" end_char="10221">.</TOKEN>
</SEG>
<SEG id="segment-68" start_char="10223" end_char="10300">
<ORIGINAL_TEXT>All of this argues against the idea of the new virus having come out of a lab.</ORIGINAL_TEXT>
<TOKEN id="token-68-0" pos="word" morph="none" start_char="10223" end_char="10225">All</TOKEN>
<TOKEN id="token-68-1" pos="word" morph="none" start_char="10227" end_char="10228">of</TOKEN>
<TOKEN id="token-68-2" pos="word" morph="none" start_char="10230" end_char="10233">this</TOKEN>
<TOKEN id="token-68-3" pos="word" morph="none" start_char="10235" end_char="10240">argues</TOKEN>
<TOKEN id="token-68-4" pos="word" morph="none" start_char="10242" end_char="10248">against</TOKEN>
<TOKEN id="token-68-5" pos="word" morph="none" start_char="10250" end_char="10252">the</TOKEN>
<TOKEN id="token-68-6" pos="word" morph="none" start_char="10254" end_char="10257">idea</TOKEN>
<TOKEN id="token-68-7" pos="word" morph="none" start_char="10259" end_char="10260">of</TOKEN>
<TOKEN id="token-68-8" pos="word" morph="none" start_char="10262" end_char="10264">the</TOKEN>
<TOKEN id="token-68-9" pos="word" morph="none" start_char="10266" end_char="10268">new</TOKEN>
<TOKEN id="token-68-10" pos="word" morph="none" start_char="10270" end_char="10274">virus</TOKEN>
<TOKEN id="token-68-11" pos="word" morph="none" start_char="10276" end_char="10281">having</TOKEN>
<TOKEN id="token-68-12" pos="word" morph="none" start_char="10283" end_char="10286">come</TOKEN>
<TOKEN id="token-68-13" pos="word" morph="none" start_char="10288" end_char="10290">out</TOKEN>
<TOKEN id="token-68-14" pos="word" morph="none" start_char="10292" end_char="10293">of</TOKEN>
<TOKEN id="token-68-15" pos="word" morph="none" start_char="10295" end_char="10295">a</TOKEN>
<TOKEN id="token-68-16" pos="word" morph="none" start_char="10297" end_char="10299">lab</TOKEN>
<TOKEN id="token-68-17" pos="punct" morph="none" start_char="10300" end_char="10300">.</TOKEN>
</SEG>
<SEG id="segment-69" start_char="10303" end_char="10362">
<ORIGINAL_TEXT>Editor’s note: FactCheck.org is one of several organizations</ORIGINAL_TEXT>
<TOKEN id="token-69-0" pos="word" morph="none" start_char="10303" end_char="10310">Editor’s</TOKEN>
<TOKEN id="token-69-1" pos="word" morph="none" start_char="10312" end_char="10315">note</TOKEN>
<TOKEN id="token-69-2" pos="punct" morph="none" start_char="10316" end_char="10316">:</TOKEN>
<TOKEN id="token-69-3" pos="unknown" morph="none" start_char="10318" end_char="10330">FactCheck.org</TOKEN>
<TOKEN id="token-69-4" pos="word" morph="none" start_char="10332" end_char="10333">is</TOKEN>
<TOKEN id="token-69-5" pos="word" morph="none" start_char="10335" end_char="10337">one</TOKEN>
<TOKEN id="token-69-6" pos="word" morph="none" start_char="10339" end_char="10340">of</TOKEN>
<TOKEN id="token-69-7" pos="word" morph="none" start_char="10342" end_char="10348">several</TOKEN>
<TOKEN id="token-69-8" pos="word" morph="none" start_char="10350" end_char="10362">organizations</TOKEN>
</SEG>
<SEG id="segment-70" start_char="10365" end_char="10385">
<ORIGINAL_TEXT>working with Facebook</ORIGINAL_TEXT>
<TOKEN id="token-70-0" pos="word" morph="none" start_char="10365" end_char="10371">working</TOKEN>
<TOKEN id="token-70-1" pos="word" morph="none" start_char="10373" end_char="10376">with</TOKEN>
<TOKEN id="token-70-2" pos="word" morph="none" start_char="10378" end_char="10385">Facebook</TOKEN>
</SEG>
<SEG id="segment-71" start_char="10388" end_char="10435">
<ORIGINAL_TEXT>to debunk misinformation shared on social media.</ORIGINAL_TEXT>
<TOKEN id="token-71-0" pos="word" morph="none" start_char="10388" end_char="10389">to</TOKEN>
<TOKEN id="token-71-1" pos="word" morph="none" start_char="10391" end_char="10396">debunk</TOKEN>
<TOKEN id="token-71-2" pos="word" morph="none" start_char="10398" end_char="10411">misinformation</TOKEN>
<TOKEN id="token-71-3" pos="word" morph="none" start_char="10413" end_char="10418">shared</TOKEN>
<TOKEN id="token-71-4" pos="word" morph="none" start_char="10420" end_char="10421">on</TOKEN>
<TOKEN id="token-71-5" pos="word" morph="none" start_char="10423" end_char="10428">social</TOKEN>
<TOKEN id="token-71-6" pos="word" morph="none" start_char="10430" end_char="10434">media</TOKEN>
<TOKEN id="token-71-7" pos="punct" morph="none" start_char="10435" end_char="10435">.</TOKEN>
</SEG>
<SEG id="segment-72" start_char="10437" end_char="10475">
<ORIGINAL_TEXT>Our previous stories can be found here.</ORIGINAL_TEXT>
<TOKEN id="token-72-0" pos="word" morph="none" start_char="10437" end_char="10439">Our</TOKEN>
<TOKEN id="token-72-1" pos="word" morph="none" start_char="10441" end_char="10448">previous</TOKEN>
<TOKEN id="token-72-2" pos="word" morph="none" start_char="10450" end_char="10456">stories</TOKEN>
<TOKEN id="token-72-3" pos="word" morph="none" start_char="10458" end_char="10460">can</TOKEN>
<TOKEN id="token-72-4" pos="word" morph="none" start_char="10462" end_char="10463">be</TOKEN>
<TOKEN id="token-72-5" pos="word" morph="none" start_char="10465" end_char="10469">found</TOKEN>
<TOKEN id="token-72-6" pos="word" morph="none" start_char="10471" end_char="10474">here</TOKEN>
<TOKEN id="token-72-7" pos="punct" morph="none" start_char="10475" end_char="10475">.</TOKEN>
</SEG>
<SEG id="segment-73" start_char="10479" end_char="10485">
<ORIGINAL_TEXT>Sources</ORIGINAL_TEXT>
<TOKEN id="token-73-0" pos="word" morph="none" start_char="10479" end_char="10485">Sources</TOKEN>
</SEG>
<SEG id="segment-74" start_char="10489" end_char="10506">
<ORIGINAL_TEXT>McDonald, Jessica.</ORIGINAL_TEXT>
<TOKEN id="token-74-0" pos="word" morph="none" start_char="10489" end_char="10496">McDonald</TOKEN>
<TOKEN id="token-74-1" pos="punct" morph="none" start_char="10497" end_char="10497">,</TOKEN>
<TOKEN id="token-74-2" pos="word" morph="none" start_char="10499" end_char="10505">Jessica</TOKEN>
<TOKEN id="token-74-3" pos="punct" morph="none" start_char="10506" end_char="10506">.</TOKEN>
</SEG>
<SEG id="segment-75" start_char="10508" end_char="10536">
<ORIGINAL_TEXT>"Q on the Wuhan Coronavirus."</ORIGINAL_TEXT>
<TOKEN id="token-75-0" pos="punct" morph="none" start_char="10508" end_char="10508">"</TOKEN>
<TOKEN id="token-75-1" pos="word" morph="none" start_char="10509" end_char="10509">Q</TOKEN>
<TOKEN id="token-75-2" pos="word" morph="none" start_char="10511" end_char="10512">on</TOKEN>
<TOKEN id="token-75-3" pos="word" morph="none" start_char="10514" end_char="10516">the</TOKEN>
<TOKEN id="token-75-4" pos="word" morph="none" start_char="10518" end_char="10522">Wuhan</TOKEN>
<TOKEN id="token-75-5" pos="word" morph="none" start_char="10524" end_char="10534">Coronavirus</TOKEN>
<TOKEN id="token-75-6" pos="punct" morph="none" start_char="10535" end_char="10536">."</TOKEN>
</SEG>
<SEG id="segment-76" start_char="10538" end_char="10551">
<ORIGINAL_TEXT>FactCheck.org.</ORIGINAL_TEXT>
<TOKEN id="token-76-0" pos="unknown" morph="none" start_char="10538" end_char="10550">FactCheck.org</TOKEN>
<TOKEN id="token-76-1" pos="punct" morph="none" start_char="10551" end_char="10551">.</TOKEN>
</SEG>
<SEG id="segment-77" start_char="10553" end_char="10564">
<ORIGINAL_TEXT>30 Jan 2020.</ORIGINAL_TEXT>
<TOKEN id="token-77-0" pos="word" morph="none" start_char="10553" end_char="10554">30</TOKEN>
<TOKEN id="token-77-1" pos="word" morph="none" start_char="10556" end_char="10558">Jan</TOKEN>
<TOKEN id="token-77-2" pos="word" morph="none" start_char="10560" end_char="10563">2020</TOKEN>
<TOKEN id="token-77-3" pos="punct" morph="none" start_char="10564" end_char="10564">.</TOKEN>
</SEG>
<SEG id="segment-78" start_char="10567" end_char="10584">
<ORIGINAL_TEXT>McDonald, Jessica.</ORIGINAL_TEXT>
<TOKEN id="token-78-0" pos="word" morph="none" start_char="10567" end_char="10574">McDonald</TOKEN>
<TOKEN id="token-78-1" pos="punct" morph="none" start_char="10575" end_char="10575">,</TOKEN>
<TOKEN id="token-78-2" pos="word" morph="none" start_char="10577" end_char="10583">Jessica</TOKEN>
<TOKEN id="token-78-3" pos="punct" morph="none" start_char="10584" end_char="10584">.</TOKEN>
</SEG>
<SEG id="segment-79" start_char="10586" end_char="10649">
<ORIGINAL_TEXT>"Social Media Posts Spread Bogus Coronavirus Conspiracy Theory."</ORIGINAL_TEXT>
<TOKEN id="token-79-0" pos="punct" morph="none" start_char="10586" end_char="10586">"</TOKEN>
<TOKEN id="token-79-1" pos="word" morph="none" start_char="10587" end_char="10592">Social</TOKEN>
<TOKEN id="token-79-2" pos="word" morph="none" start_char="10594" end_char="10598">Media</TOKEN>
<TOKEN id="token-79-3" pos="word" morph="none" start_char="10600" end_char="10604">Posts</TOKEN>
<TOKEN id="token-79-4" pos="word" morph="none" start_char="10606" end_char="10611">Spread</TOKEN>
<TOKEN id="token-79-5" pos="word" morph="none" start_char="10613" end_char="10617">Bogus</TOKEN>
<TOKEN id="token-79-6" pos="word" morph="none" start_char="10619" end_char="10629">Coronavirus</TOKEN>
<TOKEN id="token-79-7" pos="word" morph="none" start_char="10631" end_char="10640">Conspiracy</TOKEN>
<TOKEN id="token-79-8" pos="word" morph="none" start_char="10642" end_char="10647">Theory</TOKEN>
<TOKEN id="token-79-9" pos="punct" morph="none" start_char="10648" end_char="10649">."</TOKEN>
</SEG>
<SEG id="segment-80" start_char="10651" end_char="10664">
<ORIGINAL_TEXT>FactCheck.org.</ORIGINAL_TEXT>
<TOKEN id="token-80-0" pos="unknown" morph="none" start_char="10651" end_char="10663">FactCheck.org</TOKEN>
<TOKEN id="token-80-1" pos="punct" morph="none" start_char="10664" end_char="10664">.</TOKEN>
</SEG>
<SEG id="segment-81" start_char="10666" end_char="10677">
<ORIGINAL_TEXT>24 Jan 2020.</ORIGINAL_TEXT>
<TOKEN id="token-81-0" pos="word" morph="none" start_char="10666" end_char="10667">24</TOKEN>
<TOKEN id="token-81-1" pos="word" morph="none" start_char="10669" end_char="10671">Jan</TOKEN>
<TOKEN id="token-81-2" pos="word" morph="none" start_char="10673" end_char="10676">2020</TOKEN>
<TOKEN id="token-81-3" pos="punct" morph="none" start_char="10677" end_char="10677">.</TOKEN>
</SEG>
<SEG id="segment-82" start_char="10680" end_char="10709">
<ORIGINAL_TEXT>Novel coronavirus (2019-nCoV).</ORIGINAL_TEXT>
<TOKEN id="token-82-0" pos="word" morph="none" start_char="10680" end_char="10684">Novel</TOKEN>
<TOKEN id="token-82-1" pos="word" morph="none" start_char="10686" end_char="10696">coronavirus</TOKEN>
<TOKEN id="token-82-2" pos="punct" morph="none" start_char="10698" end_char="10698">(</TOKEN>
<TOKEN id="token-82-3" pos="unknown" morph="none" start_char="10699" end_char="10707">2019-nCoV</TOKEN>
<TOKEN id="token-82-4" pos="punct" morph="none" start_char="10708" end_char="10709">).</TOKEN>
</SEG>
<SEG id="segment-83" start_char="10711" end_char="10736">
<ORIGINAL_TEXT>World Health Organization.</ORIGINAL_TEXT>
<TOKEN id="token-83-0" pos="word" morph="none" start_char="10711" end_char="10715">World</TOKEN>
<TOKEN id="token-83-1" pos="word" morph="none" start_char="10717" end_char="10722">Health</TOKEN>
<TOKEN id="token-83-2" pos="word" morph="none" start_char="10724" end_char="10735">Organization</TOKEN>
<TOKEN id="token-83-3" pos="punct" morph="none" start_char="10736" end_char="10736">.</TOKEN>
</SEG>
<SEG id="segment-84" start_char="10738" end_char="10757">
<ORIGINAL_TEXT>Accessed 7 Feb 2020.</ORIGINAL_TEXT>
<TOKEN id="token-84-0" pos="word" morph="none" start_char="10738" end_char="10745">Accessed</TOKEN>
<TOKEN id="token-84-1" pos="word" morph="none" start_char="10747" end_char="10747">7</TOKEN>
<TOKEN id="token-84-2" pos="word" morph="none" start_char="10749" end_char="10751">Feb</TOKEN>
<TOKEN id="token-84-3" pos="word" morph="none" start_char="10753" end_char="10756">2020</TOKEN>
<TOKEN id="token-84-4" pos="punct" morph="none" start_char="10757" end_char="10757">.</TOKEN>
</SEG>
<SEG id="segment-85" start_char="10760" end_char="10780">
<ORIGINAL_TEXT>Pradhan, Prashant et.</ORIGINAL_TEXT>
<TOKEN id="token-85-0" pos="word" morph="none" start_char="10760" end_char="10766">Pradhan</TOKEN>
<TOKEN id="token-85-1" pos="punct" morph="none" start_char="10767" end_char="10767">,</TOKEN>
<TOKEN id="token-85-2" pos="word" morph="none" start_char="10769" end_char="10776">Prashant</TOKEN>
<TOKEN id="token-85-3" pos="word" morph="none" start_char="10778" end_char="10779">et</TOKEN>
<TOKEN id="token-85-4" pos="punct" morph="none" start_char="10780" end_char="10780">.</TOKEN>
</SEG>
<SEG id="segment-86" start_char="10782" end_char="10784">
<ORIGINAL_TEXT>al.</ORIGINAL_TEXT>
<TOKEN id="token-86-0" pos="word" morph="none" start_char="10782" end_char="10783">al</TOKEN>
<TOKEN id="token-86-1" pos="punct" morph="none" start_char="10784" end_char="10784">.</TOKEN>
</SEG>
<SEG id="segment-87" start_char="10786" end_char="10876">
<ORIGINAL_TEXT>Uncanny similarity of unique inserts in the 2019-nCoV spike protein to HIV-1 gp120 and Gag.</ORIGINAL_TEXT>
<TOKEN id="token-87-0" pos="word" morph="none" start_char="10786" end_char="10792">Uncanny</TOKEN>
<TOKEN id="token-87-1" pos="word" morph="none" start_char="10794" end_char="10803">similarity</TOKEN>
<TOKEN id="token-87-2" pos="word" morph="none" start_char="10805" end_char="10806">of</TOKEN>
<TOKEN id="token-87-3" pos="word" morph="none" start_char="10808" end_char="10813">unique</TOKEN>
<TOKEN id="token-87-4" pos="word" morph="none" start_char="10815" end_char="10821">inserts</TOKEN>
<TOKEN id="token-87-5" pos="word" morph="none" start_char="10823" end_char="10824">in</TOKEN>
<TOKEN id="token-87-6" pos="word" morph="none" start_char="10826" end_char="10828">the</TOKEN>
<TOKEN id="token-87-7" pos="unknown" morph="none" start_char="10830" end_char="10838">2019-nCoV</TOKEN>
<TOKEN id="token-87-8" pos="word" morph="none" start_char="10840" end_char="10844">spike</TOKEN>
<TOKEN id="token-87-9" pos="word" morph="none" start_char="10846" end_char="10852">protein</TOKEN>
<TOKEN id="token-87-10" pos="word" morph="none" start_char="10854" end_char="10855">to</TOKEN>
<TOKEN id="token-87-11" pos="unknown" morph="none" start_char="10857" end_char="10861">HIV-1</TOKEN>
<TOKEN id="token-87-12" pos="word" morph="none" start_char="10863" end_char="10867">gp120</TOKEN>
<TOKEN id="token-87-13" pos="word" morph="none" start_char="10869" end_char="10871">and</TOKEN>
<TOKEN id="token-87-14" pos="word" morph="none" start_char="10873" end_char="10875">Gag</TOKEN>
<TOKEN id="token-87-15" pos="punct" morph="none" start_char="10876" end_char="10876">.</TOKEN>
</SEG>
<SEG id="segment-88" start_char="10878" end_char="10885">
<ORIGINAL_TEXT>bioRxiv.</ORIGINAL_TEXT>
<TOKEN id="token-88-0" pos="word" morph="none" start_char="10878" end_char="10884">bioRxiv</TOKEN>
<TOKEN id="token-88-1" pos="punct" morph="none" start_char="10885" end_char="10885">.</TOKEN>
</SEG>
<SEG id="segment-89" start_char="10887" end_char="10898">
<ORIGINAL_TEXT>31 Jan 2020.</ORIGINAL_TEXT>
<TOKEN id="token-89-0" pos="word" morph="none" start_char="10887" end_char="10888">31</TOKEN>
<TOKEN id="token-89-1" pos="word" morph="none" start_char="10890" end_char="10892">Jan</TOKEN>
<TOKEN id="token-89-2" pos="word" morph="none" start_char="10894" end_char="10897">2020</TOKEN>
<TOKEN id="token-89-3" pos="punct" morph="none" start_char="10898" end_char="10898">.</TOKEN>
</SEG>
<SEG id="segment-90" start_char="10901" end_char="10933">
<ORIGINAL_TEXT>Konermann, Silvana (@SKonermann).</ORIGINAL_TEXT>
<TOKEN id="token-90-0" pos="word" morph="none" start_char="10901" end_char="10909">Konermann</TOKEN>
<TOKEN id="token-90-1" pos="punct" morph="none" start_char="10910" end_char="10910">,</TOKEN>
<TOKEN id="token-90-2" pos="word" morph="none" start_char="10912" end_char="10918">Silvana</TOKEN>
<TOKEN id="token-90-3" pos="punct" morph="none" start_char="10920" end_char="10921">(@</TOKEN>
<TOKEN id="token-90-4" pos="word" morph="none" start_char="10922" end_char="10931">SKonermann</TOKEN>
<TOKEN id="token-90-5" pos="punct" morph="none" start_char="10932" end_char="10933">).</TOKEN>
</SEG>
<SEG id="segment-91" start_char="10935" end_char="10962">
<ORIGINAL_TEXT>"Just checked their results.</ORIGINAL_TEXT>
<TOKEN id="token-91-0" pos="punct" morph="none" start_char="10935" end_char="10935">"</TOKEN>
<TOKEN id="token-91-1" pos="word" morph="none" start_char="10936" end_char="10939">Just</TOKEN>
<TOKEN id="token-91-2" pos="word" morph="none" start_char="10941" end_char="10947">checked</TOKEN>
<TOKEN id="token-91-3" pos="word" morph="none" start_char="10949" end_char="10953">their</TOKEN>
<TOKEN id="token-91-4" pos="word" morph="none" start_char="10955" end_char="10961">results</TOKEN>
<TOKEN id="token-91-5" pos="punct" morph="none" start_char="10962" end_char="10962">.</TOKEN>
</SEG>
<SEG id="segment-92" start_char="10964" end_char="10990">
<ORIGINAL_TEXT>The similarity is spurious.</ORIGINAL_TEXT>
<TOKEN id="token-92-0" pos="word" morph="none" start_char="10964" end_char="10966">The</TOKEN>
<TOKEN id="token-92-1" pos="word" morph="none" start_char="10968" end_char="10977">similarity</TOKEN>
<TOKEN id="token-92-2" pos="word" morph="none" start_char="10979" end_char="10980">is</TOKEN>
<TOKEN id="token-92-3" pos="word" morph="none" start_char="10982" end_char="10989">spurious</TOKEN>
<TOKEN id="token-92-4" pos="punct" morph="none" start_char="10990" end_char="10990">.</TOKEN>
</SEG>
<SEG id="segment-93" start_char="10992" end_char="11076">
<ORIGINAL_TEXT>Out of 4 inserts they identify between NCov and SARS, 2 are found in bat coronavirus.</ORIGINAL_TEXT>
<TOKEN id="token-93-0" pos="word" morph="none" start_char="10992" end_char="10994">Out</TOKEN>
<TOKEN id="token-93-1" pos="word" morph="none" start_char="10996" end_char="10997">of</TOKEN>
<TOKEN id="token-93-2" pos="word" morph="none" start_char="10999" end_char="10999">4</TOKEN>
<TOKEN id="token-93-3" pos="word" morph="none" start_char="11001" end_char="11007">inserts</TOKEN>
<TOKEN id="token-93-4" pos="word" morph="none" start_char="11009" end_char="11012">they</TOKEN>
<TOKEN id="token-93-5" pos="word" morph="none" start_char="11014" end_char="11021">identify</TOKEN>
<TOKEN id="token-93-6" pos="word" morph="none" start_char="11023" end_char="11029">between</TOKEN>
<TOKEN id="token-93-7" pos="word" morph="none" start_char="11031" end_char="11034">NCov</TOKEN>
<TOKEN id="token-93-8" pos="word" morph="none" start_char="11036" end_char="11038">and</TOKEN>
<TOKEN id="token-93-9" pos="word" morph="none" start_char="11040" end_char="11043">SARS</TOKEN>
<TOKEN id="token-93-10" pos="punct" morph="none" start_char="11044" end_char="11044">,</TOKEN>
<TOKEN id="token-93-11" pos="word" morph="none" start_char="11046" end_char="11046">2</TOKEN>
<TOKEN id="token-93-12" pos="word" morph="none" start_char="11048" end_char="11050">are</TOKEN>
<TOKEN id="token-93-13" pos="word" morph="none" start_char="11052" end_char="11056">found</TOKEN>
<TOKEN id="token-93-14" pos="word" morph="none" start_char="11058" end_char="11059">in</TOKEN>
<TOKEN id="token-93-15" pos="word" morph="none" start_char="11061" end_char="11063">bat</TOKEN>
<TOKEN id="token-93-16" pos="word" morph="none" start_char="11065" end_char="11075">coronavirus</TOKEN>
<TOKEN id="token-93-17" pos="punct" morph="none" start_char="11076" end_char="11076">.</TOKEN>
</SEG>
<SEG id="segment-94" start_char="11078" end_char="11217">
<ORIGINAL_TEXT>Of the remaining two, only one is most similar to HIV, and is so short (6 AA) that the similarity is not higher than chance given database."</ORIGINAL_TEXT>
<TOKEN id="token-94-0" pos="word" morph="none" start_char="11078" end_char="11079">Of</TOKEN>
<TOKEN id="token-94-1" pos="word" morph="none" start_char="11081" end_char="11083">the</TOKEN>
<TOKEN id="token-94-2" pos="word" morph="none" start_char="11085" end_char="11093">remaining</TOKEN>
<TOKEN id="token-94-3" pos="word" morph="none" start_char="11095" end_char="11097">two</TOKEN>
<TOKEN id="token-94-4" pos="punct" morph="none" start_char="11098" end_char="11098">,</TOKEN>
<TOKEN id="token-94-5" pos="word" morph="none" start_char="11100" end_char="11103">only</TOKEN>
<TOKEN id="token-94-6" pos="word" morph="none" start_char="11105" end_char="11107">one</TOKEN>
<TOKEN id="token-94-7" pos="word" morph="none" start_char="11109" end_char="11110">is</TOKEN>
<TOKEN id="token-94-8" pos="word" morph="none" start_char="11112" end_char="11115">most</TOKEN>
<TOKEN id="token-94-9" pos="word" morph="none" start_char="11117" end_char="11123">similar</TOKEN>
<TOKEN id="token-94-10" pos="word" morph="none" start_char="11125" end_char="11126">to</TOKEN>
<TOKEN id="token-94-11" pos="word" morph="none" start_char="11128" end_char="11130">HIV</TOKEN>
<TOKEN id="token-94-12" pos="punct" morph="none" start_char="11131" end_char="11131">,</TOKEN>
<TOKEN id="token-94-13" pos="word" morph="none" start_char="11133" end_char="11135">and</TOKEN>
<TOKEN id="token-94-14" pos="word" morph="none" start_char="11137" end_char="11138">is</TOKEN>
<TOKEN id="token-94-15" pos="word" morph="none" start_char="11140" end_char="11141">so</TOKEN>
<TOKEN id="token-94-16" pos="word" morph="none" start_char="11143" end_char="11147">short</TOKEN>
<TOKEN id="token-94-17" pos="punct" morph="none" start_char="11149" end_char="11149">(</TOKEN>
<TOKEN id="token-94-18" pos="word" morph="none" start_char="11150" end_char="11150">6</TOKEN>
<TOKEN id="token-94-19" pos="word" morph="none" start_char="11152" end_char="11153">AA</TOKEN>
<TOKEN id="token-94-20" pos="punct" morph="none" start_char="11154" end_char="11154">)</TOKEN>
<TOKEN id="token-94-21" pos="word" morph="none" start_char="11156" end_char="11159">that</TOKEN>
<TOKEN id="token-94-22" pos="word" morph="none" start_char="11161" end_char="11163">the</TOKEN>
<TOKEN id="token-94-23" pos="word" morph="none" start_char="11165" end_char="11174">similarity</TOKEN>
<TOKEN id="token-94-24" pos="word" morph="none" start_char="11176" end_char="11177">is</TOKEN>
<TOKEN id="token-94-25" pos="word" morph="none" start_char="11179" end_char="11181">not</TOKEN>
<TOKEN id="token-94-26" pos="word" morph="none" start_char="11183" end_char="11188">higher</TOKEN>
<TOKEN id="token-94-27" pos="word" morph="none" start_char="11190" end_char="11193">than</TOKEN>
<TOKEN id="token-94-28" pos="word" morph="none" start_char="11195" end_char="11200">chance</TOKEN>
<TOKEN id="token-94-29" pos="word" morph="none" start_char="11202" end_char="11206">given</TOKEN>
<TOKEN id="token-94-30" pos="word" morph="none" start_char="11208" end_char="11215">database</TOKEN>
<TOKEN id="token-94-31" pos="punct" morph="none" start_char="11216" end_char="11217">."</TOKEN>
</SEG>
<SEG id="segment-95" start_char="11219" end_char="11226">
<ORIGINAL_TEXT>Twitter.</ORIGINAL_TEXT>
<TOKEN id="token-95-0" pos="word" morph="none" start_char="11219" end_char="11225">Twitter</TOKEN>
<TOKEN id="token-95-1" pos="punct" morph="none" start_char="11226" end_char="11226">.</TOKEN>
</SEG>
<SEG id="segment-96" start_char="11228" end_char="11239">
<ORIGINAL_TEXT>31 Jan 2020.</ORIGINAL_TEXT>
<TOKEN id="token-96-0" pos="word" morph="none" start_char="11228" end_char="11229">31</TOKEN>
<TOKEN id="token-96-1" pos="word" morph="none" start_char="11231" end_char="11233">Jan</TOKEN>
<TOKEN id="token-96-2" pos="word" morph="none" start_char="11235" end_char="11238">2020</TOKEN>
<TOKEN id="token-96-3" pos="punct" morph="none" start_char="11239" end_char="11239">.</TOKEN>
</SEG>
<SEG id="segment-97" start_char="11242" end_char="11266">
<ORIGINAL_TEXT>Bedford, Trevor (@trvrb).</ORIGINAL_TEXT>
<TOKEN id="token-97-0" pos="word" morph="none" start_char="11242" end_char="11248">Bedford</TOKEN>
<TOKEN id="token-97-1" pos="punct" morph="none" start_char="11249" end_char="11249">,</TOKEN>
<TOKEN id="token-97-2" pos="word" morph="none" start_char="11251" end_char="11256">Trevor</TOKEN>
<TOKEN id="token-97-3" pos="punct" morph="none" start_char="11258" end_char="11259">(@</TOKEN>
<TOKEN id="token-97-4" pos="word" morph="none" start_char="11260" end_char="11264">trvrb</TOKEN>
<TOKEN id="token-97-5" pos="punct" morph="none" start_char="11265" end_char="11266">).</TOKEN>
</SEG>
<SEG id="segment-98" start_char="11268" end_char="11453">
<ORIGINAL_TEXT>"These short inserts do indeed exist in #nCoV2019 relative to its closest sequenced relative (BetaCoV/bat/Yunnan/RaTG13/2013, seen here https://nextstrain.org/groups/blab/sars-like-cov).</ORIGINAL_TEXT>
<TOKEN id="token-98-0" pos="punct" morph="none" start_char="11268" end_char="11268">"</TOKEN>
<TOKEN id="token-98-1" pos="word" morph="none" start_char="11269" end_char="11273">These</TOKEN>
<TOKEN id="token-98-2" pos="word" morph="none" start_char="11275" end_char="11279">short</TOKEN>
<TOKEN id="token-98-3" pos="word" morph="none" start_char="11281" end_char="11287">inserts</TOKEN>
<TOKEN id="token-98-4" pos="word" morph="none" start_char="11289" end_char="11290">do</TOKEN>
<TOKEN id="token-98-5" pos="word" morph="none" start_char="11292" end_char="11297">indeed</TOKEN>
<TOKEN id="token-98-6" pos="word" morph="none" start_char="11299" end_char="11303">exist</TOKEN>
<TOKEN id="token-98-7" pos="word" morph="none" start_char="11305" end_char="11306">in</TOKEN>
<TOKEN id="token-98-8" pos="tag" morph="none" start_char="11308" end_char="11316">#nCoV2019</TOKEN>
<TOKEN id="token-98-9" pos="word" morph="none" start_char="11318" end_char="11325">relative</TOKEN>
<TOKEN id="token-98-10" pos="word" morph="none" start_char="11327" end_char="11328">to</TOKEN>
<TOKEN id="token-98-11" pos="word" morph="none" start_char="11330" end_char="11332">its</TOKEN>
<TOKEN id="token-98-12" pos="word" morph="none" start_char="11334" end_char="11340">closest</TOKEN>
<TOKEN id="token-98-13" pos="word" morph="none" start_char="11342" end_char="11350">sequenced</TOKEN>
<TOKEN id="token-98-14" pos="word" morph="none" start_char="11352" end_char="11359">relative</TOKEN>
<TOKEN id="token-98-15" pos="punct" morph="none" start_char="11361" end_char="11361">(</TOKEN>
<TOKEN id="token-98-16" pos="unknown" morph="none" start_char="11362" end_char="11391">BetaCoV/bat/Yunnan/RaTG13/2013</TOKEN>
<TOKEN id="token-98-17" pos="punct" morph="none" start_char="11392" end_char="11392">,</TOKEN>
<TOKEN id="token-98-18" pos="word" morph="none" start_char="11394" end_char="11397">seen</TOKEN>
<TOKEN id="token-98-19" pos="word" morph="none" start_char="11399" end_char="11402">here</TOKEN>
<TOKEN id="token-98-20" pos="url" morph="none" start_char="11404" end_char="11453">https://nextstrain.org/groups/blab/sars-like-cov).</TOKEN>
</SEG>
<SEG id="segment-99" start_char="11455" end_char="11545">
<ORIGINAL_TEXT>However, a simple BLAST of such short sequences shows match to a huge variety of organisms.</ORIGINAL_TEXT>
<TOKEN id="token-99-0" pos="word" morph="none" start_char="11455" end_char="11461">However</TOKEN>
<TOKEN id="token-99-1" pos="punct" morph="none" start_char="11462" end_char="11462">,</TOKEN>
<TOKEN id="token-99-2" pos="word" morph="none" start_char="11464" end_char="11464">a</TOKEN>
<TOKEN id="token-99-3" pos="word" morph="none" start_char="11466" end_char="11471">simple</TOKEN>
<TOKEN id="token-99-4" pos="word" morph="none" start_char="11473" end_char="11477">BLAST</TOKEN>
<TOKEN id="token-99-5" pos="word" morph="none" start_char="11479" end_char="11480">of</TOKEN>
<TOKEN id="token-99-6" pos="word" morph="none" start_char="11482" end_char="11485">such</TOKEN>
<TOKEN id="token-99-7" pos="word" morph="none" start_char="11487" end_char="11491">short</TOKEN>
<TOKEN id="token-99-8" pos="word" morph="none" start_char="11493" end_char="11501">sequences</TOKEN>
<TOKEN id="token-99-9" pos="word" morph="none" start_char="11503" end_char="11507">shows</TOKEN>
<TOKEN id="token-99-10" pos="word" morph="none" start_char="11509" end_char="11513">match</TOKEN>
<TOKEN id="token-99-11" pos="word" morph="none" start_char="11515" end_char="11516">to</TOKEN>
<TOKEN id="token-99-12" pos="word" morph="none" start_char="11518" end_char="11518">a</TOKEN>
<TOKEN id="token-99-13" pos="word" morph="none" start_char="11520" end_char="11523">huge</TOKEN>
<TOKEN id="token-99-14" pos="word" morph="none" start_char="11525" end_char="11531">variety</TOKEN>
<TOKEN id="token-99-15" pos="word" morph="none" start_char="11533" end_char="11534">of</TOKEN>
<TOKEN id="token-99-16" pos="word" morph="none" start_char="11536" end_char="11544">organisms</TOKEN>
<TOKEN id="token-99-17" pos="punct" morph="none" start_char="11545" end_char="11545">.</TOKEN>
</SEG>
<SEG id="segment-100" start_char="11547" end_char="11573">
<ORIGINAL_TEXT>No reason to conclude HIV."</ORIGINAL_TEXT>
<TOKEN id="token-100-0" pos="word" morph="none" start_char="11547" end_char="11548">No</TOKEN>
<TOKEN id="token-100-1" pos="word" morph="none" start_char="11550" end_char="11555">reason</TOKEN>
<TOKEN id="token-100-2" pos="word" morph="none" start_char="11557" end_char="11558">to</TOKEN>
<TOKEN id="token-100-3" pos="word" morph="none" start_char="11560" end_char="11567">conclude</TOKEN>
<TOKEN id="token-100-4" pos="word" morph="none" start_char="11569" end_char="11571">HIV</TOKEN>
<TOKEN id="token-100-5" pos="punct" morph="none" start_char="11572" end_char="11573">."</TOKEN>
</SEG>
<SEG id="segment-101" start_char="11575" end_char="11582">
<ORIGINAL_TEXT>Twitter.</ORIGINAL_TEXT>
<TOKEN id="token-101-0" pos="word" morph="none" start_char="11575" end_char="11581">Twitter</TOKEN>
<TOKEN id="token-101-1" pos="punct" morph="none" start_char="11582" end_char="11582">.</TOKEN>
</SEG>
<SEG id="segment-102" start_char="11584" end_char="11595">
<ORIGINAL_TEXT>31 Jan 2020.</ORIGINAL_TEXT>
<TOKEN id="token-102-0" pos="word" morph="none" start_char="11584" end_char="11585">31</TOKEN>
<TOKEN id="token-102-1" pos="word" morph="none" start_char="11587" end_char="11589">Jan</TOKEN>
<TOKEN id="token-102-2" pos="word" morph="none" start_char="11591" end_char="11594">2020</TOKEN>
<TOKEN id="token-102-3" pos="punct" morph="none" start_char="11595" end_char="11595">.</TOKEN>
</SEG>
<SEG id="segment-103" start_char="11598" end_char="11638">
<ORIGINAL_TEXT>Severe Acute Respiratory Syndrome (SARS).</ORIGINAL_TEXT>
<TOKEN id="token-103-0" pos="word" morph="none" start_char="11598" end_char="11603">Severe</TOKEN>
<TOKEN id="token-103-1" pos="word" morph="none" start_char="11605" end_char="11609">Acute</TOKEN>
<TOKEN id="token-103-2" pos="word" morph="none" start_char="11611" end_char="11621">Respiratory</TOKEN>
<TOKEN id="token-103-3" pos="word" morph="none" start_char="11623" end_char="11630">Syndrome</TOKEN>
<TOKEN id="token-103-4" pos="punct" morph="none" start_char="11632" end_char="11632">(</TOKEN>
<TOKEN id="token-103-5" pos="word" morph="none" start_char="11633" end_char="11636">SARS</TOKEN>
<TOKEN id="token-103-6" pos="punct" morph="none" start_char="11637" end_char="11638">).</TOKEN>
</SEG>
<SEG id="segment-104" start_char="11640" end_char="11643">
<ORIGINAL_TEXT>CDC.</ORIGINAL_TEXT>
<TOKEN id="token-104-0" pos="word" morph="none" start_char="11640" end_char="11642">CDC</TOKEN>
<TOKEN id="token-104-1" pos="punct" morph="none" start_char="11643" end_char="11643">.</TOKEN>
</SEG>
<SEG id="segment-105" start_char="11645" end_char="11664">
<ORIGINAL_TEXT>Accessed 7 Feb 2020.</ORIGINAL_TEXT>
<TOKEN id="token-105-0" pos="word" morph="none" start_char="11645" end_char="11652">Accessed</TOKEN>
<TOKEN id="token-105-1" pos="word" morph="none" start_char="11654" end_char="11654">7</TOKEN>
<TOKEN id="token-105-2" pos="word" morph="none" start_char="11656" end_char="11658">Feb</TOKEN>
<TOKEN id="token-105-3" pos="word" morph="none" start_char="11660" end_char="11663">2020</TOKEN>
<TOKEN id="token-105-4" pos="punct" morph="none" start_char="11664" end_char="11664">.</TOKEN>
</SEG>
<SEG id="segment-106" start_char="11667" end_char="11691">
<ORIGINAL_TEXT>Bedford, Trevor (@trvrb).</ORIGINAL_TEXT>
<TOKEN id="token-106-0" pos="word" morph="none" start_char="11667" end_char="11673">Bedford</TOKEN>
<TOKEN id="token-106-1" pos="punct" morph="none" start_char="11674" end_char="11674">,</TOKEN>
<TOKEN id="token-106-2" pos="word" morph="none" start_char="11676" end_char="11681">Trevor</TOKEN>
<TOKEN id="token-106-3" pos="punct" morph="none" start_char="11683" end_char="11684">(@</TOKEN>
<TOKEN id="token-106-4" pos="word" morph="none" start_char="11685" end_char="11689">trvrb</TOKEN>
<TOKEN id="token-106-5" pos="punct" morph="none" start_char="11690" end_char="11691">).</TOKEN>
</SEG>
<SEG id="segment-107" start_char="11693" end_char="11809">
<ORIGINAL_TEXT>"Based on the content of my mentions, I feel like I need to further debunk crazy #nCoV2019 / HIV conspiracy preprint.</ORIGINAL_TEXT>
<TOKEN id="token-107-0" pos="punct" morph="none" start_char="11693" end_char="11693">"</TOKEN>
<TOKEN id="token-107-1" pos="word" morph="none" start_char="11694" end_char="11698">Based</TOKEN>
<TOKEN id="token-107-2" pos="word" morph="none" start_char="11700" end_char="11701">on</TOKEN>
<TOKEN id="token-107-3" pos="word" morph="none" start_char="11703" end_char="11705">the</TOKEN>
<TOKEN id="token-107-4" pos="word" morph="none" start_char="11707" end_char="11713">content</TOKEN>
<TOKEN id="token-107-5" pos="word" morph="none" start_char="11715" end_char="11716">of</TOKEN>
<TOKEN id="token-107-6" pos="word" morph="none" start_char="11718" end_char="11719">my</TOKEN>
<TOKEN id="token-107-7" pos="word" morph="none" start_char="11721" end_char="11728">mentions</TOKEN>
<TOKEN id="token-107-8" pos="punct" morph="none" start_char="11729" end_char="11729">,</TOKEN>
<TOKEN id="token-107-9" pos="word" morph="none" start_char="11731" end_char="11731">I</TOKEN>
<TOKEN id="token-107-10" pos="word" morph="none" start_char="11733" end_char="11736">feel</TOKEN>
<TOKEN id="token-107-11" pos="word" morph="none" start_char="11738" end_char="11741">like</TOKEN>
<TOKEN id="token-107-12" pos="word" morph="none" start_char="11743" end_char="11743">I</TOKEN>
<TOKEN id="token-107-13" pos="word" morph="none" start_char="11745" end_char="11748">need</TOKEN>
<TOKEN id="token-107-14" pos="word" morph="none" start_char="11750" end_char="11751">to</TOKEN>
<TOKEN id="token-107-15" pos="word" morph="none" start_char="11753" end_char="11759">further</TOKEN>
<TOKEN id="token-107-16" pos="word" morph="none" start_char="11761" end_char="11766">debunk</TOKEN>
<TOKEN id="token-107-17" pos="word" morph="none" start_char="11768" end_char="11772">crazy</TOKEN>
<TOKEN id="token-107-18" pos="tag" morph="none" start_char="11774" end_char="11782">#nCoV2019</TOKEN>
<TOKEN id="token-107-19" pos="punct" morph="none" start_char="11784" end_char="11784">/</TOKEN>
<TOKEN id="token-107-20" pos="word" morph="none" start_char="11786" end_char="11788">HIV</TOKEN>
<TOKEN id="token-107-21" pos="word" morph="none" start_char="11790" end_char="11799">conspiracy</TOKEN>
<TOKEN id="token-107-22" pos="word" morph="none" start_char="11801" end_char="11808">preprint</TOKEN>
<TOKEN id="token-107-23" pos="punct" morph="none" start_char="11809" end_char="11809">.</TOKEN>
</SEG>
<SEG id="segment-108" start_char="11811" end_char="11836">
<ORIGINAL_TEXT>This is a thread doing so.</ORIGINAL_TEXT>
<TOKEN id="token-108-0" pos="word" morph="none" start_char="11811" end_char="11814">This</TOKEN>
<TOKEN id="token-108-1" pos="word" morph="none" start_char="11816" end_char="11817">is</TOKEN>
<TOKEN id="token-108-2" pos="word" morph="none" start_char="11819" end_char="11819">a</TOKEN>
<TOKEN id="token-108-3" pos="word" morph="none" start_char="11821" end_char="11826">thread</TOKEN>
<TOKEN id="token-108-4" pos="word" morph="none" start_char="11828" end_char="11832">doing</TOKEN>
<TOKEN id="token-108-5" pos="word" morph="none" start_char="11834" end_char="11835">so</TOKEN>
<TOKEN id="token-108-6" pos="punct" morph="none" start_char="11836" end_char="11836">.</TOKEN>
</SEG>
<SEG id="segment-109" start_char="11838" end_char="11864">
<ORIGINAL_TEXT>1/9, and following thread."</ORIGINAL_TEXT>
<TOKEN id="token-109-0" pos="unknown" morph="none" start_char="11838" end_char="11840">1/9</TOKEN>
<TOKEN id="token-109-1" pos="punct" morph="none" start_char="11841" end_char="11841">,</TOKEN>
<TOKEN id="token-109-2" pos="word" morph="none" start_char="11843" end_char="11845">and</TOKEN>
<TOKEN id="token-109-3" pos="word" morph="none" start_char="11847" end_char="11855">following</TOKEN>
<TOKEN id="token-109-4" pos="word" morph="none" start_char="11857" end_char="11862">thread</TOKEN>
<TOKEN id="token-109-5" pos="punct" morph="none" start_char="11863" end_char="11864">."</TOKEN>
</SEG>
<SEG id="segment-110" start_char="11866" end_char="11873">
<ORIGINAL_TEXT>Twitter.</ORIGINAL_TEXT>
<TOKEN id="token-110-0" pos="word" morph="none" start_char="11866" end_char="11872">Twitter</TOKEN>
<TOKEN id="token-110-1" pos="punct" morph="none" start_char="11873" end_char="11873">.</TOKEN>
</SEG>
<SEG id="segment-111" start_char="11875" end_char="11885">
<ORIGINAL_TEXT>1 Feb 2020.</ORIGINAL_TEXT>
<TOKEN id="token-111-0" pos="word" morph="none" start_char="11875" end_char="11875">1</TOKEN>
<TOKEN id="token-111-1" pos="word" morph="none" start_char="11877" end_char="11879">Feb</TOKEN>
<TOKEN id="token-111-2" pos="word" morph="none" start_char="11881" end_char="11884">2020</TOKEN>
<TOKEN id="token-111-3" pos="punct" morph="none" start_char="11885" end_char="11885">.</TOKEN>
</SEG>
<SEG id="segment-112" start_char="11888" end_char="11906">
<ORIGINAL_TEXT>Andersen, Kristian.</ORIGINAL_TEXT>
<TOKEN id="token-112-0" pos="word" morph="none" start_char="11888" end_char="11895">Andersen</TOKEN>
<TOKEN id="token-112-1" pos="punct" morph="none" start_char="11896" end_char="11896">,</TOKEN>
<TOKEN id="token-112-2" pos="word" morph="none" start_char="11898" end_char="11905">Kristian</TOKEN>
<TOKEN id="token-112-3" pos="punct" morph="none" start_char="11906" end_char="11906">.</TOKEN>
</SEG>
<SEG id="segment-113" start_char="11908" end_char="12023">
<ORIGINAL_TEXT>Associate Professor, Department of Integrative Structural and Computational Biology, The Scripps Research Institute.</ORIGINAL_TEXT>
<TOKEN id="token-113-0" pos="word" morph="none" start_char="11908" end_char="11916">Associate</TOKEN>
<TOKEN id="token-113-1" pos="word" morph="none" start_char="11918" end_char="11926">Professor</TOKEN>
<TOKEN id="token-113-2" pos="punct" morph="none" start_char="11927" end_char="11927">,</TOKEN>
<TOKEN id="token-113-3" pos="word" morph="none" start_char="11929" end_char="11938">Department</TOKEN>
<TOKEN id="token-113-4" pos="word" morph="none" start_char="11940" end_char="11941">of</TOKEN>
<TOKEN id="token-113-5" pos="word" morph="none" start_char="11943" end_char="11953">Integrative</TOKEN>
<TOKEN id="token-113-6" pos="word" morph="none" start_char="11955" end_char="11964">Structural</TOKEN>
<TOKEN id="token-113-7" pos="word" morph="none" start_char="11966" end_char="11968">and</TOKEN>
<TOKEN id="token-113-8" pos="word" morph="none" start_char="11970" end_char="11982">Computational</TOKEN>
<TOKEN id="token-113-9" pos="word" morph="none" start_char="11984" end_char="11990">Biology</TOKEN>
<TOKEN id="token-113-10" pos="punct" morph="none" start_char="11991" end_char="11991">,</TOKEN>
<TOKEN id="token-113-11" pos="word" morph="none" start_char="11993" end_char="11995">The</TOKEN>
<TOKEN id="token-113-12" pos="word" morph="none" start_char="11997" end_char="12003">Scripps</TOKEN>
<TOKEN id="token-113-13" pos="word" morph="none" start_char="12005" end_char="12012">Research</TOKEN>
<TOKEN id="token-113-14" pos="word" morph="none" start_char="12014" end_char="12022">Institute</TOKEN>
<TOKEN id="token-113-15" pos="punct" morph="none" start_char="12023" end_char="12023">.</TOKEN>
</SEG>
<SEG id="segment-114" start_char="12025" end_char="12052">
<ORIGINAL_TEXT>Email sent to FactCheck.org.</ORIGINAL_TEXT>
<TOKEN id="token-114-0" pos="word" morph="none" start_char="12025" end_char="12029">Email</TOKEN>
<TOKEN id="token-114-1" pos="word" morph="none" start_char="12031" end_char="12034">sent</TOKEN>
<TOKEN id="token-114-2" pos="word" morph="none" start_char="12036" end_char="12037">to</TOKEN>
<TOKEN id="token-114-3" pos="unknown" morph="none" start_char="12039" end_char="12051">FactCheck.org</TOKEN>
<TOKEN id="token-114-4" pos="punct" morph="none" start_char="12052" end_char="12052">.</TOKEN>
</SEG>
<SEG id="segment-115" start_char="12054" end_char="12064">
<ORIGINAL_TEXT>3 Feb 2020.</ORIGINAL_TEXT>
<TOKEN id="token-115-0" pos="word" morph="none" start_char="12054" end_char="12054">3</TOKEN>
<TOKEN id="token-115-1" pos="word" morph="none" start_char="12056" end_char="12058">Feb</TOKEN>
<TOKEN id="token-115-2" pos="word" morph="none" start_char="12060" end_char="12063">2020</TOKEN>
<TOKEN id="token-115-3" pos="punct" morph="none" start_char="12064" end_char="12064">.</TOKEN>
</SEG>
<SEG id="segment-116" start_char="12067" end_char="12137">
<ORIGINAL_TEXT>"China testing HIV drug as treatment for new coronavirus, AbbVie says."</ORIGINAL_TEXT>
<TOKEN id="token-116-0" pos="punct" morph="none" start_char="12067" end_char="12067">"</TOKEN>
<TOKEN id="token-116-1" pos="word" morph="none" start_char="12068" end_char="12072">China</TOKEN>
<TOKEN id="token-116-2" pos="word" morph="none" start_char="12074" end_char="12080">testing</TOKEN>
<TOKEN id="token-116-3" pos="word" morph="none" start_char="12082" end_char="12084">HIV</TOKEN>
<TOKEN id="token-116-4" pos="word" morph="none" start_char="12086" end_char="12089">drug</TOKEN>
<TOKEN id="token-116-5" pos="word" morph="none" start_char="12091" end_char="12092">as</TOKEN>
<TOKEN id="token-116-6" pos="word" morph="none" start_char="12094" end_char="12102">treatment</TOKEN>
<TOKEN id="token-116-7" pos="word" morph="none" start_char="12104" end_char="12106">for</TOKEN>
<TOKEN id="token-116-8" pos="word" morph="none" start_char="12108" end_char="12110">new</TOKEN>
<TOKEN id="token-116-9" pos="word" morph="none" start_char="12112" end_char="12122">coronavirus</TOKEN>
<TOKEN id="token-116-10" pos="punct" morph="none" start_char="12123" end_char="12123">,</TOKEN>
<TOKEN id="token-116-11" pos="word" morph="none" start_char="12125" end_char="12130">AbbVie</TOKEN>
<TOKEN id="token-116-12" pos="word" morph="none" start_char="12132" end_char="12135">says</TOKEN>
<TOKEN id="token-116-13" pos="punct" morph="none" start_char="12136" end_char="12137">."</TOKEN>
</SEG>
<SEG id="segment-117" start_char="12139" end_char="12146">
<ORIGINAL_TEXT>Reuters.</ORIGINAL_TEXT>
<TOKEN id="token-117-0" pos="word" morph="none" start_char="12139" end_char="12145">Reuters</TOKEN>
<TOKEN id="token-117-1" pos="punct" morph="none" start_char="12146" end_char="12146">.</TOKEN>
</SEG>
<SEG id="segment-118" start_char="12148" end_char="12159">
<ORIGINAL_TEXT>26 Jan 2020.</ORIGINAL_TEXT>
<TOKEN id="token-118-0" pos="word" morph="none" start_char="12148" end_char="12149">26</TOKEN>
<TOKEN id="token-118-1" pos="word" morph="none" start_char="12151" end_char="12153">Jan</TOKEN>
<TOKEN id="token-118-2" pos="word" morph="none" start_char="12155" end_char="12158">2020</TOKEN>
<TOKEN id="token-118-3" pos="punct" morph="none" start_char="12159" end_char="12159">.</TOKEN>
</SEG>
<SEG id="segment-119" start_char="12162" end_char="12178">
<ORIGINAL_TEXT>Wongcha-um, Panu.</ORIGINAL_TEXT>
<TOKEN id="token-119-0" pos="unknown" morph="none" start_char="12162" end_char="12171">Wongcha-um</TOKEN>
<TOKEN id="token-119-1" pos="punct" morph="none" start_char="12172" end_char="12172">,</TOKEN>
<TOKEN id="token-119-2" pos="word" morph="none" start_char="12174" end_char="12177">Panu</TOKEN>
<TOKEN id="token-119-3" pos="punct" morph="none" start_char="12178" end_char="12178">.</TOKEN>
</SEG>
<SEG id="segment-120" start_char="12180" end_char="12256">
<ORIGINAL_TEXT>"Cocktail of flu, HIV drugs appears to help fight coronavirus: Thai doctors."</ORIGINAL_TEXT>
<TOKEN id="token-120-0" pos="punct" morph="none" start_char="12180" end_char="12180">"</TOKEN>
<TOKEN id="token-120-1" pos="word" morph="none" start_char="12181" end_char="12188">Cocktail</TOKEN>
<TOKEN id="token-120-2" pos="word" morph="none" start_char="12190" end_char="12191">of</TOKEN>
<TOKEN id="token-120-3" pos="word" morph="none" start_char="12193" end_char="12195">flu</TOKEN>
<TOKEN id="token-120-4" pos="punct" morph="none" start_char="12196" end_char="12196">,</TOKEN>
<TOKEN id="token-120-5" pos="word" morph="none" start_char="12198" end_char="12200">HIV</TOKEN>
<TOKEN id="token-120-6" pos="word" morph="none" start_char="12202" end_char="12206">drugs</TOKEN>
<TOKEN id="token-120-7" pos="word" morph="none" start_char="12208" end_char="12214">appears</TOKEN>
<TOKEN id="token-120-8" pos="word" morph="none" start_char="12216" end_char="12217">to</TOKEN>
<TOKEN id="token-120-9" pos="word" morph="none" start_char="12219" end_char="12222">help</TOKEN>
<TOKEN id="token-120-10" pos="word" morph="none" start_char="12224" end_char="12228">fight</TOKEN>
<TOKEN id="token-120-11" pos="word" morph="none" start_char="12230" end_char="12240">coronavirus</TOKEN>
<TOKEN id="token-120-12" pos="punct" morph="none" start_char="12241" end_char="12241">:</TOKEN>
<TOKEN id="token-120-13" pos="word" morph="none" start_char="12243" end_char="12246">Thai</TOKEN>
<TOKEN id="token-120-14" pos="word" morph="none" start_char="12248" end_char="12254">doctors</TOKEN>
<TOKEN id="token-120-15" pos="punct" morph="none" start_char="12255" end_char="12256">."</TOKEN>
</SEG>
<SEG id="segment-121" start_char="12258" end_char="12265">
<ORIGINAL_TEXT>Reuters.</ORIGINAL_TEXT>
<TOKEN id="token-121-0" pos="word" morph="none" start_char="12258" end_char="12264">Reuters</TOKEN>
<TOKEN id="token-121-1" pos="punct" morph="none" start_char="12265" end_char="12265">.</TOKEN>
</SEG>
<SEG id="segment-122" start_char="12267" end_char="12277">
<ORIGINAL_TEXT>2 Feb 2020.</ORIGINAL_TEXT>
<TOKEN id="token-122-0" pos="word" morph="none" start_char="12267" end_char="12267">2</TOKEN>
<TOKEN id="token-122-1" pos="word" morph="none" start_char="12269" end_char="12271">Feb</TOKEN>
<TOKEN id="token-122-2" pos="word" morph="none" start_char="12273" end_char="12276">2020</TOKEN>
<TOKEN id="token-122-3" pos="punct" morph="none" start_char="12277" end_char="12277">.</TOKEN>
</SEG>
<SEG id="segment-123" start_char="12280" end_char="12296">
<ORIGINAL_TEXT>Sheahan, Timothy.</ORIGINAL_TEXT>
<TOKEN id="token-123-0" pos="word" morph="none" start_char="12280" end_char="12286">Sheahan</TOKEN>
<TOKEN id="token-123-1" pos="punct" morph="none" start_char="12287" end_char="12287">,</TOKEN>
<TOKEN id="token-123-2" pos="word" morph="none" start_char="12289" end_char="12295">Timothy</TOKEN>
<TOKEN id="token-123-3" pos="punct" morph="none" start_char="12296" end_char="12296">.</TOKEN>
</SEG>
<SEG id="segment-124" start_char="12298" end_char="12435">
<ORIGINAL_TEXT>Assistant Professor, Department of Epidemiology, the Gillings School of Global Public Health, University of North Carolina at Chapel Hill.</ORIGINAL_TEXT>
<TOKEN id="token-124-0" pos="word" morph="none" start_char="12298" end_char="12306">Assistant</TOKEN>
<TOKEN id="token-124-1" pos="word" morph="none" start_char="12308" end_char="12316">Professor</TOKEN>
<TOKEN id="token-124-2" pos="punct" morph="none" start_char="12317" end_char="12317">,</TOKEN>
<TOKEN id="token-124-3" pos="word" morph="none" start_char="12319" end_char="12328">Department</TOKEN>
<TOKEN id="token-124-4" pos="word" morph="none" start_char="12330" end_char="12331">of</TOKEN>
<TOKEN id="token-124-5" pos="word" morph="none" start_char="12333" end_char="12344">Epidemiology</TOKEN>
<TOKEN id="token-124-6" pos="punct" morph="none" start_char="12345" end_char="12345">,</TOKEN>
<TOKEN id="token-124-7" pos="word" morph="none" start_char="12347" end_char="12349">the</TOKEN>
<TOKEN id="token-124-8" pos="word" morph="none" start_char="12351" end_char="12358">Gillings</TOKEN>
<TOKEN id="token-124-9" pos="word" morph="none" start_char="12360" end_char="12365">School</TOKEN>
<TOKEN id="token-124-10" pos="word" morph="none" start_char="12367" end_char="12368">of</TOKEN>
<TOKEN id="token-124-11" pos="word" morph="none" start_char="12370" end_char="12375">Global</TOKEN>
<TOKEN id="token-124-12" pos="word" morph="none" start_char="12377" end_char="12382">Public</TOKEN>
<TOKEN id="token-124-13" pos="word" morph="none" start_char="12384" end_char="12389">Health</TOKEN>
<TOKEN id="token-124-14" pos="punct" morph="none" start_char="12390" end_char="12390">,</TOKEN>
<TOKEN id="token-124-15" pos="word" morph="none" start_char="12392" end_char="12401">University</TOKEN>
<TOKEN id="token-124-16" pos="word" morph="none" start_char="12403" end_char="12404">of</TOKEN>
<TOKEN id="token-124-17" pos="word" morph="none" start_char="12406" end_char="12410">North</TOKEN>
<TOKEN id="token-124-18" pos="word" morph="none" start_char="12412" end_char="12419">Carolina</TOKEN>
<TOKEN id="token-124-19" pos="word" morph="none" start_char="12421" end_char="12422">at</TOKEN>
<TOKEN id="token-124-20" pos="word" morph="none" start_char="12424" end_char="12429">Chapel</TOKEN>
<TOKEN id="token-124-21" pos="word" morph="none" start_char="12431" end_char="12434">Hill</TOKEN>
<TOKEN id="token-124-22" pos="punct" morph="none" start_char="12435" end_char="12435">.</TOKEN>
</SEG>
<SEG id="segment-125" start_char="12437" end_char="12465">
<ORIGINAL_TEXT>Interview with FactCheck.org.</ORIGINAL_TEXT>
<TOKEN id="token-125-0" pos="word" morph="none" start_char="12437" end_char="12445">Interview</TOKEN>
<TOKEN id="token-125-1" pos="word" morph="none" start_char="12447" end_char="12450">with</TOKEN>
<TOKEN id="token-125-2" pos="unknown" morph="none" start_char="12452" end_char="12464">FactCheck.org</TOKEN>
<TOKEN id="token-125-3" pos="punct" morph="none" start_char="12465" end_char="12465">.</TOKEN>
</SEG>
<SEG id="segment-126" start_char="12467" end_char="12477">
<ORIGINAL_TEXT>5 Feb 2020.</ORIGINAL_TEXT>
<TOKEN id="token-126-0" pos="word" morph="none" start_char="12467" end_char="12467">5</TOKEN>
<TOKEN id="token-126-1" pos="word" morph="none" start_char="12469" end_char="12471">Feb</TOKEN>
<TOKEN id="token-126-2" pos="word" morph="none" start_char="12473" end_char="12476">2020</TOKEN>
<TOKEN id="token-126-3" pos="punct" morph="none" start_char="12477" end_char="12477">.</TOKEN>
</SEG>
<SEG id="segment-127" start_char="12480" end_char="12490">
<ORIGINAL_TEXT>Chu, C. et.</ORIGINAL_TEXT>
<TOKEN id="token-127-0" pos="word" morph="none" start_char="12480" end_char="12482">Chu</TOKEN>
<TOKEN id="token-127-1" pos="punct" morph="none" start_char="12483" end_char="12483">,</TOKEN>
<TOKEN id="token-127-2" pos="word" morph="none" start_char="12485" end_char="12485">C</TOKEN>
<TOKEN id="token-127-3" pos="punct" morph="none" start_char="12486" end_char="12486">.</TOKEN>
<TOKEN id="token-127-4" pos="word" morph="none" start_char="12488" end_char="12489">et</TOKEN>
<TOKEN id="token-127-5" pos="punct" morph="none" start_char="12490" end_char="12490">.</TOKEN>
</SEG>
<SEG id="segment-128" start_char="12492" end_char="12494">
<ORIGINAL_TEXT>al.</ORIGINAL_TEXT>
<TOKEN id="token-128-0" pos="word" morph="none" start_char="12492" end_char="12493">al</TOKEN>
<TOKEN id="token-128-1" pos="punct" morph="none" start_char="12494" end_char="12494">.</TOKEN>
</SEG>
<SEG id="segment-129" start_char="12496" end_char="12593">
<ORIGINAL_TEXT>"Role of lopinavir/ritonavir in the treatment of SARS: initial virological and clinical findings."</ORIGINAL_TEXT>
<TOKEN id="token-129-0" pos="punct" morph="none" start_char="12496" end_char="12496">"</TOKEN>
<TOKEN id="token-129-1" pos="word" morph="none" start_char="12497" end_char="12500">Role</TOKEN>
<TOKEN id="token-129-2" pos="word" morph="none" start_char="12502" end_char="12503">of</TOKEN>
<TOKEN id="token-129-3" pos="unknown" morph="none" start_char="12505" end_char="12523">lopinavir/ritonavir</TOKEN>
<TOKEN id="token-129-4" pos="word" morph="none" start_char="12525" end_char="12526">in</TOKEN>
<TOKEN id="token-129-5" pos="word" morph="none" start_char="12528" end_char="12530">the</TOKEN>
<TOKEN id="token-129-6" pos="word" morph="none" start_char="12532" end_char="12540">treatment</TOKEN>
<TOKEN id="token-129-7" pos="word" morph="none" start_char="12542" end_char="12543">of</TOKEN>
<TOKEN id="token-129-8" pos="word" morph="none" start_char="12545" end_char="12548">SARS</TOKEN>
<TOKEN id="token-129-9" pos="punct" morph="none" start_char="12549" end_char="12549">:</TOKEN>
<TOKEN id="token-129-10" pos="word" morph="none" start_char="12551" end_char="12557">initial</TOKEN>
<TOKEN id="token-129-11" pos="word" morph="none" start_char="12559" end_char="12569">virological</TOKEN>
<TOKEN id="token-129-12" pos="word" morph="none" start_char="12571" end_char="12573">and</TOKEN>
<TOKEN id="token-129-13" pos="word" morph="none" start_char="12575" end_char="12582">clinical</TOKEN>
<TOKEN id="token-129-14" pos="word" morph="none" start_char="12584" end_char="12591">findings</TOKEN>
<TOKEN id="token-129-15" pos="punct" morph="none" start_char="12592" end_char="12593">."</TOKEN>
</SEG>
<SEG id="segment-130" start_char="12595" end_char="12601">
<ORIGINAL_TEXT>Thorax.</ORIGINAL_TEXT>
<TOKEN id="token-130-0" pos="word" morph="none" start_char="12595" end_char="12600">Thorax</TOKEN>
<TOKEN id="token-130-1" pos="punct" morph="none" start_char="12601" end_char="12601">.</TOKEN>
</SEG>
<SEG id="segment-131" start_char="12603" end_char="12614">
<ORIGINAL_TEXT>59(3), 2004.</ORIGINAL_TEXT>
<TOKEN id="token-131-0" pos="unknown" morph="none" start_char="12603" end_char="12606">59(3</TOKEN>
<TOKEN id="token-131-1" pos="punct" morph="none" start_char="12607" end_char="12608">),</TOKEN>
<TOKEN id="token-131-2" pos="word" morph="none" start_char="12610" end_char="12613">2004</TOKEN>
<TOKEN id="token-131-3" pos="punct" morph="none" start_char="12614" end_char="12614">.</TOKEN>
</SEG>
<SEG id="segment-132" start_char="12617" end_char="12639">
<ORIGINAL_TEXT>Sheahan, Timothy P. et.</ORIGINAL_TEXT>
<TOKEN id="token-132-0" pos="word" morph="none" start_char="12617" end_char="12623">Sheahan</TOKEN>
<TOKEN id="token-132-1" pos="punct" morph="none" start_char="12624" end_char="12624">,</TOKEN>
<TOKEN id="token-132-2" pos="word" morph="none" start_char="12626" end_char="12632">Timothy</TOKEN>
<TOKEN id="token-132-3" pos="word" morph="none" start_char="12634" end_char="12634">P</TOKEN>
<TOKEN id="token-132-4" pos="punct" morph="none" start_char="12635" end_char="12635">.</TOKEN>
<TOKEN id="token-132-5" pos="word" morph="none" start_char="12637" end_char="12638">et</TOKEN>
<TOKEN id="token-132-6" pos="punct" morph="none" start_char="12639" end_char="12639">.</TOKEN>
</SEG>
<SEG id="segment-133" start_char="12641" end_char="12643">
<ORIGINAL_TEXT>al.</ORIGINAL_TEXT>
<TOKEN id="token-133-0" pos="word" morph="none" start_char="12641" end_char="12642">al</TOKEN>
<TOKEN id="token-133-1" pos="punct" morph="none" start_char="12643" end_char="12643">.</TOKEN>
</SEG>
<SEG id="segment-134" start_char="12645" end_char="12768">
<ORIGINAL_TEXT>"Comparative therapeutic efficacy of remdesivir and combination lopinavir, ritonavir, and interferon beta against MERS-CoV."</ORIGINAL_TEXT>
<TOKEN id="token-134-0" pos="punct" morph="none" start_char="12645" end_char="12645">"</TOKEN>
<TOKEN id="token-134-1" pos="word" morph="none" start_char="12646" end_char="12656">Comparative</TOKEN>
<TOKEN id="token-134-2" pos="word" morph="none" start_char="12658" end_char="12668">therapeutic</TOKEN>
<TOKEN id="token-134-3" pos="word" morph="none" start_char="12670" end_char="12677">efficacy</TOKEN>
<TOKEN id="token-134-4" pos="word" morph="none" start_char="12679" end_char="12680">of</TOKEN>
<TOKEN id="token-134-5" pos="word" morph="none" start_char="12682" end_char="12691">remdesivir</TOKEN>
<TOKEN id="token-134-6" pos="word" morph="none" start_char="12693" end_char="12695">and</TOKEN>
<TOKEN id="token-134-7" pos="word" morph="none" start_char="12697" end_char="12707">combination</TOKEN>
<TOKEN id="token-134-8" pos="word" morph="none" start_char="12709" end_char="12717">lopinavir</TOKEN>
<TOKEN id="token-134-9" pos="punct" morph="none" start_char="12718" end_char="12718">,</TOKEN>
<TOKEN id="token-134-10" pos="word" morph="none" start_char="12720" end_char="12728">ritonavir</TOKEN>
<TOKEN id="token-134-11" pos="punct" morph="none" start_char="12729" end_char="12729">,</TOKEN>
<TOKEN id="token-134-12" pos="word" morph="none" start_char="12731" end_char="12733">and</TOKEN>
<TOKEN id="token-134-13" pos="word" morph="none" start_char="12735" end_char="12744">interferon</TOKEN>
<TOKEN id="token-134-14" pos="word" morph="none" start_char="12746" end_char="12749">beta</TOKEN>
<TOKEN id="token-134-15" pos="word" morph="none" start_char="12751" end_char="12757">against</TOKEN>
<TOKEN id="token-134-16" pos="unknown" morph="none" start_char="12759" end_char="12766">MERS-CoV</TOKEN>
<TOKEN id="token-134-17" pos="punct" morph="none" start_char="12767" end_char="12768">."</TOKEN>
</SEG>
<SEG id="segment-135" start_char="12770" end_char="12791">
<ORIGINAL_TEXT>Nature Communications.</ORIGINAL_TEXT>
<TOKEN id="token-135-0" pos="word" morph="none" start_char="12770" end_char="12775">Nature</TOKEN>
<TOKEN id="token-135-1" pos="word" morph="none" start_char="12777" end_char="12790">Communications</TOKEN>
<TOKEN id="token-135-2" pos="punct" morph="none" start_char="12791" end_char="12791">.</TOKEN>
</SEG>
<SEG id="segment-136" start_char="12793" end_char="12801">
<ORIGINAL_TEXT>11, 2020.</ORIGINAL_TEXT>
<TOKEN id="token-136-0" pos="word" morph="none" start_char="12793" end_char="12794">11</TOKEN>
<TOKEN id="token-136-1" pos="punct" morph="none" start_char="12795" end_char="12795">,</TOKEN>
<TOKEN id="token-136-2" pos="word" morph="none" start_char="12797" end_char="12800">2020</TOKEN>
<TOKEN id="token-136-3" pos="punct" morph="none" start_char="12801" end_char="12801">.</TOKEN>
</SEG>
<SEG id="segment-137" start_char="12804" end_char="12823">
<ORIGINAL_TEXT>Arabi, Yaseen M. et.</ORIGINAL_TEXT>
<TOKEN id="token-137-0" pos="word" morph="none" start_char="12804" end_char="12808">Arabi</TOKEN>
<TOKEN id="token-137-1" pos="punct" morph="none" start_char="12809" end_char="12809">,</TOKEN>
<TOKEN id="token-137-2" pos="word" morph="none" start_char="12811" end_char="12816">Yaseen</TOKEN>
<TOKEN id="token-137-3" pos="word" morph="none" start_char="12818" end_char="12818">M</TOKEN>
<TOKEN id="token-137-4" pos="punct" morph="none" start_char="12819" end_char="12819">.</TOKEN>
<TOKEN id="token-137-5" pos="word" morph="none" start_char="12821" end_char="12822">et</TOKEN>
<TOKEN id="token-137-6" pos="punct" morph="none" start_char="12823" end_char="12823">.</TOKEN>
</SEG>
<SEG id="segment-138" start_char="12825" end_char="12827">
<ORIGINAL_TEXT>al.</ORIGINAL_TEXT>
<TOKEN id="token-138-0" pos="word" morph="none" start_char="12825" end_char="12826">al</TOKEN>
<TOKEN id="token-138-1" pos="punct" morph="none" start_char="12827" end_char="12827">.</TOKEN>
</SEG>
<SEG id="segment-139" start_char="12829" end_char="13003">
<ORIGINAL_TEXT>"Treatment of Middle East Respiratory Syndrome with a combination of lopinavir-ritonavir and interferon-β1b (MIRACLE trial): study protocol for a randomized controlled trial."</ORIGINAL_TEXT>
<TOKEN id="token-139-0" pos="punct" morph="none" start_char="12829" end_char="12829">"</TOKEN>
<TOKEN id="token-139-1" pos="word" morph="none" start_char="12830" end_char="12838">Treatment</TOKEN>
<TOKEN id="token-139-2" pos="word" morph="none" start_char="12840" end_char="12841">of</TOKEN>
<TOKEN id="token-139-3" pos="word" morph="none" start_char="12843" end_char="12848">Middle</TOKEN>
<TOKEN id="token-139-4" pos="word" morph="none" start_char="12850" end_char="12853">East</TOKEN>
<TOKEN id="token-139-5" pos="word" morph="none" start_char="12855" end_char="12865">Respiratory</TOKEN>
<TOKEN id="token-139-6" pos="word" morph="none" start_char="12867" end_char="12874">Syndrome</TOKEN>
<TOKEN id="token-139-7" pos="word" morph="none" start_char="12876" end_char="12879">with</TOKEN>
<TOKEN id="token-139-8" pos="word" morph="none" start_char="12881" end_char="12881">a</TOKEN>
<TOKEN id="token-139-9" pos="word" morph="none" start_char="12883" end_char="12893">combination</TOKEN>
<TOKEN id="token-139-10" pos="word" morph="none" start_char="12895" end_char="12896">of</TOKEN>
<TOKEN id="token-139-11" pos="unknown" morph="none" start_char="12898" end_char="12916">lopinavir-ritonavir</TOKEN>
<TOKEN id="token-139-12" pos="word" morph="none" start_char="12918" end_char="12920">and</TOKEN>
<TOKEN id="token-139-13" pos="unknown" morph="none" start_char="12922" end_char="12935">interferon-β1b</TOKEN>
<TOKEN id="token-139-14" pos="punct" morph="none" start_char="12937" end_char="12937">(</TOKEN>
<TOKEN id="token-139-15" pos="word" morph="none" start_char="12938" end_char="12944">MIRACLE</TOKEN>
<TOKEN id="token-139-16" pos="word" morph="none" start_char="12946" end_char="12950">trial</TOKEN>
<TOKEN id="token-139-17" pos="punct" morph="none" start_char="12951" end_char="12952">):</TOKEN>
<TOKEN id="token-139-18" pos="word" morph="none" start_char="12954" end_char="12958">study</TOKEN>
<TOKEN id="token-139-19" pos="word" morph="none" start_char="12960" end_char="12967">protocol</TOKEN>
<TOKEN id="token-139-20" pos="word" morph="none" start_char="12969" end_char="12971">for</TOKEN>
<TOKEN id="token-139-21" pos="word" morph="none" start_char="12973" end_char="12973">a</TOKEN>
<TOKEN id="token-139-22" pos="word" morph="none" start_char="12975" end_char="12984">randomized</TOKEN>
<TOKEN id="token-139-23" pos="word" morph="none" start_char="12986" end_char="12995">controlled</TOKEN>
<TOKEN id="token-139-24" pos="word" morph="none" start_char="12997" end_char="13001">trial</TOKEN>
<TOKEN id="token-139-25" pos="punct" morph="none" start_char="13002" end_char="13003">."</TOKEN>
</SEG>
<SEG id="segment-140" start_char="13005" end_char="13011">
<ORIGINAL_TEXT>Trials.</ORIGINAL_TEXT>
<TOKEN id="token-140-0" pos="word" morph="none" start_char="13005" end_char="13010">Trials</TOKEN>
<TOKEN id="token-140-1" pos="punct" morph="none" start_char="13011" end_char="13011">.</TOKEN>
</SEG>
<SEG id="segment-141" start_char="13013" end_char="13025">
<ORIGINAL_TEXT>19(81), 2018.</ORIGINAL_TEXT>
<TOKEN id="token-141-0" pos="unknown" morph="none" start_char="13013" end_char="13017">19(81</TOKEN>
<TOKEN id="token-141-1" pos="punct" morph="none" start_char="13018" end_char="13019">),</TOKEN>
<TOKEN id="token-141-2" pos="word" morph="none" start_char="13021" end_char="13024">2018</TOKEN>
<TOKEN id="token-141-3" pos="punct" morph="none" start_char="13025" end_char="13025">.</TOKEN>
</SEG>
<SEG id="segment-142" start_char="13028" end_char="13047">
<ORIGINAL_TEXT>Arabi, Yaseen M. et.</ORIGINAL_TEXT>
<TOKEN id="token-142-0" pos="word" morph="none" start_char="13028" end_char="13032">Arabi</TOKEN>
<TOKEN id="token-142-1" pos="punct" morph="none" start_char="13033" end_char="13033">,</TOKEN>
<TOKEN id="token-142-2" pos="word" morph="none" start_char="13035" end_char="13040">Yaseen</TOKEN>
<TOKEN id="token-142-3" pos="word" morph="none" start_char="13042" end_char="13042">M</TOKEN>
<TOKEN id="token-142-4" pos="punct" morph="none" start_char="13043" end_char="13043">.</TOKEN>
<TOKEN id="token-142-5" pos="word" morph="none" start_char="13045" end_char="13046">et</TOKEN>
<TOKEN id="token-142-6" pos="punct" morph="none" start_char="13047" end_char="13047">.</TOKEN>
</SEG>
<SEG id="segment-143" start_char="13049" end_char="13051">
<ORIGINAL_TEXT>al.</ORIGINAL_TEXT>
<TOKEN id="token-143-0" pos="word" morph="none" start_char="13049" end_char="13050">al</TOKEN>
<TOKEN id="token-143-1" pos="punct" morph="none" start_char="13051" end_char="13051">.</TOKEN>
</SEG>
<SEG id="segment-144" start_char="13053" end_char="13275">
<ORIGINAL_TEXT>"Treatment of Middle East respiratory syndrome with a combination of lopinavir/ritonavir and interferon-β1b (MIRACLE trial): statistical analysis plan for a recursive two-stage group sequential randomized controlled trial."</ORIGINAL_TEXT>
<TOKEN id="token-144-0" pos="punct" morph="none" start_char="13053" end_char="13053">"</TOKEN>
<TOKEN id="token-144-1" pos="word" morph="none" start_char="13054" end_char="13062">Treatment</TOKEN>
<TOKEN id="token-144-2" pos="word" morph="none" start_char="13064" end_char="13065">of</TOKEN>
<TOKEN id="token-144-3" pos="word" morph="none" start_char="13067" end_char="13072">Middle</TOKEN>
<TOKEN id="token-144-4" pos="word" morph="none" start_char="13074" end_char="13077">East</TOKEN>
<TOKEN id="token-144-5" pos="word" morph="none" start_char="13079" end_char="13089">respiratory</TOKEN>
<TOKEN id="token-144-6" pos="word" morph="none" start_char="13091" end_char="13098">syndrome</TOKEN>
<TOKEN id="token-144-7" pos="word" morph="none" start_char="13100" end_char="13103">with</TOKEN>
<TOKEN id="token-144-8" pos="word" morph="none" start_char="13105" end_char="13105">a</TOKEN>
<TOKEN id="token-144-9" pos="word" morph="none" start_char="13107" end_char="13117">combination</TOKEN>
<TOKEN id="token-144-10" pos="word" morph="none" start_char="13119" end_char="13120">of</TOKEN>
<TOKEN id="token-144-11" pos="unknown" morph="none" start_char="13122" end_char="13140">lopinavir/ritonavir</TOKEN>
<TOKEN id="token-144-12" pos="word" morph="none" start_char="13142" end_char="13144">and</TOKEN>
<TOKEN id="token-144-13" pos="unknown" morph="none" start_char="13146" end_char="13159">interferon-β1b</TOKEN>
<TOKEN id="token-144-14" pos="punct" morph="none" start_char="13161" end_char="13161">(</TOKEN>
<TOKEN id="token-144-15" pos="word" morph="none" start_char="13162" end_char="13168">MIRACLE</TOKEN>
<TOKEN id="token-144-16" pos="word" morph="none" start_char="13170" end_char="13174">trial</TOKEN>
<TOKEN id="token-144-17" pos="punct" morph="none" start_char="13175" end_char="13176">):</TOKEN>
<TOKEN id="token-144-18" pos="word" morph="none" start_char="13178" end_char="13188">statistical</TOKEN>
<TOKEN id="token-144-19" pos="word" morph="none" start_char="13190" end_char="13197">analysis</TOKEN>
<TOKEN id="token-144-20" pos="word" morph="none" start_char="13199" end_char="13202">plan</TOKEN>
<TOKEN id="token-144-21" pos="word" morph="none" start_char="13204" end_char="13206">for</TOKEN>
<TOKEN id="token-144-22" pos="word" morph="none" start_char="13208" end_char="13208">a</TOKEN>
<TOKEN id="token-144-23" pos="word" morph="none" start_char="13210" end_char="13218">recursive</TOKEN>
<TOKEN id="token-144-24" pos="unknown" morph="none" start_char="13220" end_char="13228">two-stage</TOKEN>
<TOKEN id="token-144-25" pos="word" morph="none" start_char="13230" end_char="13234">group</TOKEN>
<TOKEN id="token-144-26" pos="word" morph="none" start_char="13236" end_char="13245">sequential</TOKEN>
<TOKEN id="token-144-27" pos="word" morph="none" start_char="13247" end_char="13256">randomized</TOKEN>
<TOKEN id="token-144-28" pos="word" morph="none" start_char="13258" end_char="13267">controlled</TOKEN>
<TOKEN id="token-144-29" pos="word" morph="none" start_char="13269" end_char="13273">trial</TOKEN>
<TOKEN id="token-144-30" pos="punct" morph="none" start_char="13274" end_char="13275">."</TOKEN>
</SEG>
<SEG id="segment-145" start_char="13277" end_char="13283">
<ORIGINAL_TEXT>Trials.</ORIGINAL_TEXT>
<TOKEN id="token-145-0" pos="word" morph="none" start_char="13277" end_char="13282">Trials</TOKEN>
<TOKEN id="token-145-1" pos="punct" morph="none" start_char="13283" end_char="13283">.</TOKEN>
</SEG>
<SEG id="segment-146" start_char="13285" end_char="13296">
<ORIGINAL_TEXT>21(8), 2020.</ORIGINAL_TEXT>
<TOKEN id="token-146-0" pos="unknown" morph="none" start_char="13285" end_char="13288">21(8</TOKEN>
<TOKEN id="token-146-1" pos="punct" morph="none" start_char="13289" end_char="13290">),</TOKEN>
<TOKEN id="token-146-2" pos="word" morph="none" start_char="13292" end_char="13295">2020</TOKEN>
<TOKEN id="token-146-3" pos="punct" morph="none" start_char="13296" end_char="13296">.</TOKEN>
</SEG>
<SEG id="segment-147" start_char="13299" end_char="13312">
<ORIGINAL_TEXT>Zhou, Peng et.</ORIGINAL_TEXT>
<TOKEN id="token-147-0" pos="word" morph="none" start_char="13299" end_char="13302">Zhou</TOKEN>
<TOKEN id="token-147-1" pos="punct" morph="none" start_char="13303" end_char="13303">,</TOKEN>
<TOKEN id="token-147-2" pos="word" morph="none" start_char="13305" end_char="13308">Peng</TOKEN>
<TOKEN id="token-147-3" pos="word" morph="none" start_char="13310" end_char="13311">et</TOKEN>
<TOKEN id="token-147-4" pos="punct" morph="none" start_char="13312" end_char="13312">.</TOKEN>
</SEG>
<SEG id="segment-148" start_char="13314" end_char="13316">
<ORIGINAL_TEXT>al.</ORIGINAL_TEXT>
<TOKEN id="token-148-0" pos="word" morph="none" start_char="13314" end_char="13315">al</TOKEN>
<TOKEN id="token-148-1" pos="punct" morph="none" start_char="13316" end_char="13316">.</TOKEN>
</SEG>
<SEG id="segment-149" start_char="13318" end_char="13397">
<ORIGINAL_TEXT>"A pneumonia outbreak associated with a new coronavirus of probable bat origin."</ORIGINAL_TEXT>
<TOKEN id="token-149-0" pos="punct" morph="none" start_char="13318" end_char="13318">"</TOKEN>
<TOKEN id="token-149-1" pos="word" morph="none" start_char="13319" end_char="13319">A</TOKEN>
<TOKEN id="token-149-2" pos="word" morph="none" start_char="13321" end_char="13329">pneumonia</TOKEN>
<TOKEN id="token-149-3" pos="word" morph="none" start_char="13331" end_char="13338">outbreak</TOKEN>
<TOKEN id="token-149-4" pos="word" morph="none" start_char="13340" end_char="13349">associated</TOKEN>
<TOKEN id="token-149-5" pos="word" morph="none" start_char="13351" end_char="13354">with</TOKEN>
<TOKEN id="token-149-6" pos="word" morph="none" start_char="13356" end_char="13356">a</TOKEN>
<TOKEN id="token-149-7" pos="word" morph="none" start_char="13358" end_char="13360">new</TOKEN>
<TOKEN id="token-149-8" pos="word" morph="none" start_char="13362" end_char="13372">coronavirus</TOKEN>
<TOKEN id="token-149-9" pos="word" morph="none" start_char="13374" end_char="13375">of</TOKEN>
<TOKEN id="token-149-10" pos="word" morph="none" start_char="13377" end_char="13384">probable</TOKEN>
<TOKEN id="token-149-11" pos="word" morph="none" start_char="13386" end_char="13388">bat</TOKEN>
<TOKEN id="token-149-12" pos="word" morph="none" start_char="13390" end_char="13395">origin</TOKEN>
<TOKEN id="token-149-13" pos="punct" morph="none" start_char="13396" end_char="13397">."</TOKEN>
</SEG>
<SEG id="segment-150" start_char="13399" end_char="13411">
<ORIGINAL_TEXT>Nature, 2020.</ORIGINAL_TEXT>
<TOKEN id="token-150-0" pos="word" morph="none" start_char="13399" end_char="13404">Nature</TOKEN>
<TOKEN id="token-150-1" pos="punct" morph="none" start_char="13405" end_char="13405">,</TOKEN>
<TOKEN id="token-150-2" pos="word" morph="none" start_char="13407" end_char="13410">2020</TOKEN>
<TOKEN id="token-150-3" pos="punct" morph="none" start_char="13411" end_char="13411">.</TOKEN>
</SEG>
<SEG id="segment-151" start_char="13414" end_char="13438">
<ORIGINAL_TEXT>Bedford, Trevor (@trvrb).</ORIGINAL_TEXT>
<TOKEN id="token-151-0" pos="word" morph="none" start_char="13414" end_char="13420">Bedford</TOKEN>
<TOKEN id="token-151-1" pos="punct" morph="none" start_char="13421" end_char="13421">,</TOKEN>
<TOKEN id="token-151-2" pos="word" morph="none" start_char="13423" end_char="13428">Trevor</TOKEN>
<TOKEN id="token-151-3" pos="punct" morph="none" start_char="13430" end_char="13431">(@</TOKEN>
<TOKEN id="token-151-4" pos="word" morph="none" start_char="13432" end_char="13436">trvrb</TOKEN>
<TOKEN id="token-151-5" pos="punct" morph="none" start_char="13437" end_char="13438">).</TOKEN>
</SEG>
<SEG id="segment-152" start_char="13440" end_char="13684">
<ORIGINAL_TEXT>"I thought to examine signal of natural evolution in #nCoV2019 by looking at distribution of mutations in the lineage leading to nCoV and compare this distribution to mutations occurring in other SARS-like bat viruses 1/9, and following thread."</ORIGINAL_TEXT>
<TOKEN id="token-152-0" pos="punct" morph="none" start_char="13440" end_char="13440">"</TOKEN>
<TOKEN id="token-152-1" pos="word" morph="none" start_char="13441" end_char="13441">I</TOKEN>
<TOKEN id="token-152-2" pos="word" morph="none" start_char="13443" end_char="13449">thought</TOKEN>
<TOKEN id="token-152-3" pos="word" morph="none" start_char="13451" end_char="13452">to</TOKEN>
<TOKEN id="token-152-4" pos="word" morph="none" start_char="13454" end_char="13460">examine</TOKEN>
<TOKEN id="token-152-5" pos="word" morph="none" start_char="13462" end_char="13467">signal</TOKEN>
<TOKEN id="token-152-6" pos="word" morph="none" start_char="13469" end_char="13470">of</TOKEN>
<TOKEN id="token-152-7" pos="word" morph="none" start_char="13472" end_char="13478">natural</TOKEN>
<TOKEN id="token-152-8" pos="word" morph="none" start_char="13480" end_char="13488">evolution</TOKEN>
<TOKEN id="token-152-9" pos="word" morph="none" start_char="13490" end_char="13491">in</TOKEN>
<TOKEN id="token-152-10" pos="tag" morph="none" start_char="13493" end_char="13501">#nCoV2019</TOKEN>
<TOKEN id="token-152-11" pos="word" morph="none" start_char="13503" end_char="13504">by</TOKEN>
<TOKEN id="token-152-12" pos="word" morph="none" start_char="13506" end_char="13512">looking</TOKEN>
<TOKEN id="token-152-13" pos="word" morph="none" start_char="13514" end_char="13515">at</TOKEN>
<TOKEN id="token-152-14" pos="word" morph="none" start_char="13517" end_char="13528">distribution</TOKEN>
<TOKEN id="token-152-15" pos="word" morph="none" start_char="13530" end_char="13531">of</TOKEN>
<TOKEN id="token-152-16" pos="word" morph="none" start_char="13533" end_char="13541">mutations</TOKEN>
<TOKEN id="token-152-17" pos="word" morph="none" start_char="13543" end_char="13544">in</TOKEN>
<TOKEN id="token-152-18" pos="word" morph="none" start_char="13546" end_char="13548">the</TOKEN>
<TOKEN id="token-152-19" pos="word" morph="none" start_char="13550" end_char="13556">lineage</TOKEN>
<TOKEN id="token-152-20" pos="word" morph="none" start_char="13558" end_char="13564">leading</TOKEN>
<TOKEN id="token-152-21" pos="word" morph="none" start_char="13566" end_char="13567">to</TOKEN>
<TOKEN id="token-152-22" pos="word" morph="none" start_char="13569" end_char="13572">nCoV</TOKEN>
<TOKEN id="token-152-23" pos="word" morph="none" start_char="13574" end_char="13576">and</TOKEN>
<TOKEN id="token-152-24" pos="word" morph="none" start_char="13578" end_char="13584">compare</TOKEN>
<TOKEN id="token-152-25" pos="word" morph="none" start_char="13586" end_char="13589">this</TOKEN>
<TOKEN id="token-152-26" pos="word" morph="none" start_char="13591" end_char="13602">distribution</TOKEN>
<TOKEN id="token-152-27" pos="word" morph="none" start_char="13604" end_char="13605">to</TOKEN>
<TOKEN id="token-152-28" pos="word" morph="none" start_char="13607" end_char="13615">mutations</TOKEN>
<TOKEN id="token-152-29" pos="word" morph="none" start_char="13617" end_char="13625">occurring</TOKEN>
<TOKEN id="token-152-30" pos="word" morph="none" start_char="13627" end_char="13628">in</TOKEN>
<TOKEN id="token-152-31" pos="word" morph="none" start_char="13630" end_char="13634">other</TOKEN>
<TOKEN id="token-152-32" pos="unknown" morph="none" start_char="13636" end_char="13644">SARS-like</TOKEN>
<TOKEN id="token-152-33" pos="word" morph="none" start_char="13646" end_char="13648">bat</TOKEN>
<TOKEN id="token-152-34" pos="word" morph="none" start_char="13650" end_char="13656">viruses</TOKEN>
<TOKEN id="token-152-35" pos="unknown" morph="none" start_char="13658" end_char="13660">1/9</TOKEN>
<TOKEN id="token-152-36" pos="punct" morph="none" start_char="13661" end_char="13661">,</TOKEN>
<TOKEN id="token-152-37" pos="word" morph="none" start_char="13663" end_char="13665">and</TOKEN>
<TOKEN id="token-152-38" pos="word" morph="none" start_char="13667" end_char="13675">following</TOKEN>
<TOKEN id="token-152-39" pos="word" morph="none" start_char="13677" end_char="13682">thread</TOKEN>
<TOKEN id="token-152-40" pos="punct" morph="none" start_char="13683" end_char="13684">."</TOKEN>
</SEG>
<SEG id="segment-153" start_char="13686" end_char="13693">
<ORIGINAL_TEXT>Twitter.</ORIGINAL_TEXT>
<TOKEN id="token-153-0" pos="word" morph="none" start_char="13686" end_char="13692">Twitter</TOKEN>
<TOKEN id="token-153-1" pos="punct" morph="none" start_char="13693" end_char="13693">.</TOKEN>
</SEG>
<SEG id="segment-154" start_char="13695" end_char="13705">
<ORIGINAL_TEXT>3 Feb 2020.</ORIGINAL_TEXT>
<TOKEN id="token-154-0" pos="word" morph="none" start_char="13695" end_char="13695">3</TOKEN>
<TOKEN id="token-154-1" pos="word" morph="none" start_char="13697" end_char="13699">Feb</TOKEN>
<TOKEN id="token-154-2" pos="word" morph="none" start_char="13701" end_char="13704">2020</TOKEN>
<TOKEN id="token-154-3" pos="punct" morph="none" start_char="13705" end_char="13705">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
