<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CAAX" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="5071" raw_text_md5="a0bb9c97814b4dc318218f7e2167c7dd">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="76">
<ORIGINAL_TEXT>Chinese expert cites unverified study to say coronavirus originated in Spain</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="7">Chinese</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="9" end_char="14">expert</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="16" end_char="20">cites</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="22" end_char="31">unverified</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="33" end_char="37">study</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="39" end_char="40">to</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="42" end_char="44">say</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="46" end_char="56">coronavirus</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="58" end_char="67">originated</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="69" end_char="70">in</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="72" end_char="76">Spain</TOKEN>
</SEG>
<SEG id="segment-1" start_char="80" end_char="231">
<ORIGINAL_TEXT>This undated electron microscope image made available by the U.S. National Institutes of Health in February 2020 shows the Novel Coronavirus SARS-CoV-2.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="80" end_char="83">This</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="85" end_char="91">undated</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="93" end_char="100">electron</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="102" end_char="111">microscope</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="113" end_char="117">image</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="119" end_char="122">made</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="124" end_char="132">available</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="134" end_char="135">by</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="137" end_char="139">the</TOKEN>
<TOKEN id="token-1-9" pos="unknown" morph="none" start_char="141" end_char="143">U.S</TOKEN>
<TOKEN id="token-1-10" pos="punct" morph="none" start_char="144" end_char="144">.</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="146" end_char="153">National</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="155" end_char="164">Institutes</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="166" end_char="167">of</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="169" end_char="174">Health</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="176" end_char="177">in</TOKEN>
<TOKEN id="token-1-16" pos="word" morph="none" start_char="179" end_char="186">February</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="188" end_char="191">2020</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="193" end_char="197">shows</TOKEN>
<TOKEN id="token-1-19" pos="word" morph="none" start_char="199" end_char="201">the</TOKEN>
<TOKEN id="token-1-20" pos="word" morph="none" start_char="203" end_char="207">Novel</TOKEN>
<TOKEN id="token-1-21" pos="word" morph="none" start_char="209" end_char="219">Coronavirus</TOKEN>
<TOKEN id="token-1-22" pos="unknown" morph="none" start_char="221" end_char="230">SARS-CoV-2</TOKEN>
<TOKEN id="token-1-23" pos="punct" morph="none" start_char="231" end_char="231">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="233" end_char="283">
<ORIGINAL_TEXT>Also known as 2019-nCoV, the virus causes COVID-19.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="233" end_char="236">Also</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="238" end_char="242">known</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="244" end_char="245">as</TOKEN>
<TOKEN id="token-2-3" pos="unknown" morph="none" start_char="247" end_char="255">2019-nCoV</TOKEN>
<TOKEN id="token-2-4" pos="punct" morph="none" start_char="256" end_char="256">,</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="258" end_char="260">the</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="262" end_char="266">virus</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="268" end_char="273">causes</TOKEN>
<TOKEN id="token-2-8" pos="unknown" morph="none" start_char="275" end_char="282">COVID-19</TOKEN>
<TOKEN id="token-2-9" pos="punct" morph="none" start_char="283" end_char="283">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="285" end_char="352">
<ORIGINAL_TEXT>The sample was isolated from a patient in the U.S | NIAID-RML via AP</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="285" end_char="287">The</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="289" end_char="294">sample</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="296" end_char="298">was</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="300" end_char="307">isolated</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="309" end_char="312">from</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="314" end_char="314">a</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="316" end_char="322">patient</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="324" end_char="325">in</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="327" end_char="329">the</TOKEN>
<TOKEN id="token-3-9" pos="unknown" morph="none" start_char="331" end_char="333">U.S</TOKEN>
<TOKEN id="token-3-10" pos="unknown" morph="none" start_char="335" end_char="335">|</TOKEN>
<TOKEN id="token-3-11" pos="unknown" morph="none" start_char="337" end_char="345">NIAID-RML</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="347" end_char="349">via</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="351" end_char="352">AP</TOKEN>
</SEG>
<SEG id="segment-4" start_char="356" end_char="608">
<ORIGINAL_TEXT>As the World Health Organisation prepares to visit China next week for a "scoping mission" into the origins of the novel coronavirus, a senior health advisor with the Chinese government has suggested the WHO look at other countries too, including Spain.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="356" end_char="357">As</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="359" end_char="361">the</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="363" end_char="367">World</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="369" end_char="374">Health</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="376" end_char="387">Organisation</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="389" end_char="396">prepares</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="398" end_char="399">to</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="401" end_char="405">visit</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="407" end_char="411">China</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="413" end_char="416">next</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="418" end_char="421">week</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="423" end_char="425">for</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="427" end_char="427">a</TOKEN>
<TOKEN id="token-4-13" pos="punct" morph="none" start_char="429" end_char="429">"</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="430" end_char="436">scoping</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="438" end_char="444">mission</TOKEN>
<TOKEN id="token-4-16" pos="punct" morph="none" start_char="445" end_char="445">"</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="447" end_char="450">into</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="452" end_char="454">the</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="456" end_char="462">origins</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="464" end_char="465">of</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="467" end_char="469">the</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="471" end_char="475">novel</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="477" end_char="487">coronavirus</TOKEN>
<TOKEN id="token-4-24" pos="punct" morph="none" start_char="488" end_char="488">,</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="490" end_char="490">a</TOKEN>
<TOKEN id="token-4-26" pos="word" morph="none" start_char="492" end_char="497">senior</TOKEN>
<TOKEN id="token-4-27" pos="word" morph="none" start_char="499" end_char="504">health</TOKEN>
<TOKEN id="token-4-28" pos="word" morph="none" start_char="506" end_char="512">advisor</TOKEN>
<TOKEN id="token-4-29" pos="word" morph="none" start_char="514" end_char="517">with</TOKEN>
<TOKEN id="token-4-30" pos="word" morph="none" start_char="519" end_char="521">the</TOKEN>
<TOKEN id="token-4-31" pos="word" morph="none" start_char="523" end_char="529">Chinese</TOKEN>
<TOKEN id="token-4-32" pos="word" morph="none" start_char="531" end_char="540">government</TOKEN>
<TOKEN id="token-4-33" pos="word" morph="none" start_char="542" end_char="544">has</TOKEN>
<TOKEN id="token-4-34" pos="word" morph="none" start_char="546" end_char="554">suggested</TOKEN>
<TOKEN id="token-4-35" pos="word" morph="none" start_char="556" end_char="558">the</TOKEN>
<TOKEN id="token-4-36" pos="word" morph="none" start_char="560" end_char="562">WHO</TOKEN>
<TOKEN id="token-4-37" pos="word" morph="none" start_char="564" end_char="567">look</TOKEN>
<TOKEN id="token-4-38" pos="word" morph="none" start_char="569" end_char="570">at</TOKEN>
<TOKEN id="token-4-39" pos="word" morph="none" start_char="572" end_char="576">other</TOKEN>
<TOKEN id="token-4-40" pos="word" morph="none" start_char="578" end_char="586">countries</TOKEN>
<TOKEN id="token-4-41" pos="word" morph="none" start_char="588" end_char="590">too</TOKEN>
<TOKEN id="token-4-42" pos="punct" morph="none" start_char="591" end_char="591">,</TOKEN>
<TOKEN id="token-4-43" pos="word" morph="none" start_char="593" end_char="601">including</TOKEN>
<TOKEN id="token-4-44" pos="word" morph="none" start_char="603" end_char="607">Spain</TOKEN>
<TOKEN id="token-4-45" pos="punct" morph="none" start_char="608" end_char="608">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="611" end_char="627">
<ORIGINAL_TEXT>Wang Guangfa told</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="611" end_char="614">Wang</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="616" end_char="622">Guangfa</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="624" end_char="627">told</TOKEN>
</SEG>
<SEG id="segment-6" start_char="630" end_char="641">
<ORIGINAL_TEXT>Global Times</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="630" end_char="635">Global</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="637" end_char="641">Times</TOKEN>
</SEG>
<SEG id="segment-7" start_char="644" end_char="904">
<ORIGINAL_TEXT>that China is only a link in the virus transmission chain, and said that the WHO should go to "more countries such as Spain, which reported coronavirus in its wastewater sample collected in March 2019, for more comprehensive investigations on the virus origin".</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="644" end_char="647">that</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="649" end_char="653">China</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="655" end_char="656">is</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="658" end_char="661">only</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="663" end_char="663">a</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="665" end_char="668">link</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="670" end_char="671">in</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="673" end_char="675">the</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="677" end_char="681">virus</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="683" end_char="694">transmission</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="696" end_char="700">chain</TOKEN>
<TOKEN id="token-7-11" pos="punct" morph="none" start_char="701" end_char="701">,</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="703" end_char="705">and</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="707" end_char="710">said</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="712" end_char="715">that</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="717" end_char="719">the</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="721" end_char="723">WHO</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="725" end_char="730">should</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="732" end_char="733">go</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="735" end_char="736">to</TOKEN>
<TOKEN id="token-7-20" pos="punct" morph="none" start_char="738" end_char="738">"</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="739" end_char="742">more</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="744" end_char="752">countries</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="754" end_char="757">such</TOKEN>
<TOKEN id="token-7-24" pos="word" morph="none" start_char="759" end_char="760">as</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="762" end_char="766">Spain</TOKEN>
<TOKEN id="token-7-26" pos="punct" morph="none" start_char="767" end_char="767">,</TOKEN>
<TOKEN id="token-7-27" pos="word" morph="none" start_char="769" end_char="773">which</TOKEN>
<TOKEN id="token-7-28" pos="word" morph="none" start_char="775" end_char="782">reported</TOKEN>
<TOKEN id="token-7-29" pos="word" morph="none" start_char="784" end_char="794">coronavirus</TOKEN>
<TOKEN id="token-7-30" pos="word" morph="none" start_char="796" end_char="797">in</TOKEN>
<TOKEN id="token-7-31" pos="word" morph="none" start_char="799" end_char="801">its</TOKEN>
<TOKEN id="token-7-32" pos="word" morph="none" start_char="803" end_char="812">wastewater</TOKEN>
<TOKEN id="token-7-33" pos="word" morph="none" start_char="814" end_char="819">sample</TOKEN>
<TOKEN id="token-7-34" pos="word" morph="none" start_char="821" end_char="829">collected</TOKEN>
<TOKEN id="token-7-35" pos="word" morph="none" start_char="831" end_char="832">in</TOKEN>
<TOKEN id="token-7-36" pos="word" morph="none" start_char="834" end_char="838">March</TOKEN>
<TOKEN id="token-7-37" pos="word" morph="none" start_char="840" end_char="843">2019</TOKEN>
<TOKEN id="token-7-38" pos="punct" morph="none" start_char="844" end_char="844">,</TOKEN>
<TOKEN id="token-7-39" pos="word" morph="none" start_char="846" end_char="848">for</TOKEN>
<TOKEN id="token-7-40" pos="word" morph="none" start_char="850" end_char="853">more</TOKEN>
<TOKEN id="token-7-41" pos="word" morph="none" start_char="855" end_char="867">comprehensive</TOKEN>
<TOKEN id="token-7-42" pos="word" morph="none" start_char="869" end_char="882">investigations</TOKEN>
<TOKEN id="token-7-43" pos="word" morph="none" start_char="884" end_char="885">on</TOKEN>
<TOKEN id="token-7-44" pos="word" morph="none" start_char="887" end_char="889">the</TOKEN>
<TOKEN id="token-7-45" pos="word" morph="none" start_char="891" end_char="895">virus</TOKEN>
<TOKEN id="token-7-46" pos="word" morph="none" start_char="897" end_char="902">origin</TOKEN>
<TOKEN id="token-7-47" pos="punct" morph="none" start_char="903" end_char="904">".</TOKEN>
</SEG>
<SEG id="segment-8" start_char="907" end_char="1175">
<ORIGINAL_TEXT>Wang’s claim has since been reported by several news outlets to suggest that China referred to a pre-print study by researchers from the University of Barcelona, which claimed to find evidence of COVID-19 being present in Barcelona’s wastewater as early as March, 2019.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="907" end_char="912">Wang’s</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="914" end_char="918">claim</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="920" end_char="922">has</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="924" end_char="928">since</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="930" end_char="933">been</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="935" end_char="942">reported</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="944" end_char="945">by</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="947" end_char="953">several</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="955" end_char="958">news</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="960" end_char="966">outlets</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="968" end_char="969">to</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="971" end_char="977">suggest</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="979" end_char="982">that</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="984" end_char="988">China</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="990" end_char="997">referred</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="999" end_char="1000">to</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="1002" end_char="1002">a</TOKEN>
<TOKEN id="token-8-17" pos="unknown" morph="none" start_char="1004" end_char="1012">pre-print</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="1014" end_char="1018">study</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="1020" end_char="1021">by</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="1023" end_char="1033">researchers</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="1035" end_char="1038">from</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="1040" end_char="1042">the</TOKEN>
<TOKEN id="token-8-23" pos="word" morph="none" start_char="1044" end_char="1053">University</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="1055" end_char="1056">of</TOKEN>
<TOKEN id="token-8-25" pos="word" morph="none" start_char="1058" end_char="1066">Barcelona</TOKEN>
<TOKEN id="token-8-26" pos="punct" morph="none" start_char="1067" end_char="1067">,</TOKEN>
<TOKEN id="token-8-27" pos="word" morph="none" start_char="1069" end_char="1073">which</TOKEN>
<TOKEN id="token-8-28" pos="word" morph="none" start_char="1075" end_char="1081">claimed</TOKEN>
<TOKEN id="token-8-29" pos="word" morph="none" start_char="1083" end_char="1084">to</TOKEN>
<TOKEN id="token-8-30" pos="word" morph="none" start_char="1086" end_char="1089">find</TOKEN>
<TOKEN id="token-8-31" pos="word" morph="none" start_char="1091" end_char="1098">evidence</TOKEN>
<TOKEN id="token-8-32" pos="word" morph="none" start_char="1100" end_char="1101">of</TOKEN>
<TOKEN id="token-8-33" pos="unknown" morph="none" start_char="1103" end_char="1110">COVID-19</TOKEN>
<TOKEN id="token-8-34" pos="word" morph="none" start_char="1112" end_char="1116">being</TOKEN>
<TOKEN id="token-8-35" pos="word" morph="none" start_char="1118" end_char="1124">present</TOKEN>
<TOKEN id="token-8-36" pos="word" morph="none" start_char="1126" end_char="1127">in</TOKEN>
<TOKEN id="token-8-37" pos="word" morph="none" start_char="1129" end_char="1139">Barcelona’s</TOKEN>
<TOKEN id="token-8-38" pos="word" morph="none" start_char="1141" end_char="1150">wastewater</TOKEN>
<TOKEN id="token-8-39" pos="word" morph="none" start_char="1152" end_char="1153">as</TOKEN>
<TOKEN id="token-8-40" pos="word" morph="none" start_char="1155" end_char="1159">early</TOKEN>
<TOKEN id="token-8-41" pos="word" morph="none" start_char="1161" end_char="1162">as</TOKEN>
<TOKEN id="token-8-42" pos="word" morph="none" start_char="1164" end_char="1168">March</TOKEN>
<TOKEN id="token-8-43" pos="punct" morph="none" start_char="1169" end_char="1169">,</TOKEN>
<TOKEN id="token-8-44" pos="word" morph="none" start_char="1171" end_char="1174">2019</TOKEN>
<TOKEN id="token-8-45" pos="punct" morph="none" start_char="1175" end_char="1175">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1178" end_char="1393">
<ORIGINAL_TEXT>The study, which was uploaded as a pre-print—before facing peer-review—on MedRxiv, tested samples of Barcelona wastewater for presence of the virus, using WHO-recommended real-time RT-PCR assays to detect SARS-CoV-2.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1178" end_char="1180">The</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1182" end_char="1186">study</TOKEN>
<TOKEN id="token-9-2" pos="punct" morph="none" start_char="1187" end_char="1187">,</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1189" end_char="1193">which</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1195" end_char="1197">was</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1199" end_char="1206">uploaded</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1208" end_char="1209">as</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1211" end_char="1211">a</TOKEN>
<TOKEN id="token-9-8" pos="unknown" morph="none" start_char="1213" end_char="1228">pre-print—before</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="1230" end_char="1235">facing</TOKEN>
<TOKEN id="token-9-10" pos="unknown" morph="none" start_char="1237" end_char="1250">peer-review—on</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1252" end_char="1258">MedRxiv</TOKEN>
<TOKEN id="token-9-12" pos="punct" morph="none" start_char="1259" end_char="1259">,</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1261" end_char="1266">tested</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1268" end_char="1274">samples</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1276" end_char="1277">of</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="1279" end_char="1287">Barcelona</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1289" end_char="1298">wastewater</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1300" end_char="1302">for</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1304" end_char="1311">presence</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1313" end_char="1314">of</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1316" end_char="1318">the</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1320" end_char="1324">virus</TOKEN>
<TOKEN id="token-9-23" pos="punct" morph="none" start_char="1325" end_char="1325">,</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1327" end_char="1331">using</TOKEN>
<TOKEN id="token-9-25" pos="unknown" morph="none" start_char="1333" end_char="1347">WHO-recommended</TOKEN>
<TOKEN id="token-9-26" pos="unknown" morph="none" start_char="1349" end_char="1357">real-time</TOKEN>
<TOKEN id="token-9-27" pos="unknown" morph="none" start_char="1359" end_char="1364">RT-PCR</TOKEN>
<TOKEN id="token-9-28" pos="word" morph="none" start_char="1366" end_char="1371">assays</TOKEN>
<TOKEN id="token-9-29" pos="word" morph="none" start_char="1373" end_char="1374">to</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1376" end_char="1381">detect</TOKEN>
<TOKEN id="token-9-31" pos="unknown" morph="none" start_char="1383" end_char="1392">SARS-CoV-2</TOKEN>
<TOKEN id="token-9-32" pos="punct" morph="none" start_char="1393" end_char="1393">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1396" end_char="1714">
<ORIGINAL_TEXT>While the first known cases of the virus in Spain were see in February, with the coronavirus widely believed to have originated in Wuhan around December in 2019, the study found, from one frozen sample of wastewater from Barcelona dating to March 2019, presence of the virus nine months before it was detected in Spain.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1396" end_char="1400">While</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1402" end_char="1404">the</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1406" end_char="1410">first</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1412" end_char="1416">known</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1418" end_char="1422">cases</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1424" end_char="1425">of</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1427" end_char="1429">the</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1431" end_char="1435">virus</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1437" end_char="1438">in</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1440" end_char="1444">Spain</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1446" end_char="1449">were</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1451" end_char="1453">see</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1455" end_char="1456">in</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1458" end_char="1465">February</TOKEN>
<TOKEN id="token-10-14" pos="punct" morph="none" start_char="1466" end_char="1466">,</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1468" end_char="1471">with</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1473" end_char="1475">the</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1477" end_char="1487">coronavirus</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1489" end_char="1494">widely</TOKEN>
<TOKEN id="token-10-19" pos="word" morph="none" start_char="1496" end_char="1503">believed</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="1505" end_char="1506">to</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="1508" end_char="1511">have</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="1513" end_char="1522">originated</TOKEN>
<TOKEN id="token-10-23" pos="word" morph="none" start_char="1524" end_char="1525">in</TOKEN>
<TOKEN id="token-10-24" pos="word" morph="none" start_char="1527" end_char="1531">Wuhan</TOKEN>
<TOKEN id="token-10-25" pos="word" morph="none" start_char="1533" end_char="1538">around</TOKEN>
<TOKEN id="token-10-26" pos="word" morph="none" start_char="1540" end_char="1547">December</TOKEN>
<TOKEN id="token-10-27" pos="word" morph="none" start_char="1549" end_char="1550">in</TOKEN>
<TOKEN id="token-10-28" pos="word" morph="none" start_char="1552" end_char="1555">2019</TOKEN>
<TOKEN id="token-10-29" pos="punct" morph="none" start_char="1556" end_char="1556">,</TOKEN>
<TOKEN id="token-10-30" pos="word" morph="none" start_char="1558" end_char="1560">the</TOKEN>
<TOKEN id="token-10-31" pos="word" morph="none" start_char="1562" end_char="1566">study</TOKEN>
<TOKEN id="token-10-32" pos="word" morph="none" start_char="1568" end_char="1572">found</TOKEN>
<TOKEN id="token-10-33" pos="punct" morph="none" start_char="1573" end_char="1573">,</TOKEN>
<TOKEN id="token-10-34" pos="word" morph="none" start_char="1575" end_char="1578">from</TOKEN>
<TOKEN id="token-10-35" pos="word" morph="none" start_char="1580" end_char="1582">one</TOKEN>
<TOKEN id="token-10-36" pos="word" morph="none" start_char="1584" end_char="1589">frozen</TOKEN>
<TOKEN id="token-10-37" pos="word" morph="none" start_char="1591" end_char="1596">sample</TOKEN>
<TOKEN id="token-10-38" pos="word" morph="none" start_char="1598" end_char="1599">of</TOKEN>
<TOKEN id="token-10-39" pos="word" morph="none" start_char="1601" end_char="1610">wastewater</TOKEN>
<TOKEN id="token-10-40" pos="word" morph="none" start_char="1612" end_char="1615">from</TOKEN>
<TOKEN id="token-10-41" pos="word" morph="none" start_char="1617" end_char="1625">Barcelona</TOKEN>
<TOKEN id="token-10-42" pos="word" morph="none" start_char="1627" end_char="1632">dating</TOKEN>
<TOKEN id="token-10-43" pos="word" morph="none" start_char="1634" end_char="1635">to</TOKEN>
<TOKEN id="token-10-44" pos="word" morph="none" start_char="1637" end_char="1641">March</TOKEN>
<TOKEN id="token-10-45" pos="word" morph="none" start_char="1643" end_char="1646">2019</TOKEN>
<TOKEN id="token-10-46" pos="punct" morph="none" start_char="1647" end_char="1647">,</TOKEN>
<TOKEN id="token-10-47" pos="word" morph="none" start_char="1649" end_char="1656">presence</TOKEN>
<TOKEN id="token-10-48" pos="word" morph="none" start_char="1658" end_char="1659">of</TOKEN>
<TOKEN id="token-10-49" pos="word" morph="none" start_char="1661" end_char="1663">the</TOKEN>
<TOKEN id="token-10-50" pos="word" morph="none" start_char="1665" end_char="1669">virus</TOKEN>
<TOKEN id="token-10-51" pos="word" morph="none" start_char="1671" end_char="1674">nine</TOKEN>
<TOKEN id="token-10-52" pos="word" morph="none" start_char="1676" end_char="1681">months</TOKEN>
<TOKEN id="token-10-53" pos="word" morph="none" start_char="1683" end_char="1688">before</TOKEN>
<TOKEN id="token-10-54" pos="word" morph="none" start_char="1690" end_char="1691">it</TOKEN>
<TOKEN id="token-10-55" pos="word" morph="none" start_char="1693" end_char="1695">was</TOKEN>
<TOKEN id="token-10-56" pos="word" morph="none" start_char="1697" end_char="1704">detected</TOKEN>
<TOKEN id="token-10-57" pos="word" morph="none" start_char="1706" end_char="1707">in</TOKEN>
<TOKEN id="token-10-58" pos="word" morph="none" start_char="1709" end_char="1713">Spain</TOKEN>
<TOKEN id="token-10-59" pos="punct" morph="none" start_char="1714" end_char="1714">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1717" end_char="1893">
<ORIGINAL_TEXT>Since the virus can be detected in human feces, wastewater analyses have proven a useful tool for measuring when and in what quantity the virus was present in different regions.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1717" end_char="1721">Since</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1723" end_char="1725">the</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1727" end_char="1731">virus</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1733" end_char="1735">can</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1737" end_char="1738">be</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1740" end_char="1747">detected</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1749" end_char="1750">in</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1752" end_char="1756">human</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1758" end_char="1762">feces</TOKEN>
<TOKEN id="token-11-9" pos="punct" morph="none" start_char="1763" end_char="1763">,</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1765" end_char="1774">wastewater</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1776" end_char="1783">analyses</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1785" end_char="1788">have</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1790" end_char="1795">proven</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1797" end_char="1797">a</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1799" end_char="1804">useful</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1806" end_char="1809">tool</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1811" end_char="1813">for</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1815" end_char="1823">measuring</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1825" end_char="1828">when</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1830" end_char="1832">and</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1834" end_char="1835">in</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1837" end_char="1840">what</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1842" end_char="1849">quantity</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1851" end_char="1853">the</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1855" end_char="1859">virus</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="1861" end_char="1863">was</TOKEN>
<TOKEN id="token-11-27" pos="word" morph="none" start_char="1865" end_char="1871">present</TOKEN>
<TOKEN id="token-11-28" pos="word" morph="none" start_char="1873" end_char="1874">in</TOKEN>
<TOKEN id="token-11-29" pos="word" morph="none" start_char="1876" end_char="1884">different</TOKEN>
<TOKEN id="token-11-30" pos="word" morph="none" start_char="1886" end_char="1892">regions</TOKEN>
<TOKEN id="token-11-31" pos="punct" morph="none" start_char="1893" end_char="1893">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1895" end_char="2007">
<ORIGINAL_TEXT>A wastewater study in Northern Italy found the presence of the virus in wastewater from before Christmas of 2019.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1895" end_char="1895">A</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1897" end_char="1906">wastewater</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1908" end_char="1912">study</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1914" end_char="1915">in</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1917" end_char="1924">Northern</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1926" end_char="1930">Italy</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1932" end_char="1936">found</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1938" end_char="1940">the</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1942" end_char="1949">presence</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1951" end_char="1952">of</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1954" end_char="1956">the</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1958" end_char="1962">virus</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1964" end_char="1965">in</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1967" end_char="1976">wastewater</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1978" end_char="1981">from</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1983" end_char="1988">before</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1990" end_char="1998">Christmas</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="2000" end_char="2001">of</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="2003" end_char="2006">2019</TOKEN>
<TOKEN id="token-12-19" pos="punct" morph="none" start_char="2007" end_char="2007">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="2009" end_char="2123">
<ORIGINAL_TEXT>However, the University of Barcelona (UoB) study suggests a much earlier date of incidence than any other analysis.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="2009" end_char="2015">However</TOKEN>
<TOKEN id="token-13-1" pos="punct" morph="none" start_char="2016" end_char="2016">,</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="2018" end_char="2020">the</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="2022" end_char="2031">University</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="2033" end_char="2034">of</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="2036" end_char="2044">Barcelona</TOKEN>
<TOKEN id="token-13-6" pos="punct" morph="none" start_char="2046" end_char="2046">(</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="2047" end_char="2049">UoB</TOKEN>
<TOKEN id="token-13-8" pos="punct" morph="none" start_char="2050" end_char="2050">)</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="2052" end_char="2056">study</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="2058" end_char="2065">suggests</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="2067" end_char="2067">a</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="2069" end_char="2072">much</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="2074" end_char="2080">earlier</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="2082" end_char="2085">date</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="2087" end_char="2088">of</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="2090" end_char="2098">incidence</TOKEN>
<TOKEN id="token-13-17" pos="word" morph="none" start_char="2100" end_char="2103">than</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="2105" end_char="2107">any</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="2109" end_char="2113">other</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="2115" end_char="2122">analysis</TOKEN>
<TOKEN id="token-13-21" pos="punct" morph="none" start_char="2123" end_char="2123">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="2126" end_char="2303">
<ORIGINAL_TEXT>"Most COVID-19 cases show mild influenza-like symptoms and it has been suggested that some uncharacterised influenza cases may have masked COVID-19 cases in the 2019-2020 season.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="punct" morph="none" start_char="2126" end_char="2126">"</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="2127" end_char="2130">Most</TOKEN>
<TOKEN id="token-14-2" pos="unknown" morph="none" start_char="2132" end_char="2139">COVID-19</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="2141" end_char="2145">cases</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="2147" end_char="2150">show</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="2152" end_char="2155">mild</TOKEN>
<TOKEN id="token-14-6" pos="unknown" morph="none" start_char="2157" end_char="2170">influenza-like</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="2172" end_char="2179">symptoms</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="2181" end_char="2183">and</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="2185" end_char="2186">it</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="2188" end_char="2190">has</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="2192" end_char="2195">been</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="2197" end_char="2205">suggested</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="2207" end_char="2210">that</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="2212" end_char="2215">some</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="2217" end_char="2231">uncharacterised</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="2233" end_char="2241">influenza</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="2243" end_char="2247">cases</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="2249" end_char="2251">may</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="2253" end_char="2256">have</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="2258" end_char="2263">masked</TOKEN>
<TOKEN id="token-14-21" pos="unknown" morph="none" start_char="2265" end_char="2272">COVID-19</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="2274" end_char="2278">cases</TOKEN>
<TOKEN id="token-14-23" pos="word" morph="none" start_char="2280" end_char="2281">in</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="2283" end_char="2285">the</TOKEN>
<TOKEN id="token-14-25" pos="unknown" morph="none" start_char="2287" end_char="2295">2019-2020</TOKEN>
<TOKEN id="token-14-26" pos="word" morph="none" start_char="2297" end_char="2302">season</TOKEN>
<TOKEN id="token-14-27" pos="punct" morph="none" start_char="2303" end_char="2303">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="2305" end_char="2406">
<ORIGINAL_TEXT>This possibility prompted us to analyse some archival WWTP samples from January 2018 to December 2019.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="2305" end_char="2308">This</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="2310" end_char="2320">possibility</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="2322" end_char="2329">prompted</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="2331" end_char="2332">us</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="2334" end_char="2335">to</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="2337" end_char="2343">analyse</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="2345" end_char="2348">some</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="2350" end_char="2357">archival</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="2359" end_char="2362">WWTP</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="2364" end_char="2370">samples</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="2372" end_char="2375">from</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="2377" end_char="2383">January</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="2385" end_char="2388">2018</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="2390" end_char="2391">to</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="2393" end_char="2400">December</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="2402" end_char="2405">2019</TOKEN>
<TOKEN id="token-15-16" pos="punct" morph="none" start_char="2406" end_char="2406">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="2408" end_char="2574">
<ORIGINAL_TEXT>All samples came out to be negative for the presence of SARS-CoV-2 genomes with the exception of March 12, 2019, in which both IP2 and IP4 target assays were positive.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="2408" end_char="2410">All</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="2412" end_char="2418">samples</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="2420" end_char="2423">came</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="2425" end_char="2427">out</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="2429" end_char="2430">to</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="2432" end_char="2433">be</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="2435" end_char="2442">negative</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="2444" end_char="2446">for</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="2448" end_char="2450">the</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="2452" end_char="2459">presence</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="2461" end_char="2462">of</TOKEN>
<TOKEN id="token-16-11" pos="unknown" morph="none" start_char="2464" end_char="2473">SARS-CoV-2</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="2475" end_char="2481">genomes</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2483" end_char="2486">with</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="2488" end_char="2490">the</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="2492" end_char="2500">exception</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="2502" end_char="2503">of</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="2505" end_char="2509">March</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="2511" end_char="2512">12</TOKEN>
<TOKEN id="token-16-19" pos="punct" morph="none" start_char="2513" end_char="2513">,</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="2515" end_char="2518">2019</TOKEN>
<TOKEN id="token-16-21" pos="punct" morph="none" start_char="2519" end_char="2519">,</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="2521" end_char="2522">in</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="2524" end_char="2528">which</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="2530" end_char="2533">both</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="2535" end_char="2537">IP2</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="2539" end_char="2541">and</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="2543" end_char="2545">IP4</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="2547" end_char="2552">target</TOKEN>
<TOKEN id="token-16-29" pos="word" morph="none" start_char="2554" end_char="2559">assays</TOKEN>
<TOKEN id="token-16-30" pos="word" morph="none" start_char="2561" end_char="2564">were</TOKEN>
<TOKEN id="token-16-31" pos="word" morph="none" start_char="2566" end_char="2573">positive</TOKEN>
<TOKEN id="token-16-32" pos="punct" morph="none" start_char="2574" end_char="2574">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2576" end_char="2716">
<ORIGINAL_TEXT>This striking finding indicates circulation of the virus in Barcelona long before the report of any COVID-19 case worldwide," the study says.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2576" end_char="2579">This</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2581" end_char="2588">striking</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2590" end_char="2596">finding</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2598" end_char="2606">indicates</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2608" end_char="2618">circulation</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2620" end_char="2621">of</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2623" end_char="2625">the</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="2627" end_char="2631">virus</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="2633" end_char="2634">in</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="2636" end_char="2644">Barcelona</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="2646" end_char="2649">long</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="2651" end_char="2656">before</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="2658" end_char="2660">the</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="2662" end_char="2667">report</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="2669" end_char="2670">of</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="2672" end_char="2674">any</TOKEN>
<TOKEN id="token-17-16" pos="unknown" morph="none" start_char="2676" end_char="2683">COVID-19</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="2685" end_char="2688">case</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="2690" end_char="2698">worldwide</TOKEN>
<TOKEN id="token-17-19" pos="punct" morph="none" start_char="2699" end_char="2700">,"</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="2702" end_char="2704">the</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="2706" end_char="2710">study</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="2712" end_char="2715">says</TOKEN>
<TOKEN id="token-17-23" pos="punct" morph="none" start_char="2716" end_char="2716">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2719" end_char="2798">
<ORIGINAL_TEXT>The lead author, Albert Bosch, has studied wastewater viruses for over 40 years.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2719" end_char="2721">The</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2723" end_char="2726">lead</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="2728" end_char="2733">author</TOKEN>
<TOKEN id="token-18-3" pos="punct" morph="none" start_char="2734" end_char="2734">,</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2736" end_char="2741">Albert</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="2743" end_char="2747">Bosch</TOKEN>
<TOKEN id="token-18-6" pos="punct" morph="none" start_char="2748" end_char="2748">,</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2750" end_char="2752">has</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2754" end_char="2760">studied</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2762" end_char="2771">wastewater</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2773" end_char="2779">viruses</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="2781" end_char="2783">for</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="2785" end_char="2788">over</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="2790" end_char="2791">40</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="2793" end_char="2797">years</TOKEN>
<TOKEN id="token-18-15" pos="punct" morph="none" start_char="2798" end_char="2798">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2800" end_char="2849">
<ORIGINAL_TEXT>However, the study’s results are being challenged.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2800" end_char="2806">However</TOKEN>
<TOKEN id="token-19-1" pos="punct" morph="none" start_char="2807" end_char="2807">,</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2809" end_char="2811">the</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="2813" end_char="2819">study’s</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2821" end_char="2827">results</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="2829" end_char="2831">are</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2833" end_char="2837">being</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="2839" end_char="2848">challenged</TOKEN>
<TOKEN id="token-19-8" pos="punct" morph="none" start_char="2849" end_char="2849">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2851" end_char="3117">
<ORIGINAL_TEXT>Dutch microbiologist Elisabeth Bik points out in a post on the Science Integrity Digest blog that while the study appears to show evidence of the virus being present in Spain a few weeks before it was officially discovered, the test from March 2019 is not conclusive.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2851" end_char="2855">Dutch</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="2857" end_char="2870">microbiologist</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="2872" end_char="2880">Elisabeth</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="2882" end_char="2884">Bik</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2886" end_char="2891">points</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2893" end_char="2895">out</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2897" end_char="2898">in</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="2900" end_char="2900">a</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="2902" end_char="2905">post</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="2907" end_char="2908">on</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="2910" end_char="2912">the</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="2914" end_char="2920">Science</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="2922" end_char="2930">Integrity</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="2932" end_char="2937">Digest</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="2939" end_char="2942">blog</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="2944" end_char="2947">that</TOKEN>
<TOKEN id="token-20-16" pos="word" morph="none" start_char="2949" end_char="2953">while</TOKEN>
<TOKEN id="token-20-17" pos="word" morph="none" start_char="2955" end_char="2957">the</TOKEN>
<TOKEN id="token-20-18" pos="word" morph="none" start_char="2959" end_char="2963">study</TOKEN>
<TOKEN id="token-20-19" pos="word" morph="none" start_char="2965" end_char="2971">appears</TOKEN>
<TOKEN id="token-20-20" pos="word" morph="none" start_char="2973" end_char="2974">to</TOKEN>
<TOKEN id="token-20-21" pos="word" morph="none" start_char="2976" end_char="2979">show</TOKEN>
<TOKEN id="token-20-22" pos="word" morph="none" start_char="2981" end_char="2988">evidence</TOKEN>
<TOKEN id="token-20-23" pos="word" morph="none" start_char="2990" end_char="2991">of</TOKEN>
<TOKEN id="token-20-24" pos="word" morph="none" start_char="2993" end_char="2995">the</TOKEN>
<TOKEN id="token-20-25" pos="word" morph="none" start_char="2997" end_char="3001">virus</TOKEN>
<TOKEN id="token-20-26" pos="word" morph="none" start_char="3003" end_char="3007">being</TOKEN>
<TOKEN id="token-20-27" pos="word" morph="none" start_char="3009" end_char="3015">present</TOKEN>
<TOKEN id="token-20-28" pos="word" morph="none" start_char="3017" end_char="3018">in</TOKEN>
<TOKEN id="token-20-29" pos="word" morph="none" start_char="3020" end_char="3024">Spain</TOKEN>
<TOKEN id="token-20-30" pos="word" morph="none" start_char="3026" end_char="3026">a</TOKEN>
<TOKEN id="token-20-31" pos="word" morph="none" start_char="3028" end_char="3030">few</TOKEN>
<TOKEN id="token-20-32" pos="word" morph="none" start_char="3032" end_char="3036">weeks</TOKEN>
<TOKEN id="token-20-33" pos="word" morph="none" start_char="3038" end_char="3043">before</TOKEN>
<TOKEN id="token-20-34" pos="word" morph="none" start_char="3045" end_char="3046">it</TOKEN>
<TOKEN id="token-20-35" pos="word" morph="none" start_char="3048" end_char="3050">was</TOKEN>
<TOKEN id="token-20-36" pos="word" morph="none" start_char="3052" end_char="3061">officially</TOKEN>
<TOKEN id="token-20-37" pos="word" morph="none" start_char="3063" end_char="3072">discovered</TOKEN>
<TOKEN id="token-20-38" pos="punct" morph="none" start_char="3073" end_char="3073">,</TOKEN>
<TOKEN id="token-20-39" pos="word" morph="none" start_char="3075" end_char="3077">the</TOKEN>
<TOKEN id="token-20-40" pos="word" morph="none" start_char="3079" end_char="3082">test</TOKEN>
<TOKEN id="token-20-41" pos="word" morph="none" start_char="3084" end_char="3087">from</TOKEN>
<TOKEN id="token-20-42" pos="word" morph="none" start_char="3089" end_char="3093">March</TOKEN>
<TOKEN id="token-20-43" pos="word" morph="none" start_char="3095" end_char="3098">2019</TOKEN>
<TOKEN id="token-20-44" pos="word" morph="none" start_char="3100" end_char="3101">is</TOKEN>
<TOKEN id="token-20-45" pos="word" morph="none" start_char="3103" end_char="3105">not</TOKEN>
<TOKEN id="token-20-46" pos="word" morph="none" start_char="3107" end_char="3116">conclusive</TOKEN>
<TOKEN id="token-20-47" pos="punct" morph="none" start_char="3117" end_char="3117">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="3119" end_char="3250">
<ORIGINAL_TEXT>Only one sample was tested, and of the WHO-recommended tests for SARS-CoV-2, only two of five were positive, on IP2 and IP4 targets.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="3119" end_char="3122">Only</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="3124" end_char="3126">one</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="3128" end_char="3133">sample</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="3135" end_char="3137">was</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="3139" end_char="3144">tested</TOKEN>
<TOKEN id="token-21-5" pos="punct" morph="none" start_char="3145" end_char="3145">,</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="3147" end_char="3149">and</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="3151" end_char="3152">of</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="3154" end_char="3156">the</TOKEN>
<TOKEN id="token-21-9" pos="unknown" morph="none" start_char="3158" end_char="3172">WHO-recommended</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="3174" end_char="3178">tests</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="3180" end_char="3182">for</TOKEN>
<TOKEN id="token-21-12" pos="unknown" morph="none" start_char="3184" end_char="3193">SARS-CoV-2</TOKEN>
<TOKEN id="token-21-13" pos="punct" morph="none" start_char="3194" end_char="3194">,</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="3196" end_char="3199">only</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="3201" end_char="3203">two</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="3205" end_char="3206">of</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="3208" end_char="3211">five</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="3213" end_char="3216">were</TOKEN>
<TOKEN id="token-21-19" pos="word" morph="none" start_char="3218" end_char="3225">positive</TOKEN>
<TOKEN id="token-21-20" pos="punct" morph="none" start_char="3226" end_char="3226">,</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="3228" end_char="3229">on</TOKEN>
<TOKEN id="token-21-22" pos="word" morph="none" start_char="3231" end_char="3233">IP2</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="3235" end_char="3237">and</TOKEN>
<TOKEN id="token-21-24" pos="word" morph="none" start_char="3239" end_char="3241">IP4</TOKEN>
<TOKEN id="token-21-25" pos="word" morph="none" start_char="3243" end_char="3249">targets</TOKEN>
<TOKEN id="token-21-26" pos="punct" morph="none" start_char="3250" end_char="3250">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="3253" end_char="3383">
<ORIGINAL_TEXT>Bik points out that the WHO norms suggest an ‘E gene’ assay (investigative procedure to assess the presence of an analyte) be done.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="3253" end_char="3255">Bik</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="3257" end_char="3262">points</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="3264" end_char="3266">out</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="3268" end_char="3271">that</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="3273" end_char="3275">the</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="3277" end_char="3279">WHO</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="3281" end_char="3285">norms</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="3287" end_char="3293">suggest</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="3295" end_char="3296">an</TOKEN>
<TOKEN id="token-22-9" pos="punct" morph="none" start_char="3298" end_char="3298">‘</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="3299" end_char="3299">E</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="3301" end_char="3304">gene</TOKEN>
<TOKEN id="token-22-12" pos="punct" morph="none" start_char="3305" end_char="3305">’</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="3307" end_char="3311">assay</TOKEN>
<TOKEN id="token-22-14" pos="punct" morph="none" start_char="3313" end_char="3313">(</TOKEN>
<TOKEN id="token-22-15" pos="word" morph="none" start_char="3314" end_char="3326">investigative</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="3328" end_char="3336">procedure</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="3338" end_char="3339">to</TOKEN>
<TOKEN id="token-22-18" pos="word" morph="none" start_char="3341" end_char="3346">assess</TOKEN>
<TOKEN id="token-22-19" pos="word" morph="none" start_char="3348" end_char="3350">the</TOKEN>
<TOKEN id="token-22-20" pos="word" morph="none" start_char="3352" end_char="3359">presence</TOKEN>
<TOKEN id="token-22-21" pos="word" morph="none" start_char="3361" end_char="3362">of</TOKEN>
<TOKEN id="token-22-22" pos="word" morph="none" start_char="3364" end_char="3365">an</TOKEN>
<TOKEN id="token-22-23" pos="word" morph="none" start_char="3367" end_char="3373">analyte</TOKEN>
<TOKEN id="token-22-24" pos="punct" morph="none" start_char="3374" end_char="3374">)</TOKEN>
<TOKEN id="token-22-25" pos="word" morph="none" start_char="3376" end_char="3377">be</TOKEN>
<TOKEN id="token-22-26" pos="word" morph="none" start_char="3379" end_char="3382">done</TOKEN>
<TOKEN id="token-22-27" pos="punct" morph="none" start_char="3383" end_char="3383">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="3385" end_char="3438">
<ORIGINAL_TEXT>However, in the UoB study, this test came up negative.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="3385" end_char="3391">However</TOKEN>
<TOKEN id="token-23-1" pos="punct" morph="none" start_char="3392" end_char="3392">,</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="3394" end_char="3395">in</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="3397" end_char="3399">the</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="3401" end_char="3403">UoB</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="3405" end_char="3409">study</TOKEN>
<TOKEN id="token-23-6" pos="punct" morph="none" start_char="3410" end_char="3410">,</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="3412" end_char="3415">this</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="3417" end_char="3420">test</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="3422" end_char="3425">came</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="3427" end_char="3428">up</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="3430" end_char="3437">negative</TOKEN>
<TOKEN id="token-23-12" pos="punct" morph="none" start_char="3438" end_char="3438">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="3440" end_char="3615">
<ORIGINAL_TEXT>This, combined with the nine-month gap between the March sample and the next positive, suggests that more research and peer-review is required to validate the study’s findings.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="3440" end_char="3443">This</TOKEN>
<TOKEN id="token-24-1" pos="punct" morph="none" start_char="3444" end_char="3444">,</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="3446" end_char="3453">combined</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="3455" end_char="3458">with</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="3460" end_char="3462">the</TOKEN>
<TOKEN id="token-24-5" pos="unknown" morph="none" start_char="3464" end_char="3473">nine-month</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="3475" end_char="3477">gap</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="3479" end_char="3485">between</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="3487" end_char="3489">the</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="3491" end_char="3495">March</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="3497" end_char="3502">sample</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="3504" end_char="3506">and</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="3508" end_char="3510">the</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="3512" end_char="3515">next</TOKEN>
<TOKEN id="token-24-14" pos="word" morph="none" start_char="3517" end_char="3524">positive</TOKEN>
<TOKEN id="token-24-15" pos="punct" morph="none" start_char="3525" end_char="3525">,</TOKEN>
<TOKEN id="token-24-16" pos="word" morph="none" start_char="3527" end_char="3534">suggests</TOKEN>
<TOKEN id="token-24-17" pos="word" morph="none" start_char="3536" end_char="3539">that</TOKEN>
<TOKEN id="token-24-18" pos="word" morph="none" start_char="3541" end_char="3544">more</TOKEN>
<TOKEN id="token-24-19" pos="word" morph="none" start_char="3546" end_char="3553">research</TOKEN>
<TOKEN id="token-24-20" pos="word" morph="none" start_char="3555" end_char="3557">and</TOKEN>
<TOKEN id="token-24-21" pos="unknown" morph="none" start_char="3559" end_char="3569">peer-review</TOKEN>
<TOKEN id="token-24-22" pos="word" morph="none" start_char="3571" end_char="3572">is</TOKEN>
<TOKEN id="token-24-23" pos="word" morph="none" start_char="3574" end_char="3581">required</TOKEN>
<TOKEN id="token-24-24" pos="word" morph="none" start_char="3583" end_char="3584">to</TOKEN>
<TOKEN id="token-24-25" pos="word" morph="none" start_char="3586" end_char="3593">validate</TOKEN>
<TOKEN id="token-24-26" pos="word" morph="none" start_char="3595" end_char="3597">the</TOKEN>
<TOKEN id="token-24-27" pos="word" morph="none" start_char="3599" end_char="3605">study’s</TOKEN>
<TOKEN id="token-24-28" pos="word" morph="none" start_char="3607" end_char="3614">findings</TOKEN>
<TOKEN id="token-24-29" pos="punct" morph="none" start_char="3615" end_char="3615">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="3618" end_char="3723">
<ORIGINAL_TEXT>This is not the first time China has tried to suggest that the coronavirus emerged outside of its borders.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="3618" end_char="3621">This</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="3623" end_char="3624">is</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="3626" end_char="3628">not</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="3630" end_char="3632">the</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="3634" end_char="3638">first</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="3640" end_char="3643">time</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="3645" end_char="3649">China</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="3651" end_char="3653">has</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="3655" end_char="3659">tried</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="3661" end_char="3662">to</TOKEN>
<TOKEN id="token-25-10" pos="word" morph="none" start_char="3664" end_char="3670">suggest</TOKEN>
<TOKEN id="token-25-11" pos="word" morph="none" start_char="3672" end_char="3675">that</TOKEN>
<TOKEN id="token-25-12" pos="word" morph="none" start_char="3677" end_char="3679">the</TOKEN>
<TOKEN id="token-25-13" pos="word" morph="none" start_char="3681" end_char="3691">coronavirus</TOKEN>
<TOKEN id="token-25-14" pos="word" morph="none" start_char="3693" end_char="3699">emerged</TOKEN>
<TOKEN id="token-25-15" pos="word" morph="none" start_char="3701" end_char="3707">outside</TOKEN>
<TOKEN id="token-25-16" pos="word" morph="none" start_char="3709" end_char="3710">of</TOKEN>
<TOKEN id="token-25-17" pos="word" morph="none" start_char="3712" end_char="3714">its</TOKEN>
<TOKEN id="token-25-18" pos="word" morph="none" start_char="3716" end_char="3722">borders</TOKEN>
<TOKEN id="token-25-19" pos="punct" morph="none" start_char="3723" end_char="3723">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="3725" end_char="3925">
<ORIGINAL_TEXT>In March, China’s Foreign Ministry spokesperson Zhao Lijian suggested that the US Army could have brought the virus to Wuhan during the Military World games that took place in the city in October 2019.</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="3725" end_char="3726">In</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="3728" end_char="3732">March</TOKEN>
<TOKEN id="token-26-2" pos="punct" morph="none" start_char="3733" end_char="3733">,</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="3735" end_char="3741">China’s</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="3743" end_char="3749">Foreign</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="3751" end_char="3758">Ministry</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="3760" end_char="3771">spokesperson</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="3773" end_char="3776">Zhao</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="3778" end_char="3783">Lijian</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="3785" end_char="3793">suggested</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="3795" end_char="3798">that</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="3800" end_char="3802">the</TOKEN>
<TOKEN id="token-26-12" pos="word" morph="none" start_char="3804" end_char="3805">US</TOKEN>
<TOKEN id="token-26-13" pos="word" morph="none" start_char="3807" end_char="3810">Army</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="3812" end_char="3816">could</TOKEN>
<TOKEN id="token-26-15" pos="word" morph="none" start_char="3818" end_char="3821">have</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="3823" end_char="3829">brought</TOKEN>
<TOKEN id="token-26-17" pos="word" morph="none" start_char="3831" end_char="3833">the</TOKEN>
<TOKEN id="token-26-18" pos="word" morph="none" start_char="3835" end_char="3839">virus</TOKEN>
<TOKEN id="token-26-19" pos="word" morph="none" start_char="3841" end_char="3842">to</TOKEN>
<TOKEN id="token-26-20" pos="word" morph="none" start_char="3844" end_char="3848">Wuhan</TOKEN>
<TOKEN id="token-26-21" pos="word" morph="none" start_char="3850" end_char="3855">during</TOKEN>
<TOKEN id="token-26-22" pos="word" morph="none" start_char="3857" end_char="3859">the</TOKEN>
<TOKEN id="token-26-23" pos="word" morph="none" start_char="3861" end_char="3868">Military</TOKEN>
<TOKEN id="token-26-24" pos="word" morph="none" start_char="3870" end_char="3874">World</TOKEN>
<TOKEN id="token-26-25" pos="word" morph="none" start_char="3876" end_char="3880">games</TOKEN>
<TOKEN id="token-26-26" pos="word" morph="none" start_char="3882" end_char="3885">that</TOKEN>
<TOKEN id="token-26-27" pos="word" morph="none" start_char="3887" end_char="3890">took</TOKEN>
<TOKEN id="token-26-28" pos="word" morph="none" start_char="3892" end_char="3896">place</TOKEN>
<TOKEN id="token-26-29" pos="word" morph="none" start_char="3898" end_char="3899">in</TOKEN>
<TOKEN id="token-26-30" pos="word" morph="none" start_char="3901" end_char="3903">the</TOKEN>
<TOKEN id="token-26-31" pos="word" morph="none" start_char="3905" end_char="3908">city</TOKEN>
<TOKEN id="token-26-32" pos="word" morph="none" start_char="3910" end_char="3911">in</TOKEN>
<TOKEN id="token-26-33" pos="word" morph="none" start_char="3913" end_char="3919">October</TOKEN>
<TOKEN id="token-26-34" pos="word" morph="none" start_char="3921" end_char="3924">2019</TOKEN>
<TOKEN id="token-26-35" pos="punct" morph="none" start_char="3925" end_char="3925">.</TOKEN>
</SEG>
<SEG id="segment-27" start_char="3927" end_char="4062">
<ORIGINAL_TEXT>Several athletes who attended the games claimed to have suffered symptoms similar to that of the coronavirus during their stay in China.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="3927" end_char="3933">Several</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="3935" end_char="3942">athletes</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="3944" end_char="3946">who</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="3948" end_char="3955">attended</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="3957" end_char="3959">the</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="3961" end_char="3965">games</TOKEN>
<TOKEN id="token-27-6" pos="word" morph="none" start_char="3967" end_char="3973">claimed</TOKEN>
<TOKEN id="token-27-7" pos="word" morph="none" start_char="3975" end_char="3976">to</TOKEN>
<TOKEN id="token-27-8" pos="word" morph="none" start_char="3978" end_char="3981">have</TOKEN>
<TOKEN id="token-27-9" pos="word" morph="none" start_char="3983" end_char="3990">suffered</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="3992" end_char="3999">symptoms</TOKEN>
<TOKEN id="token-27-11" pos="word" morph="none" start_char="4001" end_char="4007">similar</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="4009" end_char="4010">to</TOKEN>
<TOKEN id="token-27-13" pos="word" morph="none" start_char="4012" end_char="4015">that</TOKEN>
<TOKEN id="token-27-14" pos="word" morph="none" start_char="4017" end_char="4018">of</TOKEN>
<TOKEN id="token-27-15" pos="word" morph="none" start_char="4020" end_char="4022">the</TOKEN>
<TOKEN id="token-27-16" pos="word" morph="none" start_char="4024" end_char="4034">coronavirus</TOKEN>
<TOKEN id="token-27-17" pos="word" morph="none" start_char="4036" end_char="4041">during</TOKEN>
<TOKEN id="token-27-18" pos="word" morph="none" start_char="4043" end_char="4047">their</TOKEN>
<TOKEN id="token-27-19" pos="word" morph="none" start_char="4049" end_char="4052">stay</TOKEN>
<TOKEN id="token-27-20" pos="word" morph="none" start_char="4054" end_char="4055">in</TOKEN>
<TOKEN id="token-27-21" pos="word" morph="none" start_char="4057" end_char="4061">China</TOKEN>
<TOKEN id="token-27-22" pos="punct" morph="none" start_char="4062" end_char="4062">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="4065" end_char="4151">
<ORIGINAL_TEXT>There are other reports that suggest an earlier origin to the coronavirus than thought.</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="4065" end_char="4069">There</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="4071" end_char="4073">are</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="4075" end_char="4079">other</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="4081" end_char="4087">reports</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="4089" end_char="4092">that</TOKEN>
<TOKEN id="token-28-5" pos="word" morph="none" start_char="4094" end_char="4100">suggest</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="4102" end_char="4103">an</TOKEN>
<TOKEN id="token-28-7" pos="word" morph="none" start_char="4105" end_char="4111">earlier</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="4113" end_char="4118">origin</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="4120" end_char="4121">to</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="4123" end_char="4125">the</TOKEN>
<TOKEN id="token-28-11" pos="word" morph="none" start_char="4127" end_char="4137">coronavirus</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="4139" end_char="4142">than</TOKEN>
<TOKEN id="token-28-13" pos="word" morph="none" start_char="4144" end_char="4150">thought</TOKEN>
<TOKEN id="token-28-14" pos="punct" morph="none" start_char="4151" end_char="4151">.</TOKEN>
</SEG>
<SEG id="segment-29" start_char="4153" end_char="4163">
<ORIGINAL_TEXT>A report by</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="4153" end_char="4153">A</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="4155" end_char="4160">report</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="4162" end_char="4163">by</TOKEN>
</SEG>
<SEG id="segment-30" start_char="4166" end_char="4186">
<ORIGINAL_TEXT>The American Prospect</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="4166" end_char="4168">The</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="4170" end_char="4177">American</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="4179" end_char="4186">Prospect</TOKEN>
</SEG>
<SEG id="segment-31" start_char="4189" end_char="4335">
<ORIGINAL_TEXT>suggested "a strong correlation" in COVID-19 cases reported at US military bases that are home bases of members of the US teams that went to Wuhan.</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="word" morph="none" start_char="4189" end_char="4197">suggested</TOKEN>
<TOKEN id="token-31-1" pos="punct" morph="none" start_char="4199" end_char="4199">"</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="4200" end_char="4200">a</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="4202" end_char="4207">strong</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="4209" end_char="4219">correlation</TOKEN>
<TOKEN id="token-31-5" pos="punct" morph="none" start_char="4220" end_char="4220">"</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="4222" end_char="4223">in</TOKEN>
<TOKEN id="token-31-7" pos="unknown" morph="none" start_char="4225" end_char="4232">COVID-19</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="4234" end_char="4238">cases</TOKEN>
<TOKEN id="token-31-9" pos="word" morph="none" start_char="4240" end_char="4247">reported</TOKEN>
<TOKEN id="token-31-10" pos="word" morph="none" start_char="4249" end_char="4250">at</TOKEN>
<TOKEN id="token-31-11" pos="word" morph="none" start_char="4252" end_char="4253">US</TOKEN>
<TOKEN id="token-31-12" pos="word" morph="none" start_char="4255" end_char="4262">military</TOKEN>
<TOKEN id="token-31-13" pos="word" morph="none" start_char="4264" end_char="4268">bases</TOKEN>
<TOKEN id="token-31-14" pos="word" morph="none" start_char="4270" end_char="4273">that</TOKEN>
<TOKEN id="token-31-15" pos="word" morph="none" start_char="4275" end_char="4277">are</TOKEN>
<TOKEN id="token-31-16" pos="word" morph="none" start_char="4279" end_char="4282">home</TOKEN>
<TOKEN id="token-31-17" pos="word" morph="none" start_char="4284" end_char="4288">bases</TOKEN>
<TOKEN id="token-31-18" pos="word" morph="none" start_char="4290" end_char="4291">of</TOKEN>
<TOKEN id="token-31-19" pos="word" morph="none" start_char="4293" end_char="4299">members</TOKEN>
<TOKEN id="token-31-20" pos="word" morph="none" start_char="4301" end_char="4302">of</TOKEN>
<TOKEN id="token-31-21" pos="word" morph="none" start_char="4304" end_char="4306">the</TOKEN>
<TOKEN id="token-31-22" pos="word" morph="none" start_char="4308" end_char="4309">US</TOKEN>
<TOKEN id="token-31-23" pos="word" morph="none" start_char="4311" end_char="4315">teams</TOKEN>
<TOKEN id="token-31-24" pos="word" morph="none" start_char="4317" end_char="4320">that</TOKEN>
<TOKEN id="token-31-25" pos="word" morph="none" start_char="4322" end_char="4325">went</TOKEN>
<TOKEN id="token-31-26" pos="word" morph="none" start_char="4327" end_char="4328">to</TOKEN>
<TOKEN id="token-31-27" pos="word" morph="none" start_char="4330" end_char="4334">Wuhan</TOKEN>
<TOKEN id="token-31-28" pos="punct" morph="none" start_char="4335" end_char="4335">.</TOKEN>
</SEG>
<SEG id="segment-32" start_char="4337" end_char="4611">
<ORIGINAL_TEXT>A study by researchers from Harvard Medical School and the Boston University of Public Health used satellite data of hospital parking lots in tandem with search engine queries for coronavirus symptoms to suggest that it could have emerged in China as early as August of 2019.</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="4337" end_char="4337">A</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="4339" end_char="4343">study</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="4345" end_char="4346">by</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="4348" end_char="4358">researchers</TOKEN>
<TOKEN id="token-32-4" pos="word" morph="none" start_char="4360" end_char="4363">from</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="4365" end_char="4371">Harvard</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="4373" end_char="4379">Medical</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="4381" end_char="4386">School</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="4388" end_char="4390">and</TOKEN>
<TOKEN id="token-32-9" pos="word" morph="none" start_char="4392" end_char="4394">the</TOKEN>
<TOKEN id="token-32-10" pos="word" morph="none" start_char="4396" end_char="4401">Boston</TOKEN>
<TOKEN id="token-32-11" pos="word" morph="none" start_char="4403" end_char="4412">University</TOKEN>
<TOKEN id="token-32-12" pos="word" morph="none" start_char="4414" end_char="4415">of</TOKEN>
<TOKEN id="token-32-13" pos="word" morph="none" start_char="4417" end_char="4422">Public</TOKEN>
<TOKEN id="token-32-14" pos="word" morph="none" start_char="4424" end_char="4429">Health</TOKEN>
<TOKEN id="token-32-15" pos="word" morph="none" start_char="4431" end_char="4434">used</TOKEN>
<TOKEN id="token-32-16" pos="word" morph="none" start_char="4436" end_char="4444">satellite</TOKEN>
<TOKEN id="token-32-17" pos="word" morph="none" start_char="4446" end_char="4449">data</TOKEN>
<TOKEN id="token-32-18" pos="word" morph="none" start_char="4451" end_char="4452">of</TOKEN>
<TOKEN id="token-32-19" pos="word" morph="none" start_char="4454" end_char="4461">hospital</TOKEN>
<TOKEN id="token-32-20" pos="word" morph="none" start_char="4463" end_char="4469">parking</TOKEN>
<TOKEN id="token-32-21" pos="word" morph="none" start_char="4471" end_char="4474">lots</TOKEN>
<TOKEN id="token-32-22" pos="word" morph="none" start_char="4476" end_char="4477">in</TOKEN>
<TOKEN id="token-32-23" pos="word" morph="none" start_char="4479" end_char="4484">tandem</TOKEN>
<TOKEN id="token-32-24" pos="word" morph="none" start_char="4486" end_char="4489">with</TOKEN>
<TOKEN id="token-32-25" pos="word" morph="none" start_char="4491" end_char="4496">search</TOKEN>
<TOKEN id="token-32-26" pos="word" morph="none" start_char="4498" end_char="4503">engine</TOKEN>
<TOKEN id="token-32-27" pos="word" morph="none" start_char="4505" end_char="4511">queries</TOKEN>
<TOKEN id="token-32-28" pos="word" morph="none" start_char="4513" end_char="4515">for</TOKEN>
<TOKEN id="token-32-29" pos="word" morph="none" start_char="4517" end_char="4527">coronavirus</TOKEN>
<TOKEN id="token-32-30" pos="word" morph="none" start_char="4529" end_char="4536">symptoms</TOKEN>
<TOKEN id="token-32-31" pos="word" morph="none" start_char="4538" end_char="4539">to</TOKEN>
<TOKEN id="token-32-32" pos="word" morph="none" start_char="4541" end_char="4547">suggest</TOKEN>
<TOKEN id="token-32-33" pos="word" morph="none" start_char="4549" end_char="4552">that</TOKEN>
<TOKEN id="token-32-34" pos="word" morph="none" start_char="4554" end_char="4555">it</TOKEN>
<TOKEN id="token-32-35" pos="word" morph="none" start_char="4557" end_char="4561">could</TOKEN>
<TOKEN id="token-32-36" pos="word" morph="none" start_char="4563" end_char="4566">have</TOKEN>
<TOKEN id="token-32-37" pos="word" morph="none" start_char="4568" end_char="4574">emerged</TOKEN>
<TOKEN id="token-32-38" pos="word" morph="none" start_char="4576" end_char="4577">in</TOKEN>
<TOKEN id="token-32-39" pos="word" morph="none" start_char="4579" end_char="4583">China</TOKEN>
<TOKEN id="token-32-40" pos="word" morph="none" start_char="4585" end_char="4586">as</TOKEN>
<TOKEN id="token-32-41" pos="word" morph="none" start_char="4588" end_char="4592">early</TOKEN>
<TOKEN id="token-32-42" pos="word" morph="none" start_char="4594" end_char="4595">as</TOKEN>
<TOKEN id="token-32-43" pos="word" morph="none" start_char="4597" end_char="4602">August</TOKEN>
<TOKEN id="token-32-44" pos="word" morph="none" start_char="4604" end_char="4605">of</TOKEN>
<TOKEN id="token-32-45" pos="word" morph="none" start_char="4607" end_char="4610">2019</TOKEN>
<TOKEN id="token-32-46" pos="punct" morph="none" start_char="4611" end_char="4611">.</TOKEN>
</SEG>
<SEG id="segment-33" start_char="4614" end_char="4818">
<ORIGINAL_TEXT>WHO Chief Scientist Dr Soumya Swaminathan told ANI that a WHO team would be visiting China next week to investigate the origins of the coronavirus, saying that a "thorough investigation" needed to be done.</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="word" morph="none" start_char="4614" end_char="4616">WHO</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="4618" end_char="4622">Chief</TOKEN>
<TOKEN id="token-33-2" pos="word" morph="none" start_char="4624" end_char="4632">Scientist</TOKEN>
<TOKEN id="token-33-3" pos="word" morph="none" start_char="4634" end_char="4635">Dr</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="4637" end_char="4642">Soumya</TOKEN>
<TOKEN id="token-33-5" pos="word" morph="none" start_char="4644" end_char="4654">Swaminathan</TOKEN>
<TOKEN id="token-33-6" pos="word" morph="none" start_char="4656" end_char="4659">told</TOKEN>
<TOKEN id="token-33-7" pos="word" morph="none" start_char="4661" end_char="4663">ANI</TOKEN>
<TOKEN id="token-33-8" pos="word" morph="none" start_char="4665" end_char="4668">that</TOKEN>
<TOKEN id="token-33-9" pos="word" morph="none" start_char="4670" end_char="4670">a</TOKEN>
<TOKEN id="token-33-10" pos="word" morph="none" start_char="4672" end_char="4674">WHO</TOKEN>
<TOKEN id="token-33-11" pos="word" morph="none" start_char="4676" end_char="4679">team</TOKEN>
<TOKEN id="token-33-12" pos="word" morph="none" start_char="4681" end_char="4685">would</TOKEN>
<TOKEN id="token-33-13" pos="word" morph="none" start_char="4687" end_char="4688">be</TOKEN>
<TOKEN id="token-33-14" pos="word" morph="none" start_char="4690" end_char="4697">visiting</TOKEN>
<TOKEN id="token-33-15" pos="word" morph="none" start_char="4699" end_char="4703">China</TOKEN>
<TOKEN id="token-33-16" pos="word" morph="none" start_char="4705" end_char="4708">next</TOKEN>
<TOKEN id="token-33-17" pos="word" morph="none" start_char="4710" end_char="4713">week</TOKEN>
<TOKEN id="token-33-18" pos="word" morph="none" start_char="4715" end_char="4716">to</TOKEN>
<TOKEN id="token-33-19" pos="word" morph="none" start_char="4718" end_char="4728">investigate</TOKEN>
<TOKEN id="token-33-20" pos="word" morph="none" start_char="4730" end_char="4732">the</TOKEN>
<TOKEN id="token-33-21" pos="word" morph="none" start_char="4734" end_char="4740">origins</TOKEN>
<TOKEN id="token-33-22" pos="word" morph="none" start_char="4742" end_char="4743">of</TOKEN>
<TOKEN id="token-33-23" pos="word" morph="none" start_char="4745" end_char="4747">the</TOKEN>
<TOKEN id="token-33-24" pos="word" morph="none" start_char="4749" end_char="4759">coronavirus</TOKEN>
<TOKEN id="token-33-25" pos="punct" morph="none" start_char="4760" end_char="4760">,</TOKEN>
<TOKEN id="token-33-26" pos="word" morph="none" start_char="4762" end_char="4767">saying</TOKEN>
<TOKEN id="token-33-27" pos="word" morph="none" start_char="4769" end_char="4772">that</TOKEN>
<TOKEN id="token-33-28" pos="word" morph="none" start_char="4774" end_char="4774">a</TOKEN>
<TOKEN id="token-33-29" pos="punct" morph="none" start_char="4776" end_char="4776">"</TOKEN>
<TOKEN id="token-33-30" pos="word" morph="none" start_char="4777" end_char="4784">thorough</TOKEN>
<TOKEN id="token-33-31" pos="word" morph="none" start_char="4786" end_char="4798">investigation</TOKEN>
<TOKEN id="token-33-32" pos="punct" morph="none" start_char="4799" end_char="4799">"</TOKEN>
<TOKEN id="token-33-33" pos="word" morph="none" start_char="4801" end_char="4806">needed</TOKEN>
<TOKEN id="token-33-34" pos="word" morph="none" start_char="4808" end_char="4809">to</TOKEN>
<TOKEN id="token-33-35" pos="word" morph="none" start_char="4811" end_char="4812">be</TOKEN>
<TOKEN id="token-33-36" pos="word" morph="none" start_char="4814" end_char="4817">done</TOKEN>
<TOKEN id="token-33-37" pos="punct" morph="none" start_char="4818" end_char="4818">.</TOKEN>
</SEG>
<SEG id="segment-34" start_char="4821" end_char="4948">
<ORIGINAL_TEXT>"What is needed now is a good investigation going back before December to find out where and how it jumped from animal to human.</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="punct" morph="none" start_char="4821" end_char="4821">"</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="4822" end_char="4825">What</TOKEN>
<TOKEN id="token-34-2" pos="word" morph="none" start_char="4827" end_char="4828">is</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="4830" end_char="4835">needed</TOKEN>
<TOKEN id="token-34-4" pos="word" morph="none" start_char="4837" end_char="4839">now</TOKEN>
<TOKEN id="token-34-5" pos="word" morph="none" start_char="4841" end_char="4842">is</TOKEN>
<TOKEN id="token-34-6" pos="word" morph="none" start_char="4844" end_char="4844">a</TOKEN>
<TOKEN id="token-34-7" pos="word" morph="none" start_char="4846" end_char="4849">good</TOKEN>
<TOKEN id="token-34-8" pos="word" morph="none" start_char="4851" end_char="4863">investigation</TOKEN>
<TOKEN id="token-34-9" pos="word" morph="none" start_char="4865" end_char="4869">going</TOKEN>
<TOKEN id="token-34-10" pos="word" morph="none" start_char="4871" end_char="4874">back</TOKEN>
<TOKEN id="token-34-11" pos="word" morph="none" start_char="4876" end_char="4881">before</TOKEN>
<TOKEN id="token-34-12" pos="word" morph="none" start_char="4883" end_char="4890">December</TOKEN>
<TOKEN id="token-34-13" pos="word" morph="none" start_char="4892" end_char="4893">to</TOKEN>
<TOKEN id="token-34-14" pos="word" morph="none" start_char="4895" end_char="4898">find</TOKEN>
<TOKEN id="token-34-15" pos="word" morph="none" start_char="4900" end_char="4902">out</TOKEN>
<TOKEN id="token-34-16" pos="word" morph="none" start_char="4904" end_char="4908">where</TOKEN>
<TOKEN id="token-34-17" pos="word" morph="none" start_char="4910" end_char="4912">and</TOKEN>
<TOKEN id="token-34-18" pos="word" morph="none" start_char="4914" end_char="4916">how</TOKEN>
<TOKEN id="token-34-19" pos="word" morph="none" start_char="4918" end_char="4919">it</TOKEN>
<TOKEN id="token-34-20" pos="word" morph="none" start_char="4921" end_char="4926">jumped</TOKEN>
<TOKEN id="token-34-21" pos="word" morph="none" start_char="4928" end_char="4931">from</TOKEN>
<TOKEN id="token-34-22" pos="word" morph="none" start_char="4933" end_char="4938">animal</TOKEN>
<TOKEN id="token-34-23" pos="word" morph="none" start_char="4940" end_char="4941">to</TOKEN>
<TOKEN id="token-34-24" pos="word" morph="none" start_char="4943" end_char="4947">human</TOKEN>
<TOKEN id="token-34-25" pos="punct" morph="none" start_char="4948" end_char="4948">.</TOKEN>
</SEG>
<SEG id="segment-35" start_char="4950" end_char="5056">
<ORIGINAL_TEXT>Was there any intermediate animal or not or it directly jumped from bat to humans which are also possible?"</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="4950" end_char="4952">Was</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="4954" end_char="4958">there</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="4960" end_char="4962">any</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="4964" end_char="4975">intermediate</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="4977" end_char="4982">animal</TOKEN>
<TOKEN id="token-35-5" pos="word" morph="none" start_char="4984" end_char="4985">or</TOKEN>
<TOKEN id="token-35-6" pos="word" morph="none" start_char="4987" end_char="4989">not</TOKEN>
<TOKEN id="token-35-7" pos="word" morph="none" start_char="4991" end_char="4992">or</TOKEN>
<TOKEN id="token-35-8" pos="word" morph="none" start_char="4994" end_char="4995">it</TOKEN>
<TOKEN id="token-35-9" pos="word" morph="none" start_char="4997" end_char="5004">directly</TOKEN>
<TOKEN id="token-35-10" pos="word" morph="none" start_char="5006" end_char="5011">jumped</TOKEN>
<TOKEN id="token-35-11" pos="word" morph="none" start_char="5013" end_char="5016">from</TOKEN>
<TOKEN id="token-35-12" pos="word" morph="none" start_char="5018" end_char="5020">bat</TOKEN>
<TOKEN id="token-35-13" pos="word" morph="none" start_char="5022" end_char="5023">to</TOKEN>
<TOKEN id="token-35-14" pos="word" morph="none" start_char="5025" end_char="5030">humans</TOKEN>
<TOKEN id="token-35-15" pos="word" morph="none" start_char="5032" end_char="5036">which</TOKEN>
<TOKEN id="token-35-16" pos="word" morph="none" start_char="5038" end_char="5040">are</TOKEN>
<TOKEN id="token-35-17" pos="word" morph="none" start_char="5042" end_char="5045">also</TOKEN>
<TOKEN id="token-35-18" pos="word" morph="none" start_char="5047" end_char="5054">possible</TOKEN>
<TOKEN id="token-35-19" pos="punct" morph="none" start_char="5055" end_char="5056">?"</TOKEN>
</SEG>
<SEG id="segment-36" start_char="5058" end_char="5067">
<ORIGINAL_TEXT>she asked.</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="5058" end_char="5060">she</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="5062" end_char="5066">asked</TOKEN>
<TOKEN id="token-36-2" pos="punct" morph="none" start_char="5067" end_char="5067">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
