<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CAB1" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="3434" raw_text_md5="cb422a2c2ce774ac7ea6ec4963f4eb07">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="75">
<ORIGINAL_TEXT>Chinese scientists now claim coronavirus originated in India in summer 2019</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="7">Chinese</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="9" end_char="18">scientists</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="20" end_char="22">now</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="24" end_char="28">claim</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="30" end_char="40">coronavirus</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="42" end_char="51">originated</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="53" end_char="54">in</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="56" end_char="60">India</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="62" end_char="63">in</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="65" end_char="70">summer</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="72" end_char="75">2019</TOKEN>
</SEG>
<SEG id="segment-1" start_char="80" end_char="169">
<ORIGINAL_TEXT>Beijing has for long been trying to shift the blame for the outbreak of novel coronavirus.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="80" end_char="86">Beijing</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="88" end_char="90">has</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="92" end_char="94">for</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="96" end_char="99">long</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="101" end_char="104">been</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="106" end_char="111">trying</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="113" end_char="114">to</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="116" end_char="120">shift</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="122" end_char="124">the</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="126" end_char="130">blame</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="132" end_char="134">for</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="136" end_char="138">the</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="140" end_char="147">outbreak</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="149" end_char="150">of</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="152" end_char="156">novel</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="158" end_char="168">coronavirus</TOKEN>
<TOKEN id="token-1-16" pos="punct" morph="none" start_char="169" end_char="169">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="171" end_char="274">
<ORIGINAL_TEXT>It was Europe first, and now Chinese researchers have claimed that the deadly virus originated in India.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="171" end_char="172">It</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="174" end_char="176">was</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="178" end_char="183">Europe</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="185" end_char="189">first</TOKEN>
<TOKEN id="token-2-4" pos="punct" morph="none" start_char="190" end_char="190">,</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="192" end_char="194">and</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="196" end_char="198">now</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="200" end_char="206">Chinese</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="208" end_char="218">researchers</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="220" end_char="223">have</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="225" end_char="231">claimed</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="233" end_char="236">that</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="238" end_char="240">the</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="242" end_char="247">deadly</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="249" end_char="253">virus</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="255" end_char="264">originated</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="266" end_char="267">in</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="269" end_char="273">India</TOKEN>
<TOKEN id="token-2-18" pos="punct" morph="none" start_char="274" end_char="274">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="277" end_char="443">
<ORIGINAL_TEXT>A team from the Chinese Academy of Sciences argues the virus likely originated in India in the summer of 2019 -- jumping from animals to humans via contaminated water.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="277" end_char="277">A</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="279" end_char="282">team</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="284" end_char="287">from</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="289" end_char="291">the</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="293" end_char="299">Chinese</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="301" end_char="307">Academy</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="309" end_char="310">of</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="312" end_char="319">Sciences</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="321" end_char="326">argues</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="328" end_char="330">the</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="332" end_char="336">virus</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="338" end_char="343">likely</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="345" end_char="354">originated</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="356" end_char="357">in</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="359" end_char="363">India</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="365" end_char="366">in</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="368" end_char="370">the</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="372" end_char="377">summer</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="379" end_char="380">of</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="382" end_char="385">2019</TOKEN>
<TOKEN id="token-3-20" pos="punct" morph="none" start_char="387" end_char="388">--</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="390" end_char="396">jumping</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="398" end_char="401">from</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="403" end_char="409">animals</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="411" end_char="412">to</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="414" end_char="419">humans</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="421" end_char="423">via</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="425" end_char="436">contaminated</TOKEN>
<TOKEN id="token-3-28" pos="word" morph="none" start_char="438" end_char="442">water</TOKEN>
<TOKEN id="token-3-29" pos="punct" morph="none" start_char="443" end_char="443">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="446" end_char="521">
<ORIGINAL_TEXT>They said it then travelled unnoticed to Wuhan, where it was first detected.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="446" end_char="449">They</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="451" end_char="454">said</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="456" end_char="457">it</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="459" end_char="462">then</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="464" end_char="472">travelled</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="474" end_char="482">unnoticed</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="484" end_char="485">to</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="487" end_char="491">Wuhan</TOKEN>
<TOKEN id="token-4-8" pos="punct" morph="none" start_char="492" end_char="492">,</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="494" end_char="498">where</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="500" end_char="501">it</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="503" end_char="505">was</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="507" end_char="511">first</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="513" end_char="520">detected</TOKEN>
<TOKEN id="token-4-14" pos="punct" morph="none" start_char="521" end_char="521">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="524" end_char="640">
<ORIGINAL_TEXT>Chinese authorities have earlier pointed the finger of blame to Italy, the US and Europe -- largely without evidence.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="524" end_char="530">Chinese</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="532" end_char="542">authorities</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="544" end_char="547">have</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="549" end_char="555">earlier</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="557" end_char="563">pointed</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="565" end_char="567">the</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="569" end_char="574">finger</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="576" end_char="577">of</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="579" end_char="583">blame</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="585" end_char="586">to</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="588" end_char="592">Italy</TOKEN>
<TOKEN id="token-5-11" pos="punct" morph="none" start_char="593" end_char="593">,</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="595" end_char="597">the</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="599" end_char="600">US</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="602" end_char="604">and</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="606" end_char="611">Europe</TOKEN>
<TOKEN id="token-5-16" pos="punct" morph="none" start_char="613" end_char="614">--</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="616" end_char="622">largely</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="624" end_char="630">without</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="632" end_char="639">evidence</TOKEN>
<TOKEN id="token-5-20" pos="punct" morph="none" start_char="640" end_char="640">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="643" end_char="738">
<ORIGINAL_TEXT>This new blame comes against a backdrop of increased political tensions between India and China.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="643" end_char="646">This</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="648" end_char="650">new</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="652" end_char="656">blame</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="658" end_char="662">comes</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="664" end_char="670">against</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="672" end_char="672">a</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="674" end_char="681">backdrop</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="683" end_char="684">of</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="686" end_char="694">increased</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="696" end_char="704">political</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="706" end_char="713">tensions</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="715" end_char="721">between</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="723" end_char="727">India</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="729" end_char="731">and</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="733" end_char="737">China</TOKEN>
<TOKEN id="token-6-15" pos="punct" morph="none" start_char="738" end_char="738">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="741" end_char="832">
<ORIGINAL_TEXT>In their paper, the Chinese team use phylogenetic analysis to trace the origins of Covid-19.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="741" end_char="742">In</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="744" end_char="748">their</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="750" end_char="754">paper</TOKEN>
<TOKEN id="token-7-3" pos="punct" morph="none" start_char="755" end_char="755">,</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="757" end_char="759">the</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="761" end_char="767">Chinese</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="769" end_char="772">team</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="774" end_char="776">use</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="778" end_char="789">phylogenetic</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="791" end_char="798">analysis</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="800" end_char="801">to</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="803" end_char="807">trace</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="809" end_char="811">the</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="813" end_char="819">origins</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="821" end_char="822">of</TOKEN>
<TOKEN id="token-7-15" pos="unknown" morph="none" start_char="824" end_char="831">Covid-19</TOKEN>
<TOKEN id="token-7-16" pos="punct" morph="none" start_char="832" end_char="832">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="835" end_char="961">
<ORIGINAL_TEXT>Viruses, like all cells, mutate as they reproduce, meaning tiny changes occur in their DNA each time they replicate themselves.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="835" end_char="841">Viruses</TOKEN>
<TOKEN id="token-8-1" pos="punct" morph="none" start_char="842" end_char="842">,</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="844" end_char="847">like</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="849" end_char="851">all</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="853" end_char="857">cells</TOKEN>
<TOKEN id="token-8-5" pos="punct" morph="none" start_char="858" end_char="858">,</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="860" end_char="865">mutate</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="867" end_char="868">as</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="870" end_char="873">they</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="875" end_char="883">reproduce</TOKEN>
<TOKEN id="token-8-10" pos="punct" morph="none" start_char="884" end_char="884">,</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="886" end_char="892">meaning</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="894" end_char="897">tiny</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="899" end_char="905">changes</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="907" end_char="911">occur</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="913" end_char="914">in</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="916" end_char="920">their</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="922" end_char="924">DNA</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="926" end_char="929">each</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="931" end_char="934">time</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="936" end_char="939">they</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="941" end_char="949">replicate</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="951" end_char="960">themselves</TOKEN>
<TOKEN id="token-8-23" pos="punct" morph="none" start_char="961" end_char="961">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="966" end_char="985">
<ORIGINAL_TEXT>Something went wrong</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="966" end_char="974">Something</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="976" end_char="979">went</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="981" end_char="985">wrong</TOKEN>
</SEG>
<SEG id="segment-10" start_char="988" end_char="1071">
<ORIGINAL_TEXT>Session ID 45e609d2-21a0-7450-b3a3-db2e8b6b5654:bea781ec-2258-138d-ad9e-4132553e5a8b</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="988" end_char="994">Session</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="996" end_char="997">ID</TOKEN>
<TOKEN id="token-10-2" pos="unknown" morph="none" start_char="999" end_char="1071">45e609d2-21a0-7450-b3a3-db2e8b6b5654:bea781ec-2258-138d-ad9e-4132553e5a8b</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1074" end_char="1316">
<ORIGINAL_TEXT>The scientists argue their method of investigation rules out the virus found in Wuhan as the 'original' virus, and instead points to eight other countries: Bangladesh, the USA, Greece, Australia, India, Italy, Czech Republic, Russia or Serbia.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1074" end_char="1076">The</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1078" end_char="1087">scientists</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1089" end_char="1093">argue</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1095" end_char="1099">their</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1101" end_char="1106">method</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1108" end_char="1109">of</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1111" end_char="1123">investigation</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1125" end_char="1129">rules</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1131" end_char="1133">out</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1135" end_char="1137">the</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1139" end_char="1143">virus</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1145" end_char="1149">found</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1151" end_char="1152">in</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1154" end_char="1158">Wuhan</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1160" end_char="1161">as</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1163" end_char="1165">the</TOKEN>
<TOKEN id="token-11-16" pos="punct" morph="none" start_char="1167" end_char="1167">'</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1168" end_char="1175">original</TOKEN>
<TOKEN id="token-11-18" pos="punct" morph="none" start_char="1176" end_char="1176">'</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1178" end_char="1182">virus</TOKEN>
<TOKEN id="token-11-20" pos="punct" morph="none" start_char="1183" end_char="1183">,</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1185" end_char="1187">and</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1189" end_char="1195">instead</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1197" end_char="1202">points</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1204" end_char="1205">to</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1207" end_char="1211">eight</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="1213" end_char="1217">other</TOKEN>
<TOKEN id="token-11-27" pos="word" morph="none" start_char="1219" end_char="1227">countries</TOKEN>
<TOKEN id="token-11-28" pos="punct" morph="none" start_char="1228" end_char="1228">:</TOKEN>
<TOKEN id="token-11-29" pos="word" morph="none" start_char="1230" end_char="1239">Bangladesh</TOKEN>
<TOKEN id="token-11-30" pos="punct" morph="none" start_char="1240" end_char="1240">,</TOKEN>
<TOKEN id="token-11-31" pos="word" morph="none" start_char="1242" end_char="1244">the</TOKEN>
<TOKEN id="token-11-32" pos="word" morph="none" start_char="1246" end_char="1248">USA</TOKEN>
<TOKEN id="token-11-33" pos="punct" morph="none" start_char="1249" end_char="1249">,</TOKEN>
<TOKEN id="token-11-34" pos="word" morph="none" start_char="1251" end_char="1256">Greece</TOKEN>
<TOKEN id="token-11-35" pos="punct" morph="none" start_char="1257" end_char="1257">,</TOKEN>
<TOKEN id="token-11-36" pos="word" morph="none" start_char="1259" end_char="1267">Australia</TOKEN>
<TOKEN id="token-11-37" pos="punct" morph="none" start_char="1268" end_char="1268">,</TOKEN>
<TOKEN id="token-11-38" pos="word" morph="none" start_char="1270" end_char="1274">India</TOKEN>
<TOKEN id="token-11-39" pos="punct" morph="none" start_char="1275" end_char="1275">,</TOKEN>
<TOKEN id="token-11-40" pos="word" morph="none" start_char="1277" end_char="1281">Italy</TOKEN>
<TOKEN id="token-11-41" pos="punct" morph="none" start_char="1282" end_char="1282">,</TOKEN>
<TOKEN id="token-11-42" pos="word" morph="none" start_char="1284" end_char="1288">Czech</TOKEN>
<TOKEN id="token-11-43" pos="word" morph="none" start_char="1290" end_char="1297">Republic</TOKEN>
<TOKEN id="token-11-44" pos="punct" morph="none" start_char="1298" end_char="1298">,</TOKEN>
<TOKEN id="token-11-45" pos="word" morph="none" start_char="1300" end_char="1305">Russia</TOKEN>
<TOKEN id="token-11-46" pos="word" morph="none" start_char="1307" end_char="1308">or</TOKEN>
<TOKEN id="token-11-47" pos="word" morph="none" start_char="1310" end_char="1315">Serbia</TOKEN>
<TOKEN id="token-11-48" pos="punct" morph="none" start_char="1316" end_char="1316">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1319" end_char="1507">
<ORIGINAL_TEXT>Researchers go on to argue that because India and Bangladesh both recorded samples with low mutations and are geographic neighbours, it is likely that the first transmission occurred there.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1319" end_char="1329">Researchers</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1331" end_char="1332">go</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1334" end_char="1335">on</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1337" end_char="1338">to</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1340" end_char="1344">argue</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1346" end_char="1349">that</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1351" end_char="1357">because</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1359" end_char="1363">India</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1365" end_char="1367">and</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1369" end_char="1378">Bangladesh</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1380" end_char="1383">both</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1385" end_char="1392">recorded</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1394" end_char="1400">samples</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1402" end_char="1405">with</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1407" end_char="1409">low</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1411" end_char="1419">mutations</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1421" end_char="1423">and</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1425" end_char="1427">are</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="1429" end_char="1438">geographic</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="1440" end_char="1449">neighbours</TOKEN>
<TOKEN id="token-12-20" pos="punct" morph="none" start_char="1450" end_char="1450">,</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1452" end_char="1453">it</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1455" end_char="1456">is</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1458" end_char="1463">likely</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1465" end_char="1468">that</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1470" end_char="1472">the</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="1474" end_char="1478">first</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="1480" end_char="1491">transmission</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="1493" end_char="1500">occurred</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="1502" end_char="1506">there</TOKEN>
<TOKEN id="token-12-30" pos="punct" morph="none" start_char="1507" end_char="1507">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1510" end_char="1733">
<ORIGINAL_TEXT>Their unproven theory goes on to say: "The water shortage made wild animals such as monkeys engage in the deadly fight over water among each other and would have surely increased the chance of human-wild animal interactions.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1510" end_char="1514">Their</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1516" end_char="1523">unproven</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1525" end_char="1530">theory</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1532" end_char="1535">goes</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1537" end_char="1538">on</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1540" end_char="1541">to</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1543" end_char="1545">say</TOKEN>
<TOKEN id="token-13-7" pos="punct" morph="none" start_char="1546" end_char="1546">:</TOKEN>
<TOKEN id="token-13-8" pos="punct" morph="none" start_char="1548" end_char="1548">"</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1549" end_char="1551">The</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1553" end_char="1557">water</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1559" end_char="1566">shortage</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1568" end_char="1571">made</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="1573" end_char="1576">wild</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="1578" end_char="1584">animals</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="1586" end_char="1589">such</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="1591" end_char="1592">as</TOKEN>
<TOKEN id="token-13-17" pos="word" morph="none" start_char="1594" end_char="1600">monkeys</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="1602" end_char="1607">engage</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="1609" end_char="1610">in</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="1612" end_char="1614">the</TOKEN>
<TOKEN id="token-13-21" pos="word" morph="none" start_char="1616" end_char="1621">deadly</TOKEN>
<TOKEN id="token-13-22" pos="word" morph="none" start_char="1623" end_char="1627">fight</TOKEN>
<TOKEN id="token-13-23" pos="word" morph="none" start_char="1629" end_char="1632">over</TOKEN>
<TOKEN id="token-13-24" pos="word" morph="none" start_char="1634" end_char="1638">water</TOKEN>
<TOKEN id="token-13-25" pos="word" morph="none" start_char="1640" end_char="1644">among</TOKEN>
<TOKEN id="token-13-26" pos="word" morph="none" start_char="1646" end_char="1649">each</TOKEN>
<TOKEN id="token-13-27" pos="word" morph="none" start_char="1651" end_char="1655">other</TOKEN>
<TOKEN id="token-13-28" pos="word" morph="none" start_char="1657" end_char="1659">and</TOKEN>
<TOKEN id="token-13-29" pos="word" morph="none" start_char="1661" end_char="1665">would</TOKEN>
<TOKEN id="token-13-30" pos="word" morph="none" start_char="1667" end_char="1670">have</TOKEN>
<TOKEN id="token-13-31" pos="word" morph="none" start_char="1672" end_char="1677">surely</TOKEN>
<TOKEN id="token-13-32" pos="word" morph="none" start_char="1679" end_char="1687">increased</TOKEN>
<TOKEN id="token-13-33" pos="word" morph="none" start_char="1689" end_char="1691">the</TOKEN>
<TOKEN id="token-13-34" pos="word" morph="none" start_char="1693" end_char="1698">chance</TOKEN>
<TOKEN id="token-13-35" pos="word" morph="none" start_char="1700" end_char="1701">of</TOKEN>
<TOKEN id="token-13-36" pos="unknown" morph="none" start_char="1703" end_char="1712">human-wild</TOKEN>
<TOKEN id="token-13-37" pos="word" morph="none" start_char="1714" end_char="1719">animal</TOKEN>
<TOKEN id="token-13-38" pos="word" morph="none" start_char="1721" end_char="1732">interactions</TOKEN>
<TOKEN id="token-13-39" pos="punct" morph="none" start_char="1733" end_char="1733">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1735" end_char="1850">
<ORIGINAL_TEXT>We speculated that the [animal to human] transmission of SARS-CoV-2 might be associated with this unusual heat wave.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1735" end_char="1736">We</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1738" end_char="1747">speculated</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1749" end_char="1752">that</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1754" end_char="1756">the</TOKEN>
<TOKEN id="token-14-4" pos="punct" morph="none" start_char="1758" end_char="1758">[</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1759" end_char="1764">animal</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1766" end_char="1767">to</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1769" end_char="1773">human</TOKEN>
<TOKEN id="token-14-8" pos="punct" morph="none" start_char="1774" end_char="1774">]</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1776" end_char="1787">transmission</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1789" end_char="1790">of</TOKEN>
<TOKEN id="token-14-11" pos="unknown" morph="none" start_char="1792" end_char="1801">SARS-CoV-2</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="1803" end_char="1807">might</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="1809" end_char="1810">be</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="1812" end_char="1821">associated</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="1823" end_char="1826">with</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="1828" end_char="1831">this</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="1833" end_char="1839">unusual</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="1841" end_char="1844">heat</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="1846" end_char="1849">wave</TOKEN>
<TOKEN id="token-14-20" pos="punct" morph="none" start_char="1850" end_char="1850">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1853" end_char="1964">
<ORIGINAL_TEXT>"India's poor healthcare system and young population allowed the virus to spread undetected for several months."</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="punct" morph="none" start_char="1853" end_char="1853">"</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1854" end_char="1860">India's</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1862" end_char="1865">poor</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="1867" end_char="1876">healthcare</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1878" end_char="1883">system</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1885" end_char="1887">and</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="1889" end_char="1893">young</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1895" end_char="1904">population</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1906" end_char="1912">allowed</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="1914" end_char="1916">the</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1918" end_char="1922">virus</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="1924" end_char="1925">to</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="1927" end_char="1932">spread</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="1934" end_char="1943">undetected</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="1945" end_char="1947">for</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="1949" end_char="1955">several</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="1957" end_char="1962">months</TOKEN>
<TOKEN id="token-15-17" pos="punct" morph="none" start_char="1963" end_char="1964">."</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1967" end_char="2066">
<ORIGINAL_TEXT>Despite the bizarre claims, coronavirus is believed to have first emerged in China in December 2019.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1967" end_char="1973">Despite</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1975" end_char="1977">the</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1979" end_char="1985">bizarre</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1987" end_char="1992">claims</TOKEN>
<TOKEN id="token-16-4" pos="punct" morph="none" start_char="1993" end_char="1993">,</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1995" end_char="2005">coronavirus</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="2007" end_char="2008">is</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="2010" end_char="2017">believed</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="2019" end_char="2020">to</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="2022" end_char="2025">have</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="2027" end_char="2031">first</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="2033" end_char="2039">emerged</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="2041" end_char="2042">in</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2044" end_char="2048">China</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="2050" end_char="2051">in</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="2053" end_char="2060">December</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="2062" end_char="2065">2019</TOKEN>
<TOKEN id="token-16-17" pos="punct" morph="none" start_char="2066" end_char="2066">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2069" end_char="2175">
<ORIGINAL_TEXT>It was first linked to a cluster of cases of 'pneumonia of unknown origin' at a seafood market in the city.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2069" end_char="2070">It</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2072" end_char="2074">was</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2076" end_char="2080">first</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2082" end_char="2087">linked</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2089" end_char="2090">to</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2092" end_char="2092">a</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2094" end_char="2100">cluster</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="2102" end_char="2103">of</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="2105" end_char="2109">cases</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="2111" end_char="2112">of</TOKEN>
<TOKEN id="token-17-10" pos="punct" morph="none" start_char="2114" end_char="2114">'</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="2115" end_char="2123">pneumonia</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="2125" end_char="2126">of</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="2128" end_char="2134">unknown</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="2136" end_char="2141">origin</TOKEN>
<TOKEN id="token-17-15" pos="punct" morph="none" start_char="2142" end_char="2142">'</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="2144" end_char="2145">at</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="2147" end_char="2147">a</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="2149" end_char="2155">seafood</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="2157" end_char="2162">market</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="2164" end_char="2165">in</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="2167" end_char="2169">the</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="2171" end_char="2174">city</TOKEN>
<TOKEN id="token-17-23" pos="punct" morph="none" start_char="2175" end_char="2175">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2178" end_char="2467">
<ORIGINAL_TEXT>Earlier on Friday, Chinese state media cited the presence of the novel coronavirus on imported frozen food packaging, as well as scientific papers, claiming that the coronavirus was circulating in Europe earlier than previously believed, as evidence that China may not have been its origin.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2178" end_char="2184">Earlier</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2186" end_char="2187">on</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="2189" end_char="2194">Friday</TOKEN>
<TOKEN id="token-18-3" pos="punct" morph="none" start_char="2195" end_char="2195">,</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2197" end_char="2203">Chinese</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="2205" end_char="2209">state</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2211" end_char="2215">media</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2217" end_char="2221">cited</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2223" end_char="2225">the</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2227" end_char="2234">presence</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2236" end_char="2237">of</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="2239" end_char="2241">the</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="2243" end_char="2247">novel</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="2249" end_char="2259">coronavirus</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="2261" end_char="2262">on</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="2264" end_char="2271">imported</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="2273" end_char="2278">frozen</TOKEN>
<TOKEN id="token-18-17" pos="word" morph="none" start_char="2280" end_char="2283">food</TOKEN>
<TOKEN id="token-18-18" pos="word" morph="none" start_char="2285" end_char="2293">packaging</TOKEN>
<TOKEN id="token-18-19" pos="punct" morph="none" start_char="2294" end_char="2294">,</TOKEN>
<TOKEN id="token-18-20" pos="word" morph="none" start_char="2296" end_char="2297">as</TOKEN>
<TOKEN id="token-18-21" pos="word" morph="none" start_char="2299" end_char="2302">well</TOKEN>
<TOKEN id="token-18-22" pos="word" morph="none" start_char="2304" end_char="2305">as</TOKEN>
<TOKEN id="token-18-23" pos="word" morph="none" start_char="2307" end_char="2316">scientific</TOKEN>
<TOKEN id="token-18-24" pos="word" morph="none" start_char="2318" end_char="2323">papers</TOKEN>
<TOKEN id="token-18-25" pos="punct" morph="none" start_char="2324" end_char="2324">,</TOKEN>
<TOKEN id="token-18-26" pos="word" morph="none" start_char="2326" end_char="2333">claiming</TOKEN>
<TOKEN id="token-18-27" pos="word" morph="none" start_char="2335" end_char="2338">that</TOKEN>
<TOKEN id="token-18-28" pos="word" morph="none" start_char="2340" end_char="2342">the</TOKEN>
<TOKEN id="token-18-29" pos="word" morph="none" start_char="2344" end_char="2354">coronavirus</TOKEN>
<TOKEN id="token-18-30" pos="word" morph="none" start_char="2356" end_char="2358">was</TOKEN>
<TOKEN id="token-18-31" pos="word" morph="none" start_char="2360" end_char="2370">circulating</TOKEN>
<TOKEN id="token-18-32" pos="word" morph="none" start_char="2372" end_char="2373">in</TOKEN>
<TOKEN id="token-18-33" pos="word" morph="none" start_char="2375" end_char="2380">Europe</TOKEN>
<TOKEN id="token-18-34" pos="word" morph="none" start_char="2382" end_char="2388">earlier</TOKEN>
<TOKEN id="token-18-35" pos="word" morph="none" start_char="2390" end_char="2393">than</TOKEN>
<TOKEN id="token-18-36" pos="word" morph="none" start_char="2395" end_char="2404">previously</TOKEN>
<TOKEN id="token-18-37" pos="word" morph="none" start_char="2406" end_char="2413">believed</TOKEN>
<TOKEN id="token-18-38" pos="punct" morph="none" start_char="2414" end_char="2414">,</TOKEN>
<TOKEN id="token-18-39" pos="word" morph="none" start_char="2416" end_char="2417">as</TOKEN>
<TOKEN id="token-18-40" pos="word" morph="none" start_char="2419" end_char="2426">evidence</TOKEN>
<TOKEN id="token-18-41" pos="word" morph="none" start_char="2428" end_char="2431">that</TOKEN>
<TOKEN id="token-18-42" pos="word" morph="none" start_char="2433" end_char="2437">China</TOKEN>
<TOKEN id="token-18-43" pos="word" morph="none" start_char="2439" end_char="2441">may</TOKEN>
<TOKEN id="token-18-44" pos="word" morph="none" start_char="2443" end_char="2445">not</TOKEN>
<TOKEN id="token-18-45" pos="word" morph="none" start_char="2447" end_char="2450">have</TOKEN>
<TOKEN id="token-18-46" pos="word" morph="none" start_char="2452" end_char="2455">been</TOKEN>
<TOKEN id="token-18-47" pos="word" morph="none" start_char="2457" end_char="2459">its</TOKEN>
<TOKEN id="token-18-48" pos="word" morph="none" start_char="2461" end_char="2466">origin</TOKEN>
<TOKEN id="token-18-49" pos="punct" morph="none" start_char="2467" end_char="2467">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2470" end_char="2712">
<ORIGINAL_TEXT>To this end, the World Health Organisation's top emergency expert said on Friday it would be "highly speculative" for the WHO to say the coronavirus did not emerge in China, where it was first identified in a food market in December last year.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2470" end_char="2471">To</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2473" end_char="2476">this</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2478" end_char="2480">end</TOKEN>
<TOKEN id="token-19-3" pos="punct" morph="none" start_char="2481" end_char="2481">,</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2483" end_char="2485">the</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="2487" end_char="2491">World</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2493" end_char="2498">Health</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="2500" end_char="2513">Organisation's</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="2515" end_char="2517">top</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="2519" end_char="2527">emergency</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="2529" end_char="2534">expert</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="2536" end_char="2539">said</TOKEN>
<TOKEN id="token-19-12" pos="word" morph="none" start_char="2541" end_char="2542">on</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="2544" end_char="2549">Friday</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="2551" end_char="2552">it</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="2554" end_char="2558">would</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="2560" end_char="2561">be</TOKEN>
<TOKEN id="token-19-17" pos="punct" morph="none" start_char="2563" end_char="2563">"</TOKEN>
<TOKEN id="token-19-18" pos="word" morph="none" start_char="2564" end_char="2569">highly</TOKEN>
<TOKEN id="token-19-19" pos="word" morph="none" start_char="2571" end_char="2581">speculative</TOKEN>
<TOKEN id="token-19-20" pos="punct" morph="none" start_char="2582" end_char="2582">"</TOKEN>
<TOKEN id="token-19-21" pos="word" morph="none" start_char="2584" end_char="2586">for</TOKEN>
<TOKEN id="token-19-22" pos="word" morph="none" start_char="2588" end_char="2590">the</TOKEN>
<TOKEN id="token-19-23" pos="word" morph="none" start_char="2592" end_char="2594">WHO</TOKEN>
<TOKEN id="token-19-24" pos="word" morph="none" start_char="2596" end_char="2597">to</TOKEN>
<TOKEN id="token-19-25" pos="word" morph="none" start_char="2599" end_char="2601">say</TOKEN>
<TOKEN id="token-19-26" pos="word" morph="none" start_char="2603" end_char="2605">the</TOKEN>
<TOKEN id="token-19-27" pos="word" morph="none" start_char="2607" end_char="2617">coronavirus</TOKEN>
<TOKEN id="token-19-28" pos="word" morph="none" start_char="2619" end_char="2621">did</TOKEN>
<TOKEN id="token-19-29" pos="word" morph="none" start_char="2623" end_char="2625">not</TOKEN>
<TOKEN id="token-19-30" pos="word" morph="none" start_char="2627" end_char="2632">emerge</TOKEN>
<TOKEN id="token-19-31" pos="word" morph="none" start_char="2634" end_char="2635">in</TOKEN>
<TOKEN id="token-19-32" pos="word" morph="none" start_char="2637" end_char="2641">China</TOKEN>
<TOKEN id="token-19-33" pos="punct" morph="none" start_char="2642" end_char="2642">,</TOKEN>
<TOKEN id="token-19-34" pos="word" morph="none" start_char="2644" end_char="2648">where</TOKEN>
<TOKEN id="token-19-35" pos="word" morph="none" start_char="2650" end_char="2651">it</TOKEN>
<TOKEN id="token-19-36" pos="word" morph="none" start_char="2653" end_char="2655">was</TOKEN>
<TOKEN id="token-19-37" pos="word" morph="none" start_char="2657" end_char="2661">first</TOKEN>
<TOKEN id="token-19-38" pos="word" morph="none" start_char="2663" end_char="2672">identified</TOKEN>
<TOKEN id="token-19-39" pos="word" morph="none" start_char="2674" end_char="2675">in</TOKEN>
<TOKEN id="token-19-40" pos="word" morph="none" start_char="2677" end_char="2677">a</TOKEN>
<TOKEN id="token-19-41" pos="word" morph="none" start_char="2679" end_char="2682">food</TOKEN>
<TOKEN id="token-19-42" pos="word" morph="none" start_char="2684" end_char="2689">market</TOKEN>
<TOKEN id="token-19-43" pos="word" morph="none" start_char="2691" end_char="2692">in</TOKEN>
<TOKEN id="token-19-44" pos="word" morph="none" start_char="2694" end_char="2701">December</TOKEN>
<TOKEN id="token-19-45" pos="word" morph="none" start_char="2703" end_char="2706">last</TOKEN>
<TOKEN id="token-19-46" pos="word" morph="none" start_char="2708" end_char="2711">year</TOKEN>
<TOKEN id="token-19-47" pos="punct" morph="none" start_char="2712" end_char="2712">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2715" end_char="2988">
<ORIGINAL_TEXT>China is pushing a narrative via state media that the virus existed abroad before it was discovered in the central city of Wuhan, citing the presence of coronavirus on imported frozen food packaging and scientific papers claiming it had been circulating in Europe last year.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2715" end_char="2719">China</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="2721" end_char="2722">is</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="2724" end_char="2730">pushing</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="2732" end_char="2732">a</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2734" end_char="2742">narrative</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2744" end_char="2746">via</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2748" end_char="2752">state</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="2754" end_char="2758">media</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="2760" end_char="2763">that</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="2765" end_char="2767">the</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="2769" end_char="2773">virus</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="2775" end_char="2781">existed</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="2783" end_char="2788">abroad</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="2790" end_char="2795">before</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="2797" end_char="2798">it</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="2800" end_char="2802">was</TOKEN>
<TOKEN id="token-20-16" pos="word" morph="none" start_char="2804" end_char="2813">discovered</TOKEN>
<TOKEN id="token-20-17" pos="word" morph="none" start_char="2815" end_char="2816">in</TOKEN>
<TOKEN id="token-20-18" pos="word" morph="none" start_char="2818" end_char="2820">the</TOKEN>
<TOKEN id="token-20-19" pos="word" morph="none" start_char="2822" end_char="2828">central</TOKEN>
<TOKEN id="token-20-20" pos="word" morph="none" start_char="2830" end_char="2833">city</TOKEN>
<TOKEN id="token-20-21" pos="word" morph="none" start_char="2835" end_char="2836">of</TOKEN>
<TOKEN id="token-20-22" pos="word" morph="none" start_char="2838" end_char="2842">Wuhan</TOKEN>
<TOKEN id="token-20-23" pos="punct" morph="none" start_char="2843" end_char="2843">,</TOKEN>
<TOKEN id="token-20-24" pos="word" morph="none" start_char="2845" end_char="2850">citing</TOKEN>
<TOKEN id="token-20-25" pos="word" morph="none" start_char="2852" end_char="2854">the</TOKEN>
<TOKEN id="token-20-26" pos="word" morph="none" start_char="2856" end_char="2863">presence</TOKEN>
<TOKEN id="token-20-27" pos="word" morph="none" start_char="2865" end_char="2866">of</TOKEN>
<TOKEN id="token-20-28" pos="word" morph="none" start_char="2868" end_char="2878">coronavirus</TOKEN>
<TOKEN id="token-20-29" pos="word" morph="none" start_char="2880" end_char="2881">on</TOKEN>
<TOKEN id="token-20-30" pos="word" morph="none" start_char="2883" end_char="2890">imported</TOKEN>
<TOKEN id="token-20-31" pos="word" morph="none" start_char="2892" end_char="2897">frozen</TOKEN>
<TOKEN id="token-20-32" pos="word" morph="none" start_char="2899" end_char="2902">food</TOKEN>
<TOKEN id="token-20-33" pos="word" morph="none" start_char="2904" end_char="2912">packaging</TOKEN>
<TOKEN id="token-20-34" pos="word" morph="none" start_char="2914" end_char="2916">and</TOKEN>
<TOKEN id="token-20-35" pos="word" morph="none" start_char="2918" end_char="2927">scientific</TOKEN>
<TOKEN id="token-20-36" pos="word" morph="none" start_char="2929" end_char="2934">papers</TOKEN>
<TOKEN id="token-20-37" pos="word" morph="none" start_char="2936" end_char="2943">claiming</TOKEN>
<TOKEN id="token-20-38" pos="word" morph="none" start_char="2945" end_char="2946">it</TOKEN>
<TOKEN id="token-20-39" pos="word" morph="none" start_char="2948" end_char="2950">had</TOKEN>
<TOKEN id="token-20-40" pos="word" morph="none" start_char="2952" end_char="2955">been</TOKEN>
<TOKEN id="token-20-41" pos="word" morph="none" start_char="2957" end_char="2967">circulating</TOKEN>
<TOKEN id="token-20-42" pos="word" morph="none" start_char="2969" end_char="2970">in</TOKEN>
<TOKEN id="token-20-43" pos="word" morph="none" start_char="2972" end_char="2977">Europe</TOKEN>
<TOKEN id="token-20-44" pos="word" morph="none" start_char="2979" end_char="2982">last</TOKEN>
<TOKEN id="token-20-45" pos="word" morph="none" start_char="2984" end_char="2987">year</TOKEN>
<TOKEN id="token-20-46" pos="punct" morph="none" start_char="2988" end_char="2988">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="2991" end_char="3196">
<ORIGINAL_TEXT>"I think it`s highly speculative for us to say that the disease did not emerge in China," Mike Ryan said at a virtual briefing in Geneva after being asked if COVID-19 could have first emerged outside China.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="punct" morph="none" start_char="2991" end_char="2991">"</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="2992" end_char="2992">I</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="2994" end_char="2998">think</TOKEN>
<TOKEN id="token-21-3" pos="unknown" morph="none" start_char="3000" end_char="3003">it`s</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="3005" end_char="3010">highly</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="3012" end_char="3022">speculative</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="3024" end_char="3026">for</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="3028" end_char="3029">us</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="3031" end_char="3032">to</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="3034" end_char="3036">say</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="3038" end_char="3041">that</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="3043" end_char="3045">the</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="3047" end_char="3053">disease</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="3055" end_char="3057">did</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="3059" end_char="3061">not</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="3063" end_char="3068">emerge</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="3070" end_char="3071">in</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="3073" end_char="3077">China</TOKEN>
<TOKEN id="token-21-18" pos="punct" morph="none" start_char="3078" end_char="3079">,"</TOKEN>
<TOKEN id="token-21-19" pos="word" morph="none" start_char="3081" end_char="3084">Mike</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="3086" end_char="3089">Ryan</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="3091" end_char="3094">said</TOKEN>
<TOKEN id="token-21-22" pos="word" morph="none" start_char="3096" end_char="3097">at</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="3099" end_char="3099">a</TOKEN>
<TOKEN id="token-21-24" pos="word" morph="none" start_char="3101" end_char="3107">virtual</TOKEN>
<TOKEN id="token-21-25" pos="word" morph="none" start_char="3109" end_char="3116">briefing</TOKEN>
<TOKEN id="token-21-26" pos="word" morph="none" start_char="3118" end_char="3119">in</TOKEN>
<TOKEN id="token-21-27" pos="word" morph="none" start_char="3121" end_char="3126">Geneva</TOKEN>
<TOKEN id="token-21-28" pos="word" morph="none" start_char="3128" end_char="3132">after</TOKEN>
<TOKEN id="token-21-29" pos="word" morph="none" start_char="3134" end_char="3138">being</TOKEN>
<TOKEN id="token-21-30" pos="word" morph="none" start_char="3140" end_char="3144">asked</TOKEN>
<TOKEN id="token-21-31" pos="word" morph="none" start_char="3146" end_char="3147">if</TOKEN>
<TOKEN id="token-21-32" pos="unknown" morph="none" start_char="3149" end_char="3156">COVID-19</TOKEN>
<TOKEN id="token-21-33" pos="word" morph="none" start_char="3158" end_char="3162">could</TOKEN>
<TOKEN id="token-21-34" pos="word" morph="none" start_char="3164" end_char="3167">have</TOKEN>
<TOKEN id="token-21-35" pos="word" morph="none" start_char="3169" end_char="3173">first</TOKEN>
<TOKEN id="token-21-36" pos="word" morph="none" start_char="3175" end_char="3181">emerged</TOKEN>
<TOKEN id="token-21-37" pos="word" morph="none" start_char="3183" end_char="3189">outside</TOKEN>
<TOKEN id="token-21-38" pos="word" morph="none" start_char="3191" end_char="3195">China</TOKEN>
<TOKEN id="token-21-39" pos="punct" morph="none" start_char="3196" end_char="3196">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="3199" end_char="3312">
<ORIGINAL_TEXT>He repeated that the WHO intended to send researchers to the Wuhan food market to probe the virus origins further.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="3199" end_char="3200">He</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="3202" end_char="3209">repeated</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="3211" end_char="3214">that</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="3216" end_char="3218">the</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="3220" end_char="3222">WHO</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="3224" end_char="3231">intended</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="3233" end_char="3234">to</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="3236" end_char="3239">send</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="3241" end_char="3251">researchers</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="3253" end_char="3254">to</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="3256" end_char="3258">the</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="3260" end_char="3264">Wuhan</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="3266" end_char="3269">food</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="3271" end_char="3276">market</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="3278" end_char="3279">to</TOKEN>
<TOKEN id="token-22-15" pos="word" morph="none" start_char="3281" end_char="3285">probe</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="3287" end_char="3289">the</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="3291" end_char="3295">virus</TOKEN>
<TOKEN id="token-22-18" pos="word" morph="none" start_char="3297" end_char="3303">origins</TOKEN>
<TOKEN id="token-22-19" pos="word" morph="none" start_char="3305" end_char="3311">further</TOKEN>
<TOKEN id="token-22-20" pos="punct" morph="none" start_char="3312" end_char="3312">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="3315" end_char="3430">
<ORIGINAL_TEXT>The WHO has been accused by the Trump administration of being "China-centric", allegations it has repeatedly denied.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="3315" end_char="3317">The</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="3319" end_char="3321">WHO</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="3323" end_char="3325">has</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="3327" end_char="3330">been</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="3332" end_char="3338">accused</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="3340" end_char="3341">by</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="3343" end_char="3345">the</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="3347" end_char="3351">Trump</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="3353" end_char="3366">administration</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="3368" end_char="3369">of</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="3371" end_char="3375">being</TOKEN>
<TOKEN id="token-23-11" pos="punct" morph="none" start_char="3377" end_char="3377">"</TOKEN>
<TOKEN id="token-23-12" pos="unknown" morph="none" start_char="3378" end_char="3390">China-centric</TOKEN>
<TOKEN id="token-23-13" pos="punct" morph="none" start_char="3391" end_char="3392">",</TOKEN>
<TOKEN id="token-23-14" pos="word" morph="none" start_char="3394" end_char="3404">allegations</TOKEN>
<TOKEN id="token-23-15" pos="word" morph="none" start_char="3406" end_char="3407">it</TOKEN>
<TOKEN id="token-23-16" pos="word" morph="none" start_char="3409" end_char="3411">has</TOKEN>
<TOKEN id="token-23-17" pos="word" morph="none" start_char="3413" end_char="3422">repeatedly</TOKEN>
<TOKEN id="token-23-18" pos="word" morph="none" start_char="3424" end_char="3429">denied</TOKEN>
<TOKEN id="token-23-19" pos="punct" morph="none" start_char="3430" end_char="3430">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
