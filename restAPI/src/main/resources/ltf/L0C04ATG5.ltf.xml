<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATG5" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="427" raw_text_md5="48f98efc1a70aedc3d05a52a46e28c1e">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="38">
<ORIGINAL_TEXT>Are you happy covid is getting weaker?</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="3">Are</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="5" end_char="7">you</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="9" end_char="13">happy</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="15" end_char="19">covid</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="21" end_char="22">is</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="24" end_char="30">getting</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="32" end_char="37">weaker</TOKEN>
<TOKEN id="token-0-7" pos="punct" morph="none" start_char="38" end_char="38">?</TOKEN>
</SEG>
<SEG id="segment-1" start_char="43" end_char="80">
<ORIGINAL_TEXT>Are you happy covid is getting weaker?</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="43" end_char="45">Are</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="47" end_char="49">you</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="51" end_char="55">happy</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="57" end_char="61">covid</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="63" end_char="64">is</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="66" end_char="72">getting</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="74" end_char="79">weaker</TOKEN>
<TOKEN id="token-1-7" pos="punct" morph="none" start_char="80" end_char="80">?</TOKEN>
</SEG>
<SEG id="segment-2" start_char="85" end_char="158">
<ORIGINAL_TEXT>covid is mutating at a rate faster than the medical profession can detect.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="85" end_char="89">covid</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="91" end_char="92">is</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="94" end_char="101">mutating</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="103" end_char="104">at</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="106" end_char="106">a</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="108" end_char="111">rate</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="113" end_char="118">faster</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="120" end_char="123">than</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="125" end_char="127">the</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="129" end_char="135">medical</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="137" end_char="146">profession</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="148" end_char="150">can</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="152" end_char="157">detect</TOKEN>
<TOKEN id="token-2-13" pos="punct" morph="none" start_char="158" end_char="158">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="162" end_char="170">
<ORIGINAL_TEXT>It isn't.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="162" end_char="163">It</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="165" end_char="169">isn't</TOKEN>
<TOKEN id="token-3-2" pos="punct" morph="none" start_char="170" end_char="170">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="172" end_char="226">
<ORIGINAL_TEXT>That's just your INCORRECT evaluation of the situation.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="172" end_char="177">That's</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="179" end_char="182">just</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="184" end_char="187">your</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="189" end_char="197">INCORRECT</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="199" end_char="208">evaluation</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="210" end_char="211">of</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="213" end_char="215">the</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="217" end_char="225">situation</TOKEN>
<TOKEN id="token-4-8" pos="punct" morph="none" start_char="226" end_char="226">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="230" end_char="289">
<ORIGINAL_TEXT>It's not getting weaker, especially with the new variants...</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="230" end_char="233">It's</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="235" end_char="237">not</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="239" end_char="245">getting</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="247" end_char="252">weaker</TOKEN>
<TOKEN id="token-5-4" pos="punct" morph="none" start_char="253" end_char="253">,</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="255" end_char="264">especially</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="266" end_char="269">with</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="271" end_char="273">the</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="275" end_char="277">new</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="279" end_char="286">variants</TOKEN>
<TOKEN id="token-5-10" pos="punct" morph="none" start_char="287" end_char="289">...</TOKEN>
</SEG>
<SEG id="segment-6" start_char="293" end_char="317">
<ORIGINAL_TEXT>It is not getting weaker.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="293" end_char="294">It</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="296" end_char="297">is</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="299" end_char="301">not</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="303" end_char="309">getting</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="311" end_char="316">weaker</TOKEN>
<TOKEN id="token-6-5" pos="punct" morph="none" start_char="317" end_char="317">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="319" end_char="423">
<ORIGINAL_TEXT>There is vaccine, but the virus is still virulent and potentially deadly if you have not been vaccinated.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="319" end_char="323">There</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="325" end_char="326">is</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="328" end_char="334">vaccine</TOKEN>
<TOKEN id="token-7-3" pos="punct" morph="none" start_char="335" end_char="335">,</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="337" end_char="339">but</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="341" end_char="343">the</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="345" end_char="349">virus</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="351" end_char="352">is</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="354" end_char="358">still</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="360" end_char="367">virulent</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="369" end_char="371">and</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="373" end_char="383">potentially</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="385" end_char="390">deadly</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="392" end_char="393">if</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="395" end_char="397">you</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="399" end_char="402">have</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="404" end_char="406">not</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="408" end_char="411">been</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="413" end_char="422">vaccinated</TOKEN>
<TOKEN id="token-7-19" pos="punct" morph="none" start_char="423" end_char="423">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
