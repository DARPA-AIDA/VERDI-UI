<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATBR" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="393" raw_text_md5="8d280b207c3cd14afbc11d0cd0aba25d">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="69">
<ORIGINAL_TEXT>Estudios en China identifican al pangolín como origen del coronavirus</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="8">Estudios</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="10" end_char="11">en</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="13" end_char="17">China</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="19" end_char="29">identifican</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="31" end_char="32">al</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="34" end_char="41">pangolín</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="43" end_char="46">como</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="48" end_char="53">origen</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="55" end_char="57">del</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="59" end_char="69">coronavirus</TOKEN>
</SEG>
<SEG id="segment-1" start_char="73" end_char="83">
<ORIGINAL_TEXT>See… … see…</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="73" end_char="75">See</TOKEN>
<TOKEN id="token-1-1" pos="punct" morph="none" start_char="76" end_char="76">…</TOKEN>
<TOKEN id="token-1-2" pos="punct" morph="none" start_char="78" end_char="78">…</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="80" end_char="82">see</TOKEN>
<TOKEN id="token-1-4" pos="punct" morph="none" start_char="83" end_char="83">…</TOKEN>
</SEG>
<SEG id="segment-2" start_char="87" end_char="118">
<ORIGINAL_TEXT>Ni el que lo escribió se lo cree</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="87" end_char="88">Ni</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="90" end_char="91">el</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="93" end_char="95">que</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="97" end_char="98">lo</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="100" end_char="107">escribió</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="109" end_char="110">se</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="112" end_char="113">lo</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="115" end_char="118">cree</TOKEN>
</SEG>
<SEG id="segment-3" start_char="122" end_char="226">
<ORIGINAL_TEXT>https://www.globalresearch.ca/ten-questions-for-the-u-s-where-did-the-novel-coronavirus-come-from/5707035</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="url" morph="none" start_char="122" end_char="226">https://www.globalresearch.ca/ten-questions-for-the-u-s-where-did-the-novel-coronavirus-come-from/5707035</TOKEN>
</SEG>
<SEG id="segment-4" start_char="230" end_char="353">
<ORIGINAL_TEXT>Comparto su opinion plenamente, ese «pajarito» lo llevaron a China e Iran, se escapo de la jaula y no lo pudieron controlar!</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="230" end_char="237">Comparto</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="239" end_char="240">su</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="242" end_char="248">opinion</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="250" end_char="259">plenamente</TOKEN>
<TOKEN id="token-4-4" pos="punct" morph="none" start_char="260" end_char="260">,</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="262" end_char="264">ese</TOKEN>
<TOKEN id="token-4-6" pos="punct" morph="none" start_char="266" end_char="266">«</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="267" end_char="274">pajarito</TOKEN>
<TOKEN id="token-4-8" pos="punct" morph="none" start_char="275" end_char="275">»</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="277" end_char="278">lo</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="280" end_char="287">llevaron</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="289" end_char="289">a</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="291" end_char="295">China</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="297" end_char="297">e</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="299" end_char="302">Iran</TOKEN>
<TOKEN id="token-4-15" pos="punct" morph="none" start_char="303" end_char="303">,</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="305" end_char="306">se</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="308" end_char="313">escapo</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="315" end_char="316">de</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="318" end_char="319">la</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="321" end_char="325">jaula</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="327" end_char="327">y</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="329" end_char="330">no</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="332" end_char="333">lo</TOKEN>
<TOKEN id="token-4-24" pos="word" morph="none" start_char="335" end_char="342">pudieron</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="344" end_char="352">controlar</TOKEN>
<TOKEN id="token-4-26" pos="punct" morph="none" start_char="353" end_char="353">!</TOKEN>
</SEG>
<SEG id="segment-5" start_char="355" end_char="389">
<ORIGINAL_TEXT>Gracias por el enlace, interesante.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="355" end_char="361">Gracias</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="363" end_char="365">por</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="367" end_char="368">el</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="370" end_char="375">enlace</TOKEN>
<TOKEN id="token-5-4" pos="punct" morph="none" start_char="376" end_char="376">,</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="378" end_char="388">interesante</TOKEN>
<TOKEN id="token-5-6" pos="punct" morph="none" start_char="389" end_char="389">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
