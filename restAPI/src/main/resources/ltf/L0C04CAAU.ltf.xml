<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CAAU" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="5655" raw_text_md5="f750e0652f24fa7538dc186eb7cd0046">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="81">
<ORIGINAL_TEXT>Italian Study Doesn't Contradict Theory That COVID-19 Came From Wuhan: Researcher</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="7">Italian</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="9" end_char="13">Study</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="15" end_char="21">Doesn't</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="23" end_char="32">Contradict</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="34" end_char="39">Theory</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="41" end_char="44">That</TOKEN>
<TOKEN id="token-0-6" pos="unknown" morph="none" start_char="46" end_char="53">COVID-19</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="55" end_char="58">Came</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="60" end_char="63">From</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="65" end_char="69">Wuhan</TOKEN>
<TOKEN id="token-0-10" pos="punct" morph="none" start_char="70" end_char="70">:</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="72" end_char="81">Researcher</TOKEN>
</SEG>
<SEG id="segment-1" start_char="85" end_char="192">
<ORIGINAL_TEXT>A disembarking airline passenger undergoes a test for COVID-19 at the Fiumicino Airport in Rome, Italy, Dec.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="85" end_char="85">A</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="87" end_char="98">disembarking</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="100" end_char="106">airline</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="108" end_char="116">passenger</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="118" end_char="126">undergoes</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="128" end_char="128">a</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="130" end_char="133">test</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="135" end_char="137">for</TOKEN>
<TOKEN id="token-1-8" pos="unknown" morph="none" start_char="139" end_char="146">COVID-19</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="148" end_char="149">at</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="151" end_char="153">the</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="155" end_char="163">Fiumicino</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="165" end_char="171">Airport</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="173" end_char="174">in</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="176" end_char="179">Rome</TOKEN>
<TOKEN id="token-1-15" pos="punct" morph="none" start_char="180" end_char="180">,</TOKEN>
<TOKEN id="token-1-16" pos="word" morph="none" start_char="182" end_char="186">Italy</TOKEN>
<TOKEN id="token-1-17" pos="punct" morph="none" start_char="187" end_char="187">,</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="189" end_char="191">Dec</TOKEN>
<TOKEN id="token-1-19" pos="punct" morph="none" start_char="192" end_char="192">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="194" end_char="201">
<ORIGINAL_TEXT>9, 2020.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="194" end_char="194">9</TOKEN>
<TOKEN id="token-2-1" pos="punct" morph="none" start_char="195" end_char="195">,</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="197" end_char="200">2020</TOKEN>
<TOKEN id="token-2-3" pos="punct" morph="none" start_char="201" end_char="201">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="205" end_char="376">
<ORIGINAL_TEXT>An Italian medical researcher who reported finding signs of antibodies to COVID-19 in Italian patients before China confirmed its first official case of the disease on Dec.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="205" end_char="206">An</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="208" end_char="214">Italian</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="216" end_char="222">medical</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="224" end_char="233">researcher</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="235" end_char="237">who</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="239" end_char="246">reported</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="248" end_char="254">finding</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="256" end_char="260">signs</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="262" end_char="263">of</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="265" end_char="274">antibodies</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="276" end_char="277">to</TOKEN>
<TOKEN id="token-3-11" pos="unknown" morph="none" start_char="279" end_char="286">COVID-19</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="288" end_char="289">in</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="291" end_char="297">Italian</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="299" end_char="306">patients</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="308" end_char="313">before</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="315" end_char="319">China</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="321" end_char="329">confirmed</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="331" end_char="333">its</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="335" end_char="339">first</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="341" end_char="348">official</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="350" end_char="353">case</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="355" end_char="356">of</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="358" end_char="360">the</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="362" end_char="368">disease</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="370" end_char="371">on</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="373" end_char="375">Dec</TOKEN>
<TOKEN id="token-3-27" pos="punct" morph="none" start_char="376" end_char="376">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="378" end_char="474">
<ORIGINAL_TEXT>31 have said the findings don't undermine the prevailing view that the virus originated in Wuhan.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="378" end_char="379">31</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="381" end_char="384">have</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="386" end_char="389">said</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="391" end_char="393">the</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="395" end_char="402">findings</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="404" end_char="408">don't</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="410" end_char="418">undermine</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="420" end_char="422">the</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="424" end_char="433">prevailing</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="435" end_char="438">view</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="440" end_char="443">that</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="445" end_char="447">the</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="449" end_char="453">virus</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="455" end_char="464">originated</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="466" end_char="467">in</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="469" end_char="473">Wuhan</TOKEN>
<TOKEN id="token-4-16" pos="punct" morph="none" start_char="474" end_char="474">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="477" end_char="559">
<ORIGINAL_TEXT>Giovanni Apolone, lead author of a study published in the INT’s scientific magazine</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="477" end_char="484">Giovanni</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="486" end_char="492">Apolone</TOKEN>
<TOKEN id="token-5-2" pos="punct" morph="none" start_char="493" end_char="493">,</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="495" end_char="498">lead</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="500" end_char="505">author</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="507" end_char="508">of</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="510" end_char="510">a</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="512" end_char="516">study</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="518" end_char="526">published</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="528" end_char="529">in</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="531" end_char="533">the</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="535" end_char="539">INT’s</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="541" end_char="550">scientific</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="552" end_char="559">magazine</TOKEN>
</SEG>
<SEG id="segment-6" start_char="562" end_char="575">
<ORIGINAL_TEXT>Tumori Journal</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="562" end_char="567">Tumori</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="569" end_char="575">Journal</TOKEN>
</SEG>
<SEG id="segment-7" start_char="578" end_char="848">
<ORIGINAL_TEXT>, said the fact that antibodies to SARS-CoV-2, the virus that causes COVID-19, were circulating in the Italian population prior to the first confirmed Italian case in February didn't challenge the currently prevailing view that it originated in the central city of Wuhan.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="punct" morph="none" start_char="578" end_char="578">,</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="580" end_char="583">said</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="585" end_char="587">the</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="589" end_char="592">fact</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="594" end_char="597">that</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="599" end_char="608">antibodies</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="610" end_char="611">to</TOKEN>
<TOKEN id="token-7-7" pos="unknown" morph="none" start_char="613" end_char="622">SARS-CoV-2</TOKEN>
<TOKEN id="token-7-8" pos="punct" morph="none" start_char="623" end_char="623">,</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="625" end_char="627">the</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="629" end_char="633">virus</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="635" end_char="638">that</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="640" end_char="645">causes</TOKEN>
<TOKEN id="token-7-13" pos="unknown" morph="none" start_char="647" end_char="654">COVID-19</TOKEN>
<TOKEN id="token-7-14" pos="punct" morph="none" start_char="655" end_char="655">,</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="657" end_char="660">were</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="662" end_char="672">circulating</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="674" end_char="675">in</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="677" end_char="679">the</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="681" end_char="687">Italian</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="689" end_char="698">population</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="700" end_char="704">prior</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="706" end_char="707">to</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="709" end_char="711">the</TOKEN>
<TOKEN id="token-7-24" pos="word" morph="none" start_char="713" end_char="717">first</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="719" end_char="727">confirmed</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="729" end_char="735">Italian</TOKEN>
<TOKEN id="token-7-27" pos="word" morph="none" start_char="737" end_char="740">case</TOKEN>
<TOKEN id="token-7-28" pos="word" morph="none" start_char="742" end_char="743">in</TOKEN>
<TOKEN id="token-7-29" pos="word" morph="none" start_char="745" end_char="752">February</TOKEN>
<TOKEN id="token-7-30" pos="word" morph="none" start_char="754" end_char="759">didn't</TOKEN>
<TOKEN id="token-7-31" pos="word" morph="none" start_char="761" end_char="769">challenge</TOKEN>
<TOKEN id="token-7-32" pos="word" morph="none" start_char="771" end_char="773">the</TOKEN>
<TOKEN id="token-7-33" pos="word" morph="none" start_char="775" end_char="783">currently</TOKEN>
<TOKEN id="token-7-34" pos="word" morph="none" start_char="785" end_char="794">prevailing</TOKEN>
<TOKEN id="token-7-35" pos="word" morph="none" start_char="796" end_char="799">view</TOKEN>
<TOKEN id="token-7-36" pos="word" morph="none" start_char="801" end_char="804">that</TOKEN>
<TOKEN id="token-7-37" pos="word" morph="none" start_char="806" end_char="807">it</TOKEN>
<TOKEN id="token-7-38" pos="word" morph="none" start_char="809" end_char="818">originated</TOKEN>
<TOKEN id="token-7-39" pos="word" morph="none" start_char="820" end_char="821">in</TOKEN>
<TOKEN id="token-7-40" pos="word" morph="none" start_char="823" end_char="825">the</TOKEN>
<TOKEN id="token-7-41" pos="word" morph="none" start_char="827" end_char="833">central</TOKEN>
<TOKEN id="token-7-42" pos="word" morph="none" start_char="835" end_char="838">city</TOKEN>
<TOKEN id="token-7-43" pos="word" morph="none" start_char="840" end_char="841">of</TOKEN>
<TOKEN id="token-7-44" pos="word" morph="none" start_char="843" end_char="847">Wuhan</TOKEN>
<TOKEN id="token-7-45" pos="punct" morph="none" start_char="848" end_char="848">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="851" end_char="1128">
<ORIGINAL_TEXT>The Chinese Communist Party (CCP) propaganda machine has kicked into high gear on the subject of the coronavirus' origins in recent weeks, saying the virus could have come to China before tearing through the population of Wuhan and evolving into a global public health disaster.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="851" end_char="853">The</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="855" end_char="861">Chinese</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="863" end_char="871">Communist</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="873" end_char="877">Party</TOKEN>
<TOKEN id="token-8-4" pos="punct" morph="none" start_char="879" end_char="879">(</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="880" end_char="882">CCP</TOKEN>
<TOKEN id="token-8-6" pos="punct" morph="none" start_char="883" end_char="883">)</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="885" end_char="894">propaganda</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="896" end_char="902">machine</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="904" end_char="906">has</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="908" end_char="913">kicked</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="915" end_char="918">into</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="920" end_char="923">high</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="925" end_char="928">gear</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="930" end_char="931">on</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="933" end_char="935">the</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="937" end_char="943">subject</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="945" end_char="946">of</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="948" end_char="950">the</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="952" end_char="962">coronavirus</TOKEN>
<TOKEN id="token-8-20" pos="punct" morph="none" start_char="963" end_char="963">'</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="965" end_char="971">origins</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="973" end_char="974">in</TOKEN>
<TOKEN id="token-8-23" pos="word" morph="none" start_char="976" end_char="981">recent</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="983" end_char="987">weeks</TOKEN>
<TOKEN id="token-8-25" pos="punct" morph="none" start_char="988" end_char="988">,</TOKEN>
<TOKEN id="token-8-26" pos="word" morph="none" start_char="990" end_char="995">saying</TOKEN>
<TOKEN id="token-8-27" pos="word" morph="none" start_char="997" end_char="999">the</TOKEN>
<TOKEN id="token-8-28" pos="word" morph="none" start_char="1001" end_char="1005">virus</TOKEN>
<TOKEN id="token-8-29" pos="word" morph="none" start_char="1007" end_char="1011">could</TOKEN>
<TOKEN id="token-8-30" pos="word" morph="none" start_char="1013" end_char="1016">have</TOKEN>
<TOKEN id="token-8-31" pos="word" morph="none" start_char="1018" end_char="1021">come</TOKEN>
<TOKEN id="token-8-32" pos="word" morph="none" start_char="1023" end_char="1024">to</TOKEN>
<TOKEN id="token-8-33" pos="word" morph="none" start_char="1026" end_char="1030">China</TOKEN>
<TOKEN id="token-8-34" pos="word" morph="none" start_char="1032" end_char="1037">before</TOKEN>
<TOKEN id="token-8-35" pos="word" morph="none" start_char="1039" end_char="1045">tearing</TOKEN>
<TOKEN id="token-8-36" pos="word" morph="none" start_char="1047" end_char="1053">through</TOKEN>
<TOKEN id="token-8-37" pos="word" morph="none" start_char="1055" end_char="1057">the</TOKEN>
<TOKEN id="token-8-38" pos="word" morph="none" start_char="1059" end_char="1068">population</TOKEN>
<TOKEN id="token-8-39" pos="word" morph="none" start_char="1070" end_char="1071">of</TOKEN>
<TOKEN id="token-8-40" pos="word" morph="none" start_char="1073" end_char="1077">Wuhan</TOKEN>
<TOKEN id="token-8-41" pos="word" morph="none" start_char="1079" end_char="1081">and</TOKEN>
<TOKEN id="token-8-42" pos="word" morph="none" start_char="1083" end_char="1090">evolving</TOKEN>
<TOKEN id="token-8-43" pos="word" morph="none" start_char="1092" end_char="1095">into</TOKEN>
<TOKEN id="token-8-44" pos="word" morph="none" start_char="1097" end_char="1097">a</TOKEN>
<TOKEN id="token-8-45" pos="word" morph="none" start_char="1099" end_char="1104">global</TOKEN>
<TOKEN id="token-8-46" pos="word" morph="none" start_char="1106" end_char="1111">public</TOKEN>
<TOKEN id="token-8-47" pos="word" morph="none" start_char="1113" end_char="1118">health</TOKEN>
<TOKEN id="token-8-48" pos="word" morph="none" start_char="1120" end_char="1127">disaster</TOKEN>
<TOKEN id="token-8-49" pos="punct" morph="none" start_char="1128" end_char="1128">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1131" end_char="1307">
<ORIGINAL_TEXT>SARS-CoV-2 is believed to have originated in bats and mutated via a second animal host -- possibly pangolins -- into a form capable of infecting humans, according to a report in</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="unknown" morph="none" start_char="1131" end_char="1140">SARS-CoV-2</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1142" end_char="1143">is</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="1145" end_char="1152">believed</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1154" end_char="1155">to</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1157" end_char="1160">have</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1162" end_char="1171">originated</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1173" end_char="1174">in</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1176" end_char="1179">bats</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1181" end_char="1183">and</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="1185" end_char="1191">mutated</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1193" end_char="1195">via</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1197" end_char="1197">a</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="1199" end_char="1204">second</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1206" end_char="1211">animal</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1213" end_char="1216">host</TOKEN>
<TOKEN id="token-9-15" pos="punct" morph="none" start_char="1218" end_char="1219">--</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="1221" end_char="1228">possibly</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1230" end_char="1238">pangolins</TOKEN>
<TOKEN id="token-9-18" pos="punct" morph="none" start_char="1240" end_char="1241">--</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1243" end_char="1246">into</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1248" end_char="1248">a</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1250" end_char="1253">form</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1255" end_char="1261">capable</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="1263" end_char="1264">of</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1266" end_char="1274">infecting</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="1276" end_char="1281">humans</TOKEN>
<TOKEN id="token-9-26" pos="punct" morph="none" start_char="1282" end_char="1282">,</TOKEN>
<TOKEN id="token-9-27" pos="word" morph="none" start_char="1284" end_char="1292">according</TOKEN>
<TOKEN id="token-9-28" pos="word" morph="none" start_char="1294" end_char="1295">to</TOKEN>
<TOKEN id="token-9-29" pos="word" morph="none" start_char="1297" end_char="1297">a</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1299" end_char="1304">report</TOKEN>
<TOKEN id="token-9-31" pos="word" morph="none" start_char="1306" end_char="1307">in</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1310" end_char="1319">
<ORIGINAL_TEXT>The Lancet</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1310" end_char="1312">The</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1314" end_char="1319">Lancet</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1322" end_char="1362">
<ORIGINAL_TEXT>medical journal's September 2020 edition.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1322" end_char="1328">medical</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1330" end_char="1338">journal's</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1340" end_char="1348">September</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1350" end_char="1353">2020</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1355" end_char="1361">edition</TOKEN>
<TOKEN id="token-11-5" pos="punct" morph="none" start_char="1362" end_char="1362">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1365" end_char="1576">
<ORIGINAL_TEXT>"Everything points to a bat sarbecovirus reservoir; we are very confident about this," David Robertson, head of bioinformatics at the Medical Research Council–University of Glasgow Centre for Virus Research, told</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="punct" morph="none" start_char="1365" end_char="1365">"</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1366" end_char="1375">Everything</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1377" end_char="1382">points</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1384" end_char="1385">to</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1387" end_char="1387">a</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1389" end_char="1391">bat</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1393" end_char="1404">sarbecovirus</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1406" end_char="1414">reservoir</TOKEN>
<TOKEN id="token-12-8" pos="punct" morph="none" start_char="1415" end_char="1415">;</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1417" end_char="1418">we</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1420" end_char="1422">are</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1424" end_char="1427">very</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1429" end_char="1437">confident</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1439" end_char="1443">about</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1445" end_char="1448">this</TOKEN>
<TOKEN id="token-12-15" pos="punct" morph="none" start_char="1449" end_char="1450">,"</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1452" end_char="1456">David</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1458" end_char="1466">Robertson</TOKEN>
<TOKEN id="token-12-18" pos="punct" morph="none" start_char="1467" end_char="1467">,</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="1469" end_char="1472">head</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1474" end_char="1475">of</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1477" end_char="1490">bioinformatics</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1492" end_char="1493">at</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1495" end_char="1497">the</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1499" end_char="1505">Medical</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1507" end_char="1514">Research</TOKEN>
<TOKEN id="token-12-26" pos="unknown" morph="none" start_char="1516" end_char="1533">Council–University</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="1535" end_char="1536">of</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="1538" end_char="1544">Glasgow</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="1546" end_char="1551">Centre</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="1553" end_char="1555">for</TOKEN>
<TOKEN id="token-12-31" pos="word" morph="none" start_char="1557" end_char="1561">Virus</TOKEN>
<TOKEN id="token-12-32" pos="word" morph="none" start_char="1563" end_char="1570">Research</TOKEN>
<TOKEN id="token-12-33" pos="punct" morph="none" start_char="1571" end_char="1571">,</TOKEN>
<TOKEN id="token-12-34" pos="word" morph="none" start_char="1573" end_char="1576">told</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1579" end_char="1588">
<ORIGINAL_TEXT>The Lancet</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1579" end_char="1581">The</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1583" end_char="1588">Lancet</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1591" end_char="1591">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="punct" morph="none" start_char="1591" end_char="1591">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1594" end_char="1850">
<ORIGINAL_TEXT>"The virus would not need to evolve in the pangolin, it would just need to be brought into contact with humans," Robertson said of the theory that pangolins, banned but still traded illegally in huge numbers in China, were an intermediate host for COVID-19.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="punct" morph="none" start_char="1594" end_char="1594">"</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1595" end_char="1597">The</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1599" end_char="1603">virus</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="1605" end_char="1609">would</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1611" end_char="1613">not</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1615" end_char="1618">need</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="1620" end_char="1621">to</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1623" end_char="1628">evolve</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1630" end_char="1631">in</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="1633" end_char="1635">the</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1637" end_char="1644">pangolin</TOKEN>
<TOKEN id="token-15-11" pos="punct" morph="none" start_char="1645" end_char="1645">,</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="1647" end_char="1648">it</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="1650" end_char="1654">would</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="1656" end_char="1659">just</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="1661" end_char="1664">need</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="1666" end_char="1667">to</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="1669" end_char="1670">be</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="1672" end_char="1678">brought</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="1680" end_char="1683">into</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="1685" end_char="1691">contact</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="1693" end_char="1696">with</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="1698" end_char="1703">humans</TOKEN>
<TOKEN id="token-15-23" pos="punct" morph="none" start_char="1704" end_char="1705">,"</TOKEN>
<TOKEN id="token-15-24" pos="word" morph="none" start_char="1707" end_char="1715">Robertson</TOKEN>
<TOKEN id="token-15-25" pos="word" morph="none" start_char="1717" end_char="1720">said</TOKEN>
<TOKEN id="token-15-26" pos="word" morph="none" start_char="1722" end_char="1723">of</TOKEN>
<TOKEN id="token-15-27" pos="word" morph="none" start_char="1725" end_char="1727">the</TOKEN>
<TOKEN id="token-15-28" pos="word" morph="none" start_char="1729" end_char="1734">theory</TOKEN>
<TOKEN id="token-15-29" pos="word" morph="none" start_char="1736" end_char="1739">that</TOKEN>
<TOKEN id="token-15-30" pos="word" morph="none" start_char="1741" end_char="1749">pangolins</TOKEN>
<TOKEN id="token-15-31" pos="punct" morph="none" start_char="1750" end_char="1750">,</TOKEN>
<TOKEN id="token-15-32" pos="word" morph="none" start_char="1752" end_char="1757">banned</TOKEN>
<TOKEN id="token-15-33" pos="word" morph="none" start_char="1759" end_char="1761">but</TOKEN>
<TOKEN id="token-15-34" pos="word" morph="none" start_char="1763" end_char="1767">still</TOKEN>
<TOKEN id="token-15-35" pos="word" morph="none" start_char="1769" end_char="1774">traded</TOKEN>
<TOKEN id="token-15-36" pos="word" morph="none" start_char="1776" end_char="1784">illegally</TOKEN>
<TOKEN id="token-15-37" pos="word" morph="none" start_char="1786" end_char="1787">in</TOKEN>
<TOKEN id="token-15-38" pos="word" morph="none" start_char="1789" end_char="1792">huge</TOKEN>
<TOKEN id="token-15-39" pos="word" morph="none" start_char="1794" end_char="1800">numbers</TOKEN>
<TOKEN id="token-15-40" pos="word" morph="none" start_char="1802" end_char="1803">in</TOKEN>
<TOKEN id="token-15-41" pos="word" morph="none" start_char="1805" end_char="1809">China</TOKEN>
<TOKEN id="token-15-42" pos="punct" morph="none" start_char="1810" end_char="1810">,</TOKEN>
<TOKEN id="token-15-43" pos="word" morph="none" start_char="1812" end_char="1815">were</TOKEN>
<TOKEN id="token-15-44" pos="word" morph="none" start_char="1817" end_char="1818">an</TOKEN>
<TOKEN id="token-15-45" pos="word" morph="none" start_char="1820" end_char="1831">intermediate</TOKEN>
<TOKEN id="token-15-46" pos="word" morph="none" start_char="1833" end_char="1836">host</TOKEN>
<TOKEN id="token-15-47" pos="word" morph="none" start_char="1838" end_char="1840">for</TOKEN>
<TOKEN id="token-15-48" pos="unknown" morph="none" start_char="1842" end_char="1849">COVID-19</TOKEN>
<TOKEN id="token-15-49" pos="punct" morph="none" start_char="1850" end_char="1850">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1853" end_char="2099">
<ORIGINAL_TEXT>Since these reports began appearing, Chinese state media have published a number of stories and comments claiming that while the pandemic originated in Wuhan, it didn't necessarily make its first jump to humans there, and could have been imported.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1853" end_char="1857">Since</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1859" end_char="1863">these</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1865" end_char="1871">reports</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1873" end_char="1877">began</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1879" end_char="1887">appearing</TOKEN>
<TOKEN id="token-16-5" pos="punct" morph="none" start_char="1888" end_char="1888">,</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="1890" end_char="1896">Chinese</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="1898" end_char="1902">state</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="1904" end_char="1908">media</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="1910" end_char="1913">have</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="1915" end_char="1923">published</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="1925" end_char="1925">a</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="1927" end_char="1932">number</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="1934" end_char="1935">of</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="1937" end_char="1943">stories</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="1945" end_char="1947">and</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="1949" end_char="1956">comments</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="1958" end_char="1965">claiming</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="1967" end_char="1970">that</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="1972" end_char="1976">while</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="1978" end_char="1980">the</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="1982" end_char="1989">pandemic</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="1991" end_char="2000">originated</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="2002" end_char="2003">in</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="2005" end_char="2009">Wuhan</TOKEN>
<TOKEN id="token-16-25" pos="punct" morph="none" start_char="2010" end_char="2010">,</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="2012" end_char="2013">it</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="2015" end_char="2020">didn't</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="2022" end_char="2032">necessarily</TOKEN>
<TOKEN id="token-16-29" pos="word" morph="none" start_char="2034" end_char="2037">make</TOKEN>
<TOKEN id="token-16-30" pos="word" morph="none" start_char="2039" end_char="2041">its</TOKEN>
<TOKEN id="token-16-31" pos="word" morph="none" start_char="2043" end_char="2047">first</TOKEN>
<TOKEN id="token-16-32" pos="word" morph="none" start_char="2049" end_char="2052">jump</TOKEN>
<TOKEN id="token-16-33" pos="word" morph="none" start_char="2054" end_char="2055">to</TOKEN>
<TOKEN id="token-16-34" pos="word" morph="none" start_char="2057" end_char="2062">humans</TOKEN>
<TOKEN id="token-16-35" pos="word" morph="none" start_char="2064" end_char="2068">there</TOKEN>
<TOKEN id="token-16-36" pos="punct" morph="none" start_char="2069" end_char="2069">,</TOKEN>
<TOKEN id="token-16-37" pos="word" morph="none" start_char="2071" end_char="2073">and</TOKEN>
<TOKEN id="token-16-38" pos="word" morph="none" start_char="2075" end_char="2079">could</TOKEN>
<TOKEN id="token-16-39" pos="word" morph="none" start_char="2081" end_char="2084">have</TOKEN>
<TOKEN id="token-16-40" pos="word" morph="none" start_char="2086" end_char="2089">been</TOKEN>
<TOKEN id="token-16-41" pos="word" morph="none" start_char="2091" end_char="2098">imported</TOKEN>
<TOKEN id="token-16-42" pos="punct" morph="none" start_char="2099" end_char="2099">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2102" end_char="2289">
<ORIGINAL_TEXT>Apolone said the detection of SARS-CoV-2 antibodies in Italian study participants who may have been infected as early as September 2020 didn't necessarily support Beijing's claim, however.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2102" end_char="2108">Apolone</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2110" end_char="2113">said</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2115" end_char="2117">the</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2119" end_char="2127">detection</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2129" end_char="2130">of</TOKEN>
<TOKEN id="token-17-5" pos="unknown" morph="none" start_char="2132" end_char="2141">SARS-CoV-2</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2143" end_char="2152">antibodies</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="2154" end_char="2155">in</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="2157" end_char="2163">Italian</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="2165" end_char="2169">study</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="2171" end_char="2182">participants</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="2184" end_char="2186">who</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="2188" end_char="2190">may</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="2192" end_char="2195">have</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="2197" end_char="2200">been</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="2202" end_char="2209">infected</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="2211" end_char="2212">as</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="2214" end_char="2218">early</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="2220" end_char="2221">as</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="2223" end_char="2231">September</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="2233" end_char="2236">2020</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="2238" end_char="2243">didn't</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="2245" end_char="2255">necessarily</TOKEN>
<TOKEN id="token-17-23" pos="word" morph="none" start_char="2257" end_char="2263">support</TOKEN>
<TOKEN id="token-17-24" pos="word" morph="none" start_char="2265" end_char="2273">Beijing's</TOKEN>
<TOKEN id="token-17-25" pos="word" morph="none" start_char="2275" end_char="2279">claim</TOKEN>
<TOKEN id="token-17-26" pos="punct" morph="none" start_char="2280" end_char="2280">,</TOKEN>
<TOKEN id="token-17-27" pos="word" morph="none" start_char="2282" end_char="2288">however</TOKEN>
<TOKEN id="token-17-28" pos="punct" morph="none" start_char="2289" end_char="2289">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2292" end_char="2424">
<ORIGINAL_TEXT>"All the findings so far suggest that the origin of the virus and of the pandemic is from Wuhan," he said in comments emailed to RFA.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="punct" morph="none" start_char="2292" end_char="2292">"</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2293" end_char="2295">All</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="2297" end_char="2299">the</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="2301" end_char="2308">findings</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2310" end_char="2311">so</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="2313" end_char="2315">far</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2317" end_char="2323">suggest</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2325" end_char="2328">that</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2330" end_char="2332">the</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2334" end_char="2339">origin</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2341" end_char="2342">of</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="2344" end_char="2346">the</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="2348" end_char="2352">virus</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="2354" end_char="2356">and</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="2358" end_char="2359">of</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="2361" end_char="2363">the</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="2365" end_char="2372">pandemic</TOKEN>
<TOKEN id="token-18-17" pos="word" morph="none" start_char="2374" end_char="2375">is</TOKEN>
<TOKEN id="token-18-18" pos="word" morph="none" start_char="2377" end_char="2380">from</TOKEN>
<TOKEN id="token-18-19" pos="word" morph="none" start_char="2382" end_char="2386">Wuhan</TOKEN>
<TOKEN id="token-18-20" pos="punct" morph="none" start_char="2387" end_char="2388">,"</TOKEN>
<TOKEN id="token-18-21" pos="word" morph="none" start_char="2390" end_char="2391">he</TOKEN>
<TOKEN id="token-18-22" pos="word" morph="none" start_char="2393" end_char="2396">said</TOKEN>
<TOKEN id="token-18-23" pos="word" morph="none" start_char="2398" end_char="2399">in</TOKEN>
<TOKEN id="token-18-24" pos="word" morph="none" start_char="2401" end_char="2408">comments</TOKEN>
<TOKEN id="token-18-25" pos="word" morph="none" start_char="2410" end_char="2416">emailed</TOKEN>
<TOKEN id="token-18-26" pos="word" morph="none" start_char="2418" end_char="2419">to</TOKEN>
<TOKEN id="token-18-27" pos="word" morph="none" start_char="2421" end_char="2423">RFA</TOKEN>
<TOKEN id="token-18-28" pos="punct" morph="none" start_char="2424" end_char="2424">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2427" end_char="2687">
<ORIGINAL_TEXT>He said other recent papers, including one based on data from the U.S. Centers for Disease Control and Prevention (CDC), had shown that the virus was "present and circulating" in France and the United States before the first official Chinese case was confirmed.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2427" end_char="2428">He</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2430" end_char="2433">said</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2435" end_char="2439">other</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="2441" end_char="2446">recent</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2448" end_char="2453">papers</TOKEN>
<TOKEN id="token-19-5" pos="punct" morph="none" start_char="2454" end_char="2454">,</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2456" end_char="2464">including</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="2466" end_char="2468">one</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="2470" end_char="2474">based</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="2476" end_char="2477">on</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="2479" end_char="2482">data</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="2484" end_char="2487">from</TOKEN>
<TOKEN id="token-19-12" pos="word" morph="none" start_char="2489" end_char="2491">the</TOKEN>
<TOKEN id="token-19-13" pos="unknown" morph="none" start_char="2493" end_char="2495">U.S</TOKEN>
<TOKEN id="token-19-14" pos="punct" morph="none" start_char="2496" end_char="2496">.</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="2498" end_char="2504">Centers</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="2506" end_char="2508">for</TOKEN>
<TOKEN id="token-19-17" pos="word" morph="none" start_char="2510" end_char="2516">Disease</TOKEN>
<TOKEN id="token-19-18" pos="word" morph="none" start_char="2518" end_char="2524">Control</TOKEN>
<TOKEN id="token-19-19" pos="word" morph="none" start_char="2526" end_char="2528">and</TOKEN>
<TOKEN id="token-19-20" pos="word" morph="none" start_char="2530" end_char="2539">Prevention</TOKEN>
<TOKEN id="token-19-21" pos="punct" morph="none" start_char="2541" end_char="2541">(</TOKEN>
<TOKEN id="token-19-22" pos="word" morph="none" start_char="2542" end_char="2544">CDC</TOKEN>
<TOKEN id="token-19-23" pos="punct" morph="none" start_char="2545" end_char="2546">),</TOKEN>
<TOKEN id="token-19-24" pos="word" morph="none" start_char="2548" end_char="2550">had</TOKEN>
<TOKEN id="token-19-25" pos="word" morph="none" start_char="2552" end_char="2556">shown</TOKEN>
<TOKEN id="token-19-26" pos="word" morph="none" start_char="2558" end_char="2561">that</TOKEN>
<TOKEN id="token-19-27" pos="word" morph="none" start_char="2563" end_char="2565">the</TOKEN>
<TOKEN id="token-19-28" pos="word" morph="none" start_char="2567" end_char="2571">virus</TOKEN>
<TOKEN id="token-19-29" pos="word" morph="none" start_char="2573" end_char="2575">was</TOKEN>
<TOKEN id="token-19-30" pos="punct" morph="none" start_char="2577" end_char="2577">"</TOKEN>
<TOKEN id="token-19-31" pos="word" morph="none" start_char="2578" end_char="2584">present</TOKEN>
<TOKEN id="token-19-32" pos="word" morph="none" start_char="2586" end_char="2588">and</TOKEN>
<TOKEN id="token-19-33" pos="word" morph="none" start_char="2590" end_char="2600">circulating</TOKEN>
<TOKEN id="token-19-34" pos="punct" morph="none" start_char="2601" end_char="2601">"</TOKEN>
<TOKEN id="token-19-35" pos="word" morph="none" start_char="2603" end_char="2604">in</TOKEN>
<TOKEN id="token-19-36" pos="word" morph="none" start_char="2606" end_char="2611">France</TOKEN>
<TOKEN id="token-19-37" pos="word" morph="none" start_char="2613" end_char="2615">and</TOKEN>
<TOKEN id="token-19-38" pos="word" morph="none" start_char="2617" end_char="2619">the</TOKEN>
<TOKEN id="token-19-39" pos="word" morph="none" start_char="2621" end_char="2626">United</TOKEN>
<TOKEN id="token-19-40" pos="word" morph="none" start_char="2628" end_char="2633">States</TOKEN>
<TOKEN id="token-19-41" pos="word" morph="none" start_char="2635" end_char="2640">before</TOKEN>
<TOKEN id="token-19-42" pos="word" morph="none" start_char="2642" end_char="2644">the</TOKEN>
<TOKEN id="token-19-43" pos="word" morph="none" start_char="2646" end_char="2650">first</TOKEN>
<TOKEN id="token-19-44" pos="word" morph="none" start_char="2652" end_char="2659">official</TOKEN>
<TOKEN id="token-19-45" pos="word" morph="none" start_char="2661" end_char="2667">Chinese</TOKEN>
<TOKEN id="token-19-46" pos="word" morph="none" start_char="2669" end_char="2672">case</TOKEN>
<TOKEN id="token-19-47" pos="word" morph="none" start_char="2674" end_char="2676">was</TOKEN>
<TOKEN id="token-19-48" pos="word" morph="none" start_char="2678" end_char="2686">confirmed</TOKEN>
<TOKEN id="token-19-49" pos="punct" morph="none" start_char="2687" end_char="2687">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2690" end_char="2987">
<ORIGINAL_TEXT>"The fact that the virus was present and circulating in Italy, France, and USA ...much before the official communication by China ... is likely due to two factors: the lack of identification of the problem during the first phase and/or a delay of communication for political reasons," Apolone said.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="punct" morph="none" start_char="2690" end_char="2690">"</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="2691" end_char="2693">The</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="2695" end_char="2698">fact</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="2700" end_char="2703">that</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2705" end_char="2707">the</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2709" end_char="2713">virus</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2715" end_char="2717">was</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="2719" end_char="2725">present</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="2727" end_char="2729">and</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="2731" end_char="2741">circulating</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="2743" end_char="2744">in</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="2746" end_char="2750">Italy</TOKEN>
<TOKEN id="token-20-12" pos="punct" morph="none" start_char="2751" end_char="2751">,</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="2753" end_char="2758">France</TOKEN>
<TOKEN id="token-20-14" pos="punct" morph="none" start_char="2759" end_char="2759">,</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="2761" end_char="2763">and</TOKEN>
<TOKEN id="token-20-16" pos="word" morph="none" start_char="2765" end_char="2767">USA</TOKEN>
<TOKEN id="token-20-17" pos="punct" morph="none" start_char="2769" end_char="2771">...</TOKEN>
<TOKEN id="token-20-18" pos="word" morph="none" start_char="2772" end_char="2775">much</TOKEN>
<TOKEN id="token-20-19" pos="word" morph="none" start_char="2777" end_char="2782">before</TOKEN>
<TOKEN id="token-20-20" pos="word" morph="none" start_char="2784" end_char="2786">the</TOKEN>
<TOKEN id="token-20-21" pos="word" morph="none" start_char="2788" end_char="2795">official</TOKEN>
<TOKEN id="token-20-22" pos="word" morph="none" start_char="2797" end_char="2809">communication</TOKEN>
<TOKEN id="token-20-23" pos="word" morph="none" start_char="2811" end_char="2812">by</TOKEN>
<TOKEN id="token-20-24" pos="word" morph="none" start_char="2814" end_char="2818">China</TOKEN>
<TOKEN id="token-20-25" pos="punct" morph="none" start_char="2820" end_char="2822">...</TOKEN>
<TOKEN id="token-20-26" pos="word" morph="none" start_char="2824" end_char="2825">is</TOKEN>
<TOKEN id="token-20-27" pos="word" morph="none" start_char="2827" end_char="2832">likely</TOKEN>
<TOKEN id="token-20-28" pos="word" morph="none" start_char="2834" end_char="2836">due</TOKEN>
<TOKEN id="token-20-29" pos="word" morph="none" start_char="2838" end_char="2839">to</TOKEN>
<TOKEN id="token-20-30" pos="word" morph="none" start_char="2841" end_char="2843">two</TOKEN>
<TOKEN id="token-20-31" pos="word" morph="none" start_char="2845" end_char="2851">factors</TOKEN>
<TOKEN id="token-20-32" pos="punct" morph="none" start_char="2852" end_char="2852">:</TOKEN>
<TOKEN id="token-20-33" pos="word" morph="none" start_char="2854" end_char="2856">the</TOKEN>
<TOKEN id="token-20-34" pos="word" morph="none" start_char="2858" end_char="2861">lack</TOKEN>
<TOKEN id="token-20-35" pos="word" morph="none" start_char="2863" end_char="2864">of</TOKEN>
<TOKEN id="token-20-36" pos="word" morph="none" start_char="2866" end_char="2879">identification</TOKEN>
<TOKEN id="token-20-37" pos="word" morph="none" start_char="2881" end_char="2882">of</TOKEN>
<TOKEN id="token-20-38" pos="word" morph="none" start_char="2884" end_char="2886">the</TOKEN>
<TOKEN id="token-20-39" pos="word" morph="none" start_char="2888" end_char="2894">problem</TOKEN>
<TOKEN id="token-20-40" pos="word" morph="none" start_char="2896" end_char="2901">during</TOKEN>
<TOKEN id="token-20-41" pos="word" morph="none" start_char="2903" end_char="2905">the</TOKEN>
<TOKEN id="token-20-42" pos="word" morph="none" start_char="2907" end_char="2911">first</TOKEN>
<TOKEN id="token-20-43" pos="word" morph="none" start_char="2913" end_char="2917">phase</TOKEN>
<TOKEN id="token-20-44" pos="unknown" morph="none" start_char="2919" end_char="2924">and/or</TOKEN>
<TOKEN id="token-20-45" pos="word" morph="none" start_char="2926" end_char="2926">a</TOKEN>
<TOKEN id="token-20-46" pos="word" morph="none" start_char="2928" end_char="2932">delay</TOKEN>
<TOKEN id="token-20-47" pos="word" morph="none" start_char="2934" end_char="2935">of</TOKEN>
<TOKEN id="token-20-48" pos="word" morph="none" start_char="2937" end_char="2949">communication</TOKEN>
<TOKEN id="token-20-49" pos="word" morph="none" start_char="2951" end_char="2953">for</TOKEN>
<TOKEN id="token-20-50" pos="word" morph="none" start_char="2955" end_char="2963">political</TOKEN>
<TOKEN id="token-20-51" pos="word" morph="none" start_char="2965" end_char="2971">reasons</TOKEN>
<TOKEN id="token-20-52" pos="punct" morph="none" start_char="2972" end_char="2973">,"</TOKEN>
<TOKEN id="token-20-53" pos="word" morph="none" start_char="2975" end_char="2981">Apolone</TOKEN>
<TOKEN id="token-20-54" pos="word" morph="none" start_char="2983" end_char="2986">said</TOKEN>
<TOKEN id="token-20-55" pos="punct" morph="none" start_char="2987" end_char="2987">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="2990" end_char="3113">
<ORIGINAL_TEXT>The Italian study, the CDC study, and a French study based on the retesting of old pneumonia samples were all used in a Dec.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="2990" end_char="2992">The</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="2994" end_char="3000">Italian</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="3002" end_char="3006">study</TOKEN>
<TOKEN id="token-21-3" pos="punct" morph="none" start_char="3007" end_char="3007">,</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="3009" end_char="3011">the</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="3013" end_char="3015">CDC</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="3017" end_char="3021">study</TOKEN>
<TOKEN id="token-21-7" pos="punct" morph="none" start_char="3022" end_char="3022">,</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="3024" end_char="3026">and</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="3028" end_char="3028">a</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="3030" end_char="3035">French</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="3037" end_char="3041">study</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="3043" end_char="3047">based</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="3049" end_char="3050">on</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="3052" end_char="3054">the</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="3056" end_char="3064">retesting</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="3066" end_char="3067">of</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="3069" end_char="3071">old</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="3073" end_char="3081">pneumonia</TOKEN>
<TOKEN id="token-21-19" pos="word" morph="none" start_char="3083" end_char="3089">samples</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="3091" end_char="3094">were</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="3096" end_char="3098">all</TOKEN>
<TOKEN id="token-21-22" pos="word" morph="none" start_char="3100" end_char="3103">used</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="3105" end_char="3106">in</TOKEN>
<TOKEN id="token-21-24" pos="word" morph="none" start_char="3108" end_char="3108">a</TOKEN>
<TOKEN id="token-21-25" pos="word" morph="none" start_char="3110" end_char="3112">Dec</TOKEN>
<TOKEN id="token-21-26" pos="punct" morph="none" start_char="3113" end_char="3113">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="3115" end_char="3234">
<ORIGINAL_TEXT>2 Facebook video published by China's state news agency Xinhua to support claims of a non-Chinese origin for SARS-CoV-2.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="3115" end_char="3115">2</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="3117" end_char="3124">Facebook</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="3126" end_char="3130">video</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="3132" end_char="3140">published</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="3142" end_char="3143">by</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="3145" end_char="3151">China's</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="3153" end_char="3157">state</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="3159" end_char="3162">news</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="3164" end_char="3169">agency</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="3171" end_char="3176">Xinhua</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="3178" end_char="3179">to</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="3181" end_char="3187">support</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="3189" end_char="3194">claims</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="3196" end_char="3197">of</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="3199" end_char="3199">a</TOKEN>
<TOKEN id="token-22-15" pos="unknown" morph="none" start_char="3201" end_char="3211">non-Chinese</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="3213" end_char="3218">origin</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="3220" end_char="3222">for</TOKEN>
<TOKEN id="token-22-18" pos="unknown" morph="none" start_char="3224" end_char="3233">SARS-CoV-2</TOKEN>
<TOKEN id="token-22-19" pos="punct" morph="none" start_char="3234" end_char="3234">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="3237" end_char="3263">
<ORIGINAL_TEXT>Interview censored by China</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="3237" end_char="3245">Interview</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="3247" end_char="3254">censored</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="3256" end_char="3257">by</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="3259" end_char="3263">China</TOKEN>
</SEG>
<SEG id="segment-24" start_char="3266" end_char="3372">
<ORIGINAL_TEXT>Apolone said he had been interviewed by Chinese state media, but his comments hadn't been reported in full.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="3266" end_char="3272">Apolone</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="3274" end_char="3277">said</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="3279" end_char="3280">he</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="3282" end_char="3284">had</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="3286" end_char="3289">been</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="3291" end_char="3301">interviewed</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="3303" end_char="3304">by</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="3306" end_char="3312">Chinese</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="3314" end_char="3318">state</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="3320" end_char="3324">media</TOKEN>
<TOKEN id="token-24-10" pos="punct" morph="none" start_char="3325" end_char="3325">,</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="3327" end_char="3329">but</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="3331" end_char="3333">his</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="3335" end_char="3342">comments</TOKEN>
<TOKEN id="token-24-14" pos="word" morph="none" start_char="3344" end_char="3349">hadn't</TOKEN>
<TOKEN id="token-24-15" pos="word" morph="none" start_char="3351" end_char="3354">been</TOKEN>
<TOKEN id="token-24-16" pos="word" morph="none" start_char="3356" end_char="3363">reported</TOKEN>
<TOKEN id="token-24-17" pos="word" morph="none" start_char="3365" end_char="3366">in</TOKEN>
<TOKEN id="token-24-18" pos="word" morph="none" start_char="3368" end_char="3371">full</TOKEN>
<TOKEN id="token-24-19" pos="punct" morph="none" start_char="3372" end_char="3372">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="3375" end_char="3504">
<ORIGINAL_TEXT>"I was interviewed by some Chinese media but my response about the origin of the virus was censored," he said in his email to RFA.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="punct" morph="none" start_char="3375" end_char="3375">"</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="3376" end_char="3376">I</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="3378" end_char="3380">was</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="3382" end_char="3392">interviewed</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="3394" end_char="3395">by</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="3397" end_char="3400">some</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="3402" end_char="3408">Chinese</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="3410" end_char="3414">media</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="3416" end_char="3418">but</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="3420" end_char="3421">my</TOKEN>
<TOKEN id="token-25-10" pos="word" morph="none" start_char="3423" end_char="3430">response</TOKEN>
<TOKEN id="token-25-11" pos="word" morph="none" start_char="3432" end_char="3436">about</TOKEN>
<TOKEN id="token-25-12" pos="word" morph="none" start_char="3438" end_char="3440">the</TOKEN>
<TOKEN id="token-25-13" pos="word" morph="none" start_char="3442" end_char="3447">origin</TOKEN>
<TOKEN id="token-25-14" pos="word" morph="none" start_char="3449" end_char="3450">of</TOKEN>
<TOKEN id="token-25-15" pos="word" morph="none" start_char="3452" end_char="3454">the</TOKEN>
<TOKEN id="token-25-16" pos="word" morph="none" start_char="3456" end_char="3460">virus</TOKEN>
<TOKEN id="token-25-17" pos="word" morph="none" start_char="3462" end_char="3464">was</TOKEN>
<TOKEN id="token-25-18" pos="word" morph="none" start_char="3466" end_char="3473">censored</TOKEN>
<TOKEN id="token-25-19" pos="punct" morph="none" start_char="3474" end_char="3475">,"</TOKEN>
<TOKEN id="token-25-20" pos="word" morph="none" start_char="3477" end_char="3478">he</TOKEN>
<TOKEN id="token-25-21" pos="word" morph="none" start_char="3480" end_char="3483">said</TOKEN>
<TOKEN id="token-25-22" pos="word" morph="none" start_char="3485" end_char="3486">in</TOKEN>
<TOKEN id="token-25-23" pos="word" morph="none" start_char="3488" end_char="3490">his</TOKEN>
<TOKEN id="token-25-24" pos="word" morph="none" start_char="3492" end_char="3496">email</TOKEN>
<TOKEN id="token-25-25" pos="word" morph="none" start_char="3498" end_char="3499">to</TOKEN>
<TOKEN id="token-25-26" pos="word" morph="none" start_char="3501" end_char="3503">RFA</TOKEN>
<TOKEN id="token-25-27" pos="punct" morph="none" start_char="3504" end_char="3504">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="3507" end_char="3659">
<ORIGINAL_TEXT>He said the researchers plan to follow up on the study with more in-depth interviews of participants, and that current theories have yet to be confirmed.</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="3507" end_char="3508">He</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="3510" end_char="3513">said</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="3515" end_char="3517">the</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="3519" end_char="3529">researchers</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="3531" end_char="3534">plan</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="3536" end_char="3537">to</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="3539" end_char="3544">follow</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="3546" end_char="3547">up</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="3549" end_char="3550">on</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="3552" end_char="3554">the</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="3556" end_char="3560">study</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="3562" end_char="3565">with</TOKEN>
<TOKEN id="token-26-12" pos="word" morph="none" start_char="3567" end_char="3570">more</TOKEN>
<TOKEN id="token-26-13" pos="unknown" morph="none" start_char="3572" end_char="3579">in-depth</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="3581" end_char="3590">interviews</TOKEN>
<TOKEN id="token-26-15" pos="word" morph="none" start_char="3592" end_char="3593">of</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="3595" end_char="3606">participants</TOKEN>
<TOKEN id="token-26-17" pos="punct" morph="none" start_char="3607" end_char="3607">,</TOKEN>
<TOKEN id="token-26-18" pos="word" morph="none" start_char="3609" end_char="3611">and</TOKEN>
<TOKEN id="token-26-19" pos="word" morph="none" start_char="3613" end_char="3616">that</TOKEN>
<TOKEN id="token-26-20" pos="word" morph="none" start_char="3618" end_char="3624">current</TOKEN>
<TOKEN id="token-26-21" pos="word" morph="none" start_char="3626" end_char="3633">theories</TOKEN>
<TOKEN id="token-26-22" pos="word" morph="none" start_char="3635" end_char="3638">have</TOKEN>
<TOKEN id="token-26-23" pos="word" morph="none" start_char="3640" end_char="3642">yet</TOKEN>
<TOKEN id="token-26-24" pos="word" morph="none" start_char="3644" end_char="3645">to</TOKEN>
<TOKEN id="token-26-25" pos="word" morph="none" start_char="3647" end_char="3648">be</TOKEN>
<TOKEN id="token-26-26" pos="word" morph="none" start_char="3650" end_char="3658">confirmed</TOKEN>
<TOKEN id="token-26-27" pos="punct" morph="none" start_char="3659" end_char="3659">.</TOKEN>
</SEG>
<SEG id="segment-27" start_char="3662" end_char="3791">
<ORIGINAL_TEXT>"We are conducting additional analysis to understand better the signal we found in the context of our wider study," Apolone wrote.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="punct" morph="none" start_char="3662" end_char="3662">"</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="3663" end_char="3664">We</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="3666" end_char="3668">are</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="3670" end_char="3679">conducting</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="3681" end_char="3690">additional</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="3692" end_char="3699">analysis</TOKEN>
<TOKEN id="token-27-6" pos="word" morph="none" start_char="3701" end_char="3702">to</TOKEN>
<TOKEN id="token-27-7" pos="word" morph="none" start_char="3704" end_char="3713">understand</TOKEN>
<TOKEN id="token-27-8" pos="word" morph="none" start_char="3715" end_char="3720">better</TOKEN>
<TOKEN id="token-27-9" pos="word" morph="none" start_char="3722" end_char="3724">the</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="3726" end_char="3731">signal</TOKEN>
<TOKEN id="token-27-11" pos="word" morph="none" start_char="3733" end_char="3734">we</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="3736" end_char="3740">found</TOKEN>
<TOKEN id="token-27-13" pos="word" morph="none" start_char="3742" end_char="3743">in</TOKEN>
<TOKEN id="token-27-14" pos="word" morph="none" start_char="3745" end_char="3747">the</TOKEN>
<TOKEN id="token-27-15" pos="word" morph="none" start_char="3749" end_char="3755">context</TOKEN>
<TOKEN id="token-27-16" pos="word" morph="none" start_char="3757" end_char="3758">of</TOKEN>
<TOKEN id="token-27-17" pos="word" morph="none" start_char="3760" end_char="3762">our</TOKEN>
<TOKEN id="token-27-18" pos="word" morph="none" start_char="3764" end_char="3768">wider</TOKEN>
<TOKEN id="token-27-19" pos="word" morph="none" start_char="3770" end_char="3774">study</TOKEN>
<TOKEN id="token-27-20" pos="punct" morph="none" start_char="3775" end_char="3776">,"</TOKEN>
<TOKEN id="token-27-21" pos="word" morph="none" start_char="3778" end_char="3784">Apolone</TOKEN>
<TOKEN id="token-27-22" pos="word" morph="none" start_char="3786" end_char="3790">wrote</TOKEN>
<TOKEN id="token-27-23" pos="punct" morph="none" start_char="3791" end_char="3791">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="3794" end_char="3967">
<ORIGINAL_TEXT>"I expect that after our publication other teams will report data regarding the origin, timing, and spread of the pandemic virus that will be useful to clarify the argument."</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="punct" morph="none" start_char="3794" end_char="3794">"</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="3795" end_char="3795">I</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="3797" end_char="3802">expect</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="3804" end_char="3807">that</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="3809" end_char="3813">after</TOKEN>
<TOKEN id="token-28-5" pos="word" morph="none" start_char="3815" end_char="3817">our</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="3819" end_char="3829">publication</TOKEN>
<TOKEN id="token-28-7" pos="word" morph="none" start_char="3831" end_char="3835">other</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="3837" end_char="3841">teams</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="3843" end_char="3846">will</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="3848" end_char="3853">report</TOKEN>
<TOKEN id="token-28-11" pos="word" morph="none" start_char="3855" end_char="3858">data</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="3860" end_char="3868">regarding</TOKEN>
<TOKEN id="token-28-13" pos="word" morph="none" start_char="3870" end_char="3872">the</TOKEN>
<TOKEN id="token-28-14" pos="word" morph="none" start_char="3874" end_char="3879">origin</TOKEN>
<TOKEN id="token-28-15" pos="punct" morph="none" start_char="3880" end_char="3880">,</TOKEN>
<TOKEN id="token-28-16" pos="word" morph="none" start_char="3882" end_char="3887">timing</TOKEN>
<TOKEN id="token-28-17" pos="punct" morph="none" start_char="3888" end_char="3888">,</TOKEN>
<TOKEN id="token-28-18" pos="word" morph="none" start_char="3890" end_char="3892">and</TOKEN>
<TOKEN id="token-28-19" pos="word" morph="none" start_char="3894" end_char="3899">spread</TOKEN>
<TOKEN id="token-28-20" pos="word" morph="none" start_char="3901" end_char="3902">of</TOKEN>
<TOKEN id="token-28-21" pos="word" morph="none" start_char="3904" end_char="3906">the</TOKEN>
<TOKEN id="token-28-22" pos="word" morph="none" start_char="3908" end_char="3915">pandemic</TOKEN>
<TOKEN id="token-28-23" pos="word" morph="none" start_char="3917" end_char="3921">virus</TOKEN>
<TOKEN id="token-28-24" pos="word" morph="none" start_char="3923" end_char="3926">that</TOKEN>
<TOKEN id="token-28-25" pos="word" morph="none" start_char="3928" end_char="3931">will</TOKEN>
<TOKEN id="token-28-26" pos="word" morph="none" start_char="3933" end_char="3934">be</TOKEN>
<TOKEN id="token-28-27" pos="word" morph="none" start_char="3936" end_char="3941">useful</TOKEN>
<TOKEN id="token-28-28" pos="word" morph="none" start_char="3943" end_char="3944">to</TOKEN>
<TOKEN id="token-28-29" pos="word" morph="none" start_char="3946" end_char="3952">clarify</TOKEN>
<TOKEN id="token-28-30" pos="word" morph="none" start_char="3954" end_char="3956">the</TOKEN>
<TOKEN id="token-28-31" pos="word" morph="none" start_char="3958" end_char="3965">argument</TOKEN>
<TOKEN id="token-28-32" pos="punct" morph="none" start_char="3966" end_char="3967">."</TOKEN>
</SEG>
<SEG id="segment-29" start_char="3970" end_char="4038">
<ORIGINAL_TEXT>While China didn't officially confirm its first COVID case until Dec.</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="3970" end_char="3974">While</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="3976" end_char="3980">China</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="3982" end_char="3987">didn't</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="3989" end_char="3998">officially</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="4000" end_char="4006">confirm</TOKEN>
<TOKEN id="token-29-5" pos="word" morph="none" start_char="4008" end_char="4010">its</TOKEN>
<TOKEN id="token-29-6" pos="word" morph="none" start_char="4012" end_char="4016">first</TOKEN>
<TOKEN id="token-29-7" pos="word" morph="none" start_char="4018" end_char="4022">COVID</TOKEN>
<TOKEN id="token-29-8" pos="word" morph="none" start_char="4024" end_char="4027">case</TOKEN>
<TOKEN id="token-29-9" pos="word" morph="none" start_char="4029" end_char="4033">until</TOKEN>
<TOKEN id="token-29-10" pos="word" morph="none" start_char="4035" end_char="4037">Dec</TOKEN>
<TOKEN id="token-29-11" pos="punct" morph="none" start_char="4038" end_char="4038">.</TOKEN>
</SEG>
<SEG id="segment-30" start_char="4040" end_char="4129">
<ORIGINAL_TEXT>31, a directive issued by the Wuhan municipal health committee called on hospitals on Dec.</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="4040" end_char="4041">31</TOKEN>
<TOKEN id="token-30-1" pos="punct" morph="none" start_char="4042" end_char="4042">,</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="4044" end_char="4044">a</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="4046" end_char="4054">directive</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="4056" end_char="4061">issued</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="4063" end_char="4064">by</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="4066" end_char="4068">the</TOKEN>
<TOKEN id="token-30-7" pos="word" morph="none" start_char="4070" end_char="4074">Wuhan</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="4076" end_char="4084">municipal</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="4086" end_char="4091">health</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="4093" end_char="4101">committee</TOKEN>
<TOKEN id="token-30-11" pos="word" morph="none" start_char="4103" end_char="4108">called</TOKEN>
<TOKEN id="token-30-12" pos="word" morph="none" start_char="4110" end_char="4111">on</TOKEN>
<TOKEN id="token-30-13" pos="word" morph="none" start_char="4113" end_char="4121">hospitals</TOKEN>
<TOKEN id="token-30-14" pos="word" morph="none" start_char="4123" end_char="4124">on</TOKEN>
<TOKEN id="token-30-15" pos="word" morph="none" start_char="4126" end_char="4128">Dec</TOKEN>
<TOKEN id="token-30-16" pos="punct" morph="none" start_char="4129" end_char="4129">.</TOKEN>
</SEG>
<SEG id="segment-31" start_char="4131" end_char="4244">
<ORIGINAL_TEXT>30 to follow guidelines when treating cases of "pneumonia of unknown cause that have been appearing" in hospitals.</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="word" morph="none" start_char="4131" end_char="4132">30</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="4134" end_char="4135">to</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="4137" end_char="4142">follow</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="4144" end_char="4153">guidelines</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="4155" end_char="4158">when</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="4160" end_char="4167">treating</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="4169" end_char="4173">cases</TOKEN>
<TOKEN id="token-31-7" pos="word" morph="none" start_char="4175" end_char="4176">of</TOKEN>
<TOKEN id="token-31-8" pos="punct" morph="none" start_char="4178" end_char="4178">"</TOKEN>
<TOKEN id="token-31-9" pos="word" morph="none" start_char="4179" end_char="4187">pneumonia</TOKEN>
<TOKEN id="token-31-10" pos="word" morph="none" start_char="4189" end_char="4190">of</TOKEN>
<TOKEN id="token-31-11" pos="word" morph="none" start_char="4192" end_char="4198">unknown</TOKEN>
<TOKEN id="token-31-12" pos="word" morph="none" start_char="4200" end_char="4204">cause</TOKEN>
<TOKEN id="token-31-13" pos="word" morph="none" start_char="4206" end_char="4209">that</TOKEN>
<TOKEN id="token-31-14" pos="word" morph="none" start_char="4211" end_char="4214">have</TOKEN>
<TOKEN id="token-31-15" pos="word" morph="none" start_char="4216" end_char="4219">been</TOKEN>
<TOKEN id="token-31-16" pos="word" morph="none" start_char="4221" end_char="4229">appearing</TOKEN>
<TOKEN id="token-31-17" pos="punct" morph="none" start_char="4230" end_char="4230">"</TOKEN>
<TOKEN id="token-31-18" pos="word" morph="none" start_char="4232" end_char="4233">in</TOKEN>
<TOKEN id="token-31-19" pos="word" morph="none" start_char="4235" end_char="4243">hospitals</TOKEN>
<TOKEN id="token-31-20" pos="punct" morph="none" start_char="4244" end_char="4244">.</TOKEN>
</SEG>
<SEG id="segment-32" start_char="4247" end_char="4354">
<ORIGINAL_TEXT>The directive also banned hospital staff from sharing any information on the disease with the outside world.</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="4247" end_char="4249">The</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="4251" end_char="4259">directive</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="4261" end_char="4264">also</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="4266" end_char="4271">banned</TOKEN>
<TOKEN id="token-32-4" pos="word" morph="none" start_char="4273" end_char="4280">hospital</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="4282" end_char="4286">staff</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="4288" end_char="4291">from</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="4293" end_char="4299">sharing</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="4301" end_char="4303">any</TOKEN>
<TOKEN id="token-32-9" pos="word" morph="none" start_char="4305" end_char="4315">information</TOKEN>
<TOKEN id="token-32-10" pos="word" morph="none" start_char="4317" end_char="4318">on</TOKEN>
<TOKEN id="token-32-11" pos="word" morph="none" start_char="4320" end_char="4322">the</TOKEN>
<TOKEN id="token-32-12" pos="word" morph="none" start_char="4324" end_char="4330">disease</TOKEN>
<TOKEN id="token-32-13" pos="word" morph="none" start_char="4332" end_char="4335">with</TOKEN>
<TOKEN id="token-32-14" pos="word" morph="none" start_char="4337" end_char="4339">the</TOKEN>
<TOKEN id="token-32-15" pos="word" morph="none" start_char="4341" end_char="4347">outside</TOKEN>
<TOKEN id="token-32-16" pos="word" morph="none" start_char="4349" end_char="4353">world</TOKEN>
<TOKEN id="token-32-17" pos="punct" morph="none" start_char="4354" end_char="4354">.</TOKEN>
</SEG>
<SEG id="segment-33" start_char="4357" end_char="4487">
<ORIGINAL_TEXT>"No organization or individual shall make public any medical information, unless they are authorized to do so," the directive said.</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="punct" morph="none" start_char="4357" end_char="4357">"</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="4358" end_char="4359">No</TOKEN>
<TOKEN id="token-33-2" pos="word" morph="none" start_char="4361" end_char="4372">organization</TOKEN>
<TOKEN id="token-33-3" pos="word" morph="none" start_char="4374" end_char="4375">or</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="4377" end_char="4386">individual</TOKEN>
<TOKEN id="token-33-5" pos="word" morph="none" start_char="4388" end_char="4392">shall</TOKEN>
<TOKEN id="token-33-6" pos="word" morph="none" start_char="4394" end_char="4397">make</TOKEN>
<TOKEN id="token-33-7" pos="word" morph="none" start_char="4399" end_char="4404">public</TOKEN>
<TOKEN id="token-33-8" pos="word" morph="none" start_char="4406" end_char="4408">any</TOKEN>
<TOKEN id="token-33-9" pos="word" morph="none" start_char="4410" end_char="4416">medical</TOKEN>
<TOKEN id="token-33-10" pos="word" morph="none" start_char="4418" end_char="4428">information</TOKEN>
<TOKEN id="token-33-11" pos="punct" morph="none" start_char="4429" end_char="4429">,</TOKEN>
<TOKEN id="token-33-12" pos="word" morph="none" start_char="4431" end_char="4436">unless</TOKEN>
<TOKEN id="token-33-13" pos="word" morph="none" start_char="4438" end_char="4441">they</TOKEN>
<TOKEN id="token-33-14" pos="word" morph="none" start_char="4443" end_char="4445">are</TOKEN>
<TOKEN id="token-33-15" pos="word" morph="none" start_char="4447" end_char="4456">authorized</TOKEN>
<TOKEN id="token-33-16" pos="word" morph="none" start_char="4458" end_char="4459">to</TOKEN>
<TOKEN id="token-33-17" pos="word" morph="none" start_char="4461" end_char="4462">do</TOKEN>
<TOKEN id="token-33-18" pos="word" morph="none" start_char="4464" end_char="4465">so</TOKEN>
<TOKEN id="token-33-19" pos="punct" morph="none" start_char="4466" end_char="4467">,"</TOKEN>
<TOKEN id="token-33-20" pos="word" morph="none" start_char="4469" end_char="4471">the</TOKEN>
<TOKEN id="token-33-21" pos="word" morph="none" start_char="4473" end_char="4481">directive</TOKEN>
<TOKEN id="token-33-22" pos="word" morph="none" start_char="4483" end_char="4486">said</TOKEN>
<TOKEN id="token-33-23" pos="punct" morph="none" start_char="4487" end_char="4487">.</TOKEN>
</SEG>
<SEG id="segment-34" start_char="4490" end_char="4532">
<ORIGINAL_TEXT>Hong Kong's Hospital Authority said on Jan.</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="4490" end_char="4493">Hong</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="4495" end_char="4500">Kong's</TOKEN>
<TOKEN id="token-34-2" pos="word" morph="none" start_char="4502" end_char="4509">Hospital</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="4511" end_char="4519">Authority</TOKEN>
<TOKEN id="token-34-4" pos="word" morph="none" start_char="4521" end_char="4524">said</TOKEN>
<TOKEN id="token-34-5" pos="word" morph="none" start_char="4526" end_char="4527">on</TOKEN>
<TOKEN id="token-34-6" pos="word" morph="none" start_char="4529" end_char="4531">Jan</TOKEN>
<TOKEN id="token-34-7" pos="punct" morph="none" start_char="4532" end_char="4532">.</TOKEN>
</SEG>
<SEG id="segment-35" start_char="4534" end_char="4689">
<ORIGINAL_TEXT>2 it had isolated a pneumonia patient who arrived from the central Chinese city of Wuhan, who had tested negative for SARS and avian and seasonal influenza.</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="4534" end_char="4534">2</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="4536" end_char="4537">it</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="4539" end_char="4541">had</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="4543" end_char="4550">isolated</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="4552" end_char="4552">a</TOKEN>
<TOKEN id="token-35-5" pos="word" morph="none" start_char="4554" end_char="4562">pneumonia</TOKEN>
<TOKEN id="token-35-6" pos="word" morph="none" start_char="4564" end_char="4570">patient</TOKEN>
<TOKEN id="token-35-7" pos="word" morph="none" start_char="4572" end_char="4574">who</TOKEN>
<TOKEN id="token-35-8" pos="word" morph="none" start_char="4576" end_char="4582">arrived</TOKEN>
<TOKEN id="token-35-9" pos="word" morph="none" start_char="4584" end_char="4587">from</TOKEN>
<TOKEN id="token-35-10" pos="word" morph="none" start_char="4589" end_char="4591">the</TOKEN>
<TOKEN id="token-35-11" pos="word" morph="none" start_char="4593" end_char="4599">central</TOKEN>
<TOKEN id="token-35-12" pos="word" morph="none" start_char="4601" end_char="4607">Chinese</TOKEN>
<TOKEN id="token-35-13" pos="word" morph="none" start_char="4609" end_char="4612">city</TOKEN>
<TOKEN id="token-35-14" pos="word" morph="none" start_char="4614" end_char="4615">of</TOKEN>
<TOKEN id="token-35-15" pos="word" morph="none" start_char="4617" end_char="4621">Wuhan</TOKEN>
<TOKEN id="token-35-16" pos="punct" morph="none" start_char="4622" end_char="4622">,</TOKEN>
<TOKEN id="token-35-17" pos="word" morph="none" start_char="4624" end_char="4626">who</TOKEN>
<TOKEN id="token-35-18" pos="word" morph="none" start_char="4628" end_char="4630">had</TOKEN>
<TOKEN id="token-35-19" pos="word" morph="none" start_char="4632" end_char="4637">tested</TOKEN>
<TOKEN id="token-35-20" pos="word" morph="none" start_char="4639" end_char="4646">negative</TOKEN>
<TOKEN id="token-35-21" pos="word" morph="none" start_char="4648" end_char="4650">for</TOKEN>
<TOKEN id="token-35-22" pos="word" morph="none" start_char="4652" end_char="4655">SARS</TOKEN>
<TOKEN id="token-35-23" pos="word" morph="none" start_char="4657" end_char="4659">and</TOKEN>
<TOKEN id="token-35-24" pos="word" morph="none" start_char="4661" end_char="4665">avian</TOKEN>
<TOKEN id="token-35-25" pos="word" morph="none" start_char="4667" end_char="4669">and</TOKEN>
<TOKEN id="token-35-26" pos="word" morph="none" start_char="4671" end_char="4678">seasonal</TOKEN>
<TOKEN id="token-35-27" pos="word" morph="none" start_char="4680" end_char="4688">influenza</TOKEN>
<TOKEN id="token-35-28" pos="punct" morph="none" start_char="4689" end_char="4689">.</TOKEN>
</SEG>
<SEG id="segment-36" start_char="4692" end_char="4698">
<ORIGINAL_TEXT>By Jan.</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="4692" end_char="4693">By</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="4695" end_char="4697">Jan</TOKEN>
<TOKEN id="token-36-2" pos="punct" morph="none" start_char="4698" end_char="4698">.</TOKEN>
</SEG>
<SEG id="segment-37" start_char="4700" end_char="4924">
<ORIGINAL_TEXT>6, the number of patients in Hong Kong isolation wards had risen to 21 people, none of whom had visited Wuhan's South China Seafood Market mentioned as a possible link between early cases by mainland Chinese health officials.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="4700" end_char="4700">6</TOKEN>
<TOKEN id="token-37-1" pos="punct" morph="none" start_char="4701" end_char="4701">,</TOKEN>
<TOKEN id="token-37-2" pos="word" morph="none" start_char="4703" end_char="4705">the</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="4707" end_char="4712">number</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="4714" end_char="4715">of</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="4717" end_char="4724">patients</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="4726" end_char="4727">in</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="4729" end_char="4732">Hong</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="4734" end_char="4737">Kong</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="4739" end_char="4747">isolation</TOKEN>
<TOKEN id="token-37-10" pos="word" morph="none" start_char="4749" end_char="4753">wards</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="4755" end_char="4757">had</TOKEN>
<TOKEN id="token-37-12" pos="word" morph="none" start_char="4759" end_char="4763">risen</TOKEN>
<TOKEN id="token-37-13" pos="word" morph="none" start_char="4765" end_char="4766">to</TOKEN>
<TOKEN id="token-37-14" pos="word" morph="none" start_char="4768" end_char="4769">21</TOKEN>
<TOKEN id="token-37-15" pos="word" morph="none" start_char="4771" end_char="4776">people</TOKEN>
<TOKEN id="token-37-16" pos="punct" morph="none" start_char="4777" end_char="4777">,</TOKEN>
<TOKEN id="token-37-17" pos="word" morph="none" start_char="4779" end_char="4782">none</TOKEN>
<TOKEN id="token-37-18" pos="word" morph="none" start_char="4784" end_char="4785">of</TOKEN>
<TOKEN id="token-37-19" pos="word" morph="none" start_char="4787" end_char="4790">whom</TOKEN>
<TOKEN id="token-37-20" pos="word" morph="none" start_char="4792" end_char="4794">had</TOKEN>
<TOKEN id="token-37-21" pos="word" morph="none" start_char="4796" end_char="4802">visited</TOKEN>
<TOKEN id="token-37-22" pos="word" morph="none" start_char="4804" end_char="4810">Wuhan's</TOKEN>
<TOKEN id="token-37-23" pos="word" morph="none" start_char="4812" end_char="4816">South</TOKEN>
<TOKEN id="token-37-24" pos="word" morph="none" start_char="4818" end_char="4822">China</TOKEN>
<TOKEN id="token-37-25" pos="word" morph="none" start_char="4824" end_char="4830">Seafood</TOKEN>
<TOKEN id="token-37-26" pos="word" morph="none" start_char="4832" end_char="4837">Market</TOKEN>
<TOKEN id="token-37-27" pos="word" morph="none" start_char="4839" end_char="4847">mentioned</TOKEN>
<TOKEN id="token-37-28" pos="word" morph="none" start_char="4849" end_char="4850">as</TOKEN>
<TOKEN id="token-37-29" pos="word" morph="none" start_char="4852" end_char="4852">a</TOKEN>
<TOKEN id="token-37-30" pos="word" morph="none" start_char="4854" end_char="4861">possible</TOKEN>
<TOKEN id="token-37-31" pos="word" morph="none" start_char="4863" end_char="4866">link</TOKEN>
<TOKEN id="token-37-32" pos="word" morph="none" start_char="4868" end_char="4874">between</TOKEN>
<TOKEN id="token-37-33" pos="word" morph="none" start_char="4876" end_char="4880">early</TOKEN>
<TOKEN id="token-37-34" pos="word" morph="none" start_char="4882" end_char="4886">cases</TOKEN>
<TOKEN id="token-37-35" pos="word" morph="none" start_char="4888" end_char="4889">by</TOKEN>
<TOKEN id="token-37-36" pos="word" morph="none" start_char="4891" end_char="4898">mainland</TOKEN>
<TOKEN id="token-37-37" pos="word" morph="none" start_char="4900" end_char="4906">Chinese</TOKEN>
<TOKEN id="token-37-38" pos="word" morph="none" start_char="4908" end_char="4913">health</TOKEN>
<TOKEN id="token-37-39" pos="word" morph="none" start_char="4915" end_char="4923">officials</TOKEN>
<TOKEN id="token-37-40" pos="punct" morph="none" start_char="4924" end_char="4924">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="4927" end_char="5067">
<ORIGINAL_TEXT>Hong Kong raised its response level to serious and announced that the then unknown pathogen had "public health significance" on the same day.</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="4927" end_char="4930">Hong</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="4932" end_char="4935">Kong</TOKEN>
<TOKEN id="token-38-2" pos="word" morph="none" start_char="4937" end_char="4942">raised</TOKEN>
<TOKEN id="token-38-3" pos="word" morph="none" start_char="4944" end_char="4946">its</TOKEN>
<TOKEN id="token-38-4" pos="word" morph="none" start_char="4948" end_char="4955">response</TOKEN>
<TOKEN id="token-38-5" pos="word" morph="none" start_char="4957" end_char="4961">level</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="4963" end_char="4964">to</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="4966" end_char="4972">serious</TOKEN>
<TOKEN id="token-38-8" pos="word" morph="none" start_char="4974" end_char="4976">and</TOKEN>
<TOKEN id="token-38-9" pos="word" morph="none" start_char="4978" end_char="4986">announced</TOKEN>
<TOKEN id="token-38-10" pos="word" morph="none" start_char="4988" end_char="4991">that</TOKEN>
<TOKEN id="token-38-11" pos="word" morph="none" start_char="4993" end_char="4995">the</TOKEN>
<TOKEN id="token-38-12" pos="word" morph="none" start_char="4997" end_char="5000">then</TOKEN>
<TOKEN id="token-38-13" pos="word" morph="none" start_char="5002" end_char="5008">unknown</TOKEN>
<TOKEN id="token-38-14" pos="word" morph="none" start_char="5010" end_char="5017">pathogen</TOKEN>
<TOKEN id="token-38-15" pos="word" morph="none" start_char="5019" end_char="5021">had</TOKEN>
<TOKEN id="token-38-16" pos="punct" morph="none" start_char="5023" end_char="5023">"</TOKEN>
<TOKEN id="token-38-17" pos="word" morph="none" start_char="5024" end_char="5029">public</TOKEN>
<TOKEN id="token-38-18" pos="word" morph="none" start_char="5031" end_char="5036">health</TOKEN>
<TOKEN id="token-38-19" pos="word" morph="none" start_char="5038" end_char="5049">significance</TOKEN>
<TOKEN id="token-38-20" pos="punct" morph="none" start_char="5050" end_char="5050">"</TOKEN>
<TOKEN id="token-38-21" pos="word" morph="none" start_char="5052" end_char="5053">on</TOKEN>
<TOKEN id="token-38-22" pos="word" morph="none" start_char="5055" end_char="5057">the</TOKEN>
<TOKEN id="token-38-23" pos="word" morph="none" start_char="5059" end_char="5062">same</TOKEN>
<TOKEN id="token-38-24" pos="word" morph="none" start_char="5064" end_char="5066">day</TOKEN>
<TOKEN id="token-38-25" pos="punct" morph="none" start_char="5067" end_char="5067">.</TOKEN>
</SEG>
<SEG id="segment-39" start_char="5070" end_char="5157">
<ORIGINAL_TEXT>Ho Pak-leung, head of the University of Hong Kong's Centre for Infection, warned on Jan.</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="word" morph="none" start_char="5070" end_char="5071">Ho</TOKEN>
<TOKEN id="token-39-1" pos="unknown" morph="none" start_char="5073" end_char="5081">Pak-leung</TOKEN>
<TOKEN id="token-39-2" pos="punct" morph="none" start_char="5082" end_char="5082">,</TOKEN>
<TOKEN id="token-39-3" pos="word" morph="none" start_char="5084" end_char="5087">head</TOKEN>
<TOKEN id="token-39-4" pos="word" morph="none" start_char="5089" end_char="5090">of</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="5092" end_char="5094">the</TOKEN>
<TOKEN id="token-39-6" pos="word" morph="none" start_char="5096" end_char="5105">University</TOKEN>
<TOKEN id="token-39-7" pos="word" morph="none" start_char="5107" end_char="5108">of</TOKEN>
<TOKEN id="token-39-8" pos="word" morph="none" start_char="5110" end_char="5113">Hong</TOKEN>
<TOKEN id="token-39-9" pos="word" morph="none" start_char="5115" end_char="5120">Kong's</TOKEN>
<TOKEN id="token-39-10" pos="word" morph="none" start_char="5122" end_char="5127">Centre</TOKEN>
<TOKEN id="token-39-11" pos="word" morph="none" start_char="5129" end_char="5131">for</TOKEN>
<TOKEN id="token-39-12" pos="word" morph="none" start_char="5133" end_char="5141">Infection</TOKEN>
<TOKEN id="token-39-13" pos="punct" morph="none" start_char="5142" end_char="5142">,</TOKEN>
<TOKEN id="token-39-14" pos="word" morph="none" start_char="5144" end_char="5149">warned</TOKEN>
<TOKEN id="token-39-15" pos="word" morph="none" start_char="5151" end_char="5152">on</TOKEN>
<TOKEN id="token-39-16" pos="word" morph="none" start_char="5154" end_char="5156">Jan</TOKEN>
<TOKEN id="token-39-17" pos="punct" morph="none" start_char="5157" end_char="5157">.</TOKEN>
</SEG>
<SEG id="segment-40" start_char="5159" end_char="5318">
<ORIGINAL_TEXT>4 that it was highly possible that the illness was spreading from human to human, given the sheer number of cases that had mushroomed in a short period of time.</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="word" morph="none" start_char="5159" end_char="5159">4</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="5161" end_char="5164">that</TOKEN>
<TOKEN id="token-40-2" pos="word" morph="none" start_char="5166" end_char="5167">it</TOKEN>
<TOKEN id="token-40-3" pos="word" morph="none" start_char="5169" end_char="5171">was</TOKEN>
<TOKEN id="token-40-4" pos="word" morph="none" start_char="5173" end_char="5178">highly</TOKEN>
<TOKEN id="token-40-5" pos="word" morph="none" start_char="5180" end_char="5187">possible</TOKEN>
<TOKEN id="token-40-6" pos="word" morph="none" start_char="5189" end_char="5192">that</TOKEN>
<TOKEN id="token-40-7" pos="word" morph="none" start_char="5194" end_char="5196">the</TOKEN>
<TOKEN id="token-40-8" pos="word" morph="none" start_char="5198" end_char="5204">illness</TOKEN>
<TOKEN id="token-40-9" pos="word" morph="none" start_char="5206" end_char="5208">was</TOKEN>
<TOKEN id="token-40-10" pos="word" morph="none" start_char="5210" end_char="5218">spreading</TOKEN>
<TOKEN id="token-40-11" pos="word" morph="none" start_char="5220" end_char="5223">from</TOKEN>
<TOKEN id="token-40-12" pos="word" morph="none" start_char="5225" end_char="5229">human</TOKEN>
<TOKEN id="token-40-13" pos="word" morph="none" start_char="5231" end_char="5232">to</TOKEN>
<TOKEN id="token-40-14" pos="word" morph="none" start_char="5234" end_char="5238">human</TOKEN>
<TOKEN id="token-40-15" pos="punct" morph="none" start_char="5239" end_char="5239">,</TOKEN>
<TOKEN id="token-40-16" pos="word" morph="none" start_char="5241" end_char="5245">given</TOKEN>
<TOKEN id="token-40-17" pos="word" morph="none" start_char="5247" end_char="5249">the</TOKEN>
<TOKEN id="token-40-18" pos="word" morph="none" start_char="5251" end_char="5255">sheer</TOKEN>
<TOKEN id="token-40-19" pos="word" morph="none" start_char="5257" end_char="5262">number</TOKEN>
<TOKEN id="token-40-20" pos="word" morph="none" start_char="5264" end_char="5265">of</TOKEN>
<TOKEN id="token-40-21" pos="word" morph="none" start_char="5267" end_char="5271">cases</TOKEN>
<TOKEN id="token-40-22" pos="word" morph="none" start_char="5273" end_char="5276">that</TOKEN>
<TOKEN id="token-40-23" pos="word" morph="none" start_char="5278" end_char="5280">had</TOKEN>
<TOKEN id="token-40-24" pos="word" morph="none" start_char="5282" end_char="5291">mushroomed</TOKEN>
<TOKEN id="token-40-25" pos="word" morph="none" start_char="5293" end_char="5294">in</TOKEN>
<TOKEN id="token-40-26" pos="word" morph="none" start_char="5296" end_char="5296">a</TOKEN>
<TOKEN id="token-40-27" pos="word" morph="none" start_char="5298" end_char="5302">short</TOKEN>
<TOKEN id="token-40-28" pos="word" morph="none" start_char="5304" end_char="5309">period</TOKEN>
<TOKEN id="token-40-29" pos="word" morph="none" start_char="5311" end_char="5312">of</TOKEN>
<TOKEN id="token-40-30" pos="word" morph="none" start_char="5314" end_char="5317">time</TOKEN>
<TOKEN id="token-40-31" pos="punct" morph="none" start_char="5318" end_char="5318">.</TOKEN>
</SEG>
<SEG id="segment-41" start_char="5321" end_char="5421">
<ORIGINAL_TEXT>However, China didn't publicly confirm that the virus was being transmitted between people until Jan.</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="5321" end_char="5327">However</TOKEN>
<TOKEN id="token-41-1" pos="punct" morph="none" start_char="5328" end_char="5328">,</TOKEN>
<TOKEN id="token-41-2" pos="word" morph="none" start_char="5330" end_char="5334">China</TOKEN>
<TOKEN id="token-41-3" pos="word" morph="none" start_char="5336" end_char="5341">didn't</TOKEN>
<TOKEN id="token-41-4" pos="word" morph="none" start_char="5343" end_char="5350">publicly</TOKEN>
<TOKEN id="token-41-5" pos="word" morph="none" start_char="5352" end_char="5358">confirm</TOKEN>
<TOKEN id="token-41-6" pos="word" morph="none" start_char="5360" end_char="5363">that</TOKEN>
<TOKEN id="token-41-7" pos="word" morph="none" start_char="5365" end_char="5367">the</TOKEN>
<TOKEN id="token-41-8" pos="word" morph="none" start_char="5369" end_char="5373">virus</TOKEN>
<TOKEN id="token-41-9" pos="word" morph="none" start_char="5375" end_char="5377">was</TOKEN>
<TOKEN id="token-41-10" pos="word" morph="none" start_char="5379" end_char="5383">being</TOKEN>
<TOKEN id="token-41-11" pos="word" morph="none" start_char="5385" end_char="5395">transmitted</TOKEN>
<TOKEN id="token-41-12" pos="word" morph="none" start_char="5397" end_char="5403">between</TOKEN>
<TOKEN id="token-41-13" pos="word" morph="none" start_char="5405" end_char="5410">people</TOKEN>
<TOKEN id="token-41-14" pos="word" morph="none" start_char="5412" end_char="5416">until</TOKEN>
<TOKEN id="token-41-15" pos="word" morph="none" start_char="5418" end_char="5420">Jan</TOKEN>
<TOKEN id="token-41-16" pos="punct" morph="none" start_char="5421" end_char="5421">.</TOKEN>
</SEG>
<SEG id="segment-42" start_char="5423" end_char="5558">
<ORIGINAL_TEXT>22, and Wuhan residents told RFA in early January that nobody in the city was wearing masks, nor appeared concerned about the new virus.</ORIGINAL_TEXT>
<TOKEN id="token-42-0" pos="word" morph="none" start_char="5423" end_char="5424">22</TOKEN>
<TOKEN id="token-42-1" pos="punct" morph="none" start_char="5425" end_char="5425">,</TOKEN>
<TOKEN id="token-42-2" pos="word" morph="none" start_char="5427" end_char="5429">and</TOKEN>
<TOKEN id="token-42-3" pos="word" morph="none" start_char="5431" end_char="5435">Wuhan</TOKEN>
<TOKEN id="token-42-4" pos="word" morph="none" start_char="5437" end_char="5445">residents</TOKEN>
<TOKEN id="token-42-5" pos="word" morph="none" start_char="5447" end_char="5450">told</TOKEN>
<TOKEN id="token-42-6" pos="word" morph="none" start_char="5452" end_char="5454">RFA</TOKEN>
<TOKEN id="token-42-7" pos="word" morph="none" start_char="5456" end_char="5457">in</TOKEN>
<TOKEN id="token-42-8" pos="word" morph="none" start_char="5459" end_char="5463">early</TOKEN>
<TOKEN id="token-42-9" pos="word" morph="none" start_char="5465" end_char="5471">January</TOKEN>
<TOKEN id="token-42-10" pos="word" morph="none" start_char="5473" end_char="5476">that</TOKEN>
<TOKEN id="token-42-11" pos="word" morph="none" start_char="5478" end_char="5483">nobody</TOKEN>
<TOKEN id="token-42-12" pos="word" morph="none" start_char="5485" end_char="5486">in</TOKEN>
<TOKEN id="token-42-13" pos="word" morph="none" start_char="5488" end_char="5490">the</TOKEN>
<TOKEN id="token-42-14" pos="word" morph="none" start_char="5492" end_char="5495">city</TOKEN>
<TOKEN id="token-42-15" pos="word" morph="none" start_char="5497" end_char="5499">was</TOKEN>
<TOKEN id="token-42-16" pos="word" morph="none" start_char="5501" end_char="5507">wearing</TOKEN>
<TOKEN id="token-42-17" pos="word" morph="none" start_char="5509" end_char="5513">masks</TOKEN>
<TOKEN id="token-42-18" pos="punct" morph="none" start_char="5514" end_char="5514">,</TOKEN>
<TOKEN id="token-42-19" pos="word" morph="none" start_char="5516" end_char="5518">nor</TOKEN>
<TOKEN id="token-42-20" pos="word" morph="none" start_char="5520" end_char="5527">appeared</TOKEN>
<TOKEN id="token-42-21" pos="word" morph="none" start_char="5529" end_char="5537">concerned</TOKEN>
<TOKEN id="token-42-22" pos="word" morph="none" start_char="5539" end_char="5543">about</TOKEN>
<TOKEN id="token-42-23" pos="word" morph="none" start_char="5545" end_char="5547">the</TOKEN>
<TOKEN id="token-42-24" pos="word" morph="none" start_char="5549" end_char="5551">new</TOKEN>
<TOKEN id="token-42-25" pos="word" morph="none" start_char="5553" end_char="5557">virus</TOKEN>
<TOKEN id="token-42-26" pos="punct" morph="none" start_char="5558" end_char="5558">.</TOKEN>
</SEG>
<SEG id="segment-43" start_char="5561" end_char="5610">
<ORIGINAL_TEXT>Reported by Carmen Wu for RFA's Cantonese Service.</ORIGINAL_TEXT>
<TOKEN id="token-43-0" pos="word" morph="none" start_char="5561" end_char="5568">Reported</TOKEN>
<TOKEN id="token-43-1" pos="word" morph="none" start_char="5570" end_char="5571">by</TOKEN>
<TOKEN id="token-43-2" pos="word" morph="none" start_char="5573" end_char="5578">Carmen</TOKEN>
<TOKEN id="token-43-3" pos="word" morph="none" start_char="5580" end_char="5581">Wu</TOKEN>
<TOKEN id="token-43-4" pos="word" morph="none" start_char="5583" end_char="5585">for</TOKEN>
<TOKEN id="token-43-5" pos="word" morph="none" start_char="5587" end_char="5591">RFA's</TOKEN>
<TOKEN id="token-43-6" pos="word" morph="none" start_char="5593" end_char="5601">Cantonese</TOKEN>
<TOKEN id="token-43-7" pos="word" morph="none" start_char="5603" end_char="5609">Service</TOKEN>
<TOKEN id="token-43-8" pos="punct" morph="none" start_char="5610" end_char="5610">.</TOKEN>
</SEG>
<SEG id="segment-44" start_char="5612" end_char="5651">
<ORIGINAL_TEXT>Translated and edited by Luisetta Mudie.</ORIGINAL_TEXT>
<TOKEN id="token-44-0" pos="word" morph="none" start_char="5612" end_char="5621">Translated</TOKEN>
<TOKEN id="token-44-1" pos="word" morph="none" start_char="5623" end_char="5625">and</TOKEN>
<TOKEN id="token-44-2" pos="word" morph="none" start_char="5627" end_char="5632">edited</TOKEN>
<TOKEN id="token-44-3" pos="word" morph="none" start_char="5634" end_char="5635">by</TOKEN>
<TOKEN id="token-44-4" pos="word" morph="none" start_char="5637" end_char="5644">Luisetta</TOKEN>
<TOKEN id="token-44-5" pos="word" morph="none" start_char="5646" end_char="5650">Mudie</TOKEN>
<TOKEN id="token-44-6" pos="punct" morph="none" start_char="5651" end_char="5651">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
