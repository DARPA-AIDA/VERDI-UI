<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="eng">
<DOC id="L0C049DX4" lang="eng" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="3164" raw_text_md5="147db34f7cd07a873d03357b2e0d9423">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="68">
<ORIGINAL_TEXT>French scientist who discovered HIV insists COVID-19 is lab creation</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="6">French</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="8" end_char="16">scientist</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="18" end_char="20">who</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="22" end_char="31">discovered</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="33" end_char="35">HIV</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="37" end_char="43">insists</TOKEN>
<TOKEN id="token-0-6" pos="unknown" morph="none" start_char="45" end_char="52">COVID-19</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="54" end_char="55">is</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="57" end_char="59">lab</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="61" end_char="68">creation</TOKEN>
</SEG>
<SEG id="segment-1" start_char="72" end_char="290">
<ORIGINAL_TEXT>A 2008 Nobel Prize recipient for discovering the human immunodeficiency virus (HIV) has weighed in on the controversy about the origin of the coronavirus disease 2019 (COVID-19), and insisted that it is laboratory-made.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="72" end_char="72">A</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="74" end_char="77">2008</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="79" end_char="83">Nobel</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="85" end_char="89">Prize</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="91" end_char="99">recipient</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="101" end_char="103">for</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="105" end_char="115">discovering</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="117" end_char="119">the</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="121" end_char="125">human</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="127" end_char="142">immunodeficiency</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="144" end_char="148">virus</TOKEN>
<TOKEN id="token-1-11" pos="punct" morph="none" start_char="150" end_char="150">(</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="151" end_char="153">HIV</TOKEN>
<TOKEN id="token-1-13" pos="punct" morph="none" start_char="154" end_char="154">)</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="156" end_char="158">has</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="160" end_char="166">weighed</TOKEN>
<TOKEN id="token-1-16" pos="word" morph="none" start_char="168" end_char="169">in</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="171" end_char="172">on</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="174" end_char="176">the</TOKEN>
<TOKEN id="token-1-19" pos="word" morph="none" start_char="178" end_char="188">controversy</TOKEN>
<TOKEN id="token-1-20" pos="word" morph="none" start_char="190" end_char="194">about</TOKEN>
<TOKEN id="token-1-21" pos="word" morph="none" start_char="196" end_char="198">the</TOKEN>
<TOKEN id="token-1-22" pos="word" morph="none" start_char="200" end_char="205">origin</TOKEN>
<TOKEN id="token-1-23" pos="word" morph="none" start_char="207" end_char="208">of</TOKEN>
<TOKEN id="token-1-24" pos="word" morph="none" start_char="210" end_char="212">the</TOKEN>
<TOKEN id="token-1-25" pos="word" morph="none" start_char="214" end_char="224">coronavirus</TOKEN>
<TOKEN id="token-1-26" pos="word" morph="none" start_char="226" end_char="232">disease</TOKEN>
<TOKEN id="token-1-27" pos="word" morph="none" start_char="234" end_char="237">2019</TOKEN>
<TOKEN id="token-1-28" pos="punct" morph="none" start_char="239" end_char="239">(</TOKEN>
<TOKEN id="token-1-29" pos="unknown" morph="none" start_char="240" end_char="247">COVID-19</TOKEN>
<TOKEN id="token-1-30" pos="punct" morph="none" start_char="248" end_char="249">),</TOKEN>
<TOKEN id="token-1-31" pos="word" morph="none" start_char="251" end_char="253">and</TOKEN>
<TOKEN id="token-1-32" pos="word" morph="none" start_char="255" end_char="262">insisted</TOKEN>
<TOKEN id="token-1-33" pos="word" morph="none" start_char="264" end_char="267">that</TOKEN>
<TOKEN id="token-1-34" pos="word" morph="none" start_char="269" end_char="270">it</TOKEN>
<TOKEN id="token-1-35" pos="word" morph="none" start_char="272" end_char="273">is</TOKEN>
<TOKEN id="token-1-36" pos="unknown" morph="none" start_char="275" end_char="289">laboratory-made</TOKEN>
<TOKEN id="token-1-37" pos="punct" morph="none" start_char="290" end_char="290">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="293" end_char="569">
<ORIGINAL_TEXT>Four days prior to US President Donald Trump's threat of new tariffs against Beijing, claiming there is evidence linking the coronavirus to a lab in China's ground-zero city of Wuhan, French scientist Luc Montagnier who discovered HIV said COVID-19 was a creation of an expert.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="293" end_char="296">Four</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="298" end_char="301">days</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="303" end_char="307">prior</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="309" end_char="310">to</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="312" end_char="313">US</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="315" end_char="323">President</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="325" end_char="330">Donald</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="332" end_char="338">Trump's</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="340" end_char="345">threat</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="347" end_char="348">of</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="350" end_char="352">new</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="354" end_char="360">tariffs</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="362" end_char="368">against</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="370" end_char="376">Beijing</TOKEN>
<TOKEN id="token-2-14" pos="punct" morph="none" start_char="377" end_char="377">,</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="379" end_char="386">claiming</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="388" end_char="392">there</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="394" end_char="395">is</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="397" end_char="404">evidence</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="406" end_char="412">linking</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="414" end_char="416">the</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="418" end_char="428">coronavirus</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="430" end_char="431">to</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="433" end_char="433">a</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="435" end_char="437">lab</TOKEN>
<TOKEN id="token-2-25" pos="word" morph="none" start_char="439" end_char="440">in</TOKEN>
<TOKEN id="token-2-26" pos="word" morph="none" start_char="442" end_char="448">China's</TOKEN>
<TOKEN id="token-2-27" pos="unknown" morph="none" start_char="450" end_char="460">ground-zero</TOKEN>
<TOKEN id="token-2-28" pos="word" morph="none" start_char="462" end_char="465">city</TOKEN>
<TOKEN id="token-2-29" pos="word" morph="none" start_char="467" end_char="468">of</TOKEN>
<TOKEN id="token-2-30" pos="word" morph="none" start_char="470" end_char="474">Wuhan</TOKEN>
<TOKEN id="token-2-31" pos="punct" morph="none" start_char="475" end_char="475">,</TOKEN>
<TOKEN id="token-2-32" pos="word" morph="none" start_char="477" end_char="482">French</TOKEN>
<TOKEN id="token-2-33" pos="word" morph="none" start_char="484" end_char="492">scientist</TOKEN>
<TOKEN id="token-2-34" pos="word" morph="none" start_char="494" end_char="496">Luc</TOKEN>
<TOKEN id="token-2-35" pos="word" morph="none" start_char="498" end_char="507">Montagnier</TOKEN>
<TOKEN id="token-2-36" pos="word" morph="none" start_char="509" end_char="511">who</TOKEN>
<TOKEN id="token-2-37" pos="word" morph="none" start_char="513" end_char="522">discovered</TOKEN>
<TOKEN id="token-2-38" pos="word" morph="none" start_char="524" end_char="526">HIV</TOKEN>
<TOKEN id="token-2-39" pos="word" morph="none" start_char="528" end_char="531">said</TOKEN>
<TOKEN id="token-2-40" pos="unknown" morph="none" start_char="533" end_char="540">COVID-19</TOKEN>
<TOKEN id="token-2-41" pos="word" morph="none" start_char="542" end_char="544">was</TOKEN>
<TOKEN id="token-2-42" pos="word" morph="none" start_char="546" end_char="546">a</TOKEN>
<TOKEN id="token-2-43" pos="word" morph="none" start_char="548" end_char="555">creation</TOKEN>
<TOKEN id="token-2-44" pos="word" morph="none" start_char="557" end_char="558">of</TOKEN>
<TOKEN id="token-2-45" pos="word" morph="none" start_char="560" end_char="561">an</TOKEN>
<TOKEN id="token-2-46" pos="word" morph="none" start_char="563" end_char="568">expert</TOKEN>
<TOKEN id="token-2-47" pos="punct" morph="none" start_char="569" end_char="569">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="572" end_char="804">
<ORIGINAL_TEXT>Interviewed on the CNews channel in France, Montagnier asserted that the virus had been designed by molecular biologists, stating that it contains genetic elements of HIV, and that its characteristics could not have arisen naturally.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="572" end_char="582">Interviewed</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="584" end_char="585">on</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="587" end_char="589">the</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="591" end_char="595">CNews</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="597" end_char="603">channel</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="605" end_char="606">in</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="608" end_char="613">France</TOKEN>
<TOKEN id="token-3-7" pos="punct" morph="none" start_char="614" end_char="614">,</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="616" end_char="625">Montagnier</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="627" end_char="634">asserted</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="636" end_char="639">that</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="641" end_char="643">the</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="645" end_char="649">virus</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="651" end_char="653">had</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="655" end_char="658">been</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="660" end_char="667">designed</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="669" end_char="670">by</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="672" end_char="680">molecular</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="682" end_char="691">biologists</TOKEN>
<TOKEN id="token-3-19" pos="punct" morph="none" start_char="692" end_char="692">,</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="694" end_char="700">stating</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="702" end_char="705">that</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="707" end_char="708">it</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="710" end_char="717">contains</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="719" end_char="725">genetic</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="727" end_char="734">elements</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="736" end_char="737">of</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="739" end_char="741">HIV</TOKEN>
<TOKEN id="token-3-28" pos="punct" morph="none" start_char="742" end_char="742">,</TOKEN>
<TOKEN id="token-3-29" pos="word" morph="none" start_char="744" end_char="746">and</TOKEN>
<TOKEN id="token-3-30" pos="word" morph="none" start_char="748" end_char="751">that</TOKEN>
<TOKEN id="token-3-31" pos="word" morph="none" start_char="753" end_char="755">its</TOKEN>
<TOKEN id="token-3-32" pos="word" morph="none" start_char="757" end_char="771">characteristics</TOKEN>
<TOKEN id="token-3-33" pos="word" morph="none" start_char="773" end_char="777">could</TOKEN>
<TOKEN id="token-3-34" pos="word" morph="none" start_char="779" end_char="781">not</TOKEN>
<TOKEN id="token-3-35" pos="word" morph="none" start_char="783" end_char="786">have</TOKEN>
<TOKEN id="token-3-36" pos="word" morph="none" start_char="788" end_char="793">arisen</TOKEN>
<TOKEN id="token-3-37" pos="word" morph="none" start_char="795" end_char="803">naturally</TOKEN>
<TOKEN id="token-3-38" pos="punct" morph="none" start_char="804" end_char="804">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="807" end_char="923">
<ORIGINAL_TEXT>Asked by the CNews interviewer what the goal of these molecular biologists could be, Montagnier said it wasn’t clear.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="807" end_char="811">Asked</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="813" end_char="814">by</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="816" end_char="818">the</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="820" end_char="824">CNews</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="826" end_char="836">interviewer</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="838" end_char="841">what</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="843" end_char="845">the</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="847" end_char="850">goal</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="852" end_char="853">of</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="855" end_char="859">these</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="861" end_char="869">molecular</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="871" end_char="880">biologists</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="882" end_char="886">could</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="888" end_char="889">be</TOKEN>
<TOKEN id="token-4-14" pos="punct" morph="none" start_char="890" end_char="890">,</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="892" end_char="901">Montagnier</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="903" end_char="906">said</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="908" end_char="909">it</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="911" end_char="916">wasn’t</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="918" end_char="922">clear</TOKEN>
<TOKEN id="token-4-20" pos="punct" morph="none" start_char="923" end_char="923">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="925" end_char="968">
<ORIGINAL_TEXT>"My job," he said, "is to expose the facts."</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="punct" morph="none" start_char="925" end_char="925">"</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="926" end_char="927">My</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="929" end_char="931">job</TOKEN>
<TOKEN id="token-5-3" pos="punct" morph="none" start_char="932" end_char="933">,"</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="935" end_char="936">he</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="938" end_char="941">said</TOKEN>
<TOKEN id="token-5-6" pos="punct" morph="none" start_char="942" end_char="942">,</TOKEN>
<TOKEN id="token-5-7" pos="punct" morph="none" start_char="944" end_char="944">"</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="945" end_char="946">is</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="948" end_char="949">to</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="951" end_char="956">expose</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="958" end_char="960">the</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="962" end_char="966">facts</TOKEN>
<TOKEN id="token-5-13" pos="punct" morph="none" start_char="967" end_char="968">."</TOKEN>
</SEG>
<SEG id="segment-6" start_char="971" end_char="1104">
<ORIGINAL_TEXT>Montagnier said that "he didn’t know who had done it, or why," but suggested that possibly the goal had been to make an AIDS vaccine."</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="971" end_char="980">Montagnier</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="982" end_char="985">said</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="987" end_char="990">that</TOKEN>
<TOKEN id="token-6-3" pos="punct" morph="none" start_char="992" end_char="992">"</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="993" end_char="994">he</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="996" end_char="1001">didn’t</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="1003" end_char="1006">know</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="1008" end_char="1010">who</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="1012" end_char="1014">had</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="1016" end_char="1019">done</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="1021" end_char="1022">it</TOKEN>
<TOKEN id="token-6-11" pos="punct" morph="none" start_char="1023" end_char="1023">,</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="1025" end_char="1026">or</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="1028" end_char="1030">why</TOKEN>
<TOKEN id="token-6-14" pos="punct" morph="none" start_char="1031" end_char="1032">,"</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="1034" end_char="1036">but</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="1038" end_char="1046">suggested</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="1048" end_char="1051">that</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="1053" end_char="1060">possibly</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="1062" end_char="1064">the</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="1066" end_char="1069">goal</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="1071" end_char="1073">had</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="1075" end_char="1078">been</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="1080" end_char="1081">to</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="1083" end_char="1086">make</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="1088" end_char="1089">an</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="1091" end_char="1094">AIDS</TOKEN>
<TOKEN id="token-6-27" pos="word" morph="none" start_char="1096" end_char="1102">vaccine</TOKEN>
<TOKEN id="token-6-28" pos="punct" morph="none" start_char="1103" end_char="1104">."</TOKEN>
</SEG>
<SEG id="segment-7" start_char="1107" end_char="1243">
<ORIGINAL_TEXT>But he said that the lab virus is "a professional job… a very meticulous job," describing its genome as being a "clockwork of sequences."</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="1107" end_char="1109">But</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="1111" end_char="1112">he</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="1114" end_char="1117">said</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="1119" end_char="1122">that</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="1124" end_char="1126">the</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="1128" end_char="1130">lab</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="1132" end_char="1136">virus</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="1138" end_char="1139">is</TOKEN>
<TOKEN id="token-7-8" pos="punct" morph="none" start_char="1141" end_char="1141">"</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="1142" end_char="1142">a</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="1144" end_char="1155">professional</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="1157" end_char="1159">job</TOKEN>
<TOKEN id="token-7-12" pos="punct" morph="none" start_char="1160" end_char="1160">…</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="1162" end_char="1162">a</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="1164" end_char="1167">very</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="1169" end_char="1178">meticulous</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="1180" end_char="1182">job</TOKEN>
<TOKEN id="token-7-17" pos="punct" morph="none" start_char="1183" end_char="1184">,"</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="1186" end_char="1195">describing</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="1197" end_char="1199">its</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="1201" end_char="1206">genome</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="1208" end_char="1209">as</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="1211" end_char="1215">being</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="1217" end_char="1217">a</TOKEN>
<TOKEN id="token-7-24" pos="punct" morph="none" start_char="1219" end_char="1219">"</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="1220" end_char="1228">clockwork</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="1230" end_char="1231">of</TOKEN>
<TOKEN id="token-7-27" pos="word" morph="none" start_char="1233" end_char="1241">sequences</TOKEN>
<TOKEN id="token-7-28" pos="punct" morph="none" start_char="1242" end_char="1243">."</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1246" end_char="1429">
<ORIGINAL_TEXT>"There’s a part which is obviously the classic virus, and there’s another mainly coming from the bat, but that part has added sequences, particularly from HIV–the AIDS virus," he said.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="punct" morph="none" start_char="1246" end_char="1246">"</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="1247" end_char="1253">There’s</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="1255" end_char="1255">a</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1257" end_char="1260">part</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1262" end_char="1266">which</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1268" end_char="1269">is</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="1271" end_char="1279">obviously</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="1281" end_char="1283">the</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1285" end_char="1291">classic</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="1293" end_char="1297">virus</TOKEN>
<TOKEN id="token-8-10" pos="punct" morph="none" start_char="1298" end_char="1298">,</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="1300" end_char="1302">and</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="1304" end_char="1310">there’s</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="1312" end_char="1318">another</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="1320" end_char="1325">mainly</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="1327" end_char="1332">coming</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="1334" end_char="1337">from</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="1339" end_char="1341">the</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="1343" end_char="1345">bat</TOKEN>
<TOKEN id="token-8-19" pos="punct" morph="none" start_char="1346" end_char="1346">,</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="1348" end_char="1350">but</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="1352" end_char="1355">that</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="1357" end_char="1360">part</TOKEN>
<TOKEN id="token-8-23" pos="word" morph="none" start_char="1362" end_char="1364">has</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="1366" end_char="1370">added</TOKEN>
<TOKEN id="token-8-25" pos="word" morph="none" start_char="1372" end_char="1380">sequences</TOKEN>
<TOKEN id="token-8-26" pos="punct" morph="none" start_char="1381" end_char="1381">,</TOKEN>
<TOKEN id="token-8-27" pos="word" morph="none" start_char="1383" end_char="1394">particularly</TOKEN>
<TOKEN id="token-8-28" pos="word" morph="none" start_char="1396" end_char="1399">from</TOKEN>
<TOKEN id="token-8-29" pos="unknown" morph="none" start_char="1401" end_char="1407">HIV–the</TOKEN>
<TOKEN id="token-8-30" pos="word" morph="none" start_char="1409" end_char="1412">AIDS</TOKEN>
<TOKEN id="token-8-31" pos="word" morph="none" start_char="1414" end_char="1418">virus</TOKEN>
<TOKEN id="token-8-32" pos="punct" morph="none" start_char="1419" end_char="1420">,"</TOKEN>
<TOKEN id="token-8-33" pos="word" morph="none" start_char="1422" end_char="1423">he</TOKEN>
<TOKEN id="token-8-34" pos="word" morph="none" start_char="1425" end_char="1428">said</TOKEN>
<TOKEN id="token-8-35" pos="punct" morph="none" start_char="1429" end_char="1429">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1432" end_char="1439">
<ORIGINAL_TEXT>'Absurd'</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="punct" morph="none" start_char="1432" end_char="1432">'</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1433" end_char="1438">Absurd</TOKEN>
<TOKEN id="token-9-2" pos="punct" morph="none" start_char="1439" end_char="1439">'</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1442" end_char="1540">
<ORIGINAL_TEXT>Other experts, however, are not buying Montagnier's explanation, with one even calling it "absurd."</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1442" end_char="1446">Other</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1448" end_char="1454">experts</TOKEN>
<TOKEN id="token-10-2" pos="punct" morph="none" start_char="1455" end_char="1455">,</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1457" end_char="1463">however</TOKEN>
<TOKEN id="token-10-4" pos="punct" morph="none" start_char="1464" end_char="1464">,</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1466" end_char="1468">are</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1470" end_char="1472">not</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1474" end_char="1479">buying</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1481" end_char="1492">Montagnier's</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1494" end_char="1504">explanation</TOKEN>
<TOKEN id="token-10-10" pos="punct" morph="none" start_char="1505" end_char="1505">,</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1507" end_char="1510">with</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1512" end_char="1514">one</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1516" end_char="1519">even</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1521" end_char="1527">calling</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1529" end_char="1530">it</TOKEN>
<TOKEN id="token-10-16" pos="punct" morph="none" start_char="1532" end_char="1532">"</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1533" end_char="1538">absurd</TOKEN>
<TOKEN id="token-10-18" pos="punct" morph="none" start_char="1539" end_char="1540">."</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1543" end_char="1792">
<ORIGINAL_TEXT>Virologist Etienne Simon-Lorière from the Pasteur Institute in Paris said suggesting that the virus was man-made because of the "small... pieces of genomes" in it that are anyway found in other viruses of the same family simply "does not make sense."</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1543" end_char="1552">Virologist</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1554" end_char="1560">Etienne</TOKEN>
<TOKEN id="token-11-2" pos="unknown" morph="none" start_char="1562" end_char="1574">Simon-Lorière</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1576" end_char="1579">from</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1581" end_char="1583">the</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1585" end_char="1591">Pasteur</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1593" end_char="1601">Institute</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1603" end_char="1604">in</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1606" end_char="1610">Paris</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1612" end_char="1615">said</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1617" end_char="1626">suggesting</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1628" end_char="1631">that</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1633" end_char="1635">the</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1637" end_char="1641">virus</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1643" end_char="1645">was</TOKEN>
<TOKEN id="token-11-15" pos="unknown" morph="none" start_char="1647" end_char="1654">man-made</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1656" end_char="1662">because</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1664" end_char="1665">of</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1667" end_char="1669">the</TOKEN>
<TOKEN id="token-11-19" pos="punct" morph="none" start_char="1671" end_char="1671">"</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1672" end_char="1676">small</TOKEN>
<TOKEN id="token-11-21" pos="punct" morph="none" start_char="1677" end_char="1679">...</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1681" end_char="1686">pieces</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1688" end_char="1689">of</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1691" end_char="1697">genomes</TOKEN>
<TOKEN id="token-11-25" pos="punct" morph="none" start_char="1698" end_char="1698">"</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="1700" end_char="1701">in</TOKEN>
<TOKEN id="token-11-27" pos="word" morph="none" start_char="1703" end_char="1704">it</TOKEN>
<TOKEN id="token-11-28" pos="word" morph="none" start_char="1706" end_char="1709">that</TOKEN>
<TOKEN id="token-11-29" pos="word" morph="none" start_char="1711" end_char="1713">are</TOKEN>
<TOKEN id="token-11-30" pos="word" morph="none" start_char="1715" end_char="1720">anyway</TOKEN>
<TOKEN id="token-11-31" pos="word" morph="none" start_char="1722" end_char="1726">found</TOKEN>
<TOKEN id="token-11-32" pos="word" morph="none" start_char="1728" end_char="1729">in</TOKEN>
<TOKEN id="token-11-33" pos="word" morph="none" start_char="1731" end_char="1735">other</TOKEN>
<TOKEN id="token-11-34" pos="word" morph="none" start_char="1737" end_char="1743">viruses</TOKEN>
<TOKEN id="token-11-35" pos="word" morph="none" start_char="1745" end_char="1746">of</TOKEN>
<TOKEN id="token-11-36" pos="word" morph="none" start_char="1748" end_char="1750">the</TOKEN>
<TOKEN id="token-11-37" pos="word" morph="none" start_char="1752" end_char="1755">same</TOKEN>
<TOKEN id="token-11-38" pos="word" morph="none" start_char="1757" end_char="1762">family</TOKEN>
<TOKEN id="token-11-39" pos="word" morph="none" start_char="1764" end_char="1769">simply</TOKEN>
<TOKEN id="token-11-40" pos="punct" morph="none" start_char="1771" end_char="1771">"</TOKEN>
<TOKEN id="token-11-41" pos="word" morph="none" start_char="1772" end_char="1775">does</TOKEN>
<TOKEN id="token-11-42" pos="word" morph="none" start_char="1777" end_char="1779">not</TOKEN>
<TOKEN id="token-11-43" pos="word" morph="none" start_char="1781" end_char="1784">make</TOKEN>
<TOKEN id="token-11-44" pos="word" morph="none" start_char="1786" end_char="1790">sense</TOKEN>
<TOKEN id="token-11-45" pos="punct" morph="none" start_char="1791" end_char="1792">."</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1795" end_char="1940">
<ORIGINAL_TEXT>He said the genome pieces found in the SARS-CoV-2 "actually look like lots of sequences in the genetic material of bacteria, viruses and plants...</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1795" end_char="1796">He</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1798" end_char="1801">said</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1803" end_char="1805">the</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1807" end_char="1812">genome</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1814" end_char="1819">pieces</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1821" end_char="1825">found</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1827" end_char="1828">in</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1830" end_char="1832">the</TOKEN>
<TOKEN id="token-12-8" pos="unknown" morph="none" start_char="1834" end_char="1843">SARS-CoV-2</TOKEN>
<TOKEN id="token-12-9" pos="punct" morph="none" start_char="1845" end_char="1845">"</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1846" end_char="1853">actually</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1855" end_char="1858">look</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1860" end_char="1863">like</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1865" end_char="1868">lots</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1870" end_char="1871">of</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1873" end_char="1881">sequences</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1883" end_char="1884">in</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1886" end_char="1888">the</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="1890" end_char="1896">genetic</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="1898" end_char="1905">material</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1907" end_char="1908">of</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1910" end_char="1917">bacteria</TOKEN>
<TOKEN id="token-12-22" pos="punct" morph="none" start_char="1918" end_char="1918">,</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1920" end_char="1926">viruses</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1928" end_char="1930">and</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1932" end_char="1937">plants</TOKEN>
<TOKEN id="token-12-26" pos="punct" morph="none" start_char="1938" end_char="1940">...</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1942" end_char="2064">
<ORIGINAL_TEXT>If we take a word from a book and that word resembles that of another book, can we say that one has copied from the other?"</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1942" end_char="1943">If</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1945" end_char="1946">we</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1948" end_char="1951">take</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1953" end_char="1953">a</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1955" end_char="1958">word</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1960" end_char="1963">from</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1965" end_char="1965">a</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1967" end_char="1970">book</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1972" end_char="1974">and</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1976" end_char="1979">that</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1981" end_char="1984">word</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1986" end_char="1994">resembles</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1996" end_char="1999">that</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="2001" end_char="2002">of</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="2004" end_char="2010">another</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="2012" end_char="2015">book</TOKEN>
<TOKEN id="token-13-16" pos="punct" morph="none" start_char="2016" end_char="2016">,</TOKEN>
<TOKEN id="token-13-17" pos="word" morph="none" start_char="2018" end_char="2020">can</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="2022" end_char="2023">we</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="2025" end_char="2027">say</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="2029" end_char="2032">that</TOKEN>
<TOKEN id="token-13-21" pos="word" morph="none" start_char="2034" end_char="2036">one</TOKEN>
<TOKEN id="token-13-22" pos="word" morph="none" start_char="2038" end_char="2040">has</TOKEN>
<TOKEN id="token-13-23" pos="word" morph="none" start_char="2042" end_char="2047">copied</TOKEN>
<TOKEN id="token-13-24" pos="word" morph="none" start_char="2049" end_char="2052">from</TOKEN>
<TOKEN id="token-13-25" pos="word" morph="none" start_char="2054" end_char="2056">the</TOKEN>
<TOKEN id="token-13-26" pos="word" morph="none" start_char="2058" end_char="2062">other</TOKEN>
<TOKEN id="token-13-27" pos="punct" morph="none" start_char="2063" end_char="2064">?"</TOKEN>
</SEG>
<SEG id="segment-14" start_char="2067" end_char="2242">
<ORIGINAL_TEXT>Others noted that the paper cited by Montagnier to back up his theory and published by researchers in India was not peer-reviewed and had already been withdrawn by its authors.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="2067" end_char="2072">Others</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="2074" end_char="2078">noted</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="2080" end_char="2083">that</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="2085" end_char="2087">the</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="2089" end_char="2093">paper</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="2095" end_char="2099">cited</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="2101" end_char="2102">by</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="2104" end_char="2113">Montagnier</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="2115" end_char="2116">to</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="2118" end_char="2121">back</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="2123" end_char="2124">up</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="2126" end_char="2128">his</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="2130" end_char="2135">theory</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="2137" end_char="2139">and</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="2141" end_char="2149">published</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="2151" end_char="2152">by</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="2154" end_char="2164">researchers</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="2166" end_char="2167">in</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="2169" end_char="2173">India</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="2175" end_char="2177">was</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="2179" end_char="2181">not</TOKEN>
<TOKEN id="token-14-21" pos="unknown" morph="none" start_char="2183" end_char="2195">peer-reviewed</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="2197" end_char="2199">and</TOKEN>
<TOKEN id="token-14-23" pos="word" morph="none" start_char="2201" end_char="2203">had</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="2205" end_char="2211">already</TOKEN>
<TOKEN id="token-14-25" pos="word" morph="none" start_char="2213" end_char="2216">been</TOKEN>
<TOKEN id="token-14-26" pos="word" morph="none" start_char="2218" end_char="2226">withdrawn</TOKEN>
<TOKEN id="token-14-27" pos="word" morph="none" start_char="2228" end_char="2229">by</TOKEN>
<TOKEN id="token-14-28" pos="word" morph="none" start_char="2231" end_char="2233">its</TOKEN>
<TOKEN id="token-14-29" pos="word" morph="none" start_char="2235" end_char="2241">authors</TOKEN>
<TOKEN id="token-14-30" pos="punct" morph="none" start_char="2242" end_char="2242">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="2245" end_char="2268">
<ORIGINAL_TEXT>COVID-19 origin question</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="unknown" morph="none" start_char="2245" end_char="2252">COVID-19</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="2254" end_char="2259">origin</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="2261" end_char="2268">question</TOKEN>
</SEG>
<SEG id="segment-16" start_char="2271" end_char="2473">
<ORIGINAL_TEXT>Montagnier asserted that questions on the true origin of the virus would continue to be raised for a long time as the whole world has already spent trillions of dollars in the fight against the pandemic.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="2271" end_char="2280">Montagnier</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="2282" end_char="2289">asserted</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="2291" end_char="2294">that</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="2296" end_char="2304">questions</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="2306" end_char="2307">on</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="2309" end_char="2311">the</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="2313" end_char="2316">true</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="2318" end_char="2323">origin</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="2325" end_char="2326">of</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="2328" end_char="2330">the</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="2332" end_char="2336">virus</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="2338" end_char="2342">would</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="2344" end_char="2351">continue</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2353" end_char="2354">to</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="2356" end_char="2357">be</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="2359" end_char="2364">raised</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="2366" end_char="2368">for</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="2370" end_char="2370">a</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="2372" end_char="2375">long</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="2377" end_char="2380">time</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="2382" end_char="2383">as</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="2385" end_char="2387">the</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="2389" end_char="2393">whole</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="2395" end_char="2399">world</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="2401" end_char="2403">has</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="2405" end_char="2411">already</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="2413" end_char="2417">spent</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="2419" end_char="2427">trillions</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="2429" end_char="2430">of</TOKEN>
<TOKEN id="token-16-29" pos="word" morph="none" start_char="2432" end_char="2438">dollars</TOKEN>
<TOKEN id="token-16-30" pos="word" morph="none" start_char="2440" end_char="2441">in</TOKEN>
<TOKEN id="token-16-31" pos="word" morph="none" start_char="2443" end_char="2445">the</TOKEN>
<TOKEN id="token-16-32" pos="word" morph="none" start_char="2447" end_char="2451">fight</TOKEN>
<TOKEN id="token-16-33" pos="word" morph="none" start_char="2453" end_char="2459">against</TOKEN>
<TOKEN id="token-16-34" pos="word" morph="none" start_char="2461" end_char="2463">the</TOKEN>
<TOKEN id="token-16-35" pos="word" morph="none" start_char="2465" end_char="2472">pandemic</TOKEN>
<TOKEN id="token-16-36" pos="punct" morph="none" start_char="2473" end_char="2473">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2476" end_char="2646">
<ORIGINAL_TEXT>He inferred that the allegedly man-made virus, whose genome consists of a "clockwork of sequences" and includes elements of HIV, could not have been assembled by amateurs.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2476" end_char="2477">He</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2479" end_char="2486">inferred</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2488" end_char="2491">that</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2493" end_char="2495">the</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2497" end_char="2505">allegedly</TOKEN>
<TOKEN id="token-17-5" pos="unknown" morph="none" start_char="2507" end_char="2514">man-made</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2516" end_char="2520">virus</TOKEN>
<TOKEN id="token-17-7" pos="punct" morph="none" start_char="2521" end_char="2521">,</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="2523" end_char="2527">whose</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="2529" end_char="2534">genome</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="2536" end_char="2543">consists</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="2545" end_char="2546">of</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="2548" end_char="2548">a</TOKEN>
<TOKEN id="token-17-13" pos="punct" morph="none" start_char="2550" end_char="2550">"</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="2551" end_char="2559">clockwork</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="2561" end_char="2562">of</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="2564" end_char="2572">sequences</TOKEN>
<TOKEN id="token-17-17" pos="punct" morph="none" start_char="2573" end_char="2573">"</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="2575" end_char="2577">and</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="2579" end_char="2586">includes</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="2588" end_char="2595">elements</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="2597" end_char="2598">of</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="2600" end_char="2602">HIV</TOKEN>
<TOKEN id="token-17-23" pos="punct" morph="none" start_char="2603" end_char="2603">,</TOKEN>
<TOKEN id="token-17-24" pos="word" morph="none" start_char="2605" end_char="2609">could</TOKEN>
<TOKEN id="token-17-25" pos="word" morph="none" start_char="2611" end_char="2613">not</TOKEN>
<TOKEN id="token-17-26" pos="word" morph="none" start_char="2615" end_char="2618">have</TOKEN>
<TOKEN id="token-17-27" pos="word" morph="none" start_char="2620" end_char="2623">been</TOKEN>
<TOKEN id="token-17-28" pos="word" morph="none" start_char="2625" end_char="2633">assembled</TOKEN>
<TOKEN id="token-17-29" pos="word" morph="none" start_char="2635" end_char="2636">by</TOKEN>
<TOKEN id="token-17-30" pos="word" morph="none" start_char="2638" end_char="2645">amateurs</TOKEN>
<TOKEN id="token-17-31" pos="punct" morph="none" start_char="2646" end_char="2646">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2649" end_char="2753">
<ORIGINAL_TEXT>And questions about its origin and purpose would not escape scrutiny, and are unlikely to disappear soon.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2649" end_char="2651">And</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2653" end_char="2661">questions</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="2663" end_char="2667">about</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="2669" end_char="2671">its</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2673" end_char="2678">origin</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="2680" end_char="2682">and</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2684" end_char="2690">purpose</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2692" end_char="2696">would</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2698" end_char="2700">not</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2702" end_char="2707">escape</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2709" end_char="2716">scrutiny</TOKEN>
<TOKEN id="token-18-11" pos="punct" morph="none" start_char="2717" end_char="2717">,</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="2719" end_char="2721">and</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="2723" end_char="2725">are</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="2727" end_char="2734">unlikely</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="2736" end_char="2737">to</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="2739" end_char="2747">disappear</TOKEN>
<TOKEN id="token-18-17" pos="word" morph="none" start_char="2749" end_char="2752">soon</TOKEN>
<TOKEN id="token-18-18" pos="punct" morph="none" start_char="2753" end_char="2753">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2756" end_char="2855">
<ORIGINAL_TEXT>On April 3, Blomberg reported that the global cost of COVID-19 was estimated to be at $4.1 trillion.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2756" end_char="2757">On</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2759" end_char="2763">April</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2765" end_char="2765">3</TOKEN>
<TOKEN id="token-19-3" pos="punct" morph="none" start_char="2766" end_char="2766">,</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2768" end_char="2775">Blomberg</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="2777" end_char="2784">reported</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2786" end_char="2789">that</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="2791" end_char="2793">the</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="2795" end_char="2800">global</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="2802" end_char="2805">cost</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="2807" end_char="2808">of</TOKEN>
<TOKEN id="token-19-11" pos="unknown" morph="none" start_char="2810" end_char="2817">COVID-19</TOKEN>
<TOKEN id="token-19-12" pos="word" morph="none" start_char="2819" end_char="2821">was</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="2823" end_char="2831">estimated</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="2833" end_char="2834">to</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="2836" end_char="2837">be</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="2839" end_char="2840">at</TOKEN>
<TOKEN id="token-19-17" pos="unknown" morph="none" start_char="2842" end_char="2845">$4.1</TOKEN>
<TOKEN id="token-19-18" pos="word" morph="none" start_char="2847" end_char="2854">trillion</TOKEN>
<TOKEN id="token-19-19" pos="punct" morph="none" start_char="2855" end_char="2855">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2858" end_char="3028">
<ORIGINAL_TEXT>A CNBC report on April 9 predicted that the resulting coronavirus depression would be bigger than the one in 2008, and would bleed the world of up to $20 trillion or more.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2858" end_char="2858">A</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="2860" end_char="2863">CNBC</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="2865" end_char="2870">report</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="2872" end_char="2873">on</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2875" end_char="2879">April</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2881" end_char="2881">9</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2883" end_char="2891">predicted</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="2893" end_char="2896">that</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="2898" end_char="2900">the</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="2902" end_char="2910">resulting</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="2912" end_char="2922">coronavirus</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="2924" end_char="2933">depression</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="2935" end_char="2939">would</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="2941" end_char="2942">be</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="2944" end_char="2949">bigger</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="2951" end_char="2954">than</TOKEN>
<TOKEN id="token-20-16" pos="word" morph="none" start_char="2956" end_char="2958">the</TOKEN>
<TOKEN id="token-20-17" pos="word" morph="none" start_char="2960" end_char="2962">one</TOKEN>
<TOKEN id="token-20-18" pos="word" morph="none" start_char="2964" end_char="2965">in</TOKEN>
<TOKEN id="token-20-19" pos="word" morph="none" start_char="2967" end_char="2970">2008</TOKEN>
<TOKEN id="token-20-20" pos="punct" morph="none" start_char="2971" end_char="2971">,</TOKEN>
<TOKEN id="token-20-21" pos="word" morph="none" start_char="2973" end_char="2975">and</TOKEN>
<TOKEN id="token-20-22" pos="word" morph="none" start_char="2977" end_char="2981">would</TOKEN>
<TOKEN id="token-20-23" pos="word" morph="none" start_char="2983" end_char="2987">bleed</TOKEN>
<TOKEN id="token-20-24" pos="word" morph="none" start_char="2989" end_char="2991">the</TOKEN>
<TOKEN id="token-20-25" pos="word" morph="none" start_char="2993" end_char="2997">world</TOKEN>
<TOKEN id="token-20-26" pos="word" morph="none" start_char="2999" end_char="3000">of</TOKEN>
<TOKEN id="token-20-27" pos="word" morph="none" start_char="3002" end_char="3003">up</TOKEN>
<TOKEN id="token-20-28" pos="word" morph="none" start_char="3005" end_char="3006">to</TOKEN>
<TOKEN id="token-20-29" pos="unknown" morph="none" start_char="3008" end_char="3010">$20</TOKEN>
<TOKEN id="token-20-30" pos="word" morph="none" start_char="3012" end_char="3019">trillion</TOKEN>
<TOKEN id="token-20-31" pos="word" morph="none" start_char="3021" end_char="3022">or</TOKEN>
<TOKEN id="token-20-32" pos="word" morph="none" start_char="3024" end_char="3027">more</TOKEN>
<TOKEN id="token-20-33" pos="punct" morph="none" start_char="3028" end_char="3028">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="3031" end_char="3145">
<ORIGINAL_TEXT>As of May 1, 2020 the global total of COVID-19 cases was at 3,308,233 with 234,105 deaths and 1,042,819 recoveries.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="3031" end_char="3032">As</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="3034" end_char="3035">of</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="3037" end_char="3039">May</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="3041" end_char="3041">1</TOKEN>
<TOKEN id="token-21-4" pos="punct" morph="none" start_char="3042" end_char="3042">,</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="3044" end_char="3047">2020</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="3049" end_char="3051">the</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="3053" end_char="3058">global</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="3060" end_char="3064">total</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="3066" end_char="3067">of</TOKEN>
<TOKEN id="token-21-10" pos="unknown" morph="none" start_char="3069" end_char="3076">COVID-19</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="3078" end_char="3082">cases</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="3084" end_char="3086">was</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="3088" end_char="3089">at</TOKEN>
<TOKEN id="token-21-14" pos="unknown" morph="none" start_char="3091" end_char="3099">3,308,233</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="3101" end_char="3104">with</TOKEN>
<TOKEN id="token-21-16" pos="unknown" morph="none" start_char="3106" end_char="3112">234,105</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="3114" end_char="3119">deaths</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="3121" end_char="3123">and</TOKEN>
<TOKEN id="token-21-19" pos="unknown" morph="none" start_char="3125" end_char="3133">1,042,819</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="3135" end_char="3144">recoveries</TOKEN>
<TOKEN id="token-21-21" pos="punct" morph="none" start_char="3145" end_char="3145">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="3147" end_char="3160">
<ORIGINAL_TEXT>—MDM, GMA News</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="punct" morph="none" start_char="3147" end_char="3147">—</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="3148" end_char="3150">MDM</TOKEN>
<TOKEN id="token-22-2" pos="punct" morph="none" start_char="3151" end_char="3151">,</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="3153" end_char="3155">GMA</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="3157" end_char="3160">News</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
