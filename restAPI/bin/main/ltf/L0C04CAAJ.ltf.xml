<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CAAJ" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="2849" raw_text_md5="5131fc1bf576506a14031ded6ca1190d">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="57">
<ORIGINAL_TEXT>Chinese scientists now say India is origin of coronavirus</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="7">Chinese</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="9" end_char="18">scientists</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="20" end_char="22">now</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="24" end_char="26">say</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="28" end_char="32">India</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="34" end_char="35">is</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="37" end_char="42">origin</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="44" end_char="45">of</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="47" end_char="57">coronavirus</TOKEN>
</SEG>
<SEG id="segment-1" start_char="61" end_char="81">
<ORIGINAL_TEXT>Representative image.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="61" end_char="74">Representative</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="76" end_char="80">image</TOKEN>
<TOKEN id="token-1-2" pos="punct" morph="none" start_char="81" end_char="81">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="83" end_char="99">
<ORIGINAL_TEXT>Credit: AFP Photo</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="83" end_char="88">Credit</TOKEN>
<TOKEN id="token-2-1" pos="punct" morph="none" start_char="89" end_char="89">:</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="91" end_char="93">AFP</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="95" end_char="99">Photo</TOKEN>
</SEG>
<SEG id="segment-3" start_char="103" end_char="345">
<ORIGINAL_TEXT>In what has been a fresh development in the India-China rivalry and the Covid-19 pandemic, a team from China’s top science institution, Chinese Academy of Sciences in a paper argues that SARS-CoV-2 likely originated in India in summer of 2019.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="103" end_char="104">In</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="106" end_char="109">what</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="111" end_char="113">has</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="115" end_char="118">been</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="120" end_char="120">a</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="122" end_char="126">fresh</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="128" end_char="138">development</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="140" end_char="141">in</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="143" end_char="145">the</TOKEN>
<TOKEN id="token-3-9" pos="unknown" morph="none" start_char="147" end_char="157">India-China</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="159" end_char="165">rivalry</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="167" end_char="169">and</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="171" end_char="173">the</TOKEN>
<TOKEN id="token-3-13" pos="unknown" morph="none" start_char="175" end_char="182">Covid-19</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="184" end_char="191">pandemic</TOKEN>
<TOKEN id="token-3-15" pos="punct" morph="none" start_char="192" end_char="192">,</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="194" end_char="194">a</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="196" end_char="199">team</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="201" end_char="204">from</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="206" end_char="212">China’s</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="214" end_char="216">top</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="218" end_char="224">science</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="226" end_char="236">institution</TOKEN>
<TOKEN id="token-3-23" pos="punct" morph="none" start_char="237" end_char="237">,</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="239" end_char="245">Chinese</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="247" end_char="253">Academy</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="255" end_char="256">of</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="258" end_char="265">Sciences</TOKEN>
<TOKEN id="token-3-28" pos="word" morph="none" start_char="267" end_char="268">in</TOKEN>
<TOKEN id="token-3-29" pos="word" morph="none" start_char="270" end_char="270">a</TOKEN>
<TOKEN id="token-3-30" pos="word" morph="none" start_char="272" end_char="276">paper</TOKEN>
<TOKEN id="token-3-31" pos="word" morph="none" start_char="278" end_char="283">argues</TOKEN>
<TOKEN id="token-3-32" pos="word" morph="none" start_char="285" end_char="288">that</TOKEN>
<TOKEN id="token-3-33" pos="unknown" morph="none" start_char="290" end_char="299">SARS-CoV-2</TOKEN>
<TOKEN id="token-3-34" pos="word" morph="none" start_char="301" end_char="306">likely</TOKEN>
<TOKEN id="token-3-35" pos="word" morph="none" start_char="308" end_char="317">originated</TOKEN>
<TOKEN id="token-3-36" pos="word" morph="none" start_char="319" end_char="320">in</TOKEN>
<TOKEN id="token-3-37" pos="word" morph="none" start_char="322" end_char="326">India</TOKEN>
<TOKEN id="token-3-38" pos="word" morph="none" start_char="328" end_char="329">in</TOKEN>
<TOKEN id="token-3-39" pos="word" morph="none" start_char="331" end_char="336">summer</TOKEN>
<TOKEN id="token-3-40" pos="word" morph="none" start_char="338" end_char="339">of</TOKEN>
<TOKEN id="token-3-41" pos="word" morph="none" start_char="341" end_char="344">2019</TOKEN>
<TOKEN id="token-3-42" pos="punct" morph="none" start_char="345" end_char="345">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="348" end_char="459">
<ORIGINAL_TEXT>The scientists claim that Wuhan is the place where the first human to human transmission of SARS-CoV-2 happened.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="348" end_char="350">The</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="352" end_char="361">scientists</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="363" end_char="367">claim</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="369" end_char="372">that</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="374" end_char="378">Wuhan</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="380" end_char="381">is</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="383" end_char="385">the</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="387" end_char="391">place</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="393" end_char="397">where</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="399" end_char="401">the</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="403" end_char="407">first</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="409" end_char="413">human</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="415" end_char="416">to</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="418" end_char="422">human</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="424" end_char="435">transmission</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="437" end_char="438">of</TOKEN>
<TOKEN id="token-4-16" pos="unknown" morph="none" start_char="440" end_char="449">SARS-CoV-2</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="451" end_char="458">happened</TOKEN>
<TOKEN id="token-4-18" pos="punct" morph="none" start_char="459" end_char="459">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="461" end_char="838">
<ORIGINAL_TEXT>Before the virus had spread to Wuhan, it had already seen some evolution in previous transmission between humans, says the paper, adding that, the least mutated strain’s geographic information and strain diversity suggest that the Indian subcontinent might be the place where the first human to human transmission of the virus occurred, a few months prior to the Wuhan outbreak.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="461" end_char="466">Before</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="468" end_char="470">the</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="472" end_char="476">virus</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="478" end_char="480">had</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="482" end_char="487">spread</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="489" end_char="490">to</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="492" end_char="496">Wuhan</TOKEN>
<TOKEN id="token-5-7" pos="punct" morph="none" start_char="497" end_char="497">,</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="499" end_char="500">it</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="502" end_char="504">had</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="506" end_char="512">already</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="514" end_char="517">seen</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="519" end_char="522">some</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="524" end_char="532">evolution</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="534" end_char="535">in</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="537" end_char="544">previous</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="546" end_char="557">transmission</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="559" end_char="565">between</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="567" end_char="572">humans</TOKEN>
<TOKEN id="token-5-19" pos="punct" morph="none" start_char="573" end_char="573">,</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="575" end_char="578">says</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="580" end_char="582">the</TOKEN>
<TOKEN id="token-5-22" pos="word" morph="none" start_char="584" end_char="588">paper</TOKEN>
<TOKEN id="token-5-23" pos="punct" morph="none" start_char="589" end_char="589">,</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="591" end_char="596">adding</TOKEN>
<TOKEN id="token-5-25" pos="word" morph="none" start_char="598" end_char="601">that</TOKEN>
<TOKEN id="token-5-26" pos="punct" morph="none" start_char="602" end_char="602">,</TOKEN>
<TOKEN id="token-5-27" pos="word" morph="none" start_char="604" end_char="606">the</TOKEN>
<TOKEN id="token-5-28" pos="word" morph="none" start_char="608" end_char="612">least</TOKEN>
<TOKEN id="token-5-29" pos="word" morph="none" start_char="614" end_char="620">mutated</TOKEN>
<TOKEN id="token-5-30" pos="word" morph="none" start_char="622" end_char="629">strain’s</TOKEN>
<TOKEN id="token-5-31" pos="word" morph="none" start_char="631" end_char="640">geographic</TOKEN>
<TOKEN id="token-5-32" pos="word" morph="none" start_char="642" end_char="652">information</TOKEN>
<TOKEN id="token-5-33" pos="word" morph="none" start_char="654" end_char="656">and</TOKEN>
<TOKEN id="token-5-34" pos="word" morph="none" start_char="658" end_char="663">strain</TOKEN>
<TOKEN id="token-5-35" pos="word" morph="none" start_char="665" end_char="673">diversity</TOKEN>
<TOKEN id="token-5-36" pos="word" morph="none" start_char="675" end_char="681">suggest</TOKEN>
<TOKEN id="token-5-37" pos="word" morph="none" start_char="683" end_char="686">that</TOKEN>
<TOKEN id="token-5-38" pos="word" morph="none" start_char="688" end_char="690">the</TOKEN>
<TOKEN id="token-5-39" pos="word" morph="none" start_char="692" end_char="697">Indian</TOKEN>
<TOKEN id="token-5-40" pos="word" morph="none" start_char="699" end_char="710">subcontinent</TOKEN>
<TOKEN id="token-5-41" pos="word" morph="none" start_char="712" end_char="716">might</TOKEN>
<TOKEN id="token-5-42" pos="word" morph="none" start_char="718" end_char="719">be</TOKEN>
<TOKEN id="token-5-43" pos="word" morph="none" start_char="721" end_char="723">the</TOKEN>
<TOKEN id="token-5-44" pos="word" morph="none" start_char="725" end_char="729">place</TOKEN>
<TOKEN id="token-5-45" pos="word" morph="none" start_char="731" end_char="735">where</TOKEN>
<TOKEN id="token-5-46" pos="word" morph="none" start_char="737" end_char="739">the</TOKEN>
<TOKEN id="token-5-47" pos="word" morph="none" start_char="741" end_char="745">first</TOKEN>
<TOKEN id="token-5-48" pos="word" morph="none" start_char="747" end_char="751">human</TOKEN>
<TOKEN id="token-5-49" pos="word" morph="none" start_char="753" end_char="754">to</TOKEN>
<TOKEN id="token-5-50" pos="word" morph="none" start_char="756" end_char="760">human</TOKEN>
<TOKEN id="token-5-51" pos="word" morph="none" start_char="762" end_char="773">transmission</TOKEN>
<TOKEN id="token-5-52" pos="word" morph="none" start_char="775" end_char="776">of</TOKEN>
<TOKEN id="token-5-53" pos="word" morph="none" start_char="778" end_char="780">the</TOKEN>
<TOKEN id="token-5-54" pos="word" morph="none" start_char="782" end_char="786">virus</TOKEN>
<TOKEN id="token-5-55" pos="word" morph="none" start_char="788" end_char="795">occurred</TOKEN>
<TOKEN id="token-5-56" pos="punct" morph="none" start_char="796" end_char="796">,</TOKEN>
<TOKEN id="token-5-57" pos="word" morph="none" start_char="798" end_char="798">a</TOKEN>
<TOKEN id="token-5-58" pos="word" morph="none" start_char="800" end_char="802">few</TOKEN>
<TOKEN id="token-5-59" pos="word" morph="none" start_char="804" end_char="809">months</TOKEN>
<TOKEN id="token-5-60" pos="word" morph="none" start_char="811" end_char="815">prior</TOKEN>
<TOKEN id="token-5-61" pos="word" morph="none" start_char="817" end_char="818">to</TOKEN>
<TOKEN id="token-5-62" pos="word" morph="none" start_char="820" end_char="822">the</TOKEN>
<TOKEN id="token-5-63" pos="word" morph="none" start_char="824" end_char="828">Wuhan</TOKEN>
<TOKEN id="token-5-64" pos="word" morph="none" start_char="830" end_char="837">outbreak</TOKEN>
<TOKEN id="token-5-65" pos="punct" morph="none" start_char="838" end_char="838">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="841" end_char="927">
<ORIGINAL_TEXT>Also read: Centre may purchase 300 to 400 million Covishield vaccine doses by July 2021</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="841" end_char="844">Also</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="846" end_char="849">read</TOKEN>
<TOKEN id="token-6-2" pos="punct" morph="none" start_char="850" end_char="850">:</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="852" end_char="857">Centre</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="859" end_char="861">may</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="863" end_char="870">purchase</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="872" end_char="874">300</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="876" end_char="877">to</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="879" end_char="881">400</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="883" end_char="889">million</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="891" end_char="900">Covishield</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="902" end_char="908">vaccine</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="910" end_char="914">doses</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="916" end_char="917">by</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="919" end_char="922">July</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="924" end_char="927">2021</TOKEN>
</SEG>
<SEG id="segment-7" start_char="930" end_char="1032">
<ORIGINAL_TEXT>This is not the first time China has blamed other countries for being the point of origin of the virus.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="930" end_char="933">This</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="935" end_char="936">is</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="938" end_char="940">not</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="942" end_char="944">the</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="946" end_char="950">first</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="952" end_char="955">time</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="957" end_char="961">China</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="963" end_char="965">has</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="967" end_char="972">blamed</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="974" end_char="978">other</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="980" end_char="988">countries</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="990" end_char="992">for</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="994" end_char="998">being</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="1000" end_char="1002">the</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="1004" end_char="1008">point</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="1010" end_char="1011">of</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="1013" end_char="1018">origin</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="1020" end_char="1021">of</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="1023" end_char="1025">the</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="1027" end_char="1031">virus</TOKEN>
<TOKEN id="token-7-20" pos="punct" morph="none" start_char="1032" end_char="1032">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1034" end_char="1119">
<ORIGINAL_TEXT>Earlier, they had pointed fingers at Italy and the US as the first sites of infection.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="1034" end_char="1040">Earlier</TOKEN>
<TOKEN id="token-8-1" pos="punct" morph="none" start_char="1041" end_char="1041">,</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="1043" end_char="1046">they</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1048" end_char="1050">had</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1052" end_char="1058">pointed</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1060" end_char="1066">fingers</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="1068" end_char="1069">at</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="1071" end_char="1075">Italy</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1077" end_char="1079">and</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="1081" end_char="1083">the</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="1085" end_char="1086">US</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="1088" end_char="1089">as</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="1091" end_char="1093">the</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="1095" end_char="1099">first</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="1101" end_char="1105">sites</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="1107" end_char="1108">of</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="1110" end_char="1118">infection</TOKEN>
<TOKEN id="token-8-17" pos="punct" morph="none" start_char="1119" end_char="1119">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1122" end_char="1308">
<ORIGINAL_TEXT>The paper, which has not been peer-reviewed yet, uses a method called phylogenetic analysis, a technique where scientists study the mutation of the virus to trace the origins of Covid-19.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1122" end_char="1124">The</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1126" end_char="1130">paper</TOKEN>
<TOKEN id="token-9-2" pos="punct" morph="none" start_char="1131" end_char="1131">,</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1133" end_char="1137">which</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1139" end_char="1141">has</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1143" end_char="1145">not</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1147" end_char="1150">been</TOKEN>
<TOKEN id="token-9-7" pos="unknown" morph="none" start_char="1152" end_char="1164">peer-reviewed</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1166" end_char="1168">yet</TOKEN>
<TOKEN id="token-9-9" pos="punct" morph="none" start_char="1169" end_char="1169">,</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1171" end_char="1174">uses</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1176" end_char="1176">a</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="1178" end_char="1183">method</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1185" end_char="1190">called</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1192" end_char="1203">phylogenetic</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1205" end_char="1212">analysis</TOKEN>
<TOKEN id="token-9-16" pos="punct" morph="none" start_char="1213" end_char="1213">,</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1215" end_char="1215">a</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1217" end_char="1225">technique</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1227" end_char="1231">where</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1233" end_char="1242">scientists</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1244" end_char="1248">study</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1250" end_char="1252">the</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="1254" end_char="1261">mutation</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1263" end_char="1264">of</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="1266" end_char="1268">the</TOKEN>
<TOKEN id="token-9-26" pos="word" morph="none" start_char="1270" end_char="1274">virus</TOKEN>
<TOKEN id="token-9-27" pos="word" morph="none" start_char="1276" end_char="1277">to</TOKEN>
<TOKEN id="token-9-28" pos="word" morph="none" start_char="1279" end_char="1283">trace</TOKEN>
<TOKEN id="token-9-29" pos="word" morph="none" start_char="1285" end_char="1287">the</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1289" end_char="1295">origins</TOKEN>
<TOKEN id="token-9-31" pos="word" morph="none" start_char="1297" end_char="1298">of</TOKEN>
<TOKEN id="token-9-32" pos="unknown" morph="none" start_char="1300" end_char="1307">Covid-19</TOKEN>
<TOKEN id="token-9-33" pos="punct" morph="none" start_char="1308" end_char="1308">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1310" end_char="1526">
<ORIGINAL_TEXT>According to the analysis, the method rules out Wuhan as a site of origin of the Coronavirus but nominates Bangladesh, the US, Greece, Australia, India, Italy, Czech Republic, Russia and Serbia as potential countries.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1310" end_char="1318">According</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1320" end_char="1321">to</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1323" end_char="1325">the</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1327" end_char="1334">analysis</TOKEN>
<TOKEN id="token-10-4" pos="punct" morph="none" start_char="1335" end_char="1335">,</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1337" end_char="1339">the</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1341" end_char="1346">method</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1348" end_char="1352">rules</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1354" end_char="1356">out</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1358" end_char="1362">Wuhan</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1364" end_char="1365">as</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1367" end_char="1367">a</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1369" end_char="1372">site</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1374" end_char="1375">of</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1377" end_char="1382">origin</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1384" end_char="1385">of</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1387" end_char="1389">the</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1391" end_char="1401">Coronavirus</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1403" end_char="1405">but</TOKEN>
<TOKEN id="token-10-19" pos="word" morph="none" start_char="1407" end_char="1415">nominates</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="1417" end_char="1426">Bangladesh</TOKEN>
<TOKEN id="token-10-21" pos="punct" morph="none" start_char="1427" end_char="1427">,</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="1429" end_char="1431">the</TOKEN>
<TOKEN id="token-10-23" pos="word" morph="none" start_char="1433" end_char="1434">US</TOKEN>
<TOKEN id="token-10-24" pos="punct" morph="none" start_char="1435" end_char="1435">,</TOKEN>
<TOKEN id="token-10-25" pos="word" morph="none" start_char="1437" end_char="1442">Greece</TOKEN>
<TOKEN id="token-10-26" pos="punct" morph="none" start_char="1443" end_char="1443">,</TOKEN>
<TOKEN id="token-10-27" pos="word" morph="none" start_char="1445" end_char="1453">Australia</TOKEN>
<TOKEN id="token-10-28" pos="punct" morph="none" start_char="1454" end_char="1454">,</TOKEN>
<TOKEN id="token-10-29" pos="word" morph="none" start_char="1456" end_char="1460">India</TOKEN>
<TOKEN id="token-10-30" pos="punct" morph="none" start_char="1461" end_char="1461">,</TOKEN>
<TOKEN id="token-10-31" pos="word" morph="none" start_char="1463" end_char="1467">Italy</TOKEN>
<TOKEN id="token-10-32" pos="punct" morph="none" start_char="1468" end_char="1468">,</TOKEN>
<TOKEN id="token-10-33" pos="word" morph="none" start_char="1470" end_char="1474">Czech</TOKEN>
<TOKEN id="token-10-34" pos="word" morph="none" start_char="1476" end_char="1483">Republic</TOKEN>
<TOKEN id="token-10-35" pos="punct" morph="none" start_char="1484" end_char="1484">,</TOKEN>
<TOKEN id="token-10-36" pos="word" morph="none" start_char="1486" end_char="1491">Russia</TOKEN>
<TOKEN id="token-10-37" pos="word" morph="none" start_char="1493" end_char="1495">and</TOKEN>
<TOKEN id="token-10-38" pos="word" morph="none" start_char="1497" end_char="1502">Serbia</TOKEN>
<TOKEN id="token-10-39" pos="word" morph="none" start_char="1504" end_char="1505">as</TOKEN>
<TOKEN id="token-10-40" pos="word" morph="none" start_char="1507" end_char="1515">potential</TOKEN>
<TOKEN id="token-10-41" pos="word" morph="none" start_char="1517" end_char="1525">countries</TOKEN>
<TOKEN id="token-10-42" pos="punct" morph="none" start_char="1526" end_char="1526">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1529" end_char="1729">
<ORIGINAL_TEXT>The team, however, says that since India and Bangladesh recorded the lowest number of mutations and are neighbours, it is possible that the Indian subcontinent is where the first transmission happened.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1529" end_char="1531">The</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1533" end_char="1536">team</TOKEN>
<TOKEN id="token-11-2" pos="punct" morph="none" start_char="1537" end_char="1537">,</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1539" end_char="1545">however</TOKEN>
<TOKEN id="token-11-4" pos="punct" morph="none" start_char="1546" end_char="1546">,</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1548" end_char="1551">says</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1553" end_char="1556">that</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1558" end_char="1562">since</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1564" end_char="1568">India</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1570" end_char="1572">and</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1574" end_char="1583">Bangladesh</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1585" end_char="1592">recorded</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1594" end_char="1596">the</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1598" end_char="1603">lowest</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1605" end_char="1610">number</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1612" end_char="1613">of</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1615" end_char="1623">mutations</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1625" end_char="1627">and</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1629" end_char="1631">are</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1633" end_char="1642">neighbours</TOKEN>
<TOKEN id="token-11-20" pos="punct" morph="none" start_char="1643" end_char="1643">,</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1645" end_char="1646">it</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1648" end_char="1649">is</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1651" end_char="1658">possible</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1660" end_char="1663">that</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1665" end_char="1667">the</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="1669" end_char="1674">Indian</TOKEN>
<TOKEN id="token-11-27" pos="word" morph="none" start_char="1676" end_char="1687">subcontinent</TOKEN>
<TOKEN id="token-11-28" pos="word" morph="none" start_char="1689" end_char="1690">is</TOKEN>
<TOKEN id="token-11-29" pos="word" morph="none" start_char="1692" end_char="1696">where</TOKEN>
<TOKEN id="token-11-30" pos="word" morph="none" start_char="1698" end_char="1700">the</TOKEN>
<TOKEN id="token-11-31" pos="word" morph="none" start_char="1702" end_char="1706">first</TOKEN>
<TOKEN id="token-11-32" pos="word" morph="none" start_char="1708" end_char="1719">transmission</TOKEN>
<TOKEN id="token-11-33" pos="word" morph="none" start_char="1721" end_char="1728">happened</TOKEN>
<TOKEN id="token-11-34" pos="punct" morph="none" start_char="1729" end_char="1729">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1731" end_char="1927">
<ORIGINAL_TEXT>Estimating the time it takes for the virus to mutate and comparing that time to the samples taken in the region, they also claimed that the virus first emerged in July or August 2019 in the region.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1731" end_char="1740">Estimating</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1742" end_char="1744">the</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1746" end_char="1749">time</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1751" end_char="1752">it</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1754" end_char="1758">takes</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1760" end_char="1762">for</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1764" end_char="1766">the</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1768" end_char="1772">virus</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1774" end_char="1775">to</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1777" end_char="1782">mutate</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1784" end_char="1786">and</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1788" end_char="1796">comparing</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1798" end_char="1801">that</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1803" end_char="1806">time</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1808" end_char="1809">to</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1811" end_char="1813">the</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1815" end_char="1821">samples</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1823" end_char="1827">taken</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="1829" end_char="1830">in</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="1832" end_char="1834">the</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1836" end_char="1841">region</TOKEN>
<TOKEN id="token-12-21" pos="punct" morph="none" start_char="1842" end_char="1842">,</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1844" end_char="1847">they</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1849" end_char="1852">also</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1854" end_char="1860">claimed</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1862" end_char="1865">that</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="1867" end_char="1869">the</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="1871" end_char="1875">virus</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="1877" end_char="1881">first</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="1883" end_char="1889">emerged</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="1891" end_char="1892">in</TOKEN>
<TOKEN id="token-12-31" pos="word" morph="none" start_char="1894" end_char="1897">July</TOKEN>
<TOKEN id="token-12-32" pos="word" morph="none" start_char="1899" end_char="1900">or</TOKEN>
<TOKEN id="token-12-33" pos="word" morph="none" start_char="1902" end_char="1907">August</TOKEN>
<TOKEN id="token-12-34" pos="word" morph="none" start_char="1909" end_char="1912">2019</TOKEN>
<TOKEN id="token-12-35" pos="word" morph="none" start_char="1914" end_char="1915">in</TOKEN>
<TOKEN id="token-12-36" pos="word" morph="none" start_char="1917" end_char="1919">the</TOKEN>
<TOKEN id="token-12-37" pos="word" morph="none" start_char="1921" end_char="1926">region</TOKEN>
<TOKEN id="token-12-38" pos="punct" morph="none" start_char="1927" end_char="1927">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1930" end_char="2106">
<ORIGINAL_TEXT>They add, "From May to June 2019, the second-longest recorded heat-wave had rampaged in northern-central India and Pakistan, which created a serious water crisis in this region.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1930" end_char="1933">They</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1935" end_char="1937">add</TOKEN>
<TOKEN id="token-13-2" pos="punct" morph="none" start_char="1938" end_char="1938">,</TOKEN>
<TOKEN id="token-13-3" pos="punct" morph="none" start_char="1940" end_char="1940">"</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1941" end_char="1944">From</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1946" end_char="1948">May</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1950" end_char="1951">to</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1953" end_char="1956">June</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1958" end_char="1961">2019</TOKEN>
<TOKEN id="token-13-9" pos="punct" morph="none" start_char="1962" end_char="1962">,</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1964" end_char="1966">the</TOKEN>
<TOKEN id="token-13-11" pos="unknown" morph="none" start_char="1968" end_char="1981">second-longest</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1983" end_char="1990">recorded</TOKEN>
<TOKEN id="token-13-13" pos="unknown" morph="none" start_char="1992" end_char="2000">heat-wave</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="2002" end_char="2004">had</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="2006" end_char="2013">rampaged</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="2015" end_char="2016">in</TOKEN>
<TOKEN id="token-13-17" pos="unknown" morph="none" start_char="2018" end_char="2033">northern-central</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="2035" end_char="2039">India</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="2041" end_char="2043">and</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="2045" end_char="2052">Pakistan</TOKEN>
<TOKEN id="token-13-21" pos="punct" morph="none" start_char="2053" end_char="2053">,</TOKEN>
<TOKEN id="token-13-22" pos="word" morph="none" start_char="2055" end_char="2059">which</TOKEN>
<TOKEN id="token-13-23" pos="word" morph="none" start_char="2061" end_char="2067">created</TOKEN>
<TOKEN id="token-13-24" pos="word" morph="none" start_char="2069" end_char="2069">a</TOKEN>
<TOKEN id="token-13-25" pos="word" morph="none" start_char="2071" end_char="2077">serious</TOKEN>
<TOKEN id="token-13-26" pos="word" morph="none" start_char="2079" end_char="2083">water</TOKEN>
<TOKEN id="token-13-27" pos="word" morph="none" start_char="2085" end_char="2090">crisis</TOKEN>
<TOKEN id="token-13-28" pos="word" morph="none" start_char="2092" end_char="2093">in</TOKEN>
<TOKEN id="token-13-29" pos="word" morph="none" start_char="2095" end_char="2098">this</TOKEN>
<TOKEN id="token-13-30" pos="word" morph="none" start_char="2100" end_char="2105">region</TOKEN>
<TOKEN id="token-13-31" pos="punct" morph="none" start_char="2106" end_char="2106">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="2108" end_char="2295">
<ORIGINAL_TEXT>The water shortage made wild animals, such as monkeys, engage in the deadly fight over water among each other and would have surely increased the chance of human-wild animal interactions."</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="2108" end_char="2110">The</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="2112" end_char="2116">water</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="2118" end_char="2125">shortage</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="2127" end_char="2130">made</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="2132" end_char="2135">wild</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="2137" end_char="2143">animals</TOKEN>
<TOKEN id="token-14-6" pos="punct" morph="none" start_char="2144" end_char="2144">,</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="2146" end_char="2149">such</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="2151" end_char="2152">as</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="2154" end_char="2160">monkeys</TOKEN>
<TOKEN id="token-14-10" pos="punct" morph="none" start_char="2161" end_char="2161">,</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="2163" end_char="2168">engage</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="2170" end_char="2171">in</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="2173" end_char="2175">the</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="2177" end_char="2182">deadly</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="2184" end_char="2188">fight</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="2190" end_char="2193">over</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="2195" end_char="2199">water</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="2201" end_char="2205">among</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="2207" end_char="2210">each</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="2212" end_char="2216">other</TOKEN>
<TOKEN id="token-14-21" pos="word" morph="none" start_char="2218" end_char="2220">and</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="2222" end_char="2226">would</TOKEN>
<TOKEN id="token-14-23" pos="word" morph="none" start_char="2228" end_char="2231">have</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="2233" end_char="2238">surely</TOKEN>
<TOKEN id="token-14-25" pos="word" morph="none" start_char="2240" end_char="2248">increased</TOKEN>
<TOKEN id="token-14-26" pos="word" morph="none" start_char="2250" end_char="2252">the</TOKEN>
<TOKEN id="token-14-27" pos="word" morph="none" start_char="2254" end_char="2259">chance</TOKEN>
<TOKEN id="token-14-28" pos="word" morph="none" start_char="2261" end_char="2262">of</TOKEN>
<TOKEN id="token-14-29" pos="unknown" morph="none" start_char="2264" end_char="2273">human-wild</TOKEN>
<TOKEN id="token-14-30" pos="word" morph="none" start_char="2275" end_char="2280">animal</TOKEN>
<TOKEN id="token-14-31" pos="word" morph="none" start_char="2282" end_char="2293">interactions</TOKEN>
<TOKEN id="token-14-32" pos="punct" morph="none" start_char="2294" end_char="2295">."</TOKEN>
</SEG>
<SEG id="segment-15" start_char="2298" end_char="2528">
<ORIGINAL_TEXT>Blaming a 'less efficient' healthcare system and 'imperfect' hygiene conditions, the Chinese scientists say, "As known for all, the hygiene condition is imperfect and the public medical system is less efficient in the subcontinent.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="2298" end_char="2304">Blaming</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="2306" end_char="2306">a</TOKEN>
<TOKEN id="token-15-2" pos="punct" morph="none" start_char="2308" end_char="2308">'</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="2309" end_char="2312">less</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="2314" end_char="2322">efficient</TOKEN>
<TOKEN id="token-15-5" pos="punct" morph="none" start_char="2323" end_char="2323">'</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="2325" end_char="2334">healthcare</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="2336" end_char="2341">system</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="2343" end_char="2345">and</TOKEN>
<TOKEN id="token-15-9" pos="punct" morph="none" start_char="2347" end_char="2347">'</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="2348" end_char="2356">imperfect</TOKEN>
<TOKEN id="token-15-11" pos="punct" morph="none" start_char="2357" end_char="2357">'</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="2359" end_char="2365">hygiene</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="2367" end_char="2376">conditions</TOKEN>
<TOKEN id="token-15-14" pos="punct" morph="none" start_char="2377" end_char="2377">,</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="2379" end_char="2381">the</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="2383" end_char="2389">Chinese</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="2391" end_char="2400">scientists</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="2402" end_char="2404">say</TOKEN>
<TOKEN id="token-15-19" pos="punct" morph="none" start_char="2405" end_char="2405">,</TOKEN>
<TOKEN id="token-15-20" pos="punct" morph="none" start_char="2407" end_char="2407">"</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="2408" end_char="2409">As</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="2411" end_char="2415">known</TOKEN>
<TOKEN id="token-15-23" pos="word" morph="none" start_char="2417" end_char="2419">for</TOKEN>
<TOKEN id="token-15-24" pos="word" morph="none" start_char="2421" end_char="2423">all</TOKEN>
<TOKEN id="token-15-25" pos="punct" morph="none" start_char="2424" end_char="2424">,</TOKEN>
<TOKEN id="token-15-26" pos="word" morph="none" start_char="2426" end_char="2428">the</TOKEN>
<TOKEN id="token-15-27" pos="word" morph="none" start_char="2430" end_char="2436">hygiene</TOKEN>
<TOKEN id="token-15-28" pos="word" morph="none" start_char="2438" end_char="2446">condition</TOKEN>
<TOKEN id="token-15-29" pos="word" morph="none" start_char="2448" end_char="2449">is</TOKEN>
<TOKEN id="token-15-30" pos="word" morph="none" start_char="2451" end_char="2459">imperfect</TOKEN>
<TOKEN id="token-15-31" pos="word" morph="none" start_char="2461" end_char="2463">and</TOKEN>
<TOKEN id="token-15-32" pos="word" morph="none" start_char="2465" end_char="2467">the</TOKEN>
<TOKEN id="token-15-33" pos="word" morph="none" start_char="2469" end_char="2474">public</TOKEN>
<TOKEN id="token-15-34" pos="word" morph="none" start_char="2476" end_char="2482">medical</TOKEN>
<TOKEN id="token-15-35" pos="word" morph="none" start_char="2484" end_char="2489">system</TOKEN>
<TOKEN id="token-15-36" pos="word" morph="none" start_char="2491" end_char="2492">is</TOKEN>
<TOKEN id="token-15-37" pos="word" morph="none" start_char="2494" end_char="2497">less</TOKEN>
<TOKEN id="token-15-38" pos="word" morph="none" start_char="2499" end_char="2507">efficient</TOKEN>
<TOKEN id="token-15-39" pos="word" morph="none" start_char="2509" end_char="2510">in</TOKEN>
<TOKEN id="token-15-40" pos="word" morph="none" start_char="2512" end_char="2514">the</TOKEN>
<TOKEN id="token-15-41" pos="word" morph="none" start_char="2516" end_char="2527">subcontinent</TOKEN>
<TOKEN id="token-15-42" pos="punct" morph="none" start_char="2528" end_char="2528">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="2530" end_char="2640">
<ORIGINAL_TEXT>Thus, it is conceivable that a virus with flu-like symptom could spread undetectably for several months there."</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="2530" end_char="2533">Thus</TOKEN>
<TOKEN id="token-16-1" pos="punct" morph="none" start_char="2534" end_char="2534">,</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="2536" end_char="2537">it</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="2539" end_char="2540">is</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="2542" end_char="2552">conceivable</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="2554" end_char="2557">that</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="2559" end_char="2559">a</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="2561" end_char="2565">virus</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="2567" end_char="2570">with</TOKEN>
<TOKEN id="token-16-9" pos="unknown" morph="none" start_char="2572" end_char="2579">flu-like</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="2581" end_char="2587">symptom</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="2589" end_char="2593">could</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="2595" end_char="2600">spread</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2602" end_char="2613">undetectably</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="2615" end_char="2617">for</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="2619" end_char="2625">several</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="2627" end_char="2632">months</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="2634" end_char="2638">there</TOKEN>
<TOKEN id="token-16-18" pos="punct" morph="none" start_char="2639" end_char="2640">."</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2643" end_char="2845">
<ORIGINAL_TEXT>This comes in the backdrop of a long-drawn-out stand-off between Indian and Chinese armed forces along the Line of Actual Control in Ladakh which has led to strained relations between both the countries.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2643" end_char="2646">This</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2648" end_char="2652">comes</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2654" end_char="2655">in</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2657" end_char="2659">the</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2661" end_char="2668">backdrop</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2670" end_char="2671">of</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2673" end_char="2673">a</TOKEN>
<TOKEN id="token-17-7" pos="unknown" morph="none" start_char="2675" end_char="2688">long-drawn-out</TOKEN>
<TOKEN id="token-17-8" pos="unknown" morph="none" start_char="2690" end_char="2698">stand-off</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="2700" end_char="2706">between</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="2708" end_char="2713">Indian</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="2715" end_char="2717">and</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="2719" end_char="2725">Chinese</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="2727" end_char="2731">armed</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="2733" end_char="2738">forces</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="2740" end_char="2744">along</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="2746" end_char="2748">the</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="2750" end_char="2753">Line</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="2755" end_char="2756">of</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="2758" end_char="2763">Actual</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="2765" end_char="2771">Control</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="2773" end_char="2774">in</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="2776" end_char="2781">Ladakh</TOKEN>
<TOKEN id="token-17-23" pos="word" morph="none" start_char="2783" end_char="2787">which</TOKEN>
<TOKEN id="token-17-24" pos="word" morph="none" start_char="2789" end_char="2791">has</TOKEN>
<TOKEN id="token-17-25" pos="word" morph="none" start_char="2793" end_char="2795">led</TOKEN>
<TOKEN id="token-17-26" pos="word" morph="none" start_char="2797" end_char="2798">to</TOKEN>
<TOKEN id="token-17-27" pos="word" morph="none" start_char="2800" end_char="2807">strained</TOKEN>
<TOKEN id="token-17-28" pos="word" morph="none" start_char="2809" end_char="2817">relations</TOKEN>
<TOKEN id="token-17-29" pos="word" morph="none" start_char="2819" end_char="2825">between</TOKEN>
<TOKEN id="token-17-30" pos="word" morph="none" start_char="2827" end_char="2830">both</TOKEN>
<TOKEN id="token-17-31" pos="word" morph="none" start_char="2832" end_char="2834">the</TOKEN>
<TOKEN id="token-17-32" pos="word" morph="none" start_char="2836" end_char="2844">countries</TOKEN>
<TOKEN id="token-17-33" pos="punct" morph="none" start_char="2845" end_char="2845">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
