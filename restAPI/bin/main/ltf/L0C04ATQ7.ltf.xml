<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATQ7" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="2364" raw_text_md5="8117dfeb42df1bcf100867092a811875">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="76">
<ORIGINAL_TEXT>La COVID-19 ya circulaba en Brasil antes del primer caso confirmado en Wuhan</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="2">La</TOKEN>
<TOKEN id="token-0-1" pos="unknown" morph="none" start_char="4" end_char="11">COVID-19</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="13" end_char="14">ya</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="16" end_char="24">circulaba</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="26" end_char="27">en</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="29" end_char="34">Brasil</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="36" end_char="40">antes</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="42" end_char="44">del</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="46" end_char="51">primer</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="53" end_char="56">caso</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="58" end_char="67">confirmado</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="69" end_char="70">en</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="72" end_char="76">Wuhan</TOKEN>
</SEG>
<SEG id="segment-1" start_char="80" end_char="99">
<ORIGINAL_TEXT>Fernando Bizerra EFE</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="80" end_char="87">Fernando</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="89" end_char="95">Bizerra</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="97" end_char="99">EFE</TOKEN>
</SEG>
<SEG id="segment-2" start_char="103" end_char="180">
<ORIGINAL_TEXT>El coronavirus ya circulaba antes de la confirmación del primer caso en Wuhan.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="103" end_char="104">El</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="106" end_char="116">coronavirus</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="118" end_char="119">ya</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="121" end_char="129">circulaba</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="131" end_char="135">antes</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="137" end_char="138">de</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="140" end_char="141">la</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="143" end_char="154">confirmación</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="156" end_char="158">del</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="160" end_char="165">primer</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="167" end_char="170">caso</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="172" end_char="173">en</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="175" end_char="179">Wuhan</TOKEN>
<TOKEN id="token-2-13" pos="punct" morph="none" start_char="180" end_char="180">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="182" end_char="392">
<ORIGINAL_TEXT>Es lo que ha revelado un estudio en colaboración con la Universidad Federal de Santa Catalina, en Florianópolis (Brasil), en el cual han encontrado restos del virus en las aguas fecales del país latinoamericano.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="182" end_char="183">Es</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="185" end_char="186">lo</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="188" end_char="190">que</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="192" end_char="193">ha</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="195" end_char="202">revelado</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="204" end_char="205">un</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="207" end_char="213">estudio</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="215" end_char="216">en</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="218" end_char="229">colaboración</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="231" end_char="233">con</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="235" end_char="236">la</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="238" end_char="248">Universidad</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="250" end_char="256">Federal</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="258" end_char="259">de</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="261" end_char="265">Santa</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="267" end_char="274">Catalina</TOKEN>
<TOKEN id="token-3-16" pos="punct" morph="none" start_char="275" end_char="275">,</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="277" end_char="278">en</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="280" end_char="292">Florianópolis</TOKEN>
<TOKEN id="token-3-19" pos="punct" morph="none" start_char="294" end_char="294">(</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="295" end_char="300">Brasil</TOKEN>
<TOKEN id="token-3-21" pos="punct" morph="none" start_char="301" end_char="302">),</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="304" end_char="305">en</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="307" end_char="308">el</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="310" end_char="313">cual</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="315" end_char="317">han</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="319" end_char="328">encontrado</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="330" end_char="335">restos</TOKEN>
<TOKEN id="token-3-28" pos="word" morph="none" start_char="337" end_char="339">del</TOKEN>
<TOKEN id="token-3-29" pos="word" morph="none" start_char="341" end_char="345">virus</TOKEN>
<TOKEN id="token-3-30" pos="word" morph="none" start_char="347" end_char="348">en</TOKEN>
<TOKEN id="token-3-31" pos="word" morph="none" start_char="350" end_char="352">las</TOKEN>
<TOKEN id="token-3-32" pos="word" morph="none" start_char="354" end_char="358">aguas</TOKEN>
<TOKEN id="token-3-33" pos="word" morph="none" start_char="360" end_char="366">fecales</TOKEN>
<TOKEN id="token-3-34" pos="word" morph="none" start_char="368" end_char="370">del</TOKEN>
<TOKEN id="token-3-35" pos="word" morph="none" start_char="372" end_char="375">país</TOKEN>
<TOKEN id="token-3-36" pos="word" morph="none" start_char="377" end_char="391">latinoamericano</TOKEN>
<TOKEN id="token-3-37" pos="punct" morph="none" start_char="392" end_char="392">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="394" end_char="611">
<ORIGINAL_TEXT>Este descubrimiento dataría del 27 de noviembre de 2019, no siendo hasta diciembre cuando se confirmaba el primer caso de la COVID-19 por parte de las autoridades chinas hacia la Organización Mundial de la Salud (OMS).</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="394" end_char="397">Este</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="399" end_char="412">descubrimiento</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="414" end_char="420">dataría</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="422" end_char="424">del</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="426" end_char="427">27</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="429" end_char="430">de</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="432" end_char="440">noviembre</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="442" end_char="443">de</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="445" end_char="448">2019</TOKEN>
<TOKEN id="token-4-9" pos="punct" morph="none" start_char="449" end_char="449">,</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="451" end_char="452">no</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="454" end_char="459">siendo</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="461" end_char="465">hasta</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="467" end_char="475">diciembre</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="477" end_char="482">cuando</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="484" end_char="485">se</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="487" end_char="496">confirmaba</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="498" end_char="499">el</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="501" end_char="506">primer</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="508" end_char="511">caso</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="513" end_char="514">de</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="516" end_char="517">la</TOKEN>
<TOKEN id="token-4-22" pos="unknown" morph="none" start_char="519" end_char="526">COVID-19</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="528" end_char="530">por</TOKEN>
<TOKEN id="token-4-24" pos="word" morph="none" start_char="532" end_char="536">parte</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="538" end_char="539">de</TOKEN>
<TOKEN id="token-4-26" pos="word" morph="none" start_char="541" end_char="543">las</TOKEN>
<TOKEN id="token-4-27" pos="word" morph="none" start_char="545" end_char="555">autoridades</TOKEN>
<TOKEN id="token-4-28" pos="word" morph="none" start_char="557" end_char="562">chinas</TOKEN>
<TOKEN id="token-4-29" pos="word" morph="none" start_char="564" end_char="568">hacia</TOKEN>
<TOKEN id="token-4-30" pos="word" morph="none" start_char="570" end_char="571">la</TOKEN>
<TOKEN id="token-4-31" pos="word" morph="none" start_char="573" end_char="584">Organización</TOKEN>
<TOKEN id="token-4-32" pos="word" morph="none" start_char="586" end_char="592">Mundial</TOKEN>
<TOKEN id="token-4-33" pos="word" morph="none" start_char="594" end_char="595">de</TOKEN>
<TOKEN id="token-4-34" pos="word" morph="none" start_char="597" end_char="598">la</TOKEN>
<TOKEN id="token-4-35" pos="word" morph="none" start_char="600" end_char="604">Salud</TOKEN>
<TOKEN id="token-4-36" pos="punct" morph="none" start_char="606" end_char="606">(</TOKEN>
<TOKEN id="token-4-37" pos="word" morph="none" start_char="607" end_char="609">OMS</TOKEN>
<TOKEN id="token-4-38" pos="punct" morph="none" start_char="610" end_char="611">).</TOKEN>
</SEG>
<SEG id="segment-5" start_char="614" end_char="845">
<ORIGINAL_TEXT>Así lo ha explicado David Rodríguez Lázaro, uno de los expertos que han participado en este estudio, en el cual pudieron recoger muestras desde inicio del otoño de 2019 hasta marzo de 2020, cuando se decretó la cuarentena en Brasil.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="614" end_char="616">Así</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="618" end_char="619">lo</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="621" end_char="622">ha</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="624" end_char="632">explicado</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="634" end_char="638">David</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="640" end_char="648">Rodríguez</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="650" end_char="655">Lázaro</TOKEN>
<TOKEN id="token-5-7" pos="punct" morph="none" start_char="656" end_char="656">,</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="658" end_char="660">uno</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="662" end_char="663">de</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="665" end_char="667">los</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="669" end_char="676">expertos</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="678" end_char="680">que</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="682" end_char="684">han</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="686" end_char="696">participado</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="698" end_char="699">en</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="701" end_char="704">este</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="706" end_char="712">estudio</TOKEN>
<TOKEN id="token-5-18" pos="punct" morph="none" start_char="713" end_char="713">,</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="715" end_char="716">en</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="718" end_char="719">el</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="721" end_char="724">cual</TOKEN>
<TOKEN id="token-5-22" pos="word" morph="none" start_char="726" end_char="733">pudieron</TOKEN>
<TOKEN id="token-5-23" pos="word" morph="none" start_char="735" end_char="741">recoger</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="743" end_char="750">muestras</TOKEN>
<TOKEN id="token-5-25" pos="word" morph="none" start_char="752" end_char="756">desde</TOKEN>
<TOKEN id="token-5-26" pos="word" morph="none" start_char="758" end_char="763">inicio</TOKEN>
<TOKEN id="token-5-27" pos="word" morph="none" start_char="765" end_char="767">del</TOKEN>
<TOKEN id="token-5-28" pos="word" morph="none" start_char="769" end_char="773">otoño</TOKEN>
<TOKEN id="token-5-29" pos="word" morph="none" start_char="775" end_char="776">de</TOKEN>
<TOKEN id="token-5-30" pos="word" morph="none" start_char="778" end_char="781">2019</TOKEN>
<TOKEN id="token-5-31" pos="word" morph="none" start_char="783" end_char="787">hasta</TOKEN>
<TOKEN id="token-5-32" pos="word" morph="none" start_char="789" end_char="793">marzo</TOKEN>
<TOKEN id="token-5-33" pos="word" morph="none" start_char="795" end_char="796">de</TOKEN>
<TOKEN id="token-5-34" pos="word" morph="none" start_char="798" end_char="801">2020</TOKEN>
<TOKEN id="token-5-35" pos="punct" morph="none" start_char="802" end_char="802">,</TOKEN>
<TOKEN id="token-5-36" pos="word" morph="none" start_char="804" end_char="809">cuando</TOKEN>
<TOKEN id="token-5-37" pos="word" morph="none" start_char="811" end_char="812">se</TOKEN>
<TOKEN id="token-5-38" pos="word" morph="none" start_char="814" end_char="820">decretó</TOKEN>
<TOKEN id="token-5-39" pos="word" morph="none" start_char="822" end_char="823">la</TOKEN>
<TOKEN id="token-5-40" pos="word" morph="none" start_char="825" end_char="834">cuarentena</TOKEN>
<TOKEN id="token-5-41" pos="word" morph="none" start_char="836" end_char="837">en</TOKEN>
<TOKEN id="token-5-42" pos="word" morph="none" start_char="839" end_char="844">Brasil</TOKEN>
<TOKEN id="token-5-43" pos="punct" morph="none" start_char="845" end_char="845">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="847" end_char="1081">
<ORIGINAL_TEXT>Con su análisis, bajo "un protocolo estándar de concentración de virus y posterior detección por PCR", pudieron ver que, efectivamente, el virus ya circulaba por el país hasta casi tres meses antes del primer caso confirmado en Brasil.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="847" end_char="849">Con</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="851" end_char="852">su</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="854" end_char="861">análisis</TOKEN>
<TOKEN id="token-6-3" pos="punct" morph="none" start_char="862" end_char="862">,</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="864" end_char="867">bajo</TOKEN>
<TOKEN id="token-6-5" pos="punct" morph="none" start_char="869" end_char="869">"</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="870" end_char="871">un</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="873" end_char="881">protocolo</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="883" end_char="890">estándar</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="892" end_char="893">de</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="895" end_char="907">concentración</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="909" end_char="910">de</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="912" end_char="916">virus</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="918" end_char="918">y</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="920" end_char="928">posterior</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="930" end_char="938">detección</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="940" end_char="942">por</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="944" end_char="946">PCR</TOKEN>
<TOKEN id="token-6-18" pos="punct" morph="none" start_char="947" end_char="948">",</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="950" end_char="957">pudieron</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="959" end_char="961">ver</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="963" end_char="965">que</TOKEN>
<TOKEN id="token-6-22" pos="punct" morph="none" start_char="966" end_char="966">,</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="968" end_char="980">efectivamente</TOKEN>
<TOKEN id="token-6-24" pos="punct" morph="none" start_char="981" end_char="981">,</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="983" end_char="984">el</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="986" end_char="990">virus</TOKEN>
<TOKEN id="token-6-27" pos="word" morph="none" start_char="992" end_char="993">ya</TOKEN>
<TOKEN id="token-6-28" pos="word" morph="none" start_char="995" end_char="1003">circulaba</TOKEN>
<TOKEN id="token-6-29" pos="word" morph="none" start_char="1005" end_char="1007">por</TOKEN>
<TOKEN id="token-6-30" pos="word" morph="none" start_char="1009" end_char="1010">el</TOKEN>
<TOKEN id="token-6-31" pos="word" morph="none" start_char="1012" end_char="1015">país</TOKEN>
<TOKEN id="token-6-32" pos="word" morph="none" start_char="1017" end_char="1021">hasta</TOKEN>
<TOKEN id="token-6-33" pos="word" morph="none" start_char="1023" end_char="1026">casi</TOKEN>
<TOKEN id="token-6-34" pos="word" morph="none" start_char="1028" end_char="1031">tres</TOKEN>
<TOKEN id="token-6-35" pos="word" morph="none" start_char="1033" end_char="1037">meses</TOKEN>
<TOKEN id="token-6-36" pos="word" morph="none" start_char="1039" end_char="1043">antes</TOKEN>
<TOKEN id="token-6-37" pos="word" morph="none" start_char="1045" end_char="1047">del</TOKEN>
<TOKEN id="token-6-38" pos="word" morph="none" start_char="1049" end_char="1054">primer</TOKEN>
<TOKEN id="token-6-39" pos="word" morph="none" start_char="1056" end_char="1059">caso</TOKEN>
<TOKEN id="token-6-40" pos="word" morph="none" start_char="1061" end_char="1070">confirmado</TOKEN>
<TOKEN id="token-6-41" pos="word" morph="none" start_char="1072" end_char="1073">en</TOKEN>
<TOKEN id="token-6-42" pos="word" morph="none" start_char="1075" end_char="1080">Brasil</TOKEN>
<TOKEN id="token-6-43" pos="punct" morph="none" start_char="1081" end_char="1081">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="1084" end_char="1217">
<ORIGINAL_TEXT>"Es casi dos meses, 56 días exactamente, antes de la primera descripción", comentaba el español en el programa ‘Horizonte’, de Cuatro.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="punct" morph="none" start_char="1084" end_char="1084">"</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="1085" end_char="1086">Es</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="1088" end_char="1091">casi</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="1093" end_char="1095">dos</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="1097" end_char="1101">meses</TOKEN>
<TOKEN id="token-7-5" pos="punct" morph="none" start_char="1102" end_char="1102">,</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="1104" end_char="1105">56</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="1107" end_char="1110">días</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="1112" end_char="1122">exactamente</TOKEN>
<TOKEN id="token-7-9" pos="punct" morph="none" start_char="1123" end_char="1123">,</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="1125" end_char="1129">antes</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="1131" end_char="1132">de</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="1134" end_char="1135">la</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="1137" end_char="1143">primera</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="1145" end_char="1155">descripción</TOKEN>
<TOKEN id="token-7-15" pos="punct" morph="none" start_char="1156" end_char="1157">",</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="1159" end_char="1167">comentaba</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="1169" end_char="1170">el</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="1172" end_char="1178">español</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="1180" end_char="1181">en</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="1183" end_char="1184">el</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="1186" end_char="1193">programa</TOKEN>
<TOKEN id="token-7-22" pos="punct" morph="none" start_char="1195" end_char="1195">‘</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="1196" end_char="1204">Horizonte</TOKEN>
<TOKEN id="token-7-24" pos="punct" morph="none" start_char="1205" end_char="1206">’,</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="1208" end_char="1209">de</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="1211" end_char="1216">Cuatro</TOKEN>
<TOKEN id="token-7-27" pos="punct" morph="none" start_char="1217" end_char="1217">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1219" end_char="1334">
<ORIGINAL_TEXT>"El primer mensaje que obtenemos de este estudio es que el virus estaba circulando mucho antes de lo que pensábamos.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="punct" morph="none" start_char="1219" end_char="1219">"</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="1220" end_char="1221">El</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="1223" end_char="1228">primer</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1230" end_char="1236">mensaje</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1238" end_char="1240">que</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1242" end_char="1250">obtenemos</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="1252" end_char="1253">de</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="1255" end_char="1258">este</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1260" end_char="1266">estudio</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="1268" end_char="1269">es</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="1271" end_char="1273">que</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="1275" end_char="1276">el</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="1278" end_char="1282">virus</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="1284" end_char="1289">estaba</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="1291" end_char="1300">circulando</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="1302" end_char="1306">mucho</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="1308" end_char="1312">antes</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="1314" end_char="1315">de</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="1317" end_char="1318">lo</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="1320" end_char="1322">que</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="1324" end_char="1333">pensábamos</TOKEN>
<TOKEN id="token-8-21" pos="punct" morph="none" start_char="1334" end_char="1334">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1336" end_char="1458">
<ORIGINAL_TEXT>Y lo segundo es que tenemos que plantearnos cuándo, realmente, este virus empezó a estar presente en la comunidad mundial".</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1336" end_char="1336">Y</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1338" end_char="1339">lo</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="1341" end_char="1347">segundo</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1349" end_char="1350">es</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1352" end_char="1354">que</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1356" end_char="1362">tenemos</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1364" end_char="1366">que</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1368" end_char="1378">plantearnos</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1380" end_char="1385">cuándo</TOKEN>
<TOKEN id="token-9-9" pos="punct" morph="none" start_char="1386" end_char="1386">,</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1388" end_char="1396">realmente</TOKEN>
<TOKEN id="token-9-11" pos="punct" morph="none" start_char="1397" end_char="1397">,</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="1399" end_char="1402">este</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1404" end_char="1408">virus</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1410" end_char="1415">empezó</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1417" end_char="1417">a</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="1419" end_char="1423">estar</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1425" end_char="1432">presente</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1434" end_char="1435">en</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1437" end_char="1438">la</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1440" end_char="1448">comunidad</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1450" end_char="1456">mundial</TOKEN>
<TOKEN id="token-9-22" pos="punct" morph="none" start_char="1457" end_char="1458">".</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1461" end_char="1505">
<ORIGINAL_TEXT>La fecha exacta del origen del virus, en duda</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1461" end_char="1462">La</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1464" end_char="1468">fecha</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1470" end_char="1475">exacta</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1477" end_char="1479">del</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1481" end_char="1486">origen</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1488" end_char="1490">del</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1492" end_char="1496">virus</TOKEN>
<TOKEN id="token-10-7" pos="punct" morph="none" start_char="1497" end_char="1497">,</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1499" end_char="1500">en</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1502" end_char="1505">duda</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1509" end_char="1598">
<ORIGINAL_TEXT>Estos resultados hacen que la fecha exacta de la aparición del virus sea todo un misterio.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1509" end_char="1513">Estos</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1515" end_char="1524">resultados</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1526" end_char="1530">hacen</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1532" end_char="1534">que</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1536" end_char="1537">la</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1539" end_char="1543">fecha</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1545" end_char="1550">exacta</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1552" end_char="1553">de</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1555" end_char="1556">la</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1558" end_char="1566">aparición</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1568" end_char="1570">del</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1572" end_char="1576">virus</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1578" end_char="1580">sea</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1582" end_char="1585">todo</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1587" end_char="1588">un</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1590" end_char="1597">misterio</TOKEN>
<TOKEN id="token-11-16" pos="punct" morph="none" start_char="1598" end_char="1598">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1600" end_char="1795">
<ORIGINAL_TEXT>Según Rodríguez Lázaro, el origen de la COVID-19 sigue siendo un misterio aunque, todos los expertos, coinciden en que "es indudable que se trata de un virus de origen animal, un virus zoonótico".</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1600" end_char="1604">Según</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1606" end_char="1614">Rodríguez</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1616" end_char="1621">Lázaro</TOKEN>
<TOKEN id="token-12-3" pos="punct" morph="none" start_char="1622" end_char="1622">,</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1624" end_char="1625">el</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1627" end_char="1632">origen</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1634" end_char="1635">de</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1637" end_char="1638">la</TOKEN>
<TOKEN id="token-12-8" pos="unknown" morph="none" start_char="1640" end_char="1647">COVID-19</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1649" end_char="1653">sigue</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1655" end_char="1660">siendo</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1662" end_char="1663">un</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1665" end_char="1672">misterio</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1674" end_char="1679">aunque</TOKEN>
<TOKEN id="token-12-14" pos="punct" morph="none" start_char="1680" end_char="1680">,</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1682" end_char="1686">todos</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1688" end_char="1690">los</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1692" end_char="1699">expertos</TOKEN>
<TOKEN id="token-12-18" pos="punct" morph="none" start_char="1700" end_char="1700">,</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="1702" end_char="1710">coinciden</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1712" end_char="1713">en</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1715" end_char="1717">que</TOKEN>
<TOKEN id="token-12-22" pos="punct" morph="none" start_char="1719" end_char="1719">"</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1720" end_char="1721">es</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1723" end_char="1731">indudable</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1733" end_char="1735">que</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="1737" end_char="1738">se</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="1740" end_char="1744">trata</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="1746" end_char="1747">de</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="1749" end_char="1750">un</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="1752" end_char="1756">virus</TOKEN>
<TOKEN id="token-12-31" pos="word" morph="none" start_char="1758" end_char="1759">de</TOKEN>
<TOKEN id="token-12-32" pos="word" morph="none" start_char="1761" end_char="1766">origen</TOKEN>
<TOKEN id="token-12-33" pos="word" morph="none" start_char="1768" end_char="1773">animal</TOKEN>
<TOKEN id="token-12-34" pos="punct" morph="none" start_char="1774" end_char="1774">,</TOKEN>
<TOKEN id="token-12-35" pos="word" morph="none" start_char="1776" end_char="1777">un</TOKEN>
<TOKEN id="token-12-36" pos="word" morph="none" start_char="1779" end_char="1783">virus</TOKEN>
<TOKEN id="token-12-37" pos="word" morph="none" start_char="1785" end_char="1793">zoonótico</TOKEN>
<TOKEN id="token-12-38" pos="punct" morph="none" start_char="1794" end_char="1795">".</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1797" end_char="1939">
<ORIGINAL_TEXT>Además, pese a que el mercado de Wuhan no haya sido el origen principal del coronavirus, sí ha sido "un detonante" para al expansión del mismo.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1797" end_char="1802">Además</TOKEN>
<TOKEN id="token-13-1" pos="punct" morph="none" start_char="1803" end_char="1803">,</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1805" end_char="1808">pese</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1810" end_char="1810">a</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1812" end_char="1814">que</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1816" end_char="1817">el</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1819" end_char="1825">mercado</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1827" end_char="1828">de</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1830" end_char="1834">Wuhan</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1836" end_char="1837">no</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1839" end_char="1842">haya</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1844" end_char="1847">sido</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1849" end_char="1850">el</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="1852" end_char="1857">origen</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="1859" end_char="1867">principal</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="1869" end_char="1871">del</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="1873" end_char="1883">coronavirus</TOKEN>
<TOKEN id="token-13-17" pos="punct" morph="none" start_char="1884" end_char="1884">,</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="1886" end_char="1887">sí</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="1889" end_char="1890">ha</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="1892" end_char="1895">sido</TOKEN>
<TOKEN id="token-13-21" pos="punct" morph="none" start_char="1897" end_char="1897">"</TOKEN>
<TOKEN id="token-13-22" pos="word" morph="none" start_char="1898" end_char="1899">un</TOKEN>
<TOKEN id="token-13-23" pos="word" morph="none" start_char="1901" end_char="1909">detonante</TOKEN>
<TOKEN id="token-13-24" pos="punct" morph="none" start_char="1910" end_char="1910">"</TOKEN>
<TOKEN id="token-13-25" pos="word" morph="none" start_char="1912" end_char="1915">para</TOKEN>
<TOKEN id="token-13-26" pos="word" morph="none" start_char="1917" end_char="1918">al</TOKEN>
<TOKEN id="token-13-27" pos="word" morph="none" start_char="1920" end_char="1928">expansión</TOKEN>
<TOKEN id="token-13-28" pos="word" morph="none" start_char="1930" end_char="1932">del</TOKEN>
<TOKEN id="token-13-29" pos="word" morph="none" start_char="1934" end_char="1938">mismo</TOKEN>
<TOKEN id="token-13-30" pos="punct" morph="none" start_char="1939" end_char="1939">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1941" end_char="2042">
<ORIGINAL_TEXT>"Es muy posible", indica el español, teniendo en cuenta la costumbre de "consumo de animales exótico".</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="punct" morph="none" start_char="1941" end_char="1941">"</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1942" end_char="1943">Es</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1945" end_char="1947">muy</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1949" end_char="1955">posible</TOKEN>
<TOKEN id="token-14-4" pos="punct" morph="none" start_char="1956" end_char="1957">",</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1959" end_char="1964">indica</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1966" end_char="1967">el</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1969" end_char="1975">español</TOKEN>
<TOKEN id="token-14-8" pos="punct" morph="none" start_char="1976" end_char="1976">,</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1978" end_char="1985">teniendo</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1987" end_char="1988">en</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1990" end_char="1995">cuenta</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="1997" end_char="1998">la</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="2000" end_char="2008">costumbre</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="2010" end_char="2011">de</TOKEN>
<TOKEN id="token-14-15" pos="punct" morph="none" start_char="2013" end_char="2013">"</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="2014" end_char="2020">consumo</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="2022" end_char="2023">de</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="2025" end_char="2032">animales</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="2034" end_char="2040">exótico</TOKEN>
<TOKEN id="token-14-20" pos="punct" morph="none" start_char="2041" end_char="2042">".</TOKEN>
</SEG>
<SEG id="segment-15" start_char="2045" end_char="2167">
<ORIGINAL_TEXT>"Que lo hayamos encontrado en Brasil, en noviembre de 2019, lo único que significa es que el virus estaba circulando antes.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="punct" morph="none" start_char="2045" end_char="2045">"</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="2046" end_char="2048">Que</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="2050" end_char="2051">lo</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="2053" end_char="2059">hayamos</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="2061" end_char="2070">encontrado</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="2072" end_char="2073">en</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="2075" end_char="2080">Brasil</TOKEN>
<TOKEN id="token-15-7" pos="punct" morph="none" start_char="2081" end_char="2081">,</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="2083" end_char="2084">en</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="2086" end_char="2094">noviembre</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="2096" end_char="2097">de</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="2099" end_char="2102">2019</TOKEN>
<TOKEN id="token-15-12" pos="punct" morph="none" start_char="2103" end_char="2103">,</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="2105" end_char="2106">lo</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="2108" end_char="2112">único</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="2114" end_char="2116">que</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="2118" end_char="2126">significa</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="2128" end_char="2129">es</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="2131" end_char="2133">que</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="2135" end_char="2136">el</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="2138" end_char="2142">virus</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="2144" end_char="2149">estaba</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="2151" end_char="2160">circulando</TOKEN>
<TOKEN id="token-15-23" pos="word" morph="none" start_char="2162" end_char="2166">antes</TOKEN>
<TOKEN id="token-15-24" pos="punct" morph="none" start_char="2167" end_char="2167">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="2169" end_char="2360">
<ORIGINAL_TEXT>Todos estamos de acuerdo es que no podemos datar de manera segura cuándo apareció, pero lo que podemos decir es que el virus estaba presente antes de los casos reportados", explica el experto.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="2169" end_char="2173">Todos</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="2175" end_char="2181">estamos</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="2183" end_char="2184">de</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="2186" end_char="2192">acuerdo</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="2194" end_char="2195">es</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="2197" end_char="2199">que</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="2201" end_char="2202">no</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="2204" end_char="2210">podemos</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="2212" end_char="2216">datar</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="2218" end_char="2219">de</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="2221" end_char="2226">manera</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="2228" end_char="2233">segura</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="2235" end_char="2240">cuándo</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2242" end_char="2249">apareció</TOKEN>
<TOKEN id="token-16-14" pos="punct" morph="none" start_char="2250" end_char="2250">,</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="2252" end_char="2255">pero</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="2257" end_char="2258">lo</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="2260" end_char="2262">que</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="2264" end_char="2270">podemos</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="2272" end_char="2276">decir</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="2278" end_char="2279">es</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="2281" end_char="2283">que</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="2285" end_char="2286">el</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="2288" end_char="2292">virus</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="2294" end_char="2299">estaba</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="2301" end_char="2308">presente</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="2310" end_char="2314">antes</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="2316" end_char="2317">de</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="2319" end_char="2321">los</TOKEN>
<TOKEN id="token-16-29" pos="word" morph="none" start_char="2323" end_char="2327">casos</TOKEN>
<TOKEN id="token-16-30" pos="word" morph="none" start_char="2329" end_char="2338">reportados</TOKEN>
<TOKEN id="token-16-31" pos="punct" morph="none" start_char="2339" end_char="2340">",</TOKEN>
<TOKEN id="token-16-32" pos="word" morph="none" start_char="2342" end_char="2348">explica</TOKEN>
<TOKEN id="token-16-33" pos="word" morph="none" start_char="2350" end_char="2351">el</TOKEN>
<TOKEN id="token-16-34" pos="word" morph="none" start_char="2353" end_char="2359">experto</TOKEN>
<TOKEN id="token-16-35" pos="punct" morph="none" start_char="2360" end_char="2360">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
