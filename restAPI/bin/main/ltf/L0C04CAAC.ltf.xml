<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CAAC" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="4949" raw_text_md5="ee22824752432723d1860a8735f2c98e">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="47">
<ORIGINAL_TEXT>Was coronavirus really in Europe in March 2019?</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="3">Was</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="5" end_char="15">coronavirus</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="17" end_char="22">really</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="24" end_char="25">in</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="27" end_char="32">Europe</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="34" end_char="35">in</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="37" end_char="41">March</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="43" end_char="46">2019</TOKEN>
<TOKEN id="token-0-8" pos="punct" morph="none" start_char="47" end_char="47">?</TOKEN>
</SEG>
<SEG id="segment-1" start_char="51" end_char="98">
<ORIGINAL_TEXT>Did coronavirus arrive in Spain over a year ago?</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="51" end_char="53">Did</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="55" end_char="65">coronavirus</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="67" end_char="72">arrive</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="74" end_char="75">in</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="77" end_char="81">Spain</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="83" end_char="86">over</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="88" end_char="88">a</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="90" end_char="93">year</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="95" end_char="97">ago</TOKEN>
<TOKEN id="token-1-9" pos="punct" morph="none" start_char="98" end_char="98">?</TOKEN>
</SEG>
<SEG id="segment-2" start_char="102" end_char="197">
<ORIGINAL_TEXT>The novel coronavirus – SARS-CoV-2 – may have been in Europe for longer than previously thought.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="102" end_char="104">The</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="106" end_char="110">novel</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="112" end_char="122">coronavirus</TOKEN>
<TOKEN id="token-2-3" pos="punct" morph="none" start_char="124" end_char="124">–</TOKEN>
<TOKEN id="token-2-4" pos="unknown" morph="none" start_char="126" end_char="135">SARS-CoV-2</TOKEN>
<TOKEN id="token-2-5" pos="punct" morph="none" start_char="137" end_char="137">–</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="139" end_char="141">may</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="143" end_char="146">have</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="148" end_char="151">been</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="153" end_char="154">in</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="156" end_char="161">Europe</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="163" end_char="165">for</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="167" end_char="172">longer</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="174" end_char="177">than</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="179" end_char="188">previously</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="190" end_char="196">thought</TOKEN>
<TOKEN id="token-2-16" pos="punct" morph="none" start_char="197" end_char="197">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="199" end_char="287">
<ORIGINAL_TEXT>Recent studies have suggested that it was circulating in Italy as early as December 2019.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="199" end_char="204">Recent</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="206" end_char="212">studies</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="214" end_char="217">have</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="219" end_char="227">suggested</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="229" end_char="232">that</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="234" end_char="235">it</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="237" end_char="239">was</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="241" end_char="251">circulating</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="253" end_char="254">in</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="256" end_char="260">Italy</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="262" end_char="263">as</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="265" end_char="269">early</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="271" end_char="272">as</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="274" end_char="281">December</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="283" end_char="286">2019</TOKEN>
<TOKEN id="token-3-15" pos="punct" morph="none" start_char="287" end_char="287">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="289" end_char="439">
<ORIGINAL_TEXT>More surprisingly, researchers at the University of Barcelona found traces of the virus when testing untreated wastewater samples dated March 12, 2019.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="289" end_char="292">More</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="294" end_char="305">surprisingly</TOKEN>
<TOKEN id="token-4-2" pos="punct" morph="none" start_char="306" end_char="306">,</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="308" end_char="318">researchers</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="320" end_char="321">at</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="323" end_char="325">the</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="327" end_char="336">University</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="338" end_char="339">of</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="341" end_char="349">Barcelona</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="351" end_char="355">found</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="357" end_char="362">traces</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="364" end_char="365">of</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="367" end_char="369">the</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="371" end_char="375">virus</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="377" end_char="380">when</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="382" end_char="388">testing</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="390" end_char="398">untreated</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="400" end_char="409">wastewater</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="411" end_char="417">samples</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="419" end_char="423">dated</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="425" end_char="429">March</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="431" end_char="432">12</TOKEN>
<TOKEN id="token-4-22" pos="punct" morph="none" start_char="433" end_char="433">,</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="435" end_char="438">2019</TOKEN>
<TOKEN id="token-4-24" pos="punct" morph="none" start_char="439" end_char="439">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="442" end_char="504">
<ORIGINAL_TEXT>The study was recently published on a preprint server, medRxiv.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="442" end_char="444">The</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="446" end_char="450">study</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="452" end_char="454">was</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="456" end_char="463">recently</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="465" end_char="473">published</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="475" end_char="476">on</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="478" end_char="478">a</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="480" end_char="487">preprint</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="489" end_char="494">server</TOKEN>
<TOKEN id="token-5-9" pos="punct" morph="none" start_char="495" end_char="495">,</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="497" end_char="503">medRxiv</TOKEN>
<TOKEN id="token-5-11" pos="punct" morph="none" start_char="504" end_char="504">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="506" end_char="635">
<ORIGINAL_TEXT>The paper is currently being subject to critical review by outside experts in preparation for publication in a scientific journal.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="506" end_char="508">The</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="510" end_char="514">paper</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="516" end_char="517">is</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="519" end_char="527">currently</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="529" end_char="533">being</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="535" end_char="541">subject</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="543" end_char="544">to</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="546" end_char="553">critical</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="555" end_char="560">review</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="562" end_char="563">by</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="565" end_char="571">outside</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="573" end_char="579">experts</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="581" end_char="582">in</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="584" end_char="594">preparation</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="596" end_char="598">for</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="600" end_char="610">publication</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="612" end_char="613">in</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="615" end_char="615">a</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="617" end_char="626">scientific</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="628" end_char="634">journal</TOKEN>
<TOKEN id="token-6-20" pos="punct" morph="none" start_char="635" end_char="635">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="637" end_char="744">
<ORIGINAL_TEXT>Until this process of peer review has been completed, though, the evidence needs to be treated with caution.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="637" end_char="641">Until</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="643" end_char="646">this</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="648" end_char="654">process</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="656" end_char="657">of</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="659" end_char="662">peer</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="664" end_char="669">review</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="671" end_char="673">has</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="675" end_char="678">been</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="680" end_char="688">completed</TOKEN>
<TOKEN id="token-7-9" pos="punct" morph="none" start_char="689" end_char="689">,</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="691" end_char="696">though</TOKEN>
<TOKEN id="token-7-11" pos="punct" morph="none" start_char="697" end_char="697">,</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="699" end_char="701">the</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="703" end_char="710">evidence</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="712" end_char="716">needs</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="718" end_char="719">to</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="721" end_char="722">be</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="724" end_char="730">treated</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="732" end_char="735">with</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="737" end_char="743">caution</TOKEN>
<TOKEN id="token-7-20" pos="punct" morph="none" start_char="744" end_char="744">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="747" end_char="824">
<ORIGINAL_TEXT>So, how was the experiment conducted and what exactly did the scientists find?</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="747" end_char="748">So</TOKEN>
<TOKEN id="token-8-1" pos="punct" morph="none" start_char="749" end_char="749">,</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="751" end_char="753">how</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="755" end_char="757">was</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="759" end_char="761">the</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="763" end_char="772">experiment</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="774" end_char="782">conducted</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="784" end_char="786">and</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="788" end_char="791">what</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="793" end_char="799">exactly</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="801" end_char="803">did</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="805" end_char="807">the</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="809" end_char="818">scientists</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="820" end_char="823">find</TOKEN>
<TOKEN id="token-8-14" pos="punct" morph="none" start_char="824" end_char="824">?</TOKEN>
</SEG>
<SEG id="segment-9" start_char="827" end_char="922">
<ORIGINAL_TEXT>One of the early findings about SARS-CoV-2 is that it is found in the faeces of infected people.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="827" end_char="829">One</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="831" end_char="832">of</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="834" end_char="836">the</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="838" end_char="842">early</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="844" end_char="851">findings</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="853" end_char="857">about</TOKEN>
<TOKEN id="token-9-6" pos="unknown" morph="none" start_char="859" end_char="868">SARS-CoV-2</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="870" end_char="871">is</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="873" end_char="876">that</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="878" end_char="879">it</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="881" end_char="882">is</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="884" end_char="888">found</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="890" end_char="891">in</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="893" end_char="895">the</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="897" end_char="902">faeces</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="904" end_char="905">of</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="907" end_char="914">infected</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="916" end_char="921">people</TOKEN>
<TOKEN id="token-9-18" pos="punct" morph="none" start_char="922" end_char="922">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="924" end_char="1141">
<ORIGINAL_TEXT>As the virus makes its way through the gut – where it can cause gastrointestinal symptoms – it loses its outer protein layer, but bits of genetic material called RNA survive the journey intact and are "shed" in faeces.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="924" end_char="925">As</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="927" end_char="929">the</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="931" end_char="935">virus</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="937" end_char="941">makes</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="943" end_char="945">its</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="947" end_char="949">way</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="951" end_char="957">through</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="959" end_char="961">the</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="963" end_char="965">gut</TOKEN>
<TOKEN id="token-10-9" pos="punct" morph="none" start_char="967" end_char="967">–</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="969" end_char="973">where</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="975" end_char="976">it</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="978" end_char="980">can</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="982" end_char="986">cause</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="988" end_char="1003">gastrointestinal</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1005" end_char="1012">symptoms</TOKEN>
<TOKEN id="token-10-16" pos="punct" morph="none" start_char="1014" end_char="1014">–</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1016" end_char="1017">it</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1019" end_char="1023">loses</TOKEN>
<TOKEN id="token-10-19" pos="word" morph="none" start_char="1025" end_char="1027">its</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="1029" end_char="1033">outer</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="1035" end_char="1041">protein</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="1043" end_char="1047">layer</TOKEN>
<TOKEN id="token-10-23" pos="punct" morph="none" start_char="1048" end_char="1048">,</TOKEN>
<TOKEN id="token-10-24" pos="word" morph="none" start_char="1050" end_char="1052">but</TOKEN>
<TOKEN id="token-10-25" pos="word" morph="none" start_char="1054" end_char="1057">bits</TOKEN>
<TOKEN id="token-10-26" pos="word" morph="none" start_char="1059" end_char="1060">of</TOKEN>
<TOKEN id="token-10-27" pos="word" morph="none" start_char="1062" end_char="1068">genetic</TOKEN>
<TOKEN id="token-10-28" pos="word" morph="none" start_char="1070" end_char="1077">material</TOKEN>
<TOKEN id="token-10-29" pos="word" morph="none" start_char="1079" end_char="1084">called</TOKEN>
<TOKEN id="token-10-30" pos="word" morph="none" start_char="1086" end_char="1088">RNA</TOKEN>
<TOKEN id="token-10-31" pos="word" morph="none" start_char="1090" end_char="1096">survive</TOKEN>
<TOKEN id="token-10-32" pos="word" morph="none" start_char="1098" end_char="1100">the</TOKEN>
<TOKEN id="token-10-33" pos="word" morph="none" start_char="1102" end_char="1108">journey</TOKEN>
<TOKEN id="token-10-34" pos="word" morph="none" start_char="1110" end_char="1115">intact</TOKEN>
<TOKEN id="token-10-35" pos="word" morph="none" start_char="1117" end_char="1119">and</TOKEN>
<TOKEN id="token-10-36" pos="word" morph="none" start_char="1121" end_char="1123">are</TOKEN>
<TOKEN id="token-10-37" pos="punct" morph="none" start_char="1125" end_char="1125">"</TOKEN>
<TOKEN id="token-10-38" pos="word" morph="none" start_char="1126" end_char="1129">shed</TOKEN>
<TOKEN id="token-10-39" pos="punct" morph="none" start_char="1130" end_char="1130">"</TOKEN>
<TOKEN id="token-10-40" pos="word" morph="none" start_char="1132" end_char="1133">in</TOKEN>
<TOKEN id="token-10-41" pos="word" morph="none" start_char="1135" end_char="1140">faeces</TOKEN>
<TOKEN id="token-10-42" pos="punct" morph="none" start_char="1141" end_char="1141">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1143" end_char="1222">
<ORIGINAL_TEXT>At this point, it is no longer infectious – as far as current evidence tells us.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1143" end_char="1144">At</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1146" end_char="1149">this</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1151" end_char="1155">point</TOKEN>
<TOKEN id="token-11-3" pos="punct" morph="none" start_char="1156" end_char="1156">,</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1158" end_char="1159">it</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1161" end_char="1162">is</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1164" end_char="1165">no</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1167" end_char="1172">longer</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1174" end_char="1183">infectious</TOKEN>
<TOKEN id="token-11-9" pos="punct" morph="none" start_char="1185" end_char="1185">–</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1187" end_char="1188">as</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1190" end_char="1192">far</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1194" end_char="1195">as</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1197" end_char="1203">current</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1205" end_char="1212">evidence</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1214" end_char="1218">tells</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1220" end_char="1221">us</TOKEN>
<TOKEN id="token-11-17" pos="punct" morph="none" start_char="1222" end_char="1222">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1225" end_char="1364">
<ORIGINAL_TEXT>But the fact that these bits of coronavirus RNA can be found in untreated wastewater (known as "influent") is useful for tracking outbreaks.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1225" end_char="1227">But</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1229" end_char="1231">the</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1233" end_char="1236">fact</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1238" end_char="1241">that</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1243" end_char="1247">these</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1249" end_char="1252">bits</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1254" end_char="1255">of</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1257" end_char="1267">coronavirus</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1269" end_char="1271">RNA</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1273" end_char="1275">can</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1277" end_char="1278">be</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1280" end_char="1284">found</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1286" end_char="1287">in</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1289" end_char="1297">untreated</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1299" end_char="1308">wastewater</TOKEN>
<TOKEN id="token-12-15" pos="punct" morph="none" start_char="1310" end_char="1310">(</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1311" end_char="1315">known</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1317" end_char="1318">as</TOKEN>
<TOKEN id="token-12-18" pos="punct" morph="none" start_char="1320" end_char="1320">"</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="1321" end_char="1328">influent</TOKEN>
<TOKEN id="token-12-20" pos="punct" morph="none" start_char="1329" end_char="1330">")</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1332" end_char="1333">is</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1335" end_char="1340">useful</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1342" end_char="1344">for</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1346" end_char="1353">tracking</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1355" end_char="1363">outbreaks</TOKEN>
<TOKEN id="token-12-26" pos="punct" morph="none" start_char="1364" end_char="1364">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1366" end_char="1565">
<ORIGINAL_TEXT>Indeed, they can predict where an outbreak is likely to occur a week to ten days before they show up in official figures – the reason being that people shed coronavirus before symptoms become evident.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1366" end_char="1371">Indeed</TOKEN>
<TOKEN id="token-13-1" pos="punct" morph="none" start_char="1372" end_char="1372">,</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1374" end_char="1377">they</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1379" end_char="1381">can</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1383" end_char="1389">predict</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1391" end_char="1395">where</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1397" end_char="1398">an</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1400" end_char="1407">outbreak</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1409" end_char="1410">is</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1412" end_char="1417">likely</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1419" end_char="1420">to</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1422" end_char="1426">occur</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1428" end_char="1428">a</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="1430" end_char="1433">week</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="1435" end_char="1436">to</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="1438" end_char="1440">ten</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="1442" end_char="1445">days</TOKEN>
<TOKEN id="token-13-17" pos="word" morph="none" start_char="1447" end_char="1452">before</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="1454" end_char="1457">they</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="1459" end_char="1462">show</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="1464" end_char="1465">up</TOKEN>
<TOKEN id="token-13-21" pos="word" morph="none" start_char="1467" end_char="1468">in</TOKEN>
<TOKEN id="token-13-22" pos="word" morph="none" start_char="1470" end_char="1477">official</TOKEN>
<TOKEN id="token-13-23" pos="word" morph="none" start_char="1479" end_char="1485">figures</TOKEN>
<TOKEN id="token-13-24" pos="punct" morph="none" start_char="1487" end_char="1487">–</TOKEN>
<TOKEN id="token-13-25" pos="word" morph="none" start_char="1489" end_char="1491">the</TOKEN>
<TOKEN id="token-13-26" pos="word" morph="none" start_char="1493" end_char="1498">reason</TOKEN>
<TOKEN id="token-13-27" pos="word" morph="none" start_char="1500" end_char="1504">being</TOKEN>
<TOKEN id="token-13-28" pos="word" morph="none" start_char="1506" end_char="1509">that</TOKEN>
<TOKEN id="token-13-29" pos="word" morph="none" start_char="1511" end_char="1516">people</TOKEN>
<TOKEN id="token-13-30" pos="word" morph="none" start_char="1518" end_char="1521">shed</TOKEN>
<TOKEN id="token-13-31" pos="word" morph="none" start_char="1523" end_char="1533">coronavirus</TOKEN>
<TOKEN id="token-13-32" pos="word" morph="none" start_char="1535" end_char="1540">before</TOKEN>
<TOKEN id="token-13-33" pos="word" morph="none" start_char="1542" end_char="1549">symptoms</TOKEN>
<TOKEN id="token-13-34" pos="word" morph="none" start_char="1551" end_char="1556">become</TOKEN>
<TOKEN id="token-13-35" pos="word" morph="none" start_char="1558" end_char="1564">evident</TOKEN>
<TOKEN id="token-13-36" pos="punct" morph="none" start_char="1565" end_char="1565">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1567" end_char="1735">
<ORIGINAL_TEXT>These "pre-symptomatic" people then have to get sick enough to be tested, get the results, and be admitted to a hospital as an official "case", hence the week or so lag.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1567" end_char="1571">These</TOKEN>
<TOKEN id="token-14-1" pos="punct" morph="none" start_char="1573" end_char="1573">"</TOKEN>
<TOKEN id="token-14-2" pos="unknown" morph="none" start_char="1574" end_char="1588">pre-symptomatic</TOKEN>
<TOKEN id="token-14-3" pos="punct" morph="none" start_char="1589" end_char="1589">"</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1591" end_char="1596">people</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1598" end_char="1601">then</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1603" end_char="1606">have</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1608" end_char="1609">to</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1611" end_char="1613">get</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1615" end_char="1618">sick</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1620" end_char="1625">enough</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1627" end_char="1628">to</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="1630" end_char="1631">be</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="1633" end_char="1638">tested</TOKEN>
<TOKEN id="token-14-14" pos="punct" morph="none" start_char="1639" end_char="1639">,</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="1641" end_char="1643">get</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="1645" end_char="1647">the</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="1649" end_char="1655">results</TOKEN>
<TOKEN id="token-14-18" pos="punct" morph="none" start_char="1656" end_char="1656">,</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="1658" end_char="1660">and</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="1662" end_char="1663">be</TOKEN>
<TOKEN id="token-14-21" pos="word" morph="none" start_char="1665" end_char="1672">admitted</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="1674" end_char="1675">to</TOKEN>
<TOKEN id="token-14-23" pos="word" morph="none" start_char="1677" end_char="1677">a</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="1679" end_char="1686">hospital</TOKEN>
<TOKEN id="token-14-25" pos="word" morph="none" start_char="1688" end_char="1689">as</TOKEN>
<TOKEN id="token-14-26" pos="word" morph="none" start_char="1691" end_char="1692">an</TOKEN>
<TOKEN id="token-14-27" pos="word" morph="none" start_char="1694" end_char="1701">official</TOKEN>
<TOKEN id="token-14-28" pos="punct" morph="none" start_char="1703" end_char="1703">"</TOKEN>
<TOKEN id="token-14-29" pos="word" morph="none" start_char="1704" end_char="1707">case</TOKEN>
<TOKEN id="token-14-30" pos="punct" morph="none" start_char="1708" end_char="1709">",</TOKEN>
<TOKEN id="token-14-31" pos="word" morph="none" start_char="1711" end_char="1715">hence</TOKEN>
<TOKEN id="token-14-32" pos="word" morph="none" start_char="1717" end_char="1719">the</TOKEN>
<TOKEN id="token-14-33" pos="word" morph="none" start_char="1721" end_char="1724">week</TOKEN>
<TOKEN id="token-14-34" pos="word" morph="none" start_char="1726" end_char="1727">or</TOKEN>
<TOKEN id="token-14-35" pos="word" morph="none" start_char="1729" end_char="1730">so</TOKEN>
<TOKEN id="token-14-36" pos="word" morph="none" start_char="1732" end_char="1734">lag</TOKEN>
<TOKEN id="token-14-37" pos="punct" morph="none" start_char="1735" end_char="1735">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1738" end_char="1839">
<ORIGINAL_TEXT>As a result, many countries, including Spain, are now monitoring wastewater for traces of coronavirus.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1738" end_char="1739">As</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1741" end_char="1741">a</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1743" end_char="1748">result</TOKEN>
<TOKEN id="token-15-3" pos="punct" morph="none" start_char="1749" end_char="1749">,</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1751" end_char="1754">many</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1756" end_char="1764">countries</TOKEN>
<TOKEN id="token-15-6" pos="punct" morph="none" start_char="1765" end_char="1765">,</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1767" end_char="1775">including</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1777" end_char="1781">Spain</TOKEN>
<TOKEN id="token-15-9" pos="punct" morph="none" start_char="1782" end_char="1782">,</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1784" end_char="1786">are</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="1788" end_char="1790">now</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="1792" end_char="1801">monitoring</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="1803" end_char="1812">wastewater</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="1814" end_char="1816">for</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="1818" end_char="1823">traces</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="1825" end_char="1826">of</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="1828" end_char="1838">coronavirus</TOKEN>
<TOKEN id="token-15-18" pos="punct" morph="none" start_char="1839" end_char="1839">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1841" end_char="2023">
<ORIGINAL_TEXT>In this particular study, wastewater epidemiologists were examining frozen samples of influent between January 2018 and December 2019 to see when the virus made its debut in the city.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1841" end_char="1842">In</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1844" end_char="1847">this</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1849" end_char="1858">particular</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1860" end_char="1864">study</TOKEN>
<TOKEN id="token-16-4" pos="punct" morph="none" start_char="1865" end_char="1865">,</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1867" end_char="1876">wastewater</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="1878" end_char="1892">epidemiologists</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="1894" end_char="1897">were</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="1899" end_char="1907">examining</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="1909" end_char="1914">frozen</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="1916" end_char="1922">samples</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="1924" end_char="1925">of</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="1927" end_char="1934">influent</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="1936" end_char="1942">between</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="1944" end_char="1950">January</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="1952" end_char="1955">2018</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="1957" end_char="1959">and</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="1961" end_char="1968">December</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="1970" end_char="1973">2019</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="1975" end_char="1976">to</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="1978" end_char="1980">see</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="1982" end_char="1985">when</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="1987" end_char="1989">the</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="1991" end_char="1995">virus</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="1997" end_char="2000">made</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="2002" end_char="2004">its</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="2006" end_char="2010">debut</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="2012" end_char="2013">in</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="2015" end_char="2017">the</TOKEN>
<TOKEN id="token-16-29" pos="word" morph="none" start_char="2019" end_char="2022">city</TOKEN>
<TOKEN id="token-16-30" pos="punct" morph="none" start_char="2023" end_char="2023">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2026" end_char="2101">
<ORIGINAL_TEXT>Experts around the world are monitoring wastewater for signs of coronavirus.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2026" end_char="2032">Experts</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2034" end_char="2039">around</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2041" end_char="2043">the</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2045" end_char="2049">world</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2051" end_char="2053">are</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2055" end_char="2064">monitoring</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2066" end_char="2075">wastewater</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="2077" end_char="2079">for</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="2081" end_char="2085">signs</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="2087" end_char="2088">of</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="2090" end_char="2100">coronavirus</TOKEN>
<TOKEN id="token-17-11" pos="punct" morph="none" start_char="2101" end_char="2101">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2105" end_char="2231">
<ORIGINAL_TEXT>They found evidence of the virus on January 15, 2020, 41 days before the first official case was declared on February 25, 2020.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2105" end_char="2108">They</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2110" end_char="2114">found</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="2116" end_char="2123">evidence</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="2125" end_char="2126">of</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2128" end_char="2130">the</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="2132" end_char="2136">virus</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2138" end_char="2139">on</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2141" end_char="2147">January</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2149" end_char="2150">15</TOKEN>
<TOKEN id="token-18-9" pos="punct" morph="none" start_char="2151" end_char="2151">,</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2153" end_char="2156">2020</TOKEN>
<TOKEN id="token-18-11" pos="punct" morph="none" start_char="2157" end_char="2157">,</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="2159" end_char="2160">41</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="2162" end_char="2165">days</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="2167" end_char="2172">before</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="2174" end_char="2176">the</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="2178" end_char="2182">first</TOKEN>
<TOKEN id="token-18-17" pos="word" morph="none" start_char="2184" end_char="2191">official</TOKEN>
<TOKEN id="token-18-18" pos="word" morph="none" start_char="2193" end_char="2196">case</TOKEN>
<TOKEN id="token-18-19" pos="word" morph="none" start_char="2198" end_char="2200">was</TOKEN>
<TOKEN id="token-18-20" pos="word" morph="none" start_char="2202" end_char="2209">declared</TOKEN>
<TOKEN id="token-18-21" pos="word" morph="none" start_char="2211" end_char="2212">on</TOKEN>
<TOKEN id="token-18-22" pos="word" morph="none" start_char="2214" end_char="2221">February</TOKEN>
<TOKEN id="token-18-23" pos="word" morph="none" start_char="2223" end_char="2224">25</TOKEN>
<TOKEN id="token-18-24" pos="punct" morph="none" start_char="2225" end_char="2225">,</TOKEN>
<TOKEN id="token-18-25" pos="word" morph="none" start_char="2227" end_char="2230">2020</TOKEN>
<TOKEN id="token-18-26" pos="punct" morph="none" start_char="2231" end_char="2231">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2233" end_char="2384">
<ORIGINAL_TEXT>All the samples before this date were negative, except for a sample from March 12, 2019, which gave a positive result in their PCR test for coronavirus.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2233" end_char="2235">All</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2237" end_char="2239">the</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2241" end_char="2247">samples</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="2249" end_char="2254">before</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2256" end_char="2259">this</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="2261" end_char="2264">date</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2266" end_char="2269">were</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="2271" end_char="2278">negative</TOKEN>
<TOKEN id="token-19-8" pos="punct" morph="none" start_char="2279" end_char="2279">,</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="2281" end_char="2286">except</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="2288" end_char="2290">for</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="2292" end_char="2292">a</TOKEN>
<TOKEN id="token-19-12" pos="word" morph="none" start_char="2294" end_char="2299">sample</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="2301" end_char="2304">from</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="2306" end_char="2310">March</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="2312" end_char="2313">12</TOKEN>
<TOKEN id="token-19-16" pos="punct" morph="none" start_char="2314" end_char="2314">,</TOKEN>
<TOKEN id="token-19-17" pos="word" morph="none" start_char="2316" end_char="2319">2019</TOKEN>
<TOKEN id="token-19-18" pos="punct" morph="none" start_char="2320" end_char="2320">,</TOKEN>
<TOKEN id="token-19-19" pos="word" morph="none" start_char="2322" end_char="2326">which</TOKEN>
<TOKEN id="token-19-20" pos="word" morph="none" start_char="2328" end_char="2331">gave</TOKEN>
<TOKEN id="token-19-21" pos="word" morph="none" start_char="2333" end_char="2333">a</TOKEN>
<TOKEN id="token-19-22" pos="word" morph="none" start_char="2335" end_char="2342">positive</TOKEN>
<TOKEN id="token-19-23" pos="word" morph="none" start_char="2344" end_char="2349">result</TOKEN>
<TOKEN id="token-19-24" pos="word" morph="none" start_char="2351" end_char="2352">in</TOKEN>
<TOKEN id="token-19-25" pos="word" morph="none" start_char="2354" end_char="2358">their</TOKEN>
<TOKEN id="token-19-26" pos="word" morph="none" start_char="2360" end_char="2362">PCR</TOKEN>
<TOKEN id="token-19-27" pos="word" morph="none" start_char="2364" end_char="2367">test</TOKEN>
<TOKEN id="token-19-28" pos="word" morph="none" start_char="2369" end_char="2371">for</TOKEN>
<TOKEN id="token-19-29" pos="word" morph="none" start_char="2373" end_char="2383">coronavirus</TOKEN>
<TOKEN id="token-19-30" pos="punct" morph="none" start_char="2384" end_char="2384">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2386" end_char="2464">
<ORIGINAL_TEXT>PCR is the standard way of testing to see if someone currently has the disease.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2386" end_char="2388">PCR</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="2390" end_char="2391">is</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="2393" end_char="2395">the</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="2397" end_char="2404">standard</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2406" end_char="2408">way</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2410" end_char="2411">of</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2413" end_char="2419">testing</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="2421" end_char="2422">to</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="2424" end_char="2426">see</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="2428" end_char="2429">if</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="2431" end_char="2437">someone</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="2439" end_char="2447">currently</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="2449" end_char="2451">has</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="2453" end_char="2455">the</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="2457" end_char="2463">disease</TOKEN>
<TOKEN id="token-20-15" pos="punct" morph="none" start_char="2464" end_char="2464">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="2467" end_char="2758">
<ORIGINAL_TEXT>PCR involves getting samples of saliva, mucus, frozen wastewater or whatever else the virus is thought to be lurking in, clearing all the unnecessary stuff out of the sample, then converting the RNA – which is a single strand of genetic material – into DNA (the famous double-stranded helix).</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="2467" end_char="2469">PCR</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="2471" end_char="2478">involves</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="2480" end_char="2486">getting</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="2488" end_char="2494">samples</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="2496" end_char="2497">of</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="2499" end_char="2504">saliva</TOKEN>
<TOKEN id="token-21-6" pos="punct" morph="none" start_char="2505" end_char="2505">,</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="2507" end_char="2511">mucus</TOKEN>
<TOKEN id="token-21-8" pos="punct" morph="none" start_char="2512" end_char="2512">,</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="2514" end_char="2519">frozen</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="2521" end_char="2530">wastewater</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="2532" end_char="2533">or</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="2535" end_char="2542">whatever</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="2544" end_char="2547">else</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="2549" end_char="2551">the</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="2553" end_char="2557">virus</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="2559" end_char="2560">is</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="2562" end_char="2568">thought</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="2570" end_char="2571">to</TOKEN>
<TOKEN id="token-21-19" pos="word" morph="none" start_char="2573" end_char="2574">be</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="2576" end_char="2582">lurking</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="2584" end_char="2585">in</TOKEN>
<TOKEN id="token-21-22" pos="punct" morph="none" start_char="2586" end_char="2586">,</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="2588" end_char="2595">clearing</TOKEN>
<TOKEN id="token-21-24" pos="word" morph="none" start_char="2597" end_char="2599">all</TOKEN>
<TOKEN id="token-21-25" pos="word" morph="none" start_char="2601" end_char="2603">the</TOKEN>
<TOKEN id="token-21-26" pos="word" morph="none" start_char="2605" end_char="2615">unnecessary</TOKEN>
<TOKEN id="token-21-27" pos="word" morph="none" start_char="2617" end_char="2621">stuff</TOKEN>
<TOKEN id="token-21-28" pos="word" morph="none" start_char="2623" end_char="2625">out</TOKEN>
<TOKEN id="token-21-29" pos="word" morph="none" start_char="2627" end_char="2628">of</TOKEN>
<TOKEN id="token-21-30" pos="word" morph="none" start_char="2630" end_char="2632">the</TOKEN>
<TOKEN id="token-21-31" pos="word" morph="none" start_char="2634" end_char="2639">sample</TOKEN>
<TOKEN id="token-21-32" pos="punct" morph="none" start_char="2640" end_char="2640">,</TOKEN>
<TOKEN id="token-21-33" pos="word" morph="none" start_char="2642" end_char="2645">then</TOKEN>
<TOKEN id="token-21-34" pos="word" morph="none" start_char="2647" end_char="2656">converting</TOKEN>
<TOKEN id="token-21-35" pos="word" morph="none" start_char="2658" end_char="2660">the</TOKEN>
<TOKEN id="token-21-36" pos="word" morph="none" start_char="2662" end_char="2664">RNA</TOKEN>
<TOKEN id="token-21-37" pos="punct" morph="none" start_char="2666" end_char="2666">–</TOKEN>
<TOKEN id="token-21-38" pos="word" morph="none" start_char="2668" end_char="2672">which</TOKEN>
<TOKEN id="token-21-39" pos="word" morph="none" start_char="2674" end_char="2675">is</TOKEN>
<TOKEN id="token-21-40" pos="word" morph="none" start_char="2677" end_char="2677">a</TOKEN>
<TOKEN id="token-21-41" pos="word" morph="none" start_char="2679" end_char="2684">single</TOKEN>
<TOKEN id="token-21-42" pos="word" morph="none" start_char="2686" end_char="2691">strand</TOKEN>
<TOKEN id="token-21-43" pos="word" morph="none" start_char="2693" end_char="2694">of</TOKEN>
<TOKEN id="token-21-44" pos="word" morph="none" start_char="2696" end_char="2702">genetic</TOKEN>
<TOKEN id="token-21-45" pos="word" morph="none" start_char="2704" end_char="2711">material</TOKEN>
<TOKEN id="token-21-46" pos="punct" morph="none" start_char="2713" end_char="2713">–</TOKEN>
<TOKEN id="token-21-47" pos="word" morph="none" start_char="2715" end_char="2718">into</TOKEN>
<TOKEN id="token-21-48" pos="word" morph="none" start_char="2720" end_char="2722">DNA</TOKEN>
<TOKEN id="token-21-49" pos="punct" morph="none" start_char="2724" end_char="2724">(</TOKEN>
<TOKEN id="token-21-50" pos="word" morph="none" start_char="2725" end_char="2727">the</TOKEN>
<TOKEN id="token-21-51" pos="word" morph="none" start_char="2729" end_char="2734">famous</TOKEN>
<TOKEN id="token-21-52" pos="unknown" morph="none" start_char="2736" end_char="2750">double-stranded</TOKEN>
<TOKEN id="token-21-53" pos="word" morph="none" start_char="2752" end_char="2756">helix</TOKEN>
<TOKEN id="token-21-54" pos="punct" morph="none" start_char="2757" end_char="2758">).</TOKEN>
</SEG>
<SEG id="segment-22" start_char="2760" end_char="2955">
<ORIGINAL_TEXT>The DNA is then "amplified" in successive cycles until key bits of genetic material that are known to only exist in a particular virus are plentiful enough to be detected with a fluorescent probe.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="2760" end_char="2762">The</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="2764" end_char="2766">DNA</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="2768" end_char="2769">is</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="2771" end_char="2774">then</TOKEN>
<TOKEN id="token-22-4" pos="punct" morph="none" start_char="2776" end_char="2776">"</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="2777" end_char="2785">amplified</TOKEN>
<TOKEN id="token-22-6" pos="punct" morph="none" start_char="2786" end_char="2786">"</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="2788" end_char="2789">in</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="2791" end_char="2800">successive</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="2802" end_char="2807">cycles</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="2809" end_char="2813">until</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="2815" end_char="2817">key</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="2819" end_char="2822">bits</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="2824" end_char="2825">of</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="2827" end_char="2833">genetic</TOKEN>
<TOKEN id="token-22-15" pos="word" morph="none" start_char="2835" end_char="2842">material</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="2844" end_char="2847">that</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="2849" end_char="2851">are</TOKEN>
<TOKEN id="token-22-18" pos="word" morph="none" start_char="2853" end_char="2857">known</TOKEN>
<TOKEN id="token-22-19" pos="word" morph="none" start_char="2859" end_char="2860">to</TOKEN>
<TOKEN id="token-22-20" pos="word" morph="none" start_char="2862" end_char="2865">only</TOKEN>
<TOKEN id="token-22-21" pos="word" morph="none" start_char="2867" end_char="2871">exist</TOKEN>
<TOKEN id="token-22-22" pos="word" morph="none" start_char="2873" end_char="2874">in</TOKEN>
<TOKEN id="token-22-23" pos="word" morph="none" start_char="2876" end_char="2876">a</TOKEN>
<TOKEN id="token-22-24" pos="word" morph="none" start_char="2878" end_char="2887">particular</TOKEN>
<TOKEN id="token-22-25" pos="word" morph="none" start_char="2889" end_char="2893">virus</TOKEN>
<TOKEN id="token-22-26" pos="word" morph="none" start_char="2895" end_char="2897">are</TOKEN>
<TOKEN id="token-22-27" pos="word" morph="none" start_char="2899" end_char="2907">plentiful</TOKEN>
<TOKEN id="token-22-28" pos="word" morph="none" start_char="2909" end_char="2914">enough</TOKEN>
<TOKEN id="token-22-29" pos="word" morph="none" start_char="2916" end_char="2917">to</TOKEN>
<TOKEN id="token-22-30" pos="word" morph="none" start_char="2919" end_char="2920">be</TOKEN>
<TOKEN id="token-22-31" pos="word" morph="none" start_char="2922" end_char="2929">detected</TOKEN>
<TOKEN id="token-22-32" pos="word" morph="none" start_char="2931" end_char="2934">with</TOKEN>
<TOKEN id="token-22-33" pos="word" morph="none" start_char="2936" end_char="2936">a</TOKEN>
<TOKEN id="token-22-34" pos="word" morph="none" start_char="2938" end_char="2948">fluorescent</TOKEN>
<TOKEN id="token-22-35" pos="word" morph="none" start_char="2950" end_char="2954">probe</TOKEN>
<TOKEN id="token-22-36" pos="punct" morph="none" start_char="2955" end_char="2955">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="2958" end_char="2976">
<ORIGINAL_TEXT>Not highly specific</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="2958" end_char="2960">Not</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="2962" end_char="2967">highly</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="2969" end_char="2976">specific</TOKEN>
</SEG>
<SEG id="segment-24" start_char="2980" end_char="3054">
<ORIGINAL_TEXT>In coronavirus testing, scientists typically screen for more than one gene.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="2980" end_char="2981">In</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="2983" end_char="2993">coronavirus</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="2995" end_char="3001">testing</TOKEN>
<TOKEN id="token-24-3" pos="punct" morph="none" start_char="3002" end_char="3002">,</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="3004" end_char="3013">scientists</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="3015" end_char="3023">typically</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="3025" end_char="3030">screen</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="3032" end_char="3034">for</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="3036" end_char="3039">more</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="3041" end_char="3044">than</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="3046" end_char="3048">one</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="3050" end_char="3053">gene</TOKEN>
<TOKEN id="token-24-12" pos="punct" morph="none" start_char="3054" end_char="3054">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="3056" end_char="3102">
<ORIGINAL_TEXT>In this case, the researchers tested for three.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="3056" end_char="3057">In</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="3059" end_char="3062">this</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="3064" end_char="3067">case</TOKEN>
<TOKEN id="token-25-3" pos="punct" morph="none" start_char="3068" end_char="3068">,</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="3070" end_char="3072">the</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="3074" end_char="3084">researchers</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="3086" end_char="3091">tested</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="3093" end_char="3095">for</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="3097" end_char="3101">three</TOKEN>
<TOKEN id="token-25-9" pos="punct" morph="none" start_char="3102" end_char="3102">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="3104" end_char="3205">
<ORIGINAL_TEXT>They had a positive result for the March 2019 sample in one of the three genes tested – the RdRp gene.</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="3104" end_char="3107">They</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="3109" end_char="3111">had</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="3113" end_char="3113">a</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="3115" end_char="3122">positive</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="3124" end_char="3129">result</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="3131" end_char="3133">for</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="3135" end_char="3137">the</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="3139" end_char="3143">March</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="3145" end_char="3148">2019</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="3150" end_char="3155">sample</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="3157" end_char="3158">in</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="3160" end_char="3162">one</TOKEN>
<TOKEN id="token-26-12" pos="word" morph="none" start_char="3164" end_char="3165">of</TOKEN>
<TOKEN id="token-26-13" pos="word" morph="none" start_char="3167" end_char="3169">the</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="3171" end_char="3175">three</TOKEN>
<TOKEN id="token-26-15" pos="word" morph="none" start_char="3177" end_char="3181">genes</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="3183" end_char="3188">tested</TOKEN>
<TOKEN id="token-26-17" pos="punct" morph="none" start_char="3190" end_char="3190">–</TOKEN>
<TOKEN id="token-26-18" pos="word" morph="none" start_char="3192" end_char="3194">the</TOKEN>
<TOKEN id="token-26-19" pos="word" morph="none" start_char="3196" end_char="3199">RdRp</TOKEN>
<TOKEN id="token-26-20" pos="word" morph="none" start_char="3201" end_char="3204">gene</TOKEN>
<TOKEN id="token-26-21" pos="punct" morph="none" start_char="3205" end_char="3205">.</TOKEN>
</SEG>
<SEG id="segment-27" start_char="3207" end_char="3316">
<ORIGINAL_TEXT>They screened for two regions of this gene and both were only detected around the 39th cycle of amplification.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="3207" end_char="3210">They</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="3212" end_char="3219">screened</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="3221" end_char="3223">for</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="3225" end_char="3227">two</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="3229" end_char="3235">regions</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="3237" end_char="3238">of</TOKEN>
<TOKEN id="token-27-6" pos="word" morph="none" start_char="3240" end_char="3243">this</TOKEN>
<TOKEN id="token-27-7" pos="word" morph="none" start_char="3245" end_char="3248">gene</TOKEN>
<TOKEN id="token-27-8" pos="word" morph="none" start_char="3250" end_char="3252">and</TOKEN>
<TOKEN id="token-27-9" pos="word" morph="none" start_char="3254" end_char="3257">both</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="3259" end_char="3262">were</TOKEN>
<TOKEN id="token-27-11" pos="word" morph="none" start_char="3264" end_char="3267">only</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="3269" end_char="3276">detected</TOKEN>
<TOKEN id="token-27-13" pos="word" morph="none" start_char="3278" end_char="3283">around</TOKEN>
<TOKEN id="token-27-14" pos="word" morph="none" start_char="3285" end_char="3287">the</TOKEN>
<TOKEN id="token-27-15" pos="word" morph="none" start_char="3289" end_char="3292">39th</TOKEN>
<TOKEN id="token-27-16" pos="word" morph="none" start_char="3294" end_char="3298">cycle</TOKEN>
<TOKEN id="token-27-17" pos="word" morph="none" start_char="3300" end_char="3301">of</TOKEN>
<TOKEN id="token-27-18" pos="word" morph="none" start_char="3303" end_char="3315">amplification</TOKEN>
<TOKEN id="token-27-19" pos="punct" morph="none" start_char="3316" end_char="3316">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="3318" end_char="3391">
<ORIGINAL_TEXT>(PCR tests become less "specific" with increasing rounds of amplification.</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="punct" morph="none" start_char="3318" end_char="3318">(</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="3319" end_char="3321">PCR</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="3323" end_char="3327">tests</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="3329" end_char="3334">become</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="3336" end_char="3339">less</TOKEN>
<TOKEN id="token-28-5" pos="punct" morph="none" start_char="3341" end_char="3341">"</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="3342" end_char="3349">specific</TOKEN>
<TOKEN id="token-28-7" pos="punct" morph="none" start_char="3350" end_char="3350">"</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="3352" end_char="3355">with</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="3357" end_char="3366">increasing</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="3368" end_char="3373">rounds</TOKEN>
<TOKEN id="token-28-11" pos="word" morph="none" start_char="3375" end_char="3376">of</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="3378" end_char="3390">amplification</TOKEN>
<TOKEN id="token-28-13" pos="punct" morph="none" start_char="3391" end_char="3391">.</TOKEN>
</SEG>
<SEG id="segment-29" start_char="3393" end_char="3451">
<ORIGINAL_TEXT>Scientists generally use 40 to 45 rounds of amplification.)</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="3393" end_char="3402">Scientists</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="3404" end_char="3412">generally</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="3414" end_char="3416">use</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="3418" end_char="3419">40</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="3421" end_char="3422">to</TOKEN>
<TOKEN id="token-29-5" pos="word" morph="none" start_char="3424" end_char="3425">45</TOKEN>
<TOKEN id="token-29-6" pos="word" morph="none" start_char="3427" end_char="3432">rounds</TOKEN>
<TOKEN id="token-29-7" pos="word" morph="none" start_char="3434" end_char="3435">of</TOKEN>
<TOKEN id="token-29-8" pos="word" morph="none" start_char="3437" end_char="3449">amplification</TOKEN>
<TOKEN id="token-29-9" pos="punct" morph="none" start_char="3450" end_char="3451">.)</TOKEN>
</SEG>
<SEG id="segment-30" start_char="3454" end_char="3509">
<ORIGINAL_TEXT>There are several explanations for this positive result.</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="3454" end_char="3458">There</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="3460" end_char="3462">are</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="3464" end_char="3470">several</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="3472" end_char="3483">explanations</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="3485" end_char="3487">for</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="3489" end_char="3492">this</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="3494" end_char="3501">positive</TOKEN>
<TOKEN id="token-30-7" pos="word" morph="none" start_char="3503" end_char="3508">result</TOKEN>
<TOKEN id="token-30-8" pos="punct" morph="none" start_char="3509" end_char="3509">.</TOKEN>
</SEG>
<SEG id="segment-31" start_char="3511" end_char="3578">
<ORIGINAL_TEXT>One is that SARS-CoV-2 is present in the sewage at a very low level.</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="word" morph="none" start_char="3511" end_char="3513">One</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="3515" end_char="3516">is</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="3518" end_char="3521">that</TOKEN>
<TOKEN id="token-31-3" pos="unknown" morph="none" start_char="3523" end_char="3532">SARS-CoV-2</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="3534" end_char="3535">is</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="3537" end_char="3543">present</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="3545" end_char="3546">in</TOKEN>
<TOKEN id="token-31-7" pos="word" morph="none" start_char="3548" end_char="3550">the</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="3552" end_char="3557">sewage</TOKEN>
<TOKEN id="token-31-9" pos="word" morph="none" start_char="3559" end_char="3560">at</TOKEN>
<TOKEN id="token-31-10" pos="word" morph="none" start_char="3562" end_char="3562">a</TOKEN>
<TOKEN id="token-31-11" pos="word" morph="none" start_char="3564" end_char="3567">very</TOKEN>
<TOKEN id="token-31-12" pos="word" morph="none" start_char="3569" end_char="3571">low</TOKEN>
<TOKEN id="token-31-13" pos="word" morph="none" start_char="3573" end_char="3577">level</TOKEN>
<TOKEN id="token-31-14" pos="punct" morph="none" start_char="3578" end_char="3578">.</TOKEN>
</SEG>
<SEG id="segment-32" start_char="3580" end_char="3677">
<ORIGINAL_TEXT>Another is that the test reaction was accidentally contaminated with SARS-CoV-2 in the laboratory.</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="3580" end_char="3586">Another</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="3588" end_char="3589">is</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="3591" end_char="3594">that</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="3596" end_char="3598">the</TOKEN>
<TOKEN id="token-32-4" pos="word" morph="none" start_char="3600" end_char="3603">test</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="3605" end_char="3612">reaction</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="3614" end_char="3616">was</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="3618" end_char="3629">accidentally</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="3631" end_char="3642">contaminated</TOKEN>
<TOKEN id="token-32-9" pos="word" morph="none" start_char="3644" end_char="3647">with</TOKEN>
<TOKEN id="token-32-10" pos="unknown" morph="none" start_char="3649" end_char="3658">SARS-CoV-2</TOKEN>
<TOKEN id="token-32-11" pos="word" morph="none" start_char="3660" end_char="3661">in</TOKEN>
<TOKEN id="token-32-12" pos="word" morph="none" start_char="3663" end_char="3665">the</TOKEN>
<TOKEN id="token-32-13" pos="word" morph="none" start_char="3667" end_char="3676">laboratory</TOKEN>
<TOKEN id="token-32-14" pos="punct" morph="none" start_char="3677" end_char="3677">.</TOKEN>
</SEG>
<SEG id="segment-33" start_char="3679" end_char="3851">
<ORIGINAL_TEXT>This sometimes happens in labs as positive samples are regularly being handled, and it can be difficult to prevent very small traces of positive sample contaminating others.</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="word" morph="none" start_char="3679" end_char="3682">This</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="3684" end_char="3692">sometimes</TOKEN>
<TOKEN id="token-33-2" pos="word" morph="none" start_char="3694" end_char="3700">happens</TOKEN>
<TOKEN id="token-33-3" pos="word" morph="none" start_char="3702" end_char="3703">in</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="3705" end_char="3708">labs</TOKEN>
<TOKEN id="token-33-5" pos="word" morph="none" start_char="3710" end_char="3711">as</TOKEN>
<TOKEN id="token-33-6" pos="word" morph="none" start_char="3713" end_char="3720">positive</TOKEN>
<TOKEN id="token-33-7" pos="word" morph="none" start_char="3722" end_char="3728">samples</TOKEN>
<TOKEN id="token-33-8" pos="word" morph="none" start_char="3730" end_char="3732">are</TOKEN>
<TOKEN id="token-33-9" pos="word" morph="none" start_char="3734" end_char="3742">regularly</TOKEN>
<TOKEN id="token-33-10" pos="word" morph="none" start_char="3744" end_char="3748">being</TOKEN>
<TOKEN id="token-33-11" pos="word" morph="none" start_char="3750" end_char="3756">handled</TOKEN>
<TOKEN id="token-33-12" pos="punct" morph="none" start_char="3757" end_char="3757">,</TOKEN>
<TOKEN id="token-33-13" pos="word" morph="none" start_char="3759" end_char="3761">and</TOKEN>
<TOKEN id="token-33-14" pos="word" morph="none" start_char="3763" end_char="3764">it</TOKEN>
<TOKEN id="token-33-15" pos="word" morph="none" start_char="3766" end_char="3768">can</TOKEN>
<TOKEN id="token-33-16" pos="word" morph="none" start_char="3770" end_char="3771">be</TOKEN>
<TOKEN id="token-33-17" pos="word" morph="none" start_char="3773" end_char="3781">difficult</TOKEN>
<TOKEN id="token-33-18" pos="word" morph="none" start_char="3783" end_char="3784">to</TOKEN>
<TOKEN id="token-33-19" pos="word" morph="none" start_char="3786" end_char="3792">prevent</TOKEN>
<TOKEN id="token-33-20" pos="word" morph="none" start_char="3794" end_char="3797">very</TOKEN>
<TOKEN id="token-33-21" pos="word" morph="none" start_char="3799" end_char="3803">small</TOKEN>
<TOKEN id="token-33-22" pos="word" morph="none" start_char="3805" end_char="3810">traces</TOKEN>
<TOKEN id="token-33-23" pos="word" morph="none" start_char="3812" end_char="3813">of</TOKEN>
<TOKEN id="token-33-24" pos="word" morph="none" start_char="3815" end_char="3822">positive</TOKEN>
<TOKEN id="token-33-25" pos="word" morph="none" start_char="3824" end_char="3829">sample</TOKEN>
<TOKEN id="token-33-26" pos="word" morph="none" start_char="3831" end_char="3843">contaminating</TOKEN>
<TOKEN id="token-33-27" pos="word" morph="none" start_char="3845" end_char="3850">others</TOKEN>
<TOKEN id="token-33-28" pos="punct" morph="none" start_char="3851" end_char="3851">.</TOKEN>
</SEG>
<SEG id="segment-34" start_char="3854" end_char="4032">
<ORIGINAL_TEXT>Another explanation is that there is other RNA or DNA in the sample that resembles the test target site enough for it to give a positive result at the 39th cycle of amplification.</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="3854" end_char="3860">Another</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="3862" end_char="3872">explanation</TOKEN>
<TOKEN id="token-34-2" pos="word" morph="none" start_char="3874" end_char="3875">is</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="3877" end_char="3880">that</TOKEN>
<TOKEN id="token-34-4" pos="word" morph="none" start_char="3882" end_char="3886">there</TOKEN>
<TOKEN id="token-34-5" pos="word" morph="none" start_char="3888" end_char="3889">is</TOKEN>
<TOKEN id="token-34-6" pos="word" morph="none" start_char="3891" end_char="3895">other</TOKEN>
<TOKEN id="token-34-7" pos="word" morph="none" start_char="3897" end_char="3899">RNA</TOKEN>
<TOKEN id="token-34-8" pos="word" morph="none" start_char="3901" end_char="3902">or</TOKEN>
<TOKEN id="token-34-9" pos="word" morph="none" start_char="3904" end_char="3906">DNA</TOKEN>
<TOKEN id="token-34-10" pos="word" morph="none" start_char="3908" end_char="3909">in</TOKEN>
<TOKEN id="token-34-11" pos="word" morph="none" start_char="3911" end_char="3913">the</TOKEN>
<TOKEN id="token-34-12" pos="word" morph="none" start_char="3915" end_char="3920">sample</TOKEN>
<TOKEN id="token-34-13" pos="word" morph="none" start_char="3922" end_char="3925">that</TOKEN>
<TOKEN id="token-34-14" pos="word" morph="none" start_char="3927" end_char="3935">resembles</TOKEN>
<TOKEN id="token-34-15" pos="word" morph="none" start_char="3937" end_char="3939">the</TOKEN>
<TOKEN id="token-34-16" pos="word" morph="none" start_char="3941" end_char="3944">test</TOKEN>
<TOKEN id="token-34-17" pos="word" morph="none" start_char="3946" end_char="3951">target</TOKEN>
<TOKEN id="token-34-18" pos="word" morph="none" start_char="3953" end_char="3956">site</TOKEN>
<TOKEN id="token-34-19" pos="word" morph="none" start_char="3958" end_char="3963">enough</TOKEN>
<TOKEN id="token-34-20" pos="word" morph="none" start_char="3965" end_char="3967">for</TOKEN>
<TOKEN id="token-34-21" pos="word" morph="none" start_char="3969" end_char="3970">it</TOKEN>
<TOKEN id="token-34-22" pos="word" morph="none" start_char="3972" end_char="3973">to</TOKEN>
<TOKEN id="token-34-23" pos="word" morph="none" start_char="3975" end_char="3978">give</TOKEN>
<TOKEN id="token-34-24" pos="word" morph="none" start_char="3980" end_char="3980">a</TOKEN>
<TOKEN id="token-34-25" pos="word" morph="none" start_char="3982" end_char="3989">positive</TOKEN>
<TOKEN id="token-34-26" pos="word" morph="none" start_char="3991" end_char="3996">result</TOKEN>
<TOKEN id="token-34-27" pos="word" morph="none" start_char="3998" end_char="3999">at</TOKEN>
<TOKEN id="token-34-28" pos="word" morph="none" start_char="4001" end_char="4003">the</TOKEN>
<TOKEN id="token-34-29" pos="word" morph="none" start_char="4005" end_char="4008">39th</TOKEN>
<TOKEN id="token-34-30" pos="word" morph="none" start_char="4010" end_char="4014">cycle</TOKEN>
<TOKEN id="token-34-31" pos="word" morph="none" start_char="4016" end_char="4017">of</TOKEN>
<TOKEN id="token-34-32" pos="word" morph="none" start_char="4019" end_char="4031">amplification</TOKEN>
<TOKEN id="token-34-33" pos="punct" morph="none" start_char="4032" end_char="4032">.</TOKEN>
</SEG>
<SEG id="segment-35" start_char="4035" end_char="4219">
<ORIGINAL_TEXT>Further tests need to be carried out to conclude that the sample contains SARS-CoV-2, and a finding of that magnitude would need to be replicated separately by independent laboratories.</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="4035" end_char="4041">Further</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="4043" end_char="4047">tests</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="4049" end_char="4052">need</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="4054" end_char="4055">to</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="4057" end_char="4058">be</TOKEN>
<TOKEN id="token-35-5" pos="word" morph="none" start_char="4060" end_char="4066">carried</TOKEN>
<TOKEN id="token-35-6" pos="word" morph="none" start_char="4068" end_char="4070">out</TOKEN>
<TOKEN id="token-35-7" pos="word" morph="none" start_char="4072" end_char="4073">to</TOKEN>
<TOKEN id="token-35-8" pos="word" morph="none" start_char="4075" end_char="4082">conclude</TOKEN>
<TOKEN id="token-35-9" pos="word" morph="none" start_char="4084" end_char="4087">that</TOKEN>
<TOKEN id="token-35-10" pos="word" morph="none" start_char="4089" end_char="4091">the</TOKEN>
<TOKEN id="token-35-11" pos="word" morph="none" start_char="4093" end_char="4098">sample</TOKEN>
<TOKEN id="token-35-12" pos="word" morph="none" start_char="4100" end_char="4107">contains</TOKEN>
<TOKEN id="token-35-13" pos="unknown" morph="none" start_char="4109" end_char="4118">SARS-CoV-2</TOKEN>
<TOKEN id="token-35-14" pos="punct" morph="none" start_char="4119" end_char="4119">,</TOKEN>
<TOKEN id="token-35-15" pos="word" morph="none" start_char="4121" end_char="4123">and</TOKEN>
<TOKEN id="token-35-16" pos="word" morph="none" start_char="4125" end_char="4125">a</TOKEN>
<TOKEN id="token-35-17" pos="word" morph="none" start_char="4127" end_char="4133">finding</TOKEN>
<TOKEN id="token-35-18" pos="word" morph="none" start_char="4135" end_char="4136">of</TOKEN>
<TOKEN id="token-35-19" pos="word" morph="none" start_char="4138" end_char="4141">that</TOKEN>
<TOKEN id="token-35-20" pos="word" morph="none" start_char="4143" end_char="4151">magnitude</TOKEN>
<TOKEN id="token-35-21" pos="word" morph="none" start_char="4153" end_char="4157">would</TOKEN>
<TOKEN id="token-35-22" pos="word" morph="none" start_char="4159" end_char="4162">need</TOKEN>
<TOKEN id="token-35-23" pos="word" morph="none" start_char="4164" end_char="4165">to</TOKEN>
<TOKEN id="token-35-24" pos="word" morph="none" start_char="4167" end_char="4168">be</TOKEN>
<TOKEN id="token-35-25" pos="word" morph="none" start_char="4170" end_char="4179">replicated</TOKEN>
<TOKEN id="token-35-26" pos="word" morph="none" start_char="4181" end_char="4190">separately</TOKEN>
<TOKEN id="token-35-27" pos="word" morph="none" start_char="4192" end_char="4193">by</TOKEN>
<TOKEN id="token-35-28" pos="word" morph="none" start_char="4195" end_char="4205">independent</TOKEN>
<TOKEN id="token-35-29" pos="word" morph="none" start_char="4207" end_char="4218">laboratories</TOKEN>
<TOKEN id="token-35-30" pos="punct" morph="none" start_char="4219" end_char="4219">.</TOKEN>
</SEG>
<SEG id="segment-36" start_char="4222" end_char="4246">
<ORIGINAL_TEXT>Reasons to be circumspect</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="4222" end_char="4228">Reasons</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="4230" end_char="4231">to</TOKEN>
<TOKEN id="token-36-2" pos="word" morph="none" start_char="4233" end_char="4234">be</TOKEN>
<TOKEN id="token-36-3" pos="word" morph="none" start_char="4236" end_char="4246">circumspect</TOKEN>
</SEG>
<SEG id="segment-37" start_char="4250" end_char="4347">
<ORIGINAL_TEXT>A curious thing about this finding is that it disagrees with epidemiological data about the virus.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="4250" end_char="4250">A</TOKEN>
<TOKEN id="token-37-1" pos="word" morph="none" start_char="4252" end_char="4258">curious</TOKEN>
<TOKEN id="token-37-2" pos="word" morph="none" start_char="4260" end_char="4264">thing</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="4266" end_char="4270">about</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="4272" end_char="4275">this</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="4277" end_char="4283">finding</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="4285" end_char="4286">is</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="4288" end_char="4291">that</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="4293" end_char="4294">it</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="4296" end_char="4304">disagrees</TOKEN>
<TOKEN id="token-37-10" pos="word" morph="none" start_char="4306" end_char="4309">with</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="4311" end_char="4325">epidemiological</TOKEN>
<TOKEN id="token-37-12" pos="word" morph="none" start_char="4327" end_char="4330">data</TOKEN>
<TOKEN id="token-37-13" pos="word" morph="none" start_char="4332" end_char="4336">about</TOKEN>
<TOKEN id="token-37-14" pos="word" morph="none" start_char="4338" end_char="4340">the</TOKEN>
<TOKEN id="token-37-15" pos="word" morph="none" start_char="4342" end_char="4346">virus</TOKEN>
<TOKEN id="token-37-16" pos="punct" morph="none" start_char="4347" end_char="4347">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="4349" end_char="4492">
<ORIGINAL_TEXT>The authors don’t cite reports of a spike in the number of respiratory disease cases in the local population following the date of the sampling.</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="4349" end_char="4351">The</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="4353" end_char="4359">authors</TOKEN>
<TOKEN id="token-38-2" pos="word" morph="none" start_char="4361" end_char="4365">don’t</TOKEN>
<TOKEN id="token-38-3" pos="word" morph="none" start_char="4367" end_char="4370">cite</TOKEN>
<TOKEN id="token-38-4" pos="word" morph="none" start_char="4372" end_char="4378">reports</TOKEN>
<TOKEN id="token-38-5" pos="word" morph="none" start_char="4380" end_char="4381">of</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="4383" end_char="4383">a</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="4385" end_char="4389">spike</TOKEN>
<TOKEN id="token-38-8" pos="word" morph="none" start_char="4391" end_char="4392">in</TOKEN>
<TOKEN id="token-38-9" pos="word" morph="none" start_char="4394" end_char="4396">the</TOKEN>
<TOKEN id="token-38-10" pos="word" morph="none" start_char="4398" end_char="4403">number</TOKEN>
<TOKEN id="token-38-11" pos="word" morph="none" start_char="4405" end_char="4406">of</TOKEN>
<TOKEN id="token-38-12" pos="word" morph="none" start_char="4408" end_char="4418">respiratory</TOKEN>
<TOKEN id="token-38-13" pos="word" morph="none" start_char="4420" end_char="4426">disease</TOKEN>
<TOKEN id="token-38-14" pos="word" morph="none" start_char="4428" end_char="4432">cases</TOKEN>
<TOKEN id="token-38-15" pos="word" morph="none" start_char="4434" end_char="4435">in</TOKEN>
<TOKEN id="token-38-16" pos="word" morph="none" start_char="4437" end_char="4439">the</TOKEN>
<TOKEN id="token-38-17" pos="word" morph="none" start_char="4441" end_char="4445">local</TOKEN>
<TOKEN id="token-38-18" pos="word" morph="none" start_char="4447" end_char="4456">population</TOKEN>
<TOKEN id="token-38-19" pos="word" morph="none" start_char="4458" end_char="4466">following</TOKEN>
<TOKEN id="token-38-20" pos="word" morph="none" start_char="4468" end_char="4470">the</TOKEN>
<TOKEN id="token-38-21" pos="word" morph="none" start_char="4472" end_char="4475">date</TOKEN>
<TOKEN id="token-38-22" pos="word" morph="none" start_char="4477" end_char="4478">of</TOKEN>
<TOKEN id="token-38-23" pos="word" morph="none" start_char="4480" end_char="4482">the</TOKEN>
<TOKEN id="token-38-24" pos="word" morph="none" start_char="4484" end_char="4491">sampling</TOKEN>
<TOKEN id="token-38-25" pos="punct" morph="none" start_char="4492" end_char="4492">.</TOKEN>
</SEG>
<SEG id="segment-39" start_char="4495" end_char="4576">
<ORIGINAL_TEXT>Also, we know SARS-CoV-2 to be highly transmissible, at least in its current form.</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="word" morph="none" start_char="4495" end_char="4498">Also</TOKEN>
<TOKEN id="token-39-1" pos="punct" morph="none" start_char="4499" end_char="4499">,</TOKEN>
<TOKEN id="token-39-2" pos="word" morph="none" start_char="4501" end_char="4502">we</TOKEN>
<TOKEN id="token-39-3" pos="word" morph="none" start_char="4504" end_char="4507">know</TOKEN>
<TOKEN id="token-39-4" pos="unknown" morph="none" start_char="4509" end_char="4518">SARS-CoV-2</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="4520" end_char="4521">to</TOKEN>
<TOKEN id="token-39-6" pos="word" morph="none" start_char="4523" end_char="4524">be</TOKEN>
<TOKEN id="token-39-7" pos="word" morph="none" start_char="4526" end_char="4531">highly</TOKEN>
<TOKEN id="token-39-8" pos="word" morph="none" start_char="4533" end_char="4545">transmissible</TOKEN>
<TOKEN id="token-39-9" pos="punct" morph="none" start_char="4546" end_char="4546">,</TOKEN>
<TOKEN id="token-39-10" pos="word" morph="none" start_char="4548" end_char="4549">at</TOKEN>
<TOKEN id="token-39-11" pos="word" morph="none" start_char="4551" end_char="4555">least</TOKEN>
<TOKEN id="token-39-12" pos="word" morph="none" start_char="4557" end_char="4558">in</TOKEN>
<TOKEN id="token-39-13" pos="word" morph="none" start_char="4560" end_char="4562">its</TOKEN>
<TOKEN id="token-39-14" pos="word" morph="none" start_char="4564" end_char="4570">current</TOKEN>
<TOKEN id="token-39-15" pos="word" morph="none" start_char="4572" end_char="4575">form</TOKEN>
<TOKEN id="token-39-16" pos="punct" morph="none" start_char="4576" end_char="4576">.</TOKEN>
</SEG>
<SEG id="segment-40" start_char="4578" end_char="4854">
<ORIGINAL_TEXT>If this result is a true positive it suggests the virus was present in the population at a high enough incidence to be detected in an 800ml sample of sewage, but then not present at a high enough incidence to be detected for nine months, when no control measures were in place.</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="word" morph="none" start_char="4578" end_char="4579">If</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="4581" end_char="4584">this</TOKEN>
<TOKEN id="token-40-2" pos="word" morph="none" start_char="4586" end_char="4591">result</TOKEN>
<TOKEN id="token-40-3" pos="word" morph="none" start_char="4593" end_char="4594">is</TOKEN>
<TOKEN id="token-40-4" pos="word" morph="none" start_char="4596" end_char="4596">a</TOKEN>
<TOKEN id="token-40-5" pos="word" morph="none" start_char="4598" end_char="4601">true</TOKEN>
<TOKEN id="token-40-6" pos="word" morph="none" start_char="4603" end_char="4610">positive</TOKEN>
<TOKEN id="token-40-7" pos="word" morph="none" start_char="4612" end_char="4613">it</TOKEN>
<TOKEN id="token-40-8" pos="word" morph="none" start_char="4615" end_char="4622">suggests</TOKEN>
<TOKEN id="token-40-9" pos="word" morph="none" start_char="4624" end_char="4626">the</TOKEN>
<TOKEN id="token-40-10" pos="word" morph="none" start_char="4628" end_char="4632">virus</TOKEN>
<TOKEN id="token-40-11" pos="word" morph="none" start_char="4634" end_char="4636">was</TOKEN>
<TOKEN id="token-40-12" pos="word" morph="none" start_char="4638" end_char="4644">present</TOKEN>
<TOKEN id="token-40-13" pos="word" morph="none" start_char="4646" end_char="4647">in</TOKEN>
<TOKEN id="token-40-14" pos="word" morph="none" start_char="4649" end_char="4651">the</TOKEN>
<TOKEN id="token-40-15" pos="word" morph="none" start_char="4653" end_char="4662">population</TOKEN>
<TOKEN id="token-40-16" pos="word" morph="none" start_char="4664" end_char="4665">at</TOKEN>
<TOKEN id="token-40-17" pos="word" morph="none" start_char="4667" end_char="4667">a</TOKEN>
<TOKEN id="token-40-18" pos="word" morph="none" start_char="4669" end_char="4672">high</TOKEN>
<TOKEN id="token-40-19" pos="word" morph="none" start_char="4674" end_char="4679">enough</TOKEN>
<TOKEN id="token-40-20" pos="word" morph="none" start_char="4681" end_char="4689">incidence</TOKEN>
<TOKEN id="token-40-21" pos="word" morph="none" start_char="4691" end_char="4692">to</TOKEN>
<TOKEN id="token-40-22" pos="word" morph="none" start_char="4694" end_char="4695">be</TOKEN>
<TOKEN id="token-40-23" pos="word" morph="none" start_char="4697" end_char="4704">detected</TOKEN>
<TOKEN id="token-40-24" pos="word" morph="none" start_char="4706" end_char="4707">in</TOKEN>
<TOKEN id="token-40-25" pos="word" morph="none" start_char="4709" end_char="4710">an</TOKEN>
<TOKEN id="token-40-26" pos="word" morph="none" start_char="4712" end_char="4716">800ml</TOKEN>
<TOKEN id="token-40-27" pos="word" morph="none" start_char="4718" end_char="4723">sample</TOKEN>
<TOKEN id="token-40-28" pos="word" morph="none" start_char="4725" end_char="4726">of</TOKEN>
<TOKEN id="token-40-29" pos="word" morph="none" start_char="4728" end_char="4733">sewage</TOKEN>
<TOKEN id="token-40-30" pos="punct" morph="none" start_char="4734" end_char="4734">,</TOKEN>
<TOKEN id="token-40-31" pos="word" morph="none" start_char="4736" end_char="4738">but</TOKEN>
<TOKEN id="token-40-32" pos="word" morph="none" start_char="4740" end_char="4743">then</TOKEN>
<TOKEN id="token-40-33" pos="word" morph="none" start_char="4745" end_char="4747">not</TOKEN>
<TOKEN id="token-40-34" pos="word" morph="none" start_char="4749" end_char="4755">present</TOKEN>
<TOKEN id="token-40-35" pos="word" morph="none" start_char="4757" end_char="4758">at</TOKEN>
<TOKEN id="token-40-36" pos="word" morph="none" start_char="4760" end_char="4760">a</TOKEN>
<TOKEN id="token-40-37" pos="word" morph="none" start_char="4762" end_char="4765">high</TOKEN>
<TOKEN id="token-40-38" pos="word" morph="none" start_char="4767" end_char="4772">enough</TOKEN>
<TOKEN id="token-40-39" pos="word" morph="none" start_char="4774" end_char="4782">incidence</TOKEN>
<TOKEN id="token-40-40" pos="word" morph="none" start_char="4784" end_char="4785">to</TOKEN>
<TOKEN id="token-40-41" pos="word" morph="none" start_char="4787" end_char="4788">be</TOKEN>
<TOKEN id="token-40-42" pos="word" morph="none" start_char="4790" end_char="4797">detected</TOKEN>
<TOKEN id="token-40-43" pos="word" morph="none" start_char="4799" end_char="4801">for</TOKEN>
<TOKEN id="token-40-44" pos="word" morph="none" start_char="4803" end_char="4806">nine</TOKEN>
<TOKEN id="token-40-45" pos="word" morph="none" start_char="4808" end_char="4813">months</TOKEN>
<TOKEN id="token-40-46" pos="punct" morph="none" start_char="4814" end_char="4814">,</TOKEN>
<TOKEN id="token-40-47" pos="word" morph="none" start_char="4816" end_char="4819">when</TOKEN>
<TOKEN id="token-40-48" pos="word" morph="none" start_char="4821" end_char="4822">no</TOKEN>
<TOKEN id="token-40-49" pos="word" morph="none" start_char="4824" end_char="4830">control</TOKEN>
<TOKEN id="token-40-50" pos="word" morph="none" start_char="4832" end_char="4839">measures</TOKEN>
<TOKEN id="token-40-51" pos="word" morph="none" start_char="4841" end_char="4844">were</TOKEN>
<TOKEN id="token-40-52" pos="word" morph="none" start_char="4846" end_char="4847">in</TOKEN>
<TOKEN id="token-40-53" pos="word" morph="none" start_char="4849" end_char="4853">place</TOKEN>
<TOKEN id="token-40-54" pos="punct" morph="none" start_char="4854" end_char="4854">.</TOKEN>
</SEG>
<SEG id="segment-41" start_char="4857" end_char="4945">
<ORIGINAL_TEXT>So, until further studies are carried out, it is best not to draw definitive conclusions.</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="4857" end_char="4858">So</TOKEN>
<TOKEN id="token-41-1" pos="punct" morph="none" start_char="4859" end_char="4859">,</TOKEN>
<TOKEN id="token-41-2" pos="word" morph="none" start_char="4861" end_char="4865">until</TOKEN>
<TOKEN id="token-41-3" pos="word" morph="none" start_char="4867" end_char="4873">further</TOKEN>
<TOKEN id="token-41-4" pos="word" morph="none" start_char="4875" end_char="4881">studies</TOKEN>
<TOKEN id="token-41-5" pos="word" morph="none" start_char="4883" end_char="4885">are</TOKEN>
<TOKEN id="token-41-6" pos="word" morph="none" start_char="4887" end_char="4893">carried</TOKEN>
<TOKEN id="token-41-7" pos="word" morph="none" start_char="4895" end_char="4897">out</TOKEN>
<TOKEN id="token-41-8" pos="punct" morph="none" start_char="4898" end_char="4898">,</TOKEN>
<TOKEN id="token-41-9" pos="word" morph="none" start_char="4900" end_char="4901">it</TOKEN>
<TOKEN id="token-41-10" pos="word" morph="none" start_char="4903" end_char="4904">is</TOKEN>
<TOKEN id="token-41-11" pos="word" morph="none" start_char="4906" end_char="4909">best</TOKEN>
<TOKEN id="token-41-12" pos="word" morph="none" start_char="4911" end_char="4913">not</TOKEN>
<TOKEN id="token-41-13" pos="word" morph="none" start_char="4915" end_char="4916">to</TOKEN>
<TOKEN id="token-41-14" pos="word" morph="none" start_char="4918" end_char="4921">draw</TOKEN>
<TOKEN id="token-41-15" pos="word" morph="none" start_char="4923" end_char="4932">definitive</TOKEN>
<TOKEN id="token-41-16" pos="word" morph="none" start_char="4934" end_char="4944">conclusions</TOKEN>
<TOKEN id="token-41-17" pos="punct" morph="none" start_char="4945" end_char="4945">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
