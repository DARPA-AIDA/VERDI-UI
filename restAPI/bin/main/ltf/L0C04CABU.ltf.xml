<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CABU" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="3919" raw_text_md5="09861ad1009b01089d7a5ab266b29e6c">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="88">
<ORIGINAL_TEXT>Novel coronavirus likely circulated undetected for two months before first case in Wuhan</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="5">Novel</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="7" end_char="17">coronavirus</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="19" end_char="24">likely</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="26" end_char="35">circulated</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="37" end_char="46">undetected</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="48" end_char="50">for</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="52" end_char="54">two</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="56" end_char="61">months</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="63" end_char="68">before</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="70" end_char="74">first</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="76" end_char="79">case</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="81" end_char="82">in</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="84" end_char="88">Wuhan</TOKEN>
</SEG>
<SEG id="segment-1" start_char="93" end_char="112">
<ORIGINAL_TEXT>MORE FROM THE AUTHOR</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="93" end_char="96">MORE</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="98" end_char="101">FROM</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="103" end_char="105">THE</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="107" end_char="112">AUTHOR</TOKEN>
</SEG>
<SEG id="segment-2" start_char="116" end_char="310">
<ORIGINAL_TEXT>The novel coronavirus was likely circulating undetected for at most two months before the first human cases of COVID-19 were described in Wuhan, China in late-December 2019, according to a study.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="116" end_char="118">The</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="120" end_char="124">novel</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="126" end_char="136">coronavirus</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="138" end_char="140">was</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="142" end_char="147">likely</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="149" end_char="159">circulating</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="161" end_char="170">undetected</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="172" end_char="174">for</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="176" end_char="177">at</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="179" end_char="182">most</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="184" end_char="186">two</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="188" end_char="193">months</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="195" end_char="200">before</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="202" end_char="204">the</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="206" end_char="210">first</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="212" end_char="216">human</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="218" end_char="222">cases</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="224" end_char="225">of</TOKEN>
<TOKEN id="token-2-18" pos="unknown" morph="none" start_char="227" end_char="234">COVID-19</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="236" end_char="239">were</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="241" end_char="249">described</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="251" end_char="252">in</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="254" end_char="258">Wuhan</TOKEN>
<TOKEN id="token-2-23" pos="punct" morph="none" start_char="259" end_char="259">,</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="261" end_char="265">China</TOKEN>
<TOKEN id="token-2-25" pos="word" morph="none" start_char="267" end_char="268">in</TOKEN>
<TOKEN id="token-2-26" pos="unknown" morph="none" start_char="270" end_char="282">late-December</TOKEN>
<TOKEN id="token-2-27" pos="word" morph="none" start_char="284" end_char="287">2019</TOKEN>
<TOKEN id="token-2-28" pos="punct" morph="none" start_char="288" end_char="288">,</TOKEN>
<TOKEN id="token-2-29" pos="word" morph="none" start_char="290" end_char="298">according</TOKEN>
<TOKEN id="token-2-30" pos="word" morph="none" start_char="300" end_char="301">to</TOKEN>
<TOKEN id="token-2-31" pos="word" morph="none" start_char="303" end_char="303">a</TOKEN>
<TOKEN id="token-2-32" pos="word" morph="none" start_char="305" end_char="309">study</TOKEN>
<TOKEN id="token-2-33" pos="punct" morph="none" start_char="310" end_char="310">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="312" end_char="482">
<ORIGINAL_TEXT>The research, published in the journal Science, used molecular dating tools and epidemiological simulations to date the emergence of the virus to as early as October 2019.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="312" end_char="314">The</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="316" end_char="323">research</TOKEN>
<TOKEN id="token-3-2" pos="punct" morph="none" start_char="324" end_char="324">,</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="326" end_char="334">published</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="336" end_char="337">in</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="339" end_char="341">the</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="343" end_char="349">journal</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="351" end_char="357">Science</TOKEN>
<TOKEN id="token-3-8" pos="punct" morph="none" start_char="358" end_char="358">,</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="360" end_char="363">used</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="365" end_char="373">molecular</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="375" end_char="380">dating</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="382" end_char="386">tools</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="388" end_char="390">and</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="392" end_char="406">epidemiological</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="408" end_char="418">simulations</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="420" end_char="421">to</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="423" end_char="426">date</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="428" end_char="430">the</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="432" end_char="440">emergence</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="442" end_char="443">of</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="445" end_char="447">the</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="449" end_char="453">virus</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="455" end_char="456">to</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="458" end_char="459">as</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="461" end_char="465">early</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="467" end_char="468">as</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="470" end_char="476">October</TOKEN>
<TOKEN id="token-3-28" pos="word" morph="none" start_char="478" end_char="481">2019</TOKEN>
<TOKEN id="token-3-29" pos="punct" morph="none" start_char="482" end_char="482">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="485" end_char="711">
<ORIGINAL_TEXT>The team, including researchers from University of California San Diego in the US, note that their simulations suggest that the mutating virus dies out naturally more than three-quarters of the time without causing an epidemic.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="485" end_char="487">The</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="489" end_char="492">team</TOKEN>
<TOKEN id="token-4-2" pos="punct" morph="none" start_char="493" end_char="493">,</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="495" end_char="503">including</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="505" end_char="515">researchers</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="517" end_char="520">from</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="522" end_char="531">University</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="533" end_char="534">of</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="536" end_char="545">California</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="547" end_char="549">San</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="551" end_char="555">Diego</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="557" end_char="558">in</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="560" end_char="562">the</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="564" end_char="565">US</TOKEN>
<TOKEN id="token-4-14" pos="punct" morph="none" start_char="566" end_char="566">,</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="568" end_char="571">note</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="573" end_char="576">that</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="578" end_char="582">their</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="584" end_char="594">simulations</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="596" end_char="602">suggest</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="604" end_char="607">that</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="609" end_char="611">the</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="613" end_char="620">mutating</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="622" end_char="626">virus</TOKEN>
<TOKEN id="token-4-24" pos="word" morph="none" start_char="628" end_char="631">dies</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="633" end_char="635">out</TOKEN>
<TOKEN id="token-4-26" pos="word" morph="none" start_char="637" end_char="645">naturally</TOKEN>
<TOKEN id="token-4-27" pos="word" morph="none" start_char="647" end_char="650">more</TOKEN>
<TOKEN id="token-4-28" pos="word" morph="none" start_char="652" end_char="655">than</TOKEN>
<TOKEN id="token-4-29" pos="unknown" morph="none" start_char="657" end_char="670">three-quarters</TOKEN>
<TOKEN id="token-4-30" pos="word" morph="none" start_char="672" end_char="673">of</TOKEN>
<TOKEN id="token-4-31" pos="word" morph="none" start_char="675" end_char="677">the</TOKEN>
<TOKEN id="token-4-32" pos="word" morph="none" start_char="679" end_char="682">time</TOKEN>
<TOKEN id="token-4-33" pos="word" morph="none" start_char="684" end_char="690">without</TOKEN>
<TOKEN id="token-4-34" pos="word" morph="none" start_char="692" end_char="698">causing</TOKEN>
<TOKEN id="token-4-35" pos="word" morph="none" start_char="700" end_char="701">an</TOKEN>
<TOKEN id="token-4-36" pos="word" morph="none" start_char="703" end_char="710">epidemic</TOKEN>
<TOKEN id="token-4-37" pos="punct" morph="none" start_char="711" end_char="711">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="713" end_char="940">
<ORIGINAL_TEXT>"Our study was designed to answer the question of how long could SARS-CoV-2 have circulated in China before it was discovered," said senior study author Joel O Wertheim, an associate professor at UC San Diego School of Medicine.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="punct" morph="none" start_char="713" end_char="713">"</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="714" end_char="716">Our</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="718" end_char="722">study</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="724" end_char="726">was</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="728" end_char="735">designed</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="737" end_char="738">to</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="740" end_char="745">answer</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="747" end_char="749">the</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="751" end_char="758">question</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="760" end_char="761">of</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="763" end_char="765">how</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="767" end_char="770">long</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="772" end_char="776">could</TOKEN>
<TOKEN id="token-5-13" pos="unknown" morph="none" start_char="778" end_char="787">SARS-CoV-2</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="789" end_char="792">have</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="794" end_char="803">circulated</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="805" end_char="806">in</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="808" end_char="812">China</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="814" end_char="819">before</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="821" end_char="822">it</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="824" end_char="826">was</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="828" end_char="837">discovered</TOKEN>
<TOKEN id="token-5-22" pos="punct" morph="none" start_char="838" end_char="839">,"</TOKEN>
<TOKEN id="token-5-23" pos="word" morph="none" start_char="841" end_char="844">said</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="846" end_char="851">senior</TOKEN>
<TOKEN id="token-5-25" pos="word" morph="none" start_char="853" end_char="857">study</TOKEN>
<TOKEN id="token-5-26" pos="word" morph="none" start_char="859" end_char="864">author</TOKEN>
<TOKEN id="token-5-27" pos="word" morph="none" start_char="866" end_char="869">Joel</TOKEN>
<TOKEN id="token-5-28" pos="word" morph="none" start_char="871" end_char="871">O</TOKEN>
<TOKEN id="token-5-29" pos="word" morph="none" start_char="873" end_char="880">Wertheim</TOKEN>
<TOKEN id="token-5-30" pos="punct" morph="none" start_char="881" end_char="881">,</TOKEN>
<TOKEN id="token-5-31" pos="word" morph="none" start_char="883" end_char="884">an</TOKEN>
<TOKEN id="token-5-32" pos="word" morph="none" start_char="886" end_char="894">associate</TOKEN>
<TOKEN id="token-5-33" pos="word" morph="none" start_char="896" end_char="904">professor</TOKEN>
<TOKEN id="token-5-34" pos="word" morph="none" start_char="906" end_char="907">at</TOKEN>
<TOKEN id="token-5-35" pos="word" morph="none" start_char="909" end_char="910">UC</TOKEN>
<TOKEN id="token-5-36" pos="word" morph="none" start_char="912" end_char="914">San</TOKEN>
<TOKEN id="token-5-37" pos="word" morph="none" start_char="916" end_char="920">Diego</TOKEN>
<TOKEN id="token-5-38" pos="word" morph="none" start_char="922" end_char="927">School</TOKEN>
<TOKEN id="token-5-39" pos="word" morph="none" start_char="929" end_char="930">of</TOKEN>
<TOKEN id="token-5-40" pos="word" morph="none" start_char="932" end_char="939">Medicine</TOKEN>
<TOKEN id="token-5-41" pos="punct" morph="none" start_char="940" end_char="940">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="943" end_char="1214">
<ORIGINAL_TEXT>"To answer this question, we combined three important pieces of information: a detailed understanding of how SARS-CoV-2 spread in Wuhan before the lockdown, the genetic diversity of the virus in China and reports of the earliest cases of COVID-19 in China," Wertheim said.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="punct" morph="none" start_char="943" end_char="943">"</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="944" end_char="945">To</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="947" end_char="952">answer</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="954" end_char="957">this</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="959" end_char="966">question</TOKEN>
<TOKEN id="token-6-5" pos="punct" morph="none" start_char="967" end_char="967">,</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="969" end_char="970">we</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="972" end_char="979">combined</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="981" end_char="985">three</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="987" end_char="995">important</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="997" end_char="1002">pieces</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="1004" end_char="1005">of</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="1007" end_char="1017">information</TOKEN>
<TOKEN id="token-6-13" pos="punct" morph="none" start_char="1018" end_char="1018">:</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="1020" end_char="1020">a</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="1022" end_char="1029">detailed</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="1031" end_char="1043">understanding</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="1045" end_char="1046">of</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="1048" end_char="1050">how</TOKEN>
<TOKEN id="token-6-19" pos="unknown" morph="none" start_char="1052" end_char="1061">SARS-CoV-2</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="1063" end_char="1068">spread</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="1070" end_char="1071">in</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="1073" end_char="1077">Wuhan</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="1079" end_char="1084">before</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="1086" end_char="1088">the</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="1090" end_char="1097">lockdown</TOKEN>
<TOKEN id="token-6-26" pos="punct" morph="none" start_char="1098" end_char="1098">,</TOKEN>
<TOKEN id="token-6-27" pos="word" morph="none" start_char="1100" end_char="1102">the</TOKEN>
<TOKEN id="token-6-28" pos="word" morph="none" start_char="1104" end_char="1110">genetic</TOKEN>
<TOKEN id="token-6-29" pos="word" morph="none" start_char="1112" end_char="1120">diversity</TOKEN>
<TOKEN id="token-6-30" pos="word" morph="none" start_char="1122" end_char="1123">of</TOKEN>
<TOKEN id="token-6-31" pos="word" morph="none" start_char="1125" end_char="1127">the</TOKEN>
<TOKEN id="token-6-32" pos="word" morph="none" start_char="1129" end_char="1133">virus</TOKEN>
<TOKEN id="token-6-33" pos="word" morph="none" start_char="1135" end_char="1136">in</TOKEN>
<TOKEN id="token-6-34" pos="word" morph="none" start_char="1138" end_char="1142">China</TOKEN>
<TOKEN id="token-6-35" pos="word" morph="none" start_char="1144" end_char="1146">and</TOKEN>
<TOKEN id="token-6-36" pos="word" morph="none" start_char="1148" end_char="1154">reports</TOKEN>
<TOKEN id="token-6-37" pos="word" morph="none" start_char="1156" end_char="1157">of</TOKEN>
<TOKEN id="token-6-38" pos="word" morph="none" start_char="1159" end_char="1161">the</TOKEN>
<TOKEN id="token-6-39" pos="word" morph="none" start_char="1163" end_char="1170">earliest</TOKEN>
<TOKEN id="token-6-40" pos="word" morph="none" start_char="1172" end_char="1176">cases</TOKEN>
<TOKEN id="token-6-41" pos="word" morph="none" start_char="1178" end_char="1179">of</TOKEN>
<TOKEN id="token-6-42" pos="unknown" morph="none" start_char="1181" end_char="1188">COVID-19</TOKEN>
<TOKEN id="token-6-43" pos="word" morph="none" start_char="1190" end_char="1191">in</TOKEN>
<TOKEN id="token-6-44" pos="word" morph="none" start_char="1193" end_char="1197">China</TOKEN>
<TOKEN id="token-6-45" pos="punct" morph="none" start_char="1198" end_char="1199">,"</TOKEN>
<TOKEN id="token-6-46" pos="word" morph="none" start_char="1201" end_char="1208">Wertheim</TOKEN>
<TOKEN id="token-6-47" pos="word" morph="none" start_char="1210" end_char="1213">said</TOKEN>
<TOKEN id="token-6-48" pos="punct" morph="none" start_char="1214" end_char="1214">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="1217" end_char="1390">
<ORIGINAL_TEXT>By combining these disparate lines of evidence, the researchers were able to put an upper limit of mid-October 2019 for when SARS-CoV-2 started circulating in Hubei province.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="1217" end_char="1218">By</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="1220" end_char="1228">combining</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="1230" end_char="1234">these</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="1236" end_char="1244">disparate</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="1246" end_char="1250">lines</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="1252" end_char="1253">of</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="1255" end_char="1262">evidence</TOKEN>
<TOKEN id="token-7-7" pos="punct" morph="none" start_char="1263" end_char="1263">,</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="1265" end_char="1267">the</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="1269" end_char="1279">researchers</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="1281" end_char="1284">were</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="1286" end_char="1289">able</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="1291" end_char="1292">to</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="1294" end_char="1296">put</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="1298" end_char="1299">an</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="1301" end_char="1305">upper</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="1307" end_char="1311">limit</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="1313" end_char="1314">of</TOKEN>
<TOKEN id="token-7-18" pos="unknown" morph="none" start_char="1316" end_char="1326">mid-October</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="1328" end_char="1331">2019</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="1333" end_char="1335">for</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="1337" end_char="1340">when</TOKEN>
<TOKEN id="token-7-22" pos="unknown" morph="none" start_char="1342" end_char="1351">SARS-CoV-2</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="1353" end_char="1359">started</TOKEN>
<TOKEN id="token-7-24" pos="word" morph="none" start_char="1361" end_char="1371">circulating</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="1373" end_char="1374">in</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="1376" end_char="1380">Hubei</TOKEN>
<TOKEN id="token-7-27" pos="word" morph="none" start_char="1382" end_char="1389">province</TOKEN>
<TOKEN id="token-7-28" pos="punct" morph="none" start_char="1390" end_char="1390">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1392" end_char="1508">
<ORIGINAL_TEXT>Cases of COVID-19 were first reported in late-December 2019 in Wuhan, located in the Hubei province of central China.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="1392" end_char="1396">Cases</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="1398" end_char="1399">of</TOKEN>
<TOKEN id="token-8-2" pos="unknown" morph="none" start_char="1401" end_char="1408">COVID-19</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1410" end_char="1413">were</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1415" end_char="1419">first</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1421" end_char="1428">reported</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="1430" end_char="1431">in</TOKEN>
<TOKEN id="token-8-7" pos="unknown" morph="none" start_char="1433" end_char="1445">late-December</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1447" end_char="1450">2019</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="1452" end_char="1453">in</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="1455" end_char="1459">Wuhan</TOKEN>
<TOKEN id="token-8-11" pos="punct" morph="none" start_char="1460" end_char="1460">,</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="1462" end_char="1468">located</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="1470" end_char="1471">in</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="1473" end_char="1475">the</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="1477" end_char="1481">Hubei</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="1483" end_char="1490">province</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="1492" end_char="1493">of</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="1495" end_char="1501">central</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="1503" end_char="1507">China</TOKEN>
<TOKEN id="token-8-20" pos="punct" morph="none" start_char="1508" end_char="1508">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1511" end_char="1645">
<ORIGINAL_TEXT>The researchers used molecular clock evolutionary analyses to try to home in on when the first , or index, case of SARS-CoV-2 occurred.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1511" end_char="1513">The</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1515" end_char="1525">researchers</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="1527" end_char="1530">used</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1532" end_char="1540">molecular</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1542" end_char="1546">clock</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1548" end_char="1559">evolutionary</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1561" end_char="1568">analyses</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1570" end_char="1571">to</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1573" end_char="1575">try</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="1577" end_char="1578">to</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1580" end_char="1583">home</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1585" end_char="1586">in</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="1588" end_char="1589">on</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1591" end_char="1594">when</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1596" end_char="1598">the</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1600" end_char="1604">first</TOKEN>
<TOKEN id="token-9-16" pos="punct" morph="none" start_char="1606" end_char="1606">,</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1608" end_char="1609">or</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1611" end_char="1615">index</TOKEN>
<TOKEN id="token-9-19" pos="punct" morph="none" start_char="1616" end_char="1616">,</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1618" end_char="1621">case</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1623" end_char="1624">of</TOKEN>
<TOKEN id="token-9-22" pos="unknown" morph="none" start_char="1626" end_char="1635">SARS-CoV-2</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="1637" end_char="1644">occurred</TOKEN>
<TOKEN id="token-9-24" pos="punct" morph="none" start_char="1645" end_char="1645">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1648" end_char="1913">
<ORIGINAL_TEXT>"Molecular clock" is a term for a technique that uses the mutation rate of genes to deduce when two or more life forms diverged -- in this case, when the common ancestor of all variants of SARS-CoV-2 existed, estimated in this study to as early as mid-November 2019.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="punct" morph="none" start_char="1648" end_char="1648">"</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1649" end_char="1657">Molecular</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1659" end_char="1663">clock</TOKEN>
<TOKEN id="token-10-3" pos="punct" morph="none" start_char="1664" end_char="1664">"</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1666" end_char="1667">is</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1669" end_char="1669">a</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1671" end_char="1674">term</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1676" end_char="1678">for</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1680" end_char="1680">a</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1682" end_char="1690">technique</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1692" end_char="1695">that</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1697" end_char="1700">uses</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1702" end_char="1704">the</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1706" end_char="1713">mutation</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1715" end_char="1718">rate</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1720" end_char="1721">of</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1723" end_char="1727">genes</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1729" end_char="1730">to</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1732" end_char="1737">deduce</TOKEN>
<TOKEN id="token-10-19" pos="word" morph="none" start_char="1739" end_char="1742">when</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="1744" end_char="1746">two</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="1748" end_char="1749">or</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="1751" end_char="1754">more</TOKEN>
<TOKEN id="token-10-23" pos="word" morph="none" start_char="1756" end_char="1759">life</TOKEN>
<TOKEN id="token-10-24" pos="word" morph="none" start_char="1761" end_char="1765">forms</TOKEN>
<TOKEN id="token-10-25" pos="word" morph="none" start_char="1767" end_char="1774">diverged</TOKEN>
<TOKEN id="token-10-26" pos="punct" morph="none" start_char="1776" end_char="1777">--</TOKEN>
<TOKEN id="token-10-27" pos="word" morph="none" start_char="1779" end_char="1780">in</TOKEN>
<TOKEN id="token-10-28" pos="word" morph="none" start_char="1782" end_char="1785">this</TOKEN>
<TOKEN id="token-10-29" pos="word" morph="none" start_char="1787" end_char="1790">case</TOKEN>
<TOKEN id="token-10-30" pos="punct" morph="none" start_char="1791" end_char="1791">,</TOKEN>
<TOKEN id="token-10-31" pos="word" morph="none" start_char="1793" end_char="1796">when</TOKEN>
<TOKEN id="token-10-32" pos="word" morph="none" start_char="1798" end_char="1800">the</TOKEN>
<TOKEN id="token-10-33" pos="word" morph="none" start_char="1802" end_char="1807">common</TOKEN>
<TOKEN id="token-10-34" pos="word" morph="none" start_char="1809" end_char="1816">ancestor</TOKEN>
<TOKEN id="token-10-35" pos="word" morph="none" start_char="1818" end_char="1819">of</TOKEN>
<TOKEN id="token-10-36" pos="word" morph="none" start_char="1821" end_char="1823">all</TOKEN>
<TOKEN id="token-10-37" pos="word" morph="none" start_char="1825" end_char="1832">variants</TOKEN>
<TOKEN id="token-10-38" pos="word" morph="none" start_char="1834" end_char="1835">of</TOKEN>
<TOKEN id="token-10-39" pos="unknown" morph="none" start_char="1837" end_char="1846">SARS-CoV-2</TOKEN>
<TOKEN id="token-10-40" pos="word" morph="none" start_char="1848" end_char="1854">existed</TOKEN>
<TOKEN id="token-10-41" pos="punct" morph="none" start_char="1855" end_char="1855">,</TOKEN>
<TOKEN id="token-10-42" pos="word" morph="none" start_char="1857" end_char="1865">estimated</TOKEN>
<TOKEN id="token-10-43" pos="word" morph="none" start_char="1867" end_char="1868">in</TOKEN>
<TOKEN id="token-10-44" pos="word" morph="none" start_char="1870" end_char="1873">this</TOKEN>
<TOKEN id="token-10-45" pos="word" morph="none" start_char="1875" end_char="1879">study</TOKEN>
<TOKEN id="token-10-46" pos="word" morph="none" start_char="1881" end_char="1882">to</TOKEN>
<TOKEN id="token-10-47" pos="word" morph="none" start_char="1884" end_char="1885">as</TOKEN>
<TOKEN id="token-10-48" pos="word" morph="none" start_char="1887" end_char="1891">early</TOKEN>
<TOKEN id="token-10-49" pos="word" morph="none" start_char="1893" end_char="1894">as</TOKEN>
<TOKEN id="token-10-50" pos="unknown" morph="none" start_char="1896" end_char="1907">mid-November</TOKEN>
<TOKEN id="token-10-51" pos="word" morph="none" start_char="1909" end_char="1912">2019</TOKEN>
<TOKEN id="token-10-52" pos="punct" morph="none" start_char="1913" end_char="1913">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1916" end_char="2042">
<ORIGINAL_TEXT>Molecular dating of the most recent common ancestor is often taken to be synonymous with the index case of an emerging disease.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1916" end_char="1924">Molecular</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1926" end_char="1931">dating</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1933" end_char="1934">of</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1936" end_char="1938">the</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1940" end_char="1943">most</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1945" end_char="1950">recent</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1952" end_char="1957">common</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1959" end_char="1966">ancestor</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1968" end_char="1969">is</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1971" end_char="1975">often</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1977" end_char="1981">taken</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1983" end_char="1984">to</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1986" end_char="1987">be</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1989" end_char="1998">synonymous</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="2000" end_char="2003">with</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="2005" end_char="2007">the</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="2009" end_char="2013">index</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="2015" end_char="2018">case</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="2020" end_char="2021">of</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="2023" end_char="2024">an</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="2026" end_char="2033">emerging</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="2035" end_char="2041">disease</TOKEN>
<TOKEN id="token-11-22" pos="punct" morph="none" start_char="2042" end_char="2042">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="2044" end_char="2313">
<ORIGINAL_TEXT>"The index case can conceivably predate the common ancestor -- the actual first case of this outbreak may have occurred days, weeks or even many months before the estimated common ancestor," said study co-author Michael Worobey, a professor at the University of Arizona.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="punct" morph="none" start_char="2044" end_char="2044">"</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="2045" end_char="2047">The</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="2049" end_char="2053">index</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="2055" end_char="2058">case</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="2060" end_char="2062">can</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="2064" end_char="2074">conceivably</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="2076" end_char="2082">predate</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="2084" end_char="2086">the</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="2088" end_char="2093">common</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="2095" end_char="2102">ancestor</TOKEN>
<TOKEN id="token-12-10" pos="punct" morph="none" start_char="2104" end_char="2105">--</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="2107" end_char="2109">the</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="2111" end_char="2116">actual</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="2118" end_char="2122">first</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="2124" end_char="2127">case</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="2129" end_char="2130">of</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="2132" end_char="2135">this</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="2137" end_char="2144">outbreak</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="2146" end_char="2148">may</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="2150" end_char="2153">have</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="2155" end_char="2162">occurred</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="2164" end_char="2167">days</TOKEN>
<TOKEN id="token-12-22" pos="punct" morph="none" start_char="2168" end_char="2168">,</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="2170" end_char="2174">weeks</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="2176" end_char="2177">or</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="2179" end_char="2182">even</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="2184" end_char="2187">many</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="2189" end_char="2194">months</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="2196" end_char="2201">before</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="2203" end_char="2205">the</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="2207" end_char="2215">estimated</TOKEN>
<TOKEN id="token-12-31" pos="word" morph="none" start_char="2217" end_char="2222">common</TOKEN>
<TOKEN id="token-12-32" pos="word" morph="none" start_char="2224" end_char="2231">ancestor</TOKEN>
<TOKEN id="token-12-33" pos="punct" morph="none" start_char="2232" end_char="2233">,"</TOKEN>
<TOKEN id="token-12-34" pos="word" morph="none" start_char="2235" end_char="2238">said</TOKEN>
<TOKEN id="token-12-35" pos="word" morph="none" start_char="2240" end_char="2244">study</TOKEN>
<TOKEN id="token-12-36" pos="unknown" morph="none" start_char="2246" end_char="2254">co-author</TOKEN>
<TOKEN id="token-12-37" pos="word" morph="none" start_char="2256" end_char="2262">Michael</TOKEN>
<TOKEN id="token-12-38" pos="word" morph="none" start_char="2264" end_char="2270">Worobey</TOKEN>
<TOKEN id="token-12-39" pos="punct" morph="none" start_char="2271" end_char="2271">,</TOKEN>
<TOKEN id="token-12-40" pos="word" morph="none" start_char="2273" end_char="2273">a</TOKEN>
<TOKEN id="token-12-41" pos="word" morph="none" start_char="2275" end_char="2283">professor</TOKEN>
<TOKEN id="token-12-42" pos="word" morph="none" start_char="2285" end_char="2286">at</TOKEN>
<TOKEN id="token-12-43" pos="word" morph="none" start_char="2288" end_char="2290">the</TOKEN>
<TOKEN id="token-12-44" pos="word" morph="none" start_char="2292" end_char="2301">University</TOKEN>
<TOKEN id="token-12-45" pos="word" morph="none" start_char="2303" end_char="2304">of</TOKEN>
<TOKEN id="token-12-46" pos="word" morph="none" start_char="2306" end_char="2312">Arizona</TOKEN>
<TOKEN id="token-12-47" pos="punct" morph="none" start_char="2313" end_char="2313">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="2316" end_char="2420">
<ORIGINAL_TEXT>"Determining the length of that 'phylogenetic fuse' was at the heart of our investigation," Worobey said.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="punct" morph="none" start_char="2316" end_char="2316">"</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="2317" end_char="2327">Determining</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="2329" end_char="2331">the</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="2333" end_char="2338">length</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="2340" end_char="2341">of</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="2343" end_char="2346">that</TOKEN>
<TOKEN id="token-13-6" pos="punct" morph="none" start_char="2348" end_char="2348">'</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="2349" end_char="2360">phylogenetic</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="2362" end_char="2365">fuse</TOKEN>
<TOKEN id="token-13-9" pos="punct" morph="none" start_char="2366" end_char="2366">'</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="2368" end_char="2370">was</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="2372" end_char="2373">at</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="2375" end_char="2377">the</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="2379" end_char="2383">heart</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="2385" end_char="2386">of</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="2388" end_char="2390">our</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="2392" end_char="2404">investigation</TOKEN>
<TOKEN id="token-13-17" pos="punct" morph="none" start_char="2405" end_char="2406">,"</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="2408" end_char="2414">Worobey</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="2416" end_char="2419">said</TOKEN>
<TOKEN id="token-13-20" pos="punct" morph="none" start_char="2420" end_char="2420">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="2422" end_char="2575">
<ORIGINAL_TEXT>Based on this work, the researchers estimate that the median number of persons infected with SARS-CoV-2 in China was less than one until November 4, 2019.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="2422" end_char="2426">Based</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="2428" end_char="2429">on</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="2431" end_char="2434">this</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="2436" end_char="2439">work</TOKEN>
<TOKEN id="token-14-4" pos="punct" morph="none" start_char="2440" end_char="2440">,</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="2442" end_char="2444">the</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="2446" end_char="2456">researchers</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="2458" end_char="2465">estimate</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="2467" end_char="2470">that</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="2472" end_char="2474">the</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="2476" end_char="2481">median</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="2483" end_char="2488">number</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="2490" end_char="2491">of</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="2493" end_char="2499">persons</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="2501" end_char="2508">infected</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="2510" end_char="2513">with</TOKEN>
<TOKEN id="token-14-16" pos="unknown" morph="none" start_char="2515" end_char="2524">SARS-CoV-2</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="2526" end_char="2527">in</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="2529" end_char="2533">China</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="2535" end_char="2537">was</TOKEN>
<TOKEN id="token-14-20" pos="word" morph="none" start_char="2539" end_char="2542">less</TOKEN>
<TOKEN id="token-14-21" pos="word" morph="none" start_char="2544" end_char="2547">than</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="2549" end_char="2551">one</TOKEN>
<TOKEN id="token-14-23" pos="word" morph="none" start_char="2553" end_char="2557">until</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="2559" end_char="2566">November</TOKEN>
<TOKEN id="token-14-25" pos="word" morph="none" start_char="2568" end_char="2568">4</TOKEN>
<TOKEN id="token-14-26" pos="punct" morph="none" start_char="2569" end_char="2569">,</TOKEN>
<TOKEN id="token-14-27" pos="word" morph="none" start_char="2571" end_char="2574">2019</TOKEN>
<TOKEN id="token-14-28" pos="punct" morph="none" start_char="2575" end_char="2575">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="2578" end_char="2668">
<ORIGINAL_TEXT>Thirteen days later, it was four individuals, and just nine on December 1, 2019, they said.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="2578" end_char="2585">Thirteen</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="2587" end_char="2590">days</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="2592" end_char="2596">later</TOKEN>
<TOKEN id="token-15-3" pos="punct" morph="none" start_char="2597" end_char="2597">,</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="2599" end_char="2600">it</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="2602" end_char="2604">was</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="2606" end_char="2609">four</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="2611" end_char="2621">individuals</TOKEN>
<TOKEN id="token-15-8" pos="punct" morph="none" start_char="2622" end_char="2622">,</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="2624" end_char="2626">and</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="2628" end_char="2631">just</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="2633" end_char="2636">nine</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="2638" end_char="2639">on</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="2641" end_char="2648">December</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="2650" end_char="2650">1</TOKEN>
<TOKEN id="token-15-15" pos="punct" morph="none" start_char="2651" end_char="2651">,</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="2653" end_char="2656">2019</TOKEN>
<TOKEN id="token-15-17" pos="punct" morph="none" start_char="2657" end_char="2657">,</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="2659" end_char="2662">they</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="2664" end_char="2667">said</TOKEN>
<TOKEN id="token-15-20" pos="punct" morph="none" start_char="2668" end_char="2668">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="2670" end_char="2776">
<ORIGINAL_TEXT>The first hospitalisations in Wuhan with a condition later identified as COVID-19 occurred in mid-December.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="2670" end_char="2672">The</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="2674" end_char="2678">first</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="2680" end_char="2695">hospitalisations</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="2697" end_char="2698">in</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="2700" end_char="2704">Wuhan</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="2706" end_char="2709">with</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="2711" end_char="2711">a</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="2713" end_char="2721">condition</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="2723" end_char="2727">later</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="2729" end_char="2738">identified</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="2740" end_char="2741">as</TOKEN>
<TOKEN id="token-16-11" pos="unknown" morph="none" start_char="2743" end_char="2750">COVID-19</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="2752" end_char="2759">occurred</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2761" end_char="2762">in</TOKEN>
<TOKEN id="token-16-14" pos="unknown" morph="none" start_char="2764" end_char="2775">mid-December</TOKEN>
<TOKEN id="token-16-15" pos="punct" morph="none" start_char="2776" end_char="2776">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2778" end_char="2976">
<ORIGINAL_TEXT>The researchers used a variety of analytical tools to model how the SARS-CoV-2 virus may have behaved during the initial outbreak and early days of the pandemic when it was largely an unknown entity.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2778" end_char="2780">The</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2782" end_char="2792">researchers</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2794" end_char="2797">used</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2799" end_char="2799">a</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2801" end_char="2807">variety</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2809" end_char="2810">of</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2812" end_char="2821">analytical</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="2823" end_char="2827">tools</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="2829" end_char="2830">to</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="2832" end_char="2836">model</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="2838" end_char="2840">how</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="2842" end_char="2844">the</TOKEN>
<TOKEN id="token-17-12" pos="unknown" morph="none" start_char="2846" end_char="2855">SARS-CoV-2</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="2857" end_char="2861">virus</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="2863" end_char="2865">may</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="2867" end_char="2870">have</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="2872" end_char="2878">behaved</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="2880" end_char="2885">during</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="2887" end_char="2889">the</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="2891" end_char="2897">initial</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="2899" end_char="2906">outbreak</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="2908" end_char="2910">and</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="2912" end_char="2916">early</TOKEN>
<TOKEN id="token-17-23" pos="word" morph="none" start_char="2918" end_char="2921">days</TOKEN>
<TOKEN id="token-17-24" pos="word" morph="none" start_char="2923" end_char="2924">of</TOKEN>
<TOKEN id="token-17-25" pos="word" morph="none" start_char="2926" end_char="2928">the</TOKEN>
<TOKEN id="token-17-26" pos="word" morph="none" start_char="2930" end_char="2937">pandemic</TOKEN>
<TOKEN id="token-17-27" pos="word" morph="none" start_char="2939" end_char="2942">when</TOKEN>
<TOKEN id="token-17-28" pos="word" morph="none" start_char="2944" end_char="2945">it</TOKEN>
<TOKEN id="token-17-29" pos="word" morph="none" start_char="2947" end_char="2949">was</TOKEN>
<TOKEN id="token-17-30" pos="word" morph="none" start_char="2951" end_char="2957">largely</TOKEN>
<TOKEN id="token-17-31" pos="word" morph="none" start_char="2959" end_char="2960">an</TOKEN>
<TOKEN id="token-17-32" pos="word" morph="none" start_char="2962" end_char="2968">unknown</TOKEN>
<TOKEN id="token-17-33" pos="word" morph="none" start_char="2970" end_char="2975">entity</TOKEN>
<TOKEN id="token-17-34" pos="punct" morph="none" start_char="2976" end_char="2976">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2979" end_char="3103">
<ORIGINAL_TEXT>These tools included epidemic simulations based on the virus's known biology, such as its transmissibility and other factors.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2979" end_char="2983">These</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2985" end_char="2989">tools</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="2991" end_char="2998">included</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="3000" end_char="3007">epidemic</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="3009" end_char="3019">simulations</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="3021" end_char="3025">based</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="3027" end_char="3028">on</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="3030" end_char="3032">the</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="3034" end_char="3040">virus's</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="3042" end_char="3046">known</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="3048" end_char="3054">biology</TOKEN>
<TOKEN id="token-18-11" pos="punct" morph="none" start_char="3055" end_char="3055">,</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="3057" end_char="3060">such</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="3062" end_char="3063">as</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="3065" end_char="3067">its</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="3069" end_char="3084">transmissibility</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="3086" end_char="3088">and</TOKEN>
<TOKEN id="token-18-17" pos="word" morph="none" start_char="3090" end_char="3094">other</TOKEN>
<TOKEN id="token-18-18" pos="word" morph="none" start_char="3096" end_char="3102">factors</TOKEN>
<TOKEN id="token-18-19" pos="punct" morph="none" start_char="3103" end_char="3103">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="3105" end_char="3202">
<ORIGINAL_TEXT>In just 29.7 per cent of these simulations was the virus able to create self-sustaining epidemics.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="3105" end_char="3106">In</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="3108" end_char="3111">just</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="3113" end_char="3116">29.7</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="3118" end_char="3120">per</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="3122" end_char="3125">cent</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="3127" end_char="3128">of</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="3130" end_char="3134">these</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="3136" end_char="3146">simulations</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="3148" end_char="3150">was</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="3152" end_char="3154">the</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="3156" end_char="3160">virus</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="3162" end_char="3165">able</TOKEN>
<TOKEN id="token-19-12" pos="word" morph="none" start_char="3167" end_char="3168">to</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="3170" end_char="3175">create</TOKEN>
<TOKEN id="token-19-14" pos="unknown" morph="none" start_char="3177" end_char="3191">self-sustaining</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="3193" end_char="3201">epidemics</TOKEN>
<TOKEN id="token-19-16" pos="punct" morph="none" start_char="3202" end_char="3202">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="3204" end_char="3290">
<ORIGINAL_TEXT>In the other 70.3 per cent, the virus infected relatively few persons before dying out.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="3204" end_char="3205">In</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="3207" end_char="3209">the</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="3211" end_char="3215">other</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="3217" end_char="3220">70.3</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="3222" end_char="3224">per</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="3226" end_char="3229">cent</TOKEN>
<TOKEN id="token-20-6" pos="punct" morph="none" start_char="3230" end_char="3230">,</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="3232" end_char="3234">the</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="3236" end_char="3240">virus</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="3242" end_char="3249">infected</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="3251" end_char="3260">relatively</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="3262" end_char="3264">few</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="3266" end_char="3272">persons</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="3274" end_char="3279">before</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="3281" end_char="3285">dying</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="3287" end_char="3289">out</TOKEN>
<TOKEN id="token-20-16" pos="punct" morph="none" start_char="3290" end_char="3290">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="3292" end_char="3362">
<ORIGINAL_TEXT>The average failed epidemic ended just eight days after the index case.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="3292" end_char="3294">The</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="3296" end_char="3302">average</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="3304" end_char="3309">failed</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="3311" end_char="3318">epidemic</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="3320" end_char="3324">ended</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="3326" end_char="3329">just</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="3331" end_char="3335">eight</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="3337" end_char="3340">days</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="3342" end_char="3346">after</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="3348" end_char="3350">the</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="3352" end_char="3356">index</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="3358" end_char="3361">case</TOKEN>
<TOKEN id="token-21-12" pos="punct" morph="none" start_char="3362" end_char="3362">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="3365" end_char="3448">
<ORIGINAL_TEXT>"We saw that over two-thirds of the epidemics we attempted to simulate went extinct.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="punct" morph="none" start_char="3365" end_char="3365">"</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="3366" end_char="3367">We</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="3369" end_char="3371">saw</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="3373" end_char="3376">that</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="3378" end_char="3381">over</TOKEN>
<TOKEN id="token-22-5" pos="unknown" morph="none" start_char="3383" end_char="3392">two-thirds</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="3394" end_char="3395">of</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="3397" end_char="3399">the</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="3401" end_char="3409">epidemics</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="3411" end_char="3412">we</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="3414" end_char="3422">attempted</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="3424" end_char="3425">to</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="3427" end_char="3434">simulate</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="3436" end_char="3439">went</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="3441" end_char="3447">extinct</TOKEN>
<TOKEN id="token-22-15" pos="punct" morph="none" start_char="3448" end_char="3448">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="3450" end_char="3639">
<ORIGINAL_TEXT>That means that if we could go back in time and repeat 2019 one hundred times, two out of three times, COVID-19 would have fizzled out on its own without igniting a pandemic, Wertheim noted.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="3450" end_char="3453">That</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="3455" end_char="3459">means</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="3461" end_char="3464">that</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="3466" end_char="3467">if</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="3469" end_char="3470">we</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="3472" end_char="3476">could</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="3478" end_char="3479">go</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="3481" end_char="3484">back</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="3486" end_char="3487">in</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="3489" end_char="3492">time</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="3494" end_char="3496">and</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="3498" end_char="3503">repeat</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="3505" end_char="3508">2019</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="3510" end_char="3512">one</TOKEN>
<TOKEN id="token-23-14" pos="word" morph="none" start_char="3514" end_char="3520">hundred</TOKEN>
<TOKEN id="token-23-15" pos="word" morph="none" start_char="3522" end_char="3526">times</TOKEN>
<TOKEN id="token-23-16" pos="punct" morph="none" start_char="3527" end_char="3527">,</TOKEN>
<TOKEN id="token-23-17" pos="word" morph="none" start_char="3529" end_char="3531">two</TOKEN>
<TOKEN id="token-23-18" pos="word" morph="none" start_char="3533" end_char="3535">out</TOKEN>
<TOKEN id="token-23-19" pos="word" morph="none" start_char="3537" end_char="3538">of</TOKEN>
<TOKEN id="token-23-20" pos="word" morph="none" start_char="3540" end_char="3544">three</TOKEN>
<TOKEN id="token-23-21" pos="word" morph="none" start_char="3546" end_char="3550">times</TOKEN>
<TOKEN id="token-23-22" pos="punct" morph="none" start_char="3551" end_char="3551">,</TOKEN>
<TOKEN id="token-23-23" pos="unknown" morph="none" start_char="3553" end_char="3560">COVID-19</TOKEN>
<TOKEN id="token-23-24" pos="word" morph="none" start_char="3562" end_char="3566">would</TOKEN>
<TOKEN id="token-23-25" pos="word" morph="none" start_char="3568" end_char="3571">have</TOKEN>
<TOKEN id="token-23-26" pos="word" morph="none" start_char="3573" end_char="3579">fizzled</TOKEN>
<TOKEN id="token-23-27" pos="word" morph="none" start_char="3581" end_char="3583">out</TOKEN>
<TOKEN id="token-23-28" pos="word" morph="none" start_char="3585" end_char="3586">on</TOKEN>
<TOKEN id="token-23-29" pos="word" morph="none" start_char="3588" end_char="3590">its</TOKEN>
<TOKEN id="token-23-30" pos="word" morph="none" start_char="3592" end_char="3594">own</TOKEN>
<TOKEN id="token-23-31" pos="word" morph="none" start_char="3596" end_char="3602">without</TOKEN>
<TOKEN id="token-23-32" pos="word" morph="none" start_char="3604" end_char="3611">igniting</TOKEN>
<TOKEN id="token-23-33" pos="word" morph="none" start_char="3613" end_char="3613">a</TOKEN>
<TOKEN id="token-23-34" pos="word" morph="none" start_char="3615" end_char="3622">pandemic</TOKEN>
<TOKEN id="token-23-35" pos="punct" morph="none" start_char="3623" end_char="3623">,</TOKEN>
<TOKEN id="token-23-36" pos="word" morph="none" start_char="3625" end_char="3632">Wertheim</TOKEN>
<TOKEN id="token-23-37" pos="word" morph="none" start_char="3634" end_char="3638">noted</TOKEN>
<TOKEN id="token-23-38" pos="punct" morph="none" start_char="3639" end_char="3639">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="3642" end_char="3753">
<ORIGINAL_TEXT>"This finding supports the notion that humans are constantly being bombarded with zoonotic pathogens," he added.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="punct" morph="none" start_char="3642" end_char="3642">"</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="3643" end_char="3646">This</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="3648" end_char="3654">finding</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="3656" end_char="3663">supports</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="3665" end_char="3667">the</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="3669" end_char="3674">notion</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="3676" end_char="3679">that</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="3681" end_char="3686">humans</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="3688" end_char="3690">are</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="3692" end_char="3701">constantly</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="3703" end_char="3707">being</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="3709" end_char="3717">bombarded</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="3719" end_char="3722">with</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="3724" end_char="3731">zoonotic</TOKEN>
<TOKEN id="token-24-14" pos="word" morph="none" start_char="3733" end_char="3741">pathogens</TOKEN>
<TOKEN id="token-24-15" pos="punct" morph="none" start_char="3742" end_char="3743">,"</TOKEN>
<TOKEN id="token-24-16" pos="word" morph="none" start_char="3745" end_char="3746">he</TOKEN>
<TOKEN id="token-24-17" pos="word" morph="none" start_char="3748" end_char="3752">added</TOKEN>
<TOKEN id="token-24-18" pos="punct" morph="none" start_char="3753" end_char="3753">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="3756" end_char="3833">
<ORIGINAL_TEXT>Also read: In a first, woman delivers baby with antibodies against coronavirus</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="3756" end_char="3759">Also</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="3761" end_char="3764">read</TOKEN>
<TOKEN id="token-25-2" pos="punct" morph="none" start_char="3765" end_char="3765">:</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="3767" end_char="3768">In</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="3770" end_char="3770">a</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="3772" end_char="3776">first</TOKEN>
<TOKEN id="token-25-6" pos="punct" morph="none" start_char="3777" end_char="3777">,</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="3779" end_char="3783">woman</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="3785" end_char="3792">delivers</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="3794" end_char="3797">baby</TOKEN>
<TOKEN id="token-25-10" pos="word" morph="none" start_char="3799" end_char="3802">with</TOKEN>
<TOKEN id="token-25-11" pos="word" morph="none" start_char="3804" end_char="3813">antibodies</TOKEN>
<TOKEN id="token-25-12" pos="word" morph="none" start_char="3815" end_char="3821">against</TOKEN>
<TOKEN id="token-25-13" pos="word" morph="none" start_char="3823" end_char="3833">coronavirus</TOKEN>
</SEG>
<SEG id="segment-26" start_char="3836" end_char="3915">
<ORIGINAL_TEXT>Also read: IndiGo, AirAsia crack the whip against flyers flouting COVID-19 norms</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="3836" end_char="3839">Also</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="3841" end_char="3844">read</TOKEN>
<TOKEN id="token-26-2" pos="punct" morph="none" start_char="3845" end_char="3845">:</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="3847" end_char="3852">IndiGo</TOKEN>
<TOKEN id="token-26-4" pos="punct" morph="none" start_char="3853" end_char="3853">,</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="3855" end_char="3861">AirAsia</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="3863" end_char="3867">crack</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="3869" end_char="3871">the</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="3873" end_char="3876">whip</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="3878" end_char="3884">against</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="3886" end_char="3891">flyers</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="3893" end_char="3900">flouting</TOKEN>
<TOKEN id="token-26-12" pos="unknown" morph="none" start_char="3902" end_char="3909">COVID-19</TOKEN>
<TOKEN id="token-26-13" pos="word" morph="none" start_char="3911" end_char="3915">norms</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
