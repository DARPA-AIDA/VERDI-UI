<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATQF" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="5206" raw_text_md5="4cdc93bea93b00c65758502fde853ecb">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="80">
<ORIGINAL_TEXT>Un estudio médico alerta de que la covid-19 circulaba por Europa ya en diciembre</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="2">Un</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="4" end_char="10">estudio</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="12" end_char="17">médico</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="19" end_char="24">alerta</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="26" end_char="27">de</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="29" end_char="31">que</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="33" end_char="34">la</TOKEN>
<TOKEN id="token-0-7" pos="unknown" morph="none" start_char="36" end_char="43">covid-19</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="45" end_char="53">circulaba</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="55" end_char="57">por</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="59" end_char="64">Europa</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="66" end_char="67">ya</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="69" end_char="70">en</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="72" end_char="80">diciembre</TOKEN>
</SEG>
<SEG id="segment-1" start_char="84" end_char="163">
<ORIGINAL_TEXT>Un estudio médico alerta de que la covid-19 circulaba por Europa ya en diciembre</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="84" end_char="85">Un</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="87" end_char="93">estudio</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="95" end_char="100">médico</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="102" end_char="107">alerta</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="109" end_char="110">de</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="112" end_char="114">que</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="116" end_char="117">la</TOKEN>
<TOKEN id="token-1-7" pos="unknown" morph="none" start_char="119" end_char="126">covid-19</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="128" end_char="136">circulaba</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="138" end_char="140">por</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="142" end_char="147">Europa</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="149" end_char="150">ya</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="152" end_char="153">en</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="155" end_char="163">diciembre</TOKEN>
</SEG>
<SEG id="segment-2" start_char="167" end_char="320">
<ORIGINAL_TEXT>Tiene dirección el que hizo el estudio ese o es como las empresas intermediarias fantasma que contrata el ministro Illa a dedo, cual corrupto cualquiera??</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="167" end_char="171">Tiene</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="173" end_char="181">dirección</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="183" end_char="184">el</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="186" end_char="188">que</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="190" end_char="193">hizo</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="195" end_char="196">el</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="198" end_char="204">estudio</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="206" end_char="208">ese</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="210" end_char="210">o</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="212" end_char="213">es</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="215" end_char="218">como</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="220" end_char="222">las</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="224" end_char="231">empresas</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="233" end_char="246">intermediarias</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="248" end_char="255">fantasma</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="257" end_char="259">que</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="261" end_char="268">contrata</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="270" end_char="271">el</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="273" end_char="280">ministro</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="282" end_char="285">Illa</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="287" end_char="287">a</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="289" end_char="292">dedo</TOKEN>
<TOKEN id="token-2-22" pos="punct" morph="none" start_char="293" end_char="293">,</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="295" end_char="298">cual</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="300" end_char="307">corrupto</TOKEN>
<TOKEN id="token-2-25" pos="word" morph="none" start_char="309" end_char="318">cualquiera</TOKEN>
<TOKEN id="token-2-26" pos="punct" morph="none" start_char="319" end_char="320">??</TOKEN>
</SEG>
<SEG id="segment-3" start_char="324" end_char="731">
<ORIGINAL_TEXT>En diciembre me dio un chungo terrible, 2 semanas jodidos, 3 dias en cama, me costaba respirar y reir ya ni te cuento.. jodido de la garganta, tos, fiebre...y un dolor de cuerpo desde las uñas de los pies hasta la punta de los pelos, se me fue el gusto, vamos que nada tenia sabor... y como ya he comentado..soy una persona que se cuida.. lo achaque a una gripe estacional... pero NUNCA me dio de esa manera.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="324" end_char="325">En</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="327" end_char="335">diciembre</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="337" end_char="338">me</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="340" end_char="342">dio</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="344" end_char="345">un</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="347" end_char="352">chungo</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="354" end_char="361">terrible</TOKEN>
<TOKEN id="token-3-7" pos="punct" morph="none" start_char="362" end_char="362">,</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="364" end_char="364">2</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="366" end_char="372">semanas</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="374" end_char="380">jodidos</TOKEN>
<TOKEN id="token-3-11" pos="punct" morph="none" start_char="381" end_char="381">,</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="383" end_char="383">3</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="385" end_char="388">dias</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="390" end_char="391">en</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="393" end_char="396">cama</TOKEN>
<TOKEN id="token-3-16" pos="punct" morph="none" start_char="397" end_char="397">,</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="399" end_char="400">me</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="402" end_char="408">costaba</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="410" end_char="417">respirar</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="419" end_char="419">y</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="421" end_char="424">reir</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="426" end_char="427">ya</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="429" end_char="430">ni</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="432" end_char="433">te</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="435" end_char="440">cuento</TOKEN>
<TOKEN id="token-3-26" pos="punct" morph="none" start_char="441" end_char="442">..</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="444" end_char="449">jodido</TOKEN>
<TOKEN id="token-3-28" pos="word" morph="none" start_char="451" end_char="452">de</TOKEN>
<TOKEN id="token-3-29" pos="word" morph="none" start_char="454" end_char="455">la</TOKEN>
<TOKEN id="token-3-30" pos="word" morph="none" start_char="457" end_char="464">garganta</TOKEN>
<TOKEN id="token-3-31" pos="punct" morph="none" start_char="465" end_char="465">,</TOKEN>
<TOKEN id="token-3-32" pos="word" morph="none" start_char="467" end_char="469">tos</TOKEN>
<TOKEN id="token-3-33" pos="punct" morph="none" start_char="470" end_char="470">,</TOKEN>
<TOKEN id="token-3-34" pos="unknown" morph="none" start_char="472" end_char="481">fiebre...y</TOKEN>
<TOKEN id="token-3-35" pos="word" morph="none" start_char="483" end_char="484">un</TOKEN>
<TOKEN id="token-3-36" pos="word" morph="none" start_char="486" end_char="490">dolor</TOKEN>
<TOKEN id="token-3-37" pos="word" morph="none" start_char="492" end_char="493">de</TOKEN>
<TOKEN id="token-3-38" pos="word" morph="none" start_char="495" end_char="500">cuerpo</TOKEN>
<TOKEN id="token-3-39" pos="word" morph="none" start_char="502" end_char="506">desde</TOKEN>
<TOKEN id="token-3-40" pos="word" morph="none" start_char="508" end_char="510">las</TOKEN>
<TOKEN id="token-3-41" pos="word" morph="none" start_char="512" end_char="515">uñas</TOKEN>
<TOKEN id="token-3-42" pos="word" morph="none" start_char="517" end_char="518">de</TOKEN>
<TOKEN id="token-3-43" pos="word" morph="none" start_char="520" end_char="522">los</TOKEN>
<TOKEN id="token-3-44" pos="word" morph="none" start_char="524" end_char="527">pies</TOKEN>
<TOKEN id="token-3-45" pos="word" morph="none" start_char="529" end_char="533">hasta</TOKEN>
<TOKEN id="token-3-46" pos="word" morph="none" start_char="535" end_char="536">la</TOKEN>
<TOKEN id="token-3-47" pos="word" morph="none" start_char="538" end_char="542">punta</TOKEN>
<TOKEN id="token-3-48" pos="word" morph="none" start_char="544" end_char="545">de</TOKEN>
<TOKEN id="token-3-49" pos="word" morph="none" start_char="547" end_char="549">los</TOKEN>
<TOKEN id="token-3-50" pos="word" morph="none" start_char="551" end_char="555">pelos</TOKEN>
<TOKEN id="token-3-51" pos="punct" morph="none" start_char="556" end_char="556">,</TOKEN>
<TOKEN id="token-3-52" pos="word" morph="none" start_char="558" end_char="559">se</TOKEN>
<TOKEN id="token-3-53" pos="word" morph="none" start_char="561" end_char="562">me</TOKEN>
<TOKEN id="token-3-54" pos="word" morph="none" start_char="564" end_char="566">fue</TOKEN>
<TOKEN id="token-3-55" pos="word" morph="none" start_char="568" end_char="569">el</TOKEN>
<TOKEN id="token-3-56" pos="word" morph="none" start_char="571" end_char="575">gusto</TOKEN>
<TOKEN id="token-3-57" pos="punct" morph="none" start_char="576" end_char="576">,</TOKEN>
<TOKEN id="token-3-58" pos="word" morph="none" start_char="578" end_char="582">vamos</TOKEN>
<TOKEN id="token-3-59" pos="word" morph="none" start_char="584" end_char="586">que</TOKEN>
<TOKEN id="token-3-60" pos="word" morph="none" start_char="588" end_char="591">nada</TOKEN>
<TOKEN id="token-3-61" pos="word" morph="none" start_char="593" end_char="597">tenia</TOKEN>
<TOKEN id="token-3-62" pos="word" morph="none" start_char="599" end_char="603">sabor</TOKEN>
<TOKEN id="token-3-63" pos="punct" morph="none" start_char="604" end_char="606">...</TOKEN>
<TOKEN id="token-3-64" pos="word" morph="none" start_char="608" end_char="608">y</TOKEN>
<TOKEN id="token-3-65" pos="word" morph="none" start_char="610" end_char="613">como</TOKEN>
<TOKEN id="token-3-66" pos="word" morph="none" start_char="615" end_char="616">ya</TOKEN>
<TOKEN id="token-3-67" pos="word" morph="none" start_char="618" end_char="619">he</TOKEN>
<TOKEN id="token-3-68" pos="unknown" morph="none" start_char="621" end_char="634">comentado..soy</TOKEN>
<TOKEN id="token-3-69" pos="word" morph="none" start_char="636" end_char="638">una</TOKEN>
<TOKEN id="token-3-70" pos="word" morph="none" start_char="640" end_char="646">persona</TOKEN>
<TOKEN id="token-3-71" pos="word" morph="none" start_char="648" end_char="650">que</TOKEN>
<TOKEN id="token-3-72" pos="word" morph="none" start_char="652" end_char="653">se</TOKEN>
<TOKEN id="token-3-73" pos="word" morph="none" start_char="655" end_char="659">cuida</TOKEN>
<TOKEN id="token-3-74" pos="punct" morph="none" start_char="660" end_char="661">..</TOKEN>
<TOKEN id="token-3-75" pos="word" morph="none" start_char="663" end_char="664">lo</TOKEN>
<TOKEN id="token-3-76" pos="word" morph="none" start_char="666" end_char="672">achaque</TOKEN>
<TOKEN id="token-3-77" pos="word" morph="none" start_char="674" end_char="674">a</TOKEN>
<TOKEN id="token-3-78" pos="word" morph="none" start_char="676" end_char="678">una</TOKEN>
<TOKEN id="token-3-79" pos="word" morph="none" start_char="680" end_char="684">gripe</TOKEN>
<TOKEN id="token-3-80" pos="word" morph="none" start_char="686" end_char="695">estacional</TOKEN>
<TOKEN id="token-3-81" pos="punct" morph="none" start_char="696" end_char="698">...</TOKEN>
<TOKEN id="token-3-82" pos="word" morph="none" start_char="700" end_char="703">pero</TOKEN>
<TOKEN id="token-3-83" pos="word" morph="none" start_char="705" end_char="709">NUNCA</TOKEN>
<TOKEN id="token-3-84" pos="word" morph="none" start_char="711" end_char="712">me</TOKEN>
<TOKEN id="token-3-85" pos="word" morph="none" start_char="714" end_char="716">dio</TOKEN>
<TOKEN id="token-3-86" pos="word" morph="none" start_char="718" end_char="719">de</TOKEN>
<TOKEN id="token-3-87" pos="word" morph="none" start_char="721" end_char="723">esa</TOKEN>
<TOKEN id="token-3-88" pos="word" morph="none" start_char="725" end_char="730">manera</TOKEN>
<TOKEN id="token-3-89" pos="punct" morph="none" start_char="731" end_char="731">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="735" end_char="799">
<ORIGINAL_TEXT>El bicho lleva dando vueltas al mundo desde septiembre de 2019...</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="735" end_char="736">El</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="738" end_char="742">bicho</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="744" end_char="748">lleva</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="750" end_char="754">dando</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="756" end_char="762">vueltas</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="764" end_char="765">al</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="767" end_char="771">mundo</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="773" end_char="777">desde</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="779" end_char="788">septiembre</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="790" end_char="791">de</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="793" end_char="796">2019</TOKEN>
<TOKEN id="token-4-11" pos="punct" morph="none" start_char="797" end_char="799">...</TOKEN>
</SEG>
<SEG id="segment-5" start_char="802" end_char="846">
<ORIGINAL_TEXT>Yo lo pasé en la primera semana de diciembre.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="802" end_char="803">Yo</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="805" end_char="806">lo</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="808" end_char="811">pasé</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="813" end_char="814">en</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="816" end_char="817">la</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="819" end_char="825">primera</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="827" end_char="832">semana</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="834" end_char="835">de</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="837" end_char="845">diciembre</TOKEN>
<TOKEN id="token-5-9" pos="punct" morph="none" start_char="846" end_char="846">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="848" end_char="1003">
<ORIGINAL_TEXT>Mis hijas y mi mujer llegaron de viaje via Barcelona, mi mujer nada mas bajar me dijo que medio avión tosia con sequedad...que le había llamado la atención.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="848" end_char="850">Mis</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="852" end_char="856">hijas</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="858" end_char="858">y</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="860" end_char="861">mi</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="863" end_char="867">mujer</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="869" end_char="876">llegaron</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="878" end_char="879">de</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="881" end_char="885">viaje</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="887" end_char="889">via</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="891" end_char="899">Barcelona</TOKEN>
<TOKEN id="token-6-10" pos="punct" morph="none" start_char="900" end_char="900">,</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="902" end_char="903">mi</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="905" end_char="909">mujer</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="911" end_char="914">nada</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="916" end_char="918">mas</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="920" end_char="924">bajar</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="926" end_char="927">me</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="929" end_char="932">dijo</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="934" end_char="936">que</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="938" end_char="942">medio</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="944" end_char="948">avión</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="950" end_char="954">tosia</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="956" end_char="958">con</TOKEN>
<TOKEN id="token-6-23" pos="unknown" morph="none" start_char="960" end_char="973">sequedad...que</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="975" end_char="976">le</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="978" end_char="982">había</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="984" end_char="990">llamado</TOKEN>
<TOKEN id="token-6-27" pos="word" morph="none" start_char="992" end_char="993">la</TOKEN>
<TOKEN id="token-6-28" pos="word" morph="none" start_char="995" end_char="1002">atención</TOKEN>
<TOKEN id="token-6-29" pos="punct" morph="none" start_char="1003" end_char="1003">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="1006" end_char="1164">
<ORIGINAL_TEXT>A los quince días tosían mis dos hijas y mi mujer, y servidor la primera semana de diciembre con la peor afonía y lanrigitis de mi vida...literalmente sin voz.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="1006" end_char="1006">A</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="1008" end_char="1010">los</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="1012" end_char="1017">quince</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="1019" end_char="1022">días</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="1024" end_char="1029">tosían</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="1031" end_char="1033">mis</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="1035" end_char="1037">dos</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="1039" end_char="1043">hijas</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="1045" end_char="1045">y</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="1047" end_char="1048">mi</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="1050" end_char="1054">mujer</TOKEN>
<TOKEN id="token-7-11" pos="punct" morph="none" start_char="1055" end_char="1055">,</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="1057" end_char="1057">y</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="1059" end_char="1066">servidor</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="1068" end_char="1069">la</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="1071" end_char="1077">primera</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="1079" end_char="1084">semana</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="1086" end_char="1087">de</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="1089" end_char="1097">diciembre</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="1099" end_char="1101">con</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="1103" end_char="1104">la</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="1106" end_char="1109">peor</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="1111" end_char="1116">afonía</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="1118" end_char="1118">y</TOKEN>
<TOKEN id="token-7-24" pos="word" morph="none" start_char="1120" end_char="1129">lanrigitis</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="1131" end_char="1132">de</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="1134" end_char="1135">mi</TOKEN>
<TOKEN id="token-7-27" pos="unknown" morph="none" start_char="1137" end_char="1155">vida...literalmente</TOKEN>
<TOKEN id="token-7-28" pos="word" morph="none" start_char="1157" end_char="1159">sin</TOKEN>
<TOKEN id="token-7-29" pos="word" morph="none" start_char="1161" end_char="1163">voz</TOKEN>
<TOKEN id="token-7-30" pos="punct" morph="none" start_char="1164" end_char="1164">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1166" end_char="1222">
<ORIGINAL_TEXT>Tuvieron que pincharme para que me regresara algo de voz.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="1166" end_char="1173">Tuvieron</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="1175" end_char="1177">que</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="1179" end_char="1187">pincharme</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1189" end_char="1192">para</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1194" end_char="1196">que</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1198" end_char="1199">me</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="1201" end_char="1209">regresara</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="1211" end_char="1214">algo</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1216" end_char="1217">de</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="1219" end_char="1221">voz</TOKEN>
<TOKEN id="token-8-10" pos="punct" morph="none" start_char="1222" end_char="1222">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1224" end_char="1300">
<ORIGINAL_TEXT>Mi bebe de 5 meses entonces igual que yo, tos perruna y sin voz el pobrecito.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1224" end_char="1225">Mi</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1227" end_char="1230">bebe</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="1232" end_char="1233">de</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1235" end_char="1235">5</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1237" end_char="1241">meses</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1243" end_char="1250">entonces</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1252" end_char="1256">igual</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1258" end_char="1260">que</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1262" end_char="1263">yo</TOKEN>
<TOKEN id="token-9-9" pos="punct" morph="none" start_char="1264" end_char="1264">,</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1266" end_char="1268">tos</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1270" end_char="1276">perruna</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="1278" end_char="1278">y</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1280" end_char="1282">sin</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1284" end_char="1286">voz</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1288" end_char="1289">el</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="1291" end_char="1299">pobrecito</TOKEN>
<TOKEN id="token-9-17" pos="punct" morph="none" start_char="1300" end_char="1300">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1303" end_char="1516">
<ORIGINAL_TEXT>Se nos fue...y al mes volvió, tanto el niño como yo pico febril de 38 y pico...dos días consecutivos...extrañisimo en mí que rara vez he tenido fiebre, y que con 37, 3 ya me estoy muriendo...pues dos días seguidos.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1303" end_char="1304">Se</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1306" end_char="1308">nos</TOKEN>
<TOKEN id="token-10-2" pos="unknown" morph="none" start_char="1310" end_char="1316">fue...y</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1318" end_char="1319">al</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1321" end_char="1323">mes</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1325" end_char="1330">volvió</TOKEN>
<TOKEN id="token-10-6" pos="punct" morph="none" start_char="1331" end_char="1331">,</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1333" end_char="1337">tanto</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1339" end_char="1340">el</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1342" end_char="1345">niño</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1347" end_char="1350">como</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1352" end_char="1353">yo</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1355" end_char="1358">pico</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1360" end_char="1365">febril</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1367" end_char="1368">de</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1370" end_char="1371">38</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1373" end_char="1373">y</TOKEN>
<TOKEN id="token-10-17" pos="unknown" morph="none" start_char="1375" end_char="1384">pico...dos</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1386" end_char="1389">días</TOKEN>
<TOKEN id="token-10-19" pos="unknown" morph="none" start_char="1391" end_char="1416">consecutivos...extrañisimo</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="1418" end_char="1419">en</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="1421" end_char="1422">mí</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="1424" end_char="1426">que</TOKEN>
<TOKEN id="token-10-23" pos="word" morph="none" start_char="1428" end_char="1431">rara</TOKEN>
<TOKEN id="token-10-24" pos="word" morph="none" start_char="1433" end_char="1435">vez</TOKEN>
<TOKEN id="token-10-25" pos="word" morph="none" start_char="1437" end_char="1438">he</TOKEN>
<TOKEN id="token-10-26" pos="word" morph="none" start_char="1440" end_char="1445">tenido</TOKEN>
<TOKEN id="token-10-27" pos="word" morph="none" start_char="1447" end_char="1452">fiebre</TOKEN>
<TOKEN id="token-10-28" pos="punct" morph="none" start_char="1453" end_char="1453">,</TOKEN>
<TOKEN id="token-10-29" pos="word" morph="none" start_char="1455" end_char="1455">y</TOKEN>
<TOKEN id="token-10-30" pos="word" morph="none" start_char="1457" end_char="1459">que</TOKEN>
<TOKEN id="token-10-31" pos="word" morph="none" start_char="1461" end_char="1463">con</TOKEN>
<TOKEN id="token-10-32" pos="word" morph="none" start_char="1465" end_char="1466">37</TOKEN>
<TOKEN id="token-10-33" pos="punct" morph="none" start_char="1467" end_char="1467">,</TOKEN>
<TOKEN id="token-10-34" pos="word" morph="none" start_char="1469" end_char="1469">3</TOKEN>
<TOKEN id="token-10-35" pos="word" morph="none" start_char="1471" end_char="1472">ya</TOKEN>
<TOKEN id="token-10-36" pos="word" morph="none" start_char="1474" end_char="1475">me</TOKEN>
<TOKEN id="token-10-37" pos="word" morph="none" start_char="1477" end_char="1481">estoy</TOKEN>
<TOKEN id="token-10-38" pos="unknown" morph="none" start_char="1483" end_char="1497">muriendo...pues</TOKEN>
<TOKEN id="token-10-39" pos="word" morph="none" start_char="1499" end_char="1501">dos</TOKEN>
<TOKEN id="token-10-40" pos="word" morph="none" start_char="1503" end_char="1506">días</TOKEN>
<TOKEN id="token-10-41" pos="word" morph="none" start_char="1508" end_char="1515">seguidos</TOKEN>
<TOKEN id="token-10-42" pos="punct" morph="none" start_char="1516" end_char="1516">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1519" end_char="1571">
<ORIGINAL_TEXT>Una semana con antigripales...y se fue completamente.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1519" end_char="1521">Una</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1523" end_char="1528">semana</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1530" end_char="1532">con</TOKEN>
<TOKEN id="token-11-3" pos="unknown" morph="none" start_char="1534" end_char="1549">antigripales...y</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1551" end_char="1552">se</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1554" end_char="1556">fue</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1558" end_char="1570">completamente</TOKEN>
<TOKEN id="token-11-7" pos="punct" morph="none" start_char="1571" end_char="1571">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1573" end_char="1580">
<ORIGINAL_TEXT>Por fin.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1573" end_char="1575">Por</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1577" end_char="1579">fin</TOKEN>
<TOKEN id="token-12-2" pos="punct" morph="none" start_char="1580" end_char="1580">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1582" end_char="1636">
<ORIGINAL_TEXT>La tos entre diciembre y final de enero jamás se fue...</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1582" end_char="1583">La</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1585" end_char="1587">tos</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1589" end_char="1593">entre</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1595" end_char="1603">diciembre</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1605" end_char="1605">y</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1607" end_char="1611">final</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1613" end_char="1614">de</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1616" end_char="1620">enero</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1622" end_char="1626">jamás</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1628" end_char="1629">se</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1631" end_char="1633">fue</TOKEN>
<TOKEN id="token-13-11" pos="punct" morph="none" start_char="1634" end_char="1636">...</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1639" end_char="1720">
<ORIGINAL_TEXT>A mis hijas y a mi mujer también les regresó, pero mas suave que al nene y a mí...</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1639" end_char="1639">A</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1641" end_char="1643">mis</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1645" end_char="1649">hijas</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1651" end_char="1651">y</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1653" end_char="1653">a</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1655" end_char="1656">mi</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1658" end_char="1662">mujer</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1664" end_char="1670">también</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1672" end_char="1674">les</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1676" end_char="1682">regresó</TOKEN>
<TOKEN id="token-14-10" pos="punct" morph="none" start_char="1683" end_char="1683">,</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1685" end_char="1688">pero</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="1690" end_char="1692">mas</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="1694" end_char="1698">suave</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="1700" end_char="1702">que</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="1704" end_char="1705">al</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="1707" end_char="1710">nene</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="1712" end_char="1712">y</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="1714" end_char="1714">a</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="1716" end_char="1717">mí</TOKEN>
<TOKEN id="token-14-20" pos="punct" morph="none" start_char="1718" end_char="1720">...</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1723" end_char="1811">
<ORIGINAL_TEXT>El bichito lleva muchisimas vueltas al mundo...y ya llebaba mucha en noviembre-diciembre.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1723" end_char="1724">El</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1726" end_char="1732">bichito</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1734" end_char="1738">lleva</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="1740" end_char="1749">muchisimas</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1751" end_char="1757">vueltas</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1759" end_char="1760">al</TOKEN>
<TOKEN id="token-15-6" pos="unknown" morph="none" start_char="1762" end_char="1770">mundo...y</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1772" end_char="1773">ya</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1775" end_char="1781">llebaba</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="1783" end_char="1787">mucha</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1789" end_char="1790">en</TOKEN>
<TOKEN id="token-15-11" pos="unknown" morph="none" start_char="1792" end_char="1810">noviembre-diciembre</TOKEN>
<TOKEN id="token-15-12" pos="punct" morph="none" start_char="1811" end_char="1811">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1814" end_char="1971">
<ORIGINAL_TEXT>Si oyen a Frank Cuesta en casa de Fedecojo....lo dijo bien claro: en Agosto la gente dejo de consumir pangolín porque sabían que el que lo consumía enfermaba.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1814" end_char="1815">Si</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1817" end_char="1820">oyen</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1822" end_char="1822">a</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1824" end_char="1828">Frank</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1830" end_char="1835">Cuesta</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1837" end_char="1838">en</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="1840" end_char="1843">casa</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="1845" end_char="1846">de</TOKEN>
<TOKEN id="token-16-8" pos="unknown" morph="none" start_char="1848" end_char="1861">Fedecojo....lo</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="1863" end_char="1866">dijo</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="1868" end_char="1871">bien</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="1873" end_char="1877">claro</TOKEN>
<TOKEN id="token-16-12" pos="punct" morph="none" start_char="1878" end_char="1878">:</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="1880" end_char="1881">en</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="1883" end_char="1888">Agosto</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="1890" end_char="1891">la</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="1893" end_char="1897">gente</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="1899" end_char="1902">dejo</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="1904" end_char="1905">de</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="1907" end_char="1914">consumir</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="1916" end_char="1923">pangolín</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="1925" end_char="1930">porque</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="1932" end_char="1937">sabían</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="1939" end_char="1941">que</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="1943" end_char="1944">el</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="1946" end_char="1948">que</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="1950" end_char="1951">lo</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="1953" end_char="1960">consumía</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="1962" end_char="1970">enfermaba</TOKEN>
<TOKEN id="token-16-29" pos="punct" morph="none" start_char="1971" end_char="1971">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="1973" end_char="2018">
<ORIGINAL_TEXT>Y dejamos de ver pangonlines por los mercados.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="1973" end_char="1973">Y</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="1975" end_char="1981">dejamos</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="1983" end_char="1984">de</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="1986" end_char="1988">ver</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="1990" end_char="2000">pangonlines</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2002" end_char="2004">por</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2006" end_char="2008">los</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="2010" end_char="2017">mercados</TOKEN>
<TOKEN id="token-17-8" pos="punct" morph="none" start_char="2018" end_char="2018">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2021" end_char="2090">
<ORIGINAL_TEXT>Si hacen test de anticuerpos, la mitad de la población ya los tiene...</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2021" end_char="2022">Si</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2024" end_char="2028">hacen</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="2030" end_char="2033">test</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="2035" end_char="2036">de</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2038" end_char="2048">anticuerpos</TOKEN>
<TOKEN id="token-18-5" pos="punct" morph="none" start_char="2049" end_char="2049">,</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2051" end_char="2052">la</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2054" end_char="2058">mitad</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2060" end_char="2061">de</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2063" end_char="2064">la</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2066" end_char="2074">población</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="2076" end_char="2077">ya</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="2079" end_char="2081">los</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="2083" end_char="2087">tiene</TOKEN>
<TOKEN id="token-18-14" pos="punct" morph="none" start_char="2088" end_char="2090">...</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2094" end_char="2121">
<ORIGINAL_TEXT>Y en Italia, y en Francia...</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2094" end_char="2094">Y</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2096" end_char="2097">en</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2099" end_char="2104">Italia</TOKEN>
<TOKEN id="token-19-3" pos="punct" morph="none" start_char="2105" end_char="2105">,</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2107" end_char="2107">y</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="2109" end_char="2110">en</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2112" end_char="2118">Francia</TOKEN>
<TOKEN id="token-19-7" pos="punct" morph="none" start_char="2119" end_char="2121">...</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2124" end_char="2131">
<ORIGINAL_TEXT>Francia.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2124" end_char="2130">Francia</TOKEN>
<TOKEN id="token-20-1" pos="punct" morph="none" start_char="2131" end_char="2131">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="2133" end_char="2249">
<ORIGINAL_TEXT>Detectan "+ a covid19" en muestra de enfermo de neumonía del 27 de diciembre (ergo "infectado" circa 3 semanas antes)</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="2133" end_char="2140">Detectan</TOKEN>
<TOKEN id="token-21-1" pos="unknown" morph="none" start_char="2142" end_char="2143">"+</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="2145" end_char="2145">a</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="2147" end_char="2153">covid19</TOKEN>
<TOKEN id="token-21-4" pos="punct" morph="none" start_char="2154" end_char="2154">"</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="2156" end_char="2157">en</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="2159" end_char="2165">muestra</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="2167" end_char="2168">de</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="2170" end_char="2176">enfermo</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="2178" end_char="2179">de</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="2181" end_char="2188">neumonía</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="2190" end_char="2192">del</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="2194" end_char="2195">27</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="2197" end_char="2198">de</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="2200" end_char="2208">diciembre</TOKEN>
<TOKEN id="token-21-15" pos="punct" morph="none" start_char="2210" end_char="2210">(</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="2211" end_char="2214">ergo</TOKEN>
<TOKEN id="token-21-17" pos="punct" morph="none" start_char="2216" end_char="2216">"</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="2217" end_char="2225">infectado</TOKEN>
<TOKEN id="token-21-19" pos="punct" morph="none" start_char="2226" end_char="2226">"</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="2228" end_char="2232">circa</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="2234" end_char="2234">3</TOKEN>
<TOKEN id="token-21-22" pos="word" morph="none" start_char="2236" end_char="2242">semanas</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="2244" end_char="2248">antes</TOKEN>
<TOKEN id="token-21-24" pos="punct" morph="none" start_char="2249" end_char="2249">)</TOKEN>
</SEG>
<SEG id="segment-22" start_char="2252" end_char="2348">
<ORIGINAL_TEXT>Y pronostico que si nos ponemos a estudiar muestras de hace 10 o 20 años también habrá "covid19".</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="2252" end_char="2252">Y</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="2254" end_char="2263">pronostico</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="2265" end_char="2267">que</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="2269" end_char="2270">si</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="2272" end_char="2274">nos</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="2276" end_char="2282">ponemos</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="2284" end_char="2284">a</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="2286" end_char="2293">estudiar</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="2295" end_char="2302">muestras</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="2304" end_char="2305">de</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="2307" end_char="2310">hace</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="2312" end_char="2313">10</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="2315" end_char="2315">o</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="2317" end_char="2318">20</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="2320" end_char="2323">años</TOKEN>
<TOKEN id="token-22-15" pos="word" morph="none" start_char="2325" end_char="2331">también</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="2333" end_char="2337">habrá</TOKEN>
<TOKEN id="token-22-17" pos="punct" morph="none" start_char="2339" end_char="2339">"</TOKEN>
<TOKEN id="token-22-18" pos="word" morph="none" start_char="2340" end_char="2346">covid19</TOKEN>
<TOKEN id="token-22-19" pos="punct" morph="none" start_char="2347" end_char="2348">".</TOKEN>
</SEG>
<SEG id="segment-23" start_char="2351" end_char="2404">
<ORIGINAL_TEXT>El test da un 7% de "+", testes la muestra que testes:</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="2351" end_char="2352">El</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="2354" end_char="2357">test</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="2359" end_char="2360">da</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="2362" end_char="2363">un</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="2365" end_char="2365">7</TOKEN>
<TOKEN id="token-23-5" pos="punct" morph="none" start_char="2366" end_char="2366">%</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="2368" end_char="2369">de</TOKEN>
<TOKEN id="token-23-7" pos="unknown" morph="none" start_char="2371" end_char="2374">"+",</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="2376" end_char="2381">testes</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="2383" end_char="2384">la</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="2386" end_char="2392">muestra</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="2394" end_char="2396">que</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="2398" end_char="2403">testes</TOKEN>
<TOKEN id="token-23-13" pos="punct" morph="none" start_char="2404" end_char="2404">:</TOKEN>
</SEG>
<SEG id="segment-24" start_char="2407" end_char="2551">
<ORIGINAL_TEXT>Lectura crítica de pantallazos de prospectos y literatura sobre "tests de SARS Cov 2" ("Covid19")|Infórmese antes de consentir uno de estos tests</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="2407" end_char="2413">Lectura</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="2415" end_char="2421">crítica</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="2423" end_char="2424">de</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="2426" end_char="2436">pantallazos</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="2438" end_char="2439">de</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="2441" end_char="2450">prospectos</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="2452" end_char="2452">y</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="2454" end_char="2463">literatura</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="2465" end_char="2469">sobre</TOKEN>
<TOKEN id="token-24-9" pos="punct" morph="none" start_char="2471" end_char="2471">"</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="2472" end_char="2476">tests</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="2478" end_char="2479">de</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="2481" end_char="2484">SARS</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="2486" end_char="2488">Cov</TOKEN>
<TOKEN id="token-24-14" pos="word" morph="none" start_char="2490" end_char="2490">2</TOKEN>
<TOKEN id="token-24-15" pos="punct" morph="none" start_char="2491" end_char="2491">"</TOKEN>
<TOKEN id="token-24-16" pos="punct" morph="none" start_char="2493" end_char="2494">("</TOKEN>
<TOKEN id="token-24-17" pos="unknown" morph="none" start_char="2495" end_char="2513">Covid19")|Infórmese</TOKEN>
<TOKEN id="token-24-18" pos="word" morph="none" start_char="2515" end_char="2519">antes</TOKEN>
<TOKEN id="token-24-19" pos="word" morph="none" start_char="2521" end_char="2522">de</TOKEN>
<TOKEN id="token-24-20" pos="word" morph="none" start_char="2524" end_char="2532">consentir</TOKEN>
<TOKEN id="token-24-21" pos="word" morph="none" start_char="2534" end_char="2536">uno</TOKEN>
<TOKEN id="token-24-22" pos="word" morph="none" start_char="2538" end_char="2539">de</TOKEN>
<TOKEN id="token-24-23" pos="word" morph="none" start_char="2541" end_char="2545">estos</TOKEN>
<TOKEN id="token-24-24" pos="word" morph="none" start_char="2547" end_char="2551">tests</TOKEN>
</SEG>
<SEG id="segment-25" start_char="2554" end_char="2578">
<ORIGINAL_TEXT>Los tests son los padres.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="2554" end_char="2556">Los</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="2558" end_char="2562">tests</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="2564" end_char="2566">son</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="2568" end_char="2570">los</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="2572" end_char="2577">padres</TOKEN>
<TOKEN id="token-25-5" pos="punct" morph="none" start_char="2578" end_char="2578">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="2582" end_char="2724">
<ORIGINAL_TEXT>En estos momentos se están publicando muchos "estudios" sobre el covid-19 sin contrastar y lo más probable es que la mayor parte sean erróneos.</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="2582" end_char="2583">En</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="2585" end_char="2589">estos</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="2591" end_char="2598">momentos</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="2600" end_char="2601">se</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="2603" end_char="2607">están</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="2609" end_char="2618">publicando</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="2620" end_char="2625">muchos</TOKEN>
<TOKEN id="token-26-7" pos="punct" morph="none" start_char="2627" end_char="2627">"</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="2628" end_char="2635">estudios</TOKEN>
<TOKEN id="token-26-9" pos="punct" morph="none" start_char="2636" end_char="2636">"</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="2638" end_char="2642">sobre</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="2644" end_char="2645">el</TOKEN>
<TOKEN id="token-26-12" pos="unknown" morph="none" start_char="2647" end_char="2654">covid-19</TOKEN>
<TOKEN id="token-26-13" pos="word" morph="none" start_char="2656" end_char="2658">sin</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="2660" end_char="2669">contrastar</TOKEN>
<TOKEN id="token-26-15" pos="word" morph="none" start_char="2671" end_char="2671">y</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="2673" end_char="2674">lo</TOKEN>
<TOKEN id="token-26-17" pos="word" morph="none" start_char="2676" end_char="2678">más</TOKEN>
<TOKEN id="token-26-18" pos="word" morph="none" start_char="2680" end_char="2687">probable</TOKEN>
<TOKEN id="token-26-19" pos="word" morph="none" start_char="2689" end_char="2690">es</TOKEN>
<TOKEN id="token-26-20" pos="word" morph="none" start_char="2692" end_char="2694">que</TOKEN>
<TOKEN id="token-26-21" pos="word" morph="none" start_char="2696" end_char="2697">la</TOKEN>
<TOKEN id="token-26-22" pos="word" morph="none" start_char="2699" end_char="2703">mayor</TOKEN>
<TOKEN id="token-26-23" pos="word" morph="none" start_char="2705" end_char="2709">parte</TOKEN>
<TOKEN id="token-26-24" pos="word" morph="none" start_char="2711" end_char="2714">sean</TOKEN>
<TOKEN id="token-26-25" pos="word" morph="none" start_char="2716" end_char="2723">erróneos</TOKEN>
<TOKEN id="token-26-26" pos="punct" morph="none" start_char="2724" end_char="2724">.</TOKEN>
</SEG>
<SEG id="segment-27" start_char="2726" end_char="2742">
<ORIGINAL_TEXT>Cuidado con ello.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="2726" end_char="2732">Cuidado</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="2734" end_char="2736">con</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="2738" end_char="2741">ello</TOKEN>
<TOKEN id="token-27-3" pos="punct" morph="none" start_char="2742" end_char="2742">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="2744" end_char="2874">
<ORIGINAL_TEXT>Entre investigadores buscando financiación y periodistas a la caza de noticias sensacionalistas tenemos el caos informativo actual.</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="2744" end_char="2748">Entre</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="2750" end_char="2763">investigadores</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="2765" end_char="2772">buscando</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="2774" end_char="2785">financiación</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="2787" end_char="2787">y</TOKEN>
<TOKEN id="token-28-5" pos="word" morph="none" start_char="2789" end_char="2799">periodistas</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="2801" end_char="2801">a</TOKEN>
<TOKEN id="token-28-7" pos="word" morph="none" start_char="2803" end_char="2804">la</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="2806" end_char="2809">caza</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="2811" end_char="2812">de</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="2814" end_char="2821">noticias</TOKEN>
<TOKEN id="token-28-11" pos="word" morph="none" start_char="2823" end_char="2838">sensacionalistas</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="2840" end_char="2846">tenemos</TOKEN>
<TOKEN id="token-28-13" pos="word" morph="none" start_char="2848" end_char="2849">el</TOKEN>
<TOKEN id="token-28-14" pos="word" morph="none" start_char="2851" end_char="2854">caos</TOKEN>
<TOKEN id="token-28-15" pos="word" morph="none" start_char="2856" end_char="2866">informativo</TOKEN>
<TOKEN id="token-28-16" pos="word" morph="none" start_char="2868" end_char="2873">actual</TOKEN>
<TOKEN id="token-28-17" pos="punct" morph="none" start_char="2874" end_char="2874">.</TOKEN>
</SEG>
<SEG id="segment-29" start_char="2878" end_char="2943">
<ORIGINAL_TEXT>Yo creo que lo ha pasado ya medio país o más , digan lo que digan.</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="2878" end_char="2879">Yo</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="2881" end_char="2884">creo</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="2886" end_char="2888">que</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="2890" end_char="2891">lo</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="2893" end_char="2894">ha</TOKEN>
<TOKEN id="token-29-5" pos="word" morph="none" start_char="2896" end_char="2901">pasado</TOKEN>
<TOKEN id="token-29-6" pos="word" morph="none" start_char="2903" end_char="2904">ya</TOKEN>
<TOKEN id="token-29-7" pos="word" morph="none" start_char="2906" end_char="2910">medio</TOKEN>
<TOKEN id="token-29-8" pos="word" morph="none" start_char="2912" end_char="2915">país</TOKEN>
<TOKEN id="token-29-9" pos="word" morph="none" start_char="2917" end_char="2917">o</TOKEN>
<TOKEN id="token-29-10" pos="word" morph="none" start_char="2919" end_char="2921">más</TOKEN>
<TOKEN id="token-29-11" pos="punct" morph="none" start_char="2923" end_char="2923">,</TOKEN>
<TOKEN id="token-29-12" pos="word" morph="none" start_char="2925" end_char="2929">digan</TOKEN>
<TOKEN id="token-29-13" pos="word" morph="none" start_char="2931" end_char="2932">lo</TOKEN>
<TOKEN id="token-29-14" pos="word" morph="none" start_char="2934" end_char="2936">que</TOKEN>
<TOKEN id="token-29-15" pos="word" morph="none" start_char="2938" end_char="2942">digan</TOKEN>
<TOKEN id="token-29-16" pos="punct" morph="none" start_char="2943" end_char="2943">.</TOKEN>
</SEG>
<SEG id="segment-30" start_char="2947" end_char="2980">
<ORIGINAL_TEXT>Un estudio para no culpar a China.</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="2947" end_char="2948">Un</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="2950" end_char="2956">estudio</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="2958" end_char="2961">para</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="2963" end_char="2964">no</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="2966" end_char="2971">culpar</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="2973" end_char="2973">a</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="2975" end_char="2979">China</TOKEN>
<TOKEN id="token-30-7" pos="punct" morph="none" start_char="2980" end_char="2980">.</TOKEN>
</SEG>
<SEG id="segment-31" start_char="2985" end_char="3041">
<ORIGINAL_TEXT>AYN RANDiano2 dijo: Y en Italia, y en Francia... Francia.</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="word" morph="none" start_char="2985" end_char="2987">AYN</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="2989" end_char="2997">RANDiano2</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="2999" end_char="3002">dijo</TOKEN>
<TOKEN id="token-31-3" pos="punct" morph="none" start_char="3003" end_char="3003">:</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="3005" end_char="3005">Y</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="3007" end_char="3008">en</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="3010" end_char="3015">Italia</TOKEN>
<TOKEN id="token-31-7" pos="punct" morph="none" start_char="3016" end_char="3016">,</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="3018" end_char="3018">y</TOKEN>
<TOKEN id="token-31-9" pos="word" morph="none" start_char="3020" end_char="3021">en</TOKEN>
<TOKEN id="token-31-10" pos="word" morph="none" start_char="3023" end_char="3029">Francia</TOKEN>
<TOKEN id="token-31-11" pos="punct" morph="none" start_char="3030" end_char="3032">...</TOKEN>
<TOKEN id="token-31-12" pos="word" morph="none" start_char="3034" end_char="3040">Francia</TOKEN>
<TOKEN id="token-31-13" pos="punct" morph="none" start_char="3041" end_char="3041">.</TOKEN>
</SEG>
<SEG id="segment-32" start_char="3043" end_char="3257">
<ORIGINAL_TEXT>Detectan "+ a covid19" en muestra de enfermo de neumonía del 27 de diciembre (ergo "infectado" circa 3 semanas antes) Y pronostico que si nos ponemos a estudiar muestras de hace 10 o 20 años también habrá "covid19".</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="3043" end_char="3050">Detectan</TOKEN>
<TOKEN id="token-32-1" pos="unknown" morph="none" start_char="3052" end_char="3053">"+</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="3055" end_char="3055">a</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="3057" end_char="3063">covid19</TOKEN>
<TOKEN id="token-32-4" pos="punct" morph="none" start_char="3064" end_char="3064">"</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="3066" end_char="3067">en</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="3069" end_char="3075">muestra</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="3077" end_char="3078">de</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="3080" end_char="3086">enfermo</TOKEN>
<TOKEN id="token-32-9" pos="word" morph="none" start_char="3088" end_char="3089">de</TOKEN>
<TOKEN id="token-32-10" pos="word" morph="none" start_char="3091" end_char="3098">neumonía</TOKEN>
<TOKEN id="token-32-11" pos="word" morph="none" start_char="3100" end_char="3102">del</TOKEN>
<TOKEN id="token-32-12" pos="word" morph="none" start_char="3104" end_char="3105">27</TOKEN>
<TOKEN id="token-32-13" pos="word" morph="none" start_char="3107" end_char="3108">de</TOKEN>
<TOKEN id="token-32-14" pos="word" morph="none" start_char="3110" end_char="3118">diciembre</TOKEN>
<TOKEN id="token-32-15" pos="punct" morph="none" start_char="3120" end_char="3120">(</TOKEN>
<TOKEN id="token-32-16" pos="word" morph="none" start_char="3121" end_char="3124">ergo</TOKEN>
<TOKEN id="token-32-17" pos="punct" morph="none" start_char="3126" end_char="3126">"</TOKEN>
<TOKEN id="token-32-18" pos="word" morph="none" start_char="3127" end_char="3135">infectado</TOKEN>
<TOKEN id="token-32-19" pos="punct" morph="none" start_char="3136" end_char="3136">"</TOKEN>
<TOKEN id="token-32-20" pos="word" morph="none" start_char="3138" end_char="3142">circa</TOKEN>
<TOKEN id="token-32-21" pos="word" morph="none" start_char="3144" end_char="3144">3</TOKEN>
<TOKEN id="token-32-22" pos="word" morph="none" start_char="3146" end_char="3152">semanas</TOKEN>
<TOKEN id="token-32-23" pos="word" morph="none" start_char="3154" end_char="3158">antes</TOKEN>
<TOKEN id="token-32-24" pos="punct" morph="none" start_char="3159" end_char="3159">)</TOKEN>
<TOKEN id="token-32-25" pos="word" morph="none" start_char="3161" end_char="3161">Y</TOKEN>
<TOKEN id="token-32-26" pos="word" morph="none" start_char="3163" end_char="3172">pronostico</TOKEN>
<TOKEN id="token-32-27" pos="word" morph="none" start_char="3174" end_char="3176">que</TOKEN>
<TOKEN id="token-32-28" pos="word" morph="none" start_char="3178" end_char="3179">si</TOKEN>
<TOKEN id="token-32-29" pos="word" morph="none" start_char="3181" end_char="3183">nos</TOKEN>
<TOKEN id="token-32-30" pos="word" morph="none" start_char="3185" end_char="3191">ponemos</TOKEN>
<TOKEN id="token-32-31" pos="word" morph="none" start_char="3193" end_char="3193">a</TOKEN>
<TOKEN id="token-32-32" pos="word" morph="none" start_char="3195" end_char="3202">estudiar</TOKEN>
<TOKEN id="token-32-33" pos="word" morph="none" start_char="3204" end_char="3211">muestras</TOKEN>
<TOKEN id="token-32-34" pos="word" morph="none" start_char="3213" end_char="3214">de</TOKEN>
<TOKEN id="token-32-35" pos="word" morph="none" start_char="3216" end_char="3219">hace</TOKEN>
<TOKEN id="token-32-36" pos="word" morph="none" start_char="3221" end_char="3222">10</TOKEN>
<TOKEN id="token-32-37" pos="word" morph="none" start_char="3224" end_char="3224">o</TOKEN>
<TOKEN id="token-32-38" pos="word" morph="none" start_char="3226" end_char="3227">20</TOKEN>
<TOKEN id="token-32-39" pos="word" morph="none" start_char="3229" end_char="3232">años</TOKEN>
<TOKEN id="token-32-40" pos="word" morph="none" start_char="3234" end_char="3240">también</TOKEN>
<TOKEN id="token-32-41" pos="word" morph="none" start_char="3242" end_char="3246">habrá</TOKEN>
<TOKEN id="token-32-42" pos="punct" morph="none" start_char="3248" end_char="3248">"</TOKEN>
<TOKEN id="token-32-43" pos="word" morph="none" start_char="3249" end_char="3255">covid19</TOKEN>
<TOKEN id="token-32-44" pos="punct" morph="none" start_char="3256" end_char="3257">".</TOKEN>
</SEG>
<SEG id="segment-33" start_char="3259" end_char="3484">
<ORIGINAL_TEXT>El test da un 7% de "+", testes la muestra que testes: Lectura crítica de pantallazos de prospectos y literatura sobre "tests de SARS Cov 2" ("Covid19")|Infórmese antes de consentir uno de estos tests Los tests son los padres.</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="word" morph="none" start_char="3259" end_char="3260">El</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="3262" end_char="3265">test</TOKEN>
<TOKEN id="token-33-2" pos="word" morph="none" start_char="3267" end_char="3268">da</TOKEN>
<TOKEN id="token-33-3" pos="word" morph="none" start_char="3270" end_char="3271">un</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="3273" end_char="3273">7</TOKEN>
<TOKEN id="token-33-5" pos="punct" morph="none" start_char="3274" end_char="3274">%</TOKEN>
<TOKEN id="token-33-6" pos="word" morph="none" start_char="3276" end_char="3277">de</TOKEN>
<TOKEN id="token-33-7" pos="unknown" morph="none" start_char="3279" end_char="3282">"+",</TOKEN>
<TOKEN id="token-33-8" pos="word" morph="none" start_char="3284" end_char="3289">testes</TOKEN>
<TOKEN id="token-33-9" pos="word" morph="none" start_char="3291" end_char="3292">la</TOKEN>
<TOKEN id="token-33-10" pos="word" morph="none" start_char="3294" end_char="3300">muestra</TOKEN>
<TOKEN id="token-33-11" pos="word" morph="none" start_char="3302" end_char="3304">que</TOKEN>
<TOKEN id="token-33-12" pos="word" morph="none" start_char="3306" end_char="3311">testes</TOKEN>
<TOKEN id="token-33-13" pos="punct" morph="none" start_char="3312" end_char="3312">:</TOKEN>
<TOKEN id="token-33-14" pos="word" morph="none" start_char="3314" end_char="3320">Lectura</TOKEN>
<TOKEN id="token-33-15" pos="word" morph="none" start_char="3322" end_char="3328">crítica</TOKEN>
<TOKEN id="token-33-16" pos="word" morph="none" start_char="3330" end_char="3331">de</TOKEN>
<TOKEN id="token-33-17" pos="word" morph="none" start_char="3333" end_char="3343">pantallazos</TOKEN>
<TOKEN id="token-33-18" pos="word" morph="none" start_char="3345" end_char="3346">de</TOKEN>
<TOKEN id="token-33-19" pos="word" morph="none" start_char="3348" end_char="3357">prospectos</TOKEN>
<TOKEN id="token-33-20" pos="word" morph="none" start_char="3359" end_char="3359">y</TOKEN>
<TOKEN id="token-33-21" pos="word" morph="none" start_char="3361" end_char="3370">literatura</TOKEN>
<TOKEN id="token-33-22" pos="word" morph="none" start_char="3372" end_char="3376">sobre</TOKEN>
<TOKEN id="token-33-23" pos="punct" morph="none" start_char="3378" end_char="3378">"</TOKEN>
<TOKEN id="token-33-24" pos="word" morph="none" start_char="3379" end_char="3383">tests</TOKEN>
<TOKEN id="token-33-25" pos="word" morph="none" start_char="3385" end_char="3386">de</TOKEN>
<TOKEN id="token-33-26" pos="word" morph="none" start_char="3388" end_char="3391">SARS</TOKEN>
<TOKEN id="token-33-27" pos="word" morph="none" start_char="3393" end_char="3395">Cov</TOKEN>
<TOKEN id="token-33-28" pos="word" morph="none" start_char="3397" end_char="3397">2</TOKEN>
<TOKEN id="token-33-29" pos="punct" morph="none" start_char="3398" end_char="3398">"</TOKEN>
<TOKEN id="token-33-30" pos="punct" morph="none" start_char="3400" end_char="3401">("</TOKEN>
<TOKEN id="token-33-31" pos="unknown" morph="none" start_char="3402" end_char="3420">Covid19")|Infórmese</TOKEN>
<TOKEN id="token-33-32" pos="word" morph="none" start_char="3422" end_char="3426">antes</TOKEN>
<TOKEN id="token-33-33" pos="word" morph="none" start_char="3428" end_char="3429">de</TOKEN>
<TOKEN id="token-33-34" pos="word" morph="none" start_char="3431" end_char="3439">consentir</TOKEN>
<TOKEN id="token-33-35" pos="word" morph="none" start_char="3441" end_char="3443">uno</TOKEN>
<TOKEN id="token-33-36" pos="word" morph="none" start_char="3445" end_char="3446">de</TOKEN>
<TOKEN id="token-33-37" pos="word" morph="none" start_char="3448" end_char="3452">estos</TOKEN>
<TOKEN id="token-33-38" pos="word" morph="none" start_char="3454" end_char="3458">tests</TOKEN>
<TOKEN id="token-33-39" pos="word" morph="none" start_char="3460" end_char="3462">Los</TOKEN>
<TOKEN id="token-33-40" pos="word" morph="none" start_char="3464" end_char="3468">tests</TOKEN>
<TOKEN id="token-33-41" pos="word" morph="none" start_char="3470" end_char="3472">son</TOKEN>
<TOKEN id="token-33-42" pos="word" morph="none" start_char="3474" end_char="3476">los</TOKEN>
<TOKEN id="token-33-43" pos="word" morph="none" start_char="3478" end_char="3483">padres</TOKEN>
<TOKEN id="token-33-44" pos="punct" morph="none" start_char="3484" end_char="3484">.</TOKEN>
</SEG>
<SEG id="segment-34" start_char="3486" end_char="3512">
<ORIGINAL_TEXT>Hacer clic para expandir...</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="3486" end_char="3490">Hacer</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="3492" end_char="3495">clic</TOKEN>
<TOKEN id="token-34-2" pos="word" morph="none" start_char="3497" end_char="3500">para</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="3502" end_char="3509">expandir</TOKEN>
<TOKEN id="token-34-4" pos="punct" morph="none" start_char="3510" end_char="3512">...</TOKEN>
</SEG>
<SEG id="segment-35" start_char="3515" end_char="3762">
<ORIGINAL_TEXT>Pues no te diría que no...visto lo visto, la familia de los coronavirus parece ser que ya era vieja conocida de nuestros virólogos occidentales...y que en torno a un 10-15% de las gripes de temporada son provocadas por un bichito de esta familia...</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="3515" end_char="3518">Pues</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="3520" end_char="3521">no</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="3523" end_char="3524">te</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="3526" end_char="3530">diría</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="3532" end_char="3534">que</TOKEN>
<TOKEN id="token-35-5" pos="unknown" morph="none" start_char="3536" end_char="3545">no...visto</TOKEN>
<TOKEN id="token-35-6" pos="word" morph="none" start_char="3547" end_char="3548">lo</TOKEN>
<TOKEN id="token-35-7" pos="word" morph="none" start_char="3550" end_char="3554">visto</TOKEN>
<TOKEN id="token-35-8" pos="punct" morph="none" start_char="3555" end_char="3555">,</TOKEN>
<TOKEN id="token-35-9" pos="word" morph="none" start_char="3557" end_char="3558">la</TOKEN>
<TOKEN id="token-35-10" pos="word" morph="none" start_char="3560" end_char="3566">familia</TOKEN>
<TOKEN id="token-35-11" pos="word" morph="none" start_char="3568" end_char="3569">de</TOKEN>
<TOKEN id="token-35-12" pos="word" morph="none" start_char="3571" end_char="3573">los</TOKEN>
<TOKEN id="token-35-13" pos="word" morph="none" start_char="3575" end_char="3585">coronavirus</TOKEN>
<TOKEN id="token-35-14" pos="word" morph="none" start_char="3587" end_char="3592">parece</TOKEN>
<TOKEN id="token-35-15" pos="word" morph="none" start_char="3594" end_char="3596">ser</TOKEN>
<TOKEN id="token-35-16" pos="word" morph="none" start_char="3598" end_char="3600">que</TOKEN>
<TOKEN id="token-35-17" pos="word" morph="none" start_char="3602" end_char="3603">ya</TOKEN>
<TOKEN id="token-35-18" pos="word" morph="none" start_char="3605" end_char="3607">era</TOKEN>
<TOKEN id="token-35-19" pos="word" morph="none" start_char="3609" end_char="3613">vieja</TOKEN>
<TOKEN id="token-35-20" pos="word" morph="none" start_char="3615" end_char="3622">conocida</TOKEN>
<TOKEN id="token-35-21" pos="word" morph="none" start_char="3624" end_char="3625">de</TOKEN>
<TOKEN id="token-35-22" pos="word" morph="none" start_char="3627" end_char="3634">nuestros</TOKEN>
<TOKEN id="token-35-23" pos="word" morph="none" start_char="3636" end_char="3644">virólogos</TOKEN>
<TOKEN id="token-35-24" pos="unknown" morph="none" start_char="3646" end_char="3661">occidentales...y</TOKEN>
<TOKEN id="token-35-25" pos="word" morph="none" start_char="3663" end_char="3665">que</TOKEN>
<TOKEN id="token-35-26" pos="word" morph="none" start_char="3667" end_char="3668">en</TOKEN>
<TOKEN id="token-35-27" pos="word" morph="none" start_char="3670" end_char="3674">torno</TOKEN>
<TOKEN id="token-35-28" pos="word" morph="none" start_char="3676" end_char="3676">a</TOKEN>
<TOKEN id="token-35-29" pos="word" morph="none" start_char="3678" end_char="3679">un</TOKEN>
<TOKEN id="token-35-30" pos="unknown" morph="none" start_char="3681" end_char="3685">10-15</TOKEN>
<TOKEN id="token-35-31" pos="punct" morph="none" start_char="3686" end_char="3686">%</TOKEN>
<TOKEN id="token-35-32" pos="word" morph="none" start_char="3688" end_char="3689">de</TOKEN>
<TOKEN id="token-35-33" pos="word" morph="none" start_char="3691" end_char="3693">las</TOKEN>
<TOKEN id="token-35-34" pos="word" morph="none" start_char="3695" end_char="3700">gripes</TOKEN>
<TOKEN id="token-35-35" pos="word" morph="none" start_char="3702" end_char="3703">de</TOKEN>
<TOKEN id="token-35-36" pos="word" morph="none" start_char="3705" end_char="3713">temporada</TOKEN>
<TOKEN id="token-35-37" pos="word" morph="none" start_char="3715" end_char="3717">son</TOKEN>
<TOKEN id="token-35-38" pos="word" morph="none" start_char="3719" end_char="3728">provocadas</TOKEN>
<TOKEN id="token-35-39" pos="word" morph="none" start_char="3730" end_char="3732">por</TOKEN>
<TOKEN id="token-35-40" pos="word" morph="none" start_char="3734" end_char="3735">un</TOKEN>
<TOKEN id="token-35-41" pos="word" morph="none" start_char="3737" end_char="3743">bichito</TOKEN>
<TOKEN id="token-35-42" pos="word" morph="none" start_char="3745" end_char="3746">de</TOKEN>
<TOKEN id="token-35-43" pos="word" morph="none" start_char="3748" end_char="3751">esta</TOKEN>
<TOKEN id="token-35-44" pos="word" morph="none" start_char="3753" end_char="3759">familia</TOKEN>
<TOKEN id="token-35-45" pos="punct" morph="none" start_char="3760" end_char="3762">...</TOKEN>
</SEG>
<SEG id="segment-36" start_char="3767" end_char="3888">
<ORIGINAL_TEXT>Venga ya si eso el virus llevaba circulando ya desde la primera maqueta de Metallica, a partir de ahi ya dejaron de molar.</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="3767" end_char="3771">Venga</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="3773" end_char="3774">ya</TOKEN>
<TOKEN id="token-36-2" pos="word" morph="none" start_char="3776" end_char="3777">si</TOKEN>
<TOKEN id="token-36-3" pos="word" morph="none" start_char="3779" end_char="3781">eso</TOKEN>
<TOKEN id="token-36-4" pos="word" morph="none" start_char="3783" end_char="3784">el</TOKEN>
<TOKEN id="token-36-5" pos="word" morph="none" start_char="3786" end_char="3790">virus</TOKEN>
<TOKEN id="token-36-6" pos="word" morph="none" start_char="3792" end_char="3798">llevaba</TOKEN>
<TOKEN id="token-36-7" pos="word" morph="none" start_char="3800" end_char="3809">circulando</TOKEN>
<TOKEN id="token-36-8" pos="word" morph="none" start_char="3811" end_char="3812">ya</TOKEN>
<TOKEN id="token-36-9" pos="word" morph="none" start_char="3814" end_char="3818">desde</TOKEN>
<TOKEN id="token-36-10" pos="word" morph="none" start_char="3820" end_char="3821">la</TOKEN>
<TOKEN id="token-36-11" pos="word" morph="none" start_char="3823" end_char="3829">primera</TOKEN>
<TOKEN id="token-36-12" pos="word" morph="none" start_char="3831" end_char="3837">maqueta</TOKEN>
<TOKEN id="token-36-13" pos="word" morph="none" start_char="3839" end_char="3840">de</TOKEN>
<TOKEN id="token-36-14" pos="word" morph="none" start_char="3842" end_char="3850">Metallica</TOKEN>
<TOKEN id="token-36-15" pos="punct" morph="none" start_char="3851" end_char="3851">,</TOKEN>
<TOKEN id="token-36-16" pos="word" morph="none" start_char="3853" end_char="3853">a</TOKEN>
<TOKEN id="token-36-17" pos="word" morph="none" start_char="3855" end_char="3860">partir</TOKEN>
<TOKEN id="token-36-18" pos="word" morph="none" start_char="3862" end_char="3863">de</TOKEN>
<TOKEN id="token-36-19" pos="word" morph="none" start_char="3865" end_char="3867">ahi</TOKEN>
<TOKEN id="token-36-20" pos="word" morph="none" start_char="3869" end_char="3870">ya</TOKEN>
<TOKEN id="token-36-21" pos="word" morph="none" start_char="3872" end_char="3878">dejaron</TOKEN>
<TOKEN id="token-36-22" pos="word" morph="none" start_char="3880" end_char="3881">de</TOKEN>
<TOKEN id="token-36-23" pos="word" morph="none" start_char="3883" end_char="3887">molar</TOKEN>
<TOKEN id="token-36-24" pos="punct" morph="none" start_char="3888" end_char="3888">.</TOKEN>
</SEG>
<SEG id="segment-37" start_char="3892" end_char="3989">
<ORIGINAL_TEXT>Cuanto antes mejor, significaría que es menos grave que si todo se hubiera desencadenado en marzo.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="3892" end_char="3897">Cuanto</TOKEN>
<TOKEN id="token-37-1" pos="word" morph="none" start_char="3899" end_char="3903">antes</TOKEN>
<TOKEN id="token-37-2" pos="word" morph="none" start_char="3905" end_char="3909">mejor</TOKEN>
<TOKEN id="token-37-3" pos="punct" morph="none" start_char="3910" end_char="3910">,</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="3912" end_char="3923">significaría</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="3925" end_char="3927">que</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="3929" end_char="3930">es</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="3932" end_char="3936">menos</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="3938" end_char="3942">grave</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="3944" end_char="3946">que</TOKEN>
<TOKEN id="token-37-10" pos="word" morph="none" start_char="3948" end_char="3949">si</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="3951" end_char="3954">todo</TOKEN>
<TOKEN id="token-37-12" pos="word" morph="none" start_char="3956" end_char="3957">se</TOKEN>
<TOKEN id="token-37-13" pos="word" morph="none" start_char="3959" end_char="3965">hubiera</TOKEN>
<TOKEN id="token-37-14" pos="word" morph="none" start_char="3967" end_char="3979">desencadenado</TOKEN>
<TOKEN id="token-37-15" pos="word" morph="none" start_char="3981" end_char="3982">en</TOKEN>
<TOKEN id="token-37-16" pos="word" morph="none" start_char="3984" end_char="3988">marzo</TOKEN>
<TOKEN id="token-37-17" pos="punct" morph="none" start_char="3989" end_char="3989">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="3993" end_char="4414">
<ORIGINAL_TEXT>Pajirri dijo: En diciembre me dio un chungo terrible, 2 semanas jodidos, 3 dias en cama, me costaba respirar y reir ya ni te cuento.. jodido de la garganta, tos, fiebre...y un dolor de cuerpo desde las uñas de los pies hasta la punta de los pelos, se me fue el gusto, vamos que nada tenia sabor... y como ya he comentado..soy una persona que se cuida.. lo achaque a una gripe estacional... pero NUNCA me dio de esa manera.</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="3993" end_char="3999">Pajirri</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="4001" end_char="4004">dijo</TOKEN>
<TOKEN id="token-38-2" pos="punct" morph="none" start_char="4005" end_char="4005">:</TOKEN>
<TOKEN id="token-38-3" pos="word" morph="none" start_char="4007" end_char="4008">En</TOKEN>
<TOKEN id="token-38-4" pos="word" morph="none" start_char="4010" end_char="4018">diciembre</TOKEN>
<TOKEN id="token-38-5" pos="word" morph="none" start_char="4020" end_char="4021">me</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="4023" end_char="4025">dio</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="4027" end_char="4028">un</TOKEN>
<TOKEN id="token-38-8" pos="word" morph="none" start_char="4030" end_char="4035">chungo</TOKEN>
<TOKEN id="token-38-9" pos="word" morph="none" start_char="4037" end_char="4044">terrible</TOKEN>
<TOKEN id="token-38-10" pos="punct" morph="none" start_char="4045" end_char="4045">,</TOKEN>
<TOKEN id="token-38-11" pos="word" morph="none" start_char="4047" end_char="4047">2</TOKEN>
<TOKEN id="token-38-12" pos="word" morph="none" start_char="4049" end_char="4055">semanas</TOKEN>
<TOKEN id="token-38-13" pos="word" morph="none" start_char="4057" end_char="4063">jodidos</TOKEN>
<TOKEN id="token-38-14" pos="punct" morph="none" start_char="4064" end_char="4064">,</TOKEN>
<TOKEN id="token-38-15" pos="word" morph="none" start_char="4066" end_char="4066">3</TOKEN>
<TOKEN id="token-38-16" pos="word" morph="none" start_char="4068" end_char="4071">dias</TOKEN>
<TOKEN id="token-38-17" pos="word" morph="none" start_char="4073" end_char="4074">en</TOKEN>
<TOKEN id="token-38-18" pos="word" morph="none" start_char="4076" end_char="4079">cama</TOKEN>
<TOKEN id="token-38-19" pos="punct" morph="none" start_char="4080" end_char="4080">,</TOKEN>
<TOKEN id="token-38-20" pos="word" morph="none" start_char="4082" end_char="4083">me</TOKEN>
<TOKEN id="token-38-21" pos="word" morph="none" start_char="4085" end_char="4091">costaba</TOKEN>
<TOKEN id="token-38-22" pos="word" morph="none" start_char="4093" end_char="4100">respirar</TOKEN>
<TOKEN id="token-38-23" pos="word" morph="none" start_char="4102" end_char="4102">y</TOKEN>
<TOKEN id="token-38-24" pos="word" morph="none" start_char="4104" end_char="4107">reir</TOKEN>
<TOKEN id="token-38-25" pos="word" morph="none" start_char="4109" end_char="4110">ya</TOKEN>
<TOKEN id="token-38-26" pos="word" morph="none" start_char="4112" end_char="4113">ni</TOKEN>
<TOKEN id="token-38-27" pos="word" morph="none" start_char="4115" end_char="4116">te</TOKEN>
<TOKEN id="token-38-28" pos="word" morph="none" start_char="4118" end_char="4123">cuento</TOKEN>
<TOKEN id="token-38-29" pos="punct" morph="none" start_char="4124" end_char="4125">..</TOKEN>
<TOKEN id="token-38-30" pos="word" morph="none" start_char="4127" end_char="4132">jodido</TOKEN>
<TOKEN id="token-38-31" pos="word" morph="none" start_char="4134" end_char="4135">de</TOKEN>
<TOKEN id="token-38-32" pos="word" morph="none" start_char="4137" end_char="4138">la</TOKEN>
<TOKEN id="token-38-33" pos="word" morph="none" start_char="4140" end_char="4147">garganta</TOKEN>
<TOKEN id="token-38-34" pos="punct" morph="none" start_char="4148" end_char="4148">,</TOKEN>
<TOKEN id="token-38-35" pos="word" morph="none" start_char="4150" end_char="4152">tos</TOKEN>
<TOKEN id="token-38-36" pos="punct" morph="none" start_char="4153" end_char="4153">,</TOKEN>
<TOKEN id="token-38-37" pos="unknown" morph="none" start_char="4155" end_char="4164">fiebre...y</TOKEN>
<TOKEN id="token-38-38" pos="word" morph="none" start_char="4166" end_char="4167">un</TOKEN>
<TOKEN id="token-38-39" pos="word" morph="none" start_char="4169" end_char="4173">dolor</TOKEN>
<TOKEN id="token-38-40" pos="word" morph="none" start_char="4175" end_char="4176">de</TOKEN>
<TOKEN id="token-38-41" pos="word" morph="none" start_char="4178" end_char="4183">cuerpo</TOKEN>
<TOKEN id="token-38-42" pos="word" morph="none" start_char="4185" end_char="4189">desde</TOKEN>
<TOKEN id="token-38-43" pos="word" morph="none" start_char="4191" end_char="4193">las</TOKEN>
<TOKEN id="token-38-44" pos="word" morph="none" start_char="4195" end_char="4198">uñas</TOKEN>
<TOKEN id="token-38-45" pos="word" morph="none" start_char="4200" end_char="4201">de</TOKEN>
<TOKEN id="token-38-46" pos="word" morph="none" start_char="4203" end_char="4205">los</TOKEN>
<TOKEN id="token-38-47" pos="word" morph="none" start_char="4207" end_char="4210">pies</TOKEN>
<TOKEN id="token-38-48" pos="word" morph="none" start_char="4212" end_char="4216">hasta</TOKEN>
<TOKEN id="token-38-49" pos="word" morph="none" start_char="4218" end_char="4219">la</TOKEN>
<TOKEN id="token-38-50" pos="word" morph="none" start_char="4221" end_char="4225">punta</TOKEN>
<TOKEN id="token-38-51" pos="word" morph="none" start_char="4227" end_char="4228">de</TOKEN>
<TOKEN id="token-38-52" pos="word" morph="none" start_char="4230" end_char="4232">los</TOKEN>
<TOKEN id="token-38-53" pos="word" morph="none" start_char="4234" end_char="4238">pelos</TOKEN>
<TOKEN id="token-38-54" pos="punct" morph="none" start_char="4239" end_char="4239">,</TOKEN>
<TOKEN id="token-38-55" pos="word" morph="none" start_char="4241" end_char="4242">se</TOKEN>
<TOKEN id="token-38-56" pos="word" morph="none" start_char="4244" end_char="4245">me</TOKEN>
<TOKEN id="token-38-57" pos="word" morph="none" start_char="4247" end_char="4249">fue</TOKEN>
<TOKEN id="token-38-58" pos="word" morph="none" start_char="4251" end_char="4252">el</TOKEN>
<TOKEN id="token-38-59" pos="word" morph="none" start_char="4254" end_char="4258">gusto</TOKEN>
<TOKEN id="token-38-60" pos="punct" morph="none" start_char="4259" end_char="4259">,</TOKEN>
<TOKEN id="token-38-61" pos="word" morph="none" start_char="4261" end_char="4265">vamos</TOKEN>
<TOKEN id="token-38-62" pos="word" morph="none" start_char="4267" end_char="4269">que</TOKEN>
<TOKEN id="token-38-63" pos="word" morph="none" start_char="4271" end_char="4274">nada</TOKEN>
<TOKEN id="token-38-64" pos="word" morph="none" start_char="4276" end_char="4280">tenia</TOKEN>
<TOKEN id="token-38-65" pos="word" morph="none" start_char="4282" end_char="4286">sabor</TOKEN>
<TOKEN id="token-38-66" pos="punct" morph="none" start_char="4287" end_char="4289">...</TOKEN>
<TOKEN id="token-38-67" pos="word" morph="none" start_char="4291" end_char="4291">y</TOKEN>
<TOKEN id="token-38-68" pos="word" morph="none" start_char="4293" end_char="4296">como</TOKEN>
<TOKEN id="token-38-69" pos="word" morph="none" start_char="4298" end_char="4299">ya</TOKEN>
<TOKEN id="token-38-70" pos="word" morph="none" start_char="4301" end_char="4302">he</TOKEN>
<TOKEN id="token-38-71" pos="unknown" morph="none" start_char="4304" end_char="4317">comentado..soy</TOKEN>
<TOKEN id="token-38-72" pos="word" morph="none" start_char="4319" end_char="4321">una</TOKEN>
<TOKEN id="token-38-73" pos="word" morph="none" start_char="4323" end_char="4329">persona</TOKEN>
<TOKEN id="token-38-74" pos="word" morph="none" start_char="4331" end_char="4333">que</TOKEN>
<TOKEN id="token-38-75" pos="word" morph="none" start_char="4335" end_char="4336">se</TOKEN>
<TOKEN id="token-38-76" pos="word" morph="none" start_char="4338" end_char="4342">cuida</TOKEN>
<TOKEN id="token-38-77" pos="punct" morph="none" start_char="4343" end_char="4344">..</TOKEN>
<TOKEN id="token-38-78" pos="word" morph="none" start_char="4346" end_char="4347">lo</TOKEN>
<TOKEN id="token-38-79" pos="word" morph="none" start_char="4349" end_char="4355">achaque</TOKEN>
<TOKEN id="token-38-80" pos="word" morph="none" start_char="4357" end_char="4357">a</TOKEN>
<TOKEN id="token-38-81" pos="word" morph="none" start_char="4359" end_char="4361">una</TOKEN>
<TOKEN id="token-38-82" pos="word" morph="none" start_char="4363" end_char="4367">gripe</TOKEN>
<TOKEN id="token-38-83" pos="word" morph="none" start_char="4369" end_char="4378">estacional</TOKEN>
<TOKEN id="token-38-84" pos="punct" morph="none" start_char="4379" end_char="4381">...</TOKEN>
<TOKEN id="token-38-85" pos="word" morph="none" start_char="4383" end_char="4386">pero</TOKEN>
<TOKEN id="token-38-86" pos="word" morph="none" start_char="4388" end_char="4392">NUNCA</TOKEN>
<TOKEN id="token-38-87" pos="word" morph="none" start_char="4394" end_char="4395">me</TOKEN>
<TOKEN id="token-38-88" pos="word" morph="none" start_char="4397" end_char="4399">dio</TOKEN>
<TOKEN id="token-38-89" pos="word" morph="none" start_char="4401" end_char="4402">de</TOKEN>
<TOKEN id="token-38-90" pos="word" morph="none" start_char="4404" end_char="4406">esa</TOKEN>
<TOKEN id="token-38-91" pos="word" morph="none" start_char="4408" end_char="4413">manera</TOKEN>
<TOKEN id="token-38-92" pos="punct" morph="none" start_char="4414" end_char="4414">.</TOKEN>
</SEG>
<SEG id="segment-39" start_char="4418" end_char="4449">
<ORIGINAL_TEXT>Muchos ya hemos sufrido el covid</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="word" morph="none" start_char="4418" end_char="4423">Muchos</TOKEN>
<TOKEN id="token-39-1" pos="word" morph="none" start_char="4425" end_char="4426">ya</TOKEN>
<TOKEN id="token-39-2" pos="word" morph="none" start_char="4428" end_char="4432">hemos</TOKEN>
<TOKEN id="token-39-3" pos="word" morph="none" start_char="4434" end_char="4440">sufrido</TOKEN>
<TOKEN id="token-39-4" pos="word" morph="none" start_char="4442" end_char="4443">el</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="4445" end_char="4449">covid</TOKEN>
</SEG>
<SEG id="segment-40" start_char="4453" end_char="4502">
<ORIGINAL_TEXT>Navarrete dijo: Un estudio para no culpar a China.</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="word" morph="none" start_char="4453" end_char="4461">Navarrete</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="4463" end_char="4466">dijo</TOKEN>
<TOKEN id="token-40-2" pos="punct" morph="none" start_char="4467" end_char="4467">:</TOKEN>
<TOKEN id="token-40-3" pos="word" morph="none" start_char="4469" end_char="4470">Un</TOKEN>
<TOKEN id="token-40-4" pos="word" morph="none" start_char="4472" end_char="4478">estudio</TOKEN>
<TOKEN id="token-40-5" pos="word" morph="none" start_char="4480" end_char="4483">para</TOKEN>
<TOKEN id="token-40-6" pos="word" morph="none" start_char="4485" end_char="4486">no</TOKEN>
<TOKEN id="token-40-7" pos="word" morph="none" start_char="4488" end_char="4493">culpar</TOKEN>
<TOKEN id="token-40-8" pos="word" morph="none" start_char="4495" end_char="4495">a</TOKEN>
<TOKEN id="token-40-9" pos="word" morph="none" start_char="4497" end_char="4501">China</TOKEN>
<TOKEN id="token-40-10" pos="punct" morph="none" start_char="4502" end_char="4502">.</TOKEN>
</SEG>
<SEG id="segment-41" start_char="4506" end_char="4531">
<ORIGINAL_TEXT>Los lejías sois así jajaja</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="4506" end_char="4508">Los</TOKEN>
<TOKEN id="token-41-1" pos="word" morph="none" start_char="4510" end_char="4515">lejías</TOKEN>
<TOKEN id="token-41-2" pos="word" morph="none" start_char="4517" end_char="4520">sois</TOKEN>
<TOKEN id="token-41-3" pos="word" morph="none" start_char="4522" end_char="4524">así</TOKEN>
<TOKEN id="token-41-4" pos="word" morph="none" start_char="4526" end_char="4531">jajaja</TOKEN>
</SEG>
<SEG id="segment-42" start_char="4536" end_char="4592">
<ORIGINAL_TEXT>AYN RANDiano2 dijo: Y en Italia, y en Francia... Francia.</ORIGINAL_TEXT>
<TOKEN id="token-42-0" pos="word" morph="none" start_char="4536" end_char="4538">AYN</TOKEN>
<TOKEN id="token-42-1" pos="word" morph="none" start_char="4540" end_char="4548">RANDiano2</TOKEN>
<TOKEN id="token-42-2" pos="word" morph="none" start_char="4550" end_char="4553">dijo</TOKEN>
<TOKEN id="token-42-3" pos="punct" morph="none" start_char="4554" end_char="4554">:</TOKEN>
<TOKEN id="token-42-4" pos="word" morph="none" start_char="4556" end_char="4556">Y</TOKEN>
<TOKEN id="token-42-5" pos="word" morph="none" start_char="4558" end_char="4559">en</TOKEN>
<TOKEN id="token-42-6" pos="word" morph="none" start_char="4561" end_char="4566">Italia</TOKEN>
<TOKEN id="token-42-7" pos="punct" morph="none" start_char="4567" end_char="4567">,</TOKEN>
<TOKEN id="token-42-8" pos="word" morph="none" start_char="4569" end_char="4569">y</TOKEN>
<TOKEN id="token-42-9" pos="word" morph="none" start_char="4571" end_char="4572">en</TOKEN>
<TOKEN id="token-42-10" pos="word" morph="none" start_char="4574" end_char="4580">Francia</TOKEN>
<TOKEN id="token-42-11" pos="punct" morph="none" start_char="4581" end_char="4583">...</TOKEN>
<TOKEN id="token-42-12" pos="word" morph="none" start_char="4585" end_char="4591">Francia</TOKEN>
<TOKEN id="token-42-13" pos="punct" morph="none" start_char="4592" end_char="4592">.</TOKEN>
</SEG>
<SEG id="segment-43" start_char="4594" end_char="4808">
<ORIGINAL_TEXT>Detectan "+ a covid19" en muestra de enfermo de neumonía del 27 de diciembre (ergo "infectado" circa 3 semanas antes) Y pronostico que si nos ponemos a estudiar muestras de hace 10 o 20 años también habrá "covid19".</ORIGINAL_TEXT>
<TOKEN id="token-43-0" pos="word" morph="none" start_char="4594" end_char="4601">Detectan</TOKEN>
<TOKEN id="token-43-1" pos="unknown" morph="none" start_char="4603" end_char="4604">"+</TOKEN>
<TOKEN id="token-43-2" pos="word" morph="none" start_char="4606" end_char="4606">a</TOKEN>
<TOKEN id="token-43-3" pos="word" morph="none" start_char="4608" end_char="4614">covid19</TOKEN>
<TOKEN id="token-43-4" pos="punct" morph="none" start_char="4615" end_char="4615">"</TOKEN>
<TOKEN id="token-43-5" pos="word" morph="none" start_char="4617" end_char="4618">en</TOKEN>
<TOKEN id="token-43-6" pos="word" morph="none" start_char="4620" end_char="4626">muestra</TOKEN>
<TOKEN id="token-43-7" pos="word" morph="none" start_char="4628" end_char="4629">de</TOKEN>
<TOKEN id="token-43-8" pos="word" morph="none" start_char="4631" end_char="4637">enfermo</TOKEN>
<TOKEN id="token-43-9" pos="word" morph="none" start_char="4639" end_char="4640">de</TOKEN>
<TOKEN id="token-43-10" pos="word" morph="none" start_char="4642" end_char="4649">neumonía</TOKEN>
<TOKEN id="token-43-11" pos="word" morph="none" start_char="4651" end_char="4653">del</TOKEN>
<TOKEN id="token-43-12" pos="word" morph="none" start_char="4655" end_char="4656">27</TOKEN>
<TOKEN id="token-43-13" pos="word" morph="none" start_char="4658" end_char="4659">de</TOKEN>
<TOKEN id="token-43-14" pos="word" morph="none" start_char="4661" end_char="4669">diciembre</TOKEN>
<TOKEN id="token-43-15" pos="punct" morph="none" start_char="4671" end_char="4671">(</TOKEN>
<TOKEN id="token-43-16" pos="word" morph="none" start_char="4672" end_char="4675">ergo</TOKEN>
<TOKEN id="token-43-17" pos="punct" morph="none" start_char="4677" end_char="4677">"</TOKEN>
<TOKEN id="token-43-18" pos="word" morph="none" start_char="4678" end_char="4686">infectado</TOKEN>
<TOKEN id="token-43-19" pos="punct" morph="none" start_char="4687" end_char="4687">"</TOKEN>
<TOKEN id="token-43-20" pos="word" morph="none" start_char="4689" end_char="4693">circa</TOKEN>
<TOKEN id="token-43-21" pos="word" morph="none" start_char="4695" end_char="4695">3</TOKEN>
<TOKEN id="token-43-22" pos="word" morph="none" start_char="4697" end_char="4703">semanas</TOKEN>
<TOKEN id="token-43-23" pos="word" morph="none" start_char="4705" end_char="4709">antes</TOKEN>
<TOKEN id="token-43-24" pos="punct" morph="none" start_char="4710" end_char="4710">)</TOKEN>
<TOKEN id="token-43-25" pos="word" morph="none" start_char="4712" end_char="4712">Y</TOKEN>
<TOKEN id="token-43-26" pos="word" morph="none" start_char="4714" end_char="4723">pronostico</TOKEN>
<TOKEN id="token-43-27" pos="word" morph="none" start_char="4725" end_char="4727">que</TOKEN>
<TOKEN id="token-43-28" pos="word" morph="none" start_char="4729" end_char="4730">si</TOKEN>
<TOKEN id="token-43-29" pos="word" morph="none" start_char="4732" end_char="4734">nos</TOKEN>
<TOKEN id="token-43-30" pos="word" morph="none" start_char="4736" end_char="4742">ponemos</TOKEN>
<TOKEN id="token-43-31" pos="word" morph="none" start_char="4744" end_char="4744">a</TOKEN>
<TOKEN id="token-43-32" pos="word" morph="none" start_char="4746" end_char="4753">estudiar</TOKEN>
<TOKEN id="token-43-33" pos="word" morph="none" start_char="4755" end_char="4762">muestras</TOKEN>
<TOKEN id="token-43-34" pos="word" morph="none" start_char="4764" end_char="4765">de</TOKEN>
<TOKEN id="token-43-35" pos="word" morph="none" start_char="4767" end_char="4770">hace</TOKEN>
<TOKEN id="token-43-36" pos="word" morph="none" start_char="4772" end_char="4773">10</TOKEN>
<TOKEN id="token-43-37" pos="word" morph="none" start_char="4775" end_char="4775">o</TOKEN>
<TOKEN id="token-43-38" pos="word" morph="none" start_char="4777" end_char="4778">20</TOKEN>
<TOKEN id="token-43-39" pos="word" morph="none" start_char="4780" end_char="4783">años</TOKEN>
<TOKEN id="token-43-40" pos="word" morph="none" start_char="4785" end_char="4791">también</TOKEN>
<TOKEN id="token-43-41" pos="word" morph="none" start_char="4793" end_char="4797">habrá</TOKEN>
<TOKEN id="token-43-42" pos="punct" morph="none" start_char="4799" end_char="4799">"</TOKEN>
<TOKEN id="token-43-43" pos="word" morph="none" start_char="4800" end_char="4806">covid19</TOKEN>
<TOKEN id="token-43-44" pos="punct" morph="none" start_char="4807" end_char="4808">".</TOKEN>
</SEG>
<SEG id="segment-44" start_char="4810" end_char="5035">
<ORIGINAL_TEXT>El test da un 7% de "+", testes la muestra que testes: Lectura crítica de pantallazos de prospectos y literatura sobre "tests de SARS Cov 2" ("Covid19")|Infórmese antes de consentir uno de estos tests Los tests son los padres.</ORIGINAL_TEXT>
<TOKEN id="token-44-0" pos="word" morph="none" start_char="4810" end_char="4811">El</TOKEN>
<TOKEN id="token-44-1" pos="word" morph="none" start_char="4813" end_char="4816">test</TOKEN>
<TOKEN id="token-44-2" pos="word" morph="none" start_char="4818" end_char="4819">da</TOKEN>
<TOKEN id="token-44-3" pos="word" morph="none" start_char="4821" end_char="4822">un</TOKEN>
<TOKEN id="token-44-4" pos="word" morph="none" start_char="4824" end_char="4824">7</TOKEN>
<TOKEN id="token-44-5" pos="punct" morph="none" start_char="4825" end_char="4825">%</TOKEN>
<TOKEN id="token-44-6" pos="word" morph="none" start_char="4827" end_char="4828">de</TOKEN>
<TOKEN id="token-44-7" pos="unknown" morph="none" start_char="4830" end_char="4833">"+",</TOKEN>
<TOKEN id="token-44-8" pos="word" morph="none" start_char="4835" end_char="4840">testes</TOKEN>
<TOKEN id="token-44-9" pos="word" morph="none" start_char="4842" end_char="4843">la</TOKEN>
<TOKEN id="token-44-10" pos="word" morph="none" start_char="4845" end_char="4851">muestra</TOKEN>
<TOKEN id="token-44-11" pos="word" morph="none" start_char="4853" end_char="4855">que</TOKEN>
<TOKEN id="token-44-12" pos="word" morph="none" start_char="4857" end_char="4862">testes</TOKEN>
<TOKEN id="token-44-13" pos="punct" morph="none" start_char="4863" end_char="4863">:</TOKEN>
<TOKEN id="token-44-14" pos="word" morph="none" start_char="4865" end_char="4871">Lectura</TOKEN>
<TOKEN id="token-44-15" pos="word" morph="none" start_char="4873" end_char="4879">crítica</TOKEN>
<TOKEN id="token-44-16" pos="word" morph="none" start_char="4881" end_char="4882">de</TOKEN>
<TOKEN id="token-44-17" pos="word" morph="none" start_char="4884" end_char="4894">pantallazos</TOKEN>
<TOKEN id="token-44-18" pos="word" morph="none" start_char="4896" end_char="4897">de</TOKEN>
<TOKEN id="token-44-19" pos="word" morph="none" start_char="4899" end_char="4908">prospectos</TOKEN>
<TOKEN id="token-44-20" pos="word" morph="none" start_char="4910" end_char="4910">y</TOKEN>
<TOKEN id="token-44-21" pos="word" morph="none" start_char="4912" end_char="4921">literatura</TOKEN>
<TOKEN id="token-44-22" pos="word" morph="none" start_char="4923" end_char="4927">sobre</TOKEN>
<TOKEN id="token-44-23" pos="punct" morph="none" start_char="4929" end_char="4929">"</TOKEN>
<TOKEN id="token-44-24" pos="word" morph="none" start_char="4930" end_char="4934">tests</TOKEN>
<TOKEN id="token-44-25" pos="word" morph="none" start_char="4936" end_char="4937">de</TOKEN>
<TOKEN id="token-44-26" pos="word" morph="none" start_char="4939" end_char="4942">SARS</TOKEN>
<TOKEN id="token-44-27" pos="word" morph="none" start_char="4944" end_char="4946">Cov</TOKEN>
<TOKEN id="token-44-28" pos="word" morph="none" start_char="4948" end_char="4948">2</TOKEN>
<TOKEN id="token-44-29" pos="punct" morph="none" start_char="4949" end_char="4949">"</TOKEN>
<TOKEN id="token-44-30" pos="punct" morph="none" start_char="4951" end_char="4952">("</TOKEN>
<TOKEN id="token-44-31" pos="unknown" morph="none" start_char="4953" end_char="4971">Covid19")|Infórmese</TOKEN>
<TOKEN id="token-44-32" pos="word" morph="none" start_char="4973" end_char="4977">antes</TOKEN>
<TOKEN id="token-44-33" pos="word" morph="none" start_char="4979" end_char="4980">de</TOKEN>
<TOKEN id="token-44-34" pos="word" morph="none" start_char="4982" end_char="4990">consentir</TOKEN>
<TOKEN id="token-44-35" pos="word" morph="none" start_char="4992" end_char="4994">uno</TOKEN>
<TOKEN id="token-44-36" pos="word" morph="none" start_char="4996" end_char="4997">de</TOKEN>
<TOKEN id="token-44-37" pos="word" morph="none" start_char="4999" end_char="5003">estos</TOKEN>
<TOKEN id="token-44-38" pos="word" morph="none" start_char="5005" end_char="5009">tests</TOKEN>
<TOKEN id="token-44-39" pos="word" morph="none" start_char="5011" end_char="5013">Los</TOKEN>
<TOKEN id="token-44-40" pos="word" morph="none" start_char="5015" end_char="5019">tests</TOKEN>
<TOKEN id="token-44-41" pos="word" morph="none" start_char="5021" end_char="5023">son</TOKEN>
<TOKEN id="token-44-42" pos="word" morph="none" start_char="5025" end_char="5027">los</TOKEN>
<TOKEN id="token-44-43" pos="word" morph="none" start_char="5029" end_char="5034">padres</TOKEN>
<TOKEN id="token-44-44" pos="punct" morph="none" start_char="5035" end_char="5035">.</TOKEN>
</SEG>
<SEG id="segment-45" start_char="5037" end_char="5063">
<ORIGINAL_TEXT>Hacer clic para expandir...</ORIGINAL_TEXT>
<TOKEN id="token-45-0" pos="word" morph="none" start_char="5037" end_char="5041">Hacer</TOKEN>
<TOKEN id="token-45-1" pos="word" morph="none" start_char="5043" end_char="5046">clic</TOKEN>
<TOKEN id="token-45-2" pos="word" morph="none" start_char="5048" end_char="5051">para</TOKEN>
<TOKEN id="token-45-3" pos="word" morph="none" start_char="5053" end_char="5060">expandir</TOKEN>
<TOKEN id="token-45-4" pos="punct" morph="none" start_char="5061" end_char="5063">...</TOKEN>
</SEG>
<SEG id="segment-46" start_char="5066" end_char="5084">
<ORIGINAL_TEXT>Minuto 1 del vídeo.</ORIGINAL_TEXT>
<TOKEN id="token-46-0" pos="word" morph="none" start_char="5066" end_char="5071">Minuto</TOKEN>
<TOKEN id="token-46-1" pos="word" morph="none" start_char="5073" end_char="5073">1</TOKEN>
<TOKEN id="token-46-2" pos="word" morph="none" start_char="5075" end_char="5077">del</TOKEN>
<TOKEN id="token-46-3" pos="word" morph="none" start_char="5079" end_char="5083">vídeo</TOKEN>
<TOKEN id="token-46-4" pos="punct" morph="none" start_char="5084" end_char="5084">.</TOKEN>
</SEG>
<SEG id="segment-47" start_char="5086" end_char="5137">
<ORIGINAL_TEXT>Sangre congelada del 2017 da positivo al mierdatest.</ORIGINAL_TEXT>
<TOKEN id="token-47-0" pos="word" morph="none" start_char="5086" end_char="5091">Sangre</TOKEN>
<TOKEN id="token-47-1" pos="word" morph="none" start_char="5093" end_char="5101">congelada</TOKEN>
<TOKEN id="token-47-2" pos="word" morph="none" start_char="5103" end_char="5105">del</TOKEN>
<TOKEN id="token-47-3" pos="word" morph="none" start_char="5107" end_char="5110">2017</TOKEN>
<TOKEN id="token-47-4" pos="word" morph="none" start_char="5112" end_char="5113">da</TOKEN>
<TOKEN id="token-47-5" pos="word" morph="none" start_char="5115" end_char="5122">positivo</TOKEN>
<TOKEN id="token-47-6" pos="word" morph="none" start_char="5124" end_char="5125">al</TOKEN>
<TOKEN id="token-47-7" pos="word" morph="none" start_char="5127" end_char="5136">mierdatest</TOKEN>
<TOKEN id="token-47-8" pos="punct" morph="none" start_char="5137" end_char="5137">.</TOKEN>
</SEG>
<SEG id="segment-48" start_char="5142" end_char="5177">
<ORIGINAL_TEXT>pues tal cómo funcionan los tests...</ORIGINAL_TEXT>
<TOKEN id="token-48-0" pos="word" morph="none" start_char="5142" end_char="5145">pues</TOKEN>
<TOKEN id="token-48-1" pos="word" morph="none" start_char="5147" end_char="5149">tal</TOKEN>
<TOKEN id="token-48-2" pos="word" morph="none" start_char="5151" end_char="5154">cómo</TOKEN>
<TOKEN id="token-48-3" pos="word" morph="none" start_char="5156" end_char="5164">funcionan</TOKEN>
<TOKEN id="token-48-4" pos="word" morph="none" start_char="5166" end_char="5168">los</TOKEN>
<TOKEN id="token-48-5" pos="word" morph="none" start_char="5170" end_char="5174">tests</TOKEN>
<TOKEN id="token-48-6" pos="punct" morph="none" start_char="5175" end_char="5177">...</TOKEN>
</SEG>
<SEG id="segment-49" start_char="5180" end_char="5202">
<ORIGINAL_TEXT>PA FIARSE DE LA VACUNA!</ORIGINAL_TEXT>
<TOKEN id="token-49-0" pos="word" morph="none" start_char="5180" end_char="5181">PA</TOKEN>
<TOKEN id="token-49-1" pos="word" morph="none" start_char="5183" end_char="5188">FIARSE</TOKEN>
<TOKEN id="token-49-2" pos="word" morph="none" start_char="5190" end_char="5191">DE</TOKEN>
<TOKEN id="token-49-3" pos="word" morph="none" start_char="5193" end_char="5194">LA</TOKEN>
<TOKEN id="token-49-4" pos="word" morph="none" start_char="5196" end_char="5201">VACUNA</TOKEN>
<TOKEN id="token-49-5" pos="punct" morph="none" start_char="5202" end_char="5202">!</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
