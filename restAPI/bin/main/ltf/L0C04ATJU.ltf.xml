<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATJU" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="188" raw_text_md5="1656454f41217a6e92bad552fb4db98e">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="55">
<ORIGINAL_TEXT>China reports further food-related coronavirus findings</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="5">China</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="7" end_char="13">reports</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="15" end_char="21">further</TOKEN>
<TOKEN id="token-0-3" pos="unknown" morph="none" start_char="23" end_char="34">food-related</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="36" end_char="46">coronavirus</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="48" end_char="55">findings</TOKEN>
</SEG>
<SEG id="segment-1" start_char="59" end_char="110">
<ORIGINAL_TEXT>As if nothing China exports isn't full of the virus.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="59" end_char="60">As</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="62" end_char="63">if</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="65" end_char="71">nothing</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="73" end_char="77">China</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="79" end_char="85">exports</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="87" end_char="91">isn't</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="93" end_char="96">full</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="98" end_char="99">of</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="101" end_char="103">the</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="105" end_char="109">virus</TOKEN>
<TOKEN id="token-1-10" pos="punct" morph="none" start_char="110" end_char="110">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="112" end_char="172">
<ORIGINAL_TEXT>They created this virus so let them live with it like we are!</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="112" end_char="115">They</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="117" end_char="123">created</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="125" end_char="128">this</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="130" end_char="134">virus</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="136" end_char="137">so</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="139" end_char="141">let</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="143" end_char="146">them</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="148" end_char="151">live</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="153" end_char="156">with</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="158" end_char="159">it</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="161" end_char="164">like</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="166" end_char="167">we</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="169" end_char="171">are</TOKEN>
<TOKEN id="token-2-13" pos="punct" morph="none" start_char="172" end_char="172">!</TOKEN>
</SEG>
<SEG id="segment-3" start_char="176" end_char="184">
<ORIGINAL_TEXT>I agree !</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="176" end_char="176">I</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="178" end_char="182">agree</TOKEN>
<TOKEN id="token-3-2" pos="punct" morph="none" start_char="184" end_char="184">!</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
