<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATKS" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="1341" raw_text_md5="cd78567a4c5b584990c71dd38a1b64ac">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="44">
<ORIGINAL_TEXT>¿Es necesario llevar dos mascarillas juntas?</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="punct" morph="none" start_char="1" end_char="1">¿</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="2" end_char="3">Es</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="5" end_char="13">necesario</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="15" end_char="20">llevar</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="22" end_char="24">dos</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="26" end_char="36">mascarillas</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="38" end_char="43">juntas</TOKEN>
<TOKEN id="token-0-7" pos="punct" morph="none" start_char="44" end_char="44">?</TOKEN>
</SEG>
<SEG id="segment-1" start_char="48" end_char="329">
<ORIGINAL_TEXT>Un aporte muy interesante, siempre he tenido dudas de la eficacia de usar una mascarilla quirúrgica sobre una FPP2… A día de hoy me sorprende que haya tanta gente que use mascarillas de tela caseras con distintos estampados para ir a la moda o «conjuntados» con el resto de la ropa.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="48" end_char="49">Un</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="51" end_char="56">aporte</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="58" end_char="60">muy</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="62" end_char="72">interesante</TOKEN>
<TOKEN id="token-1-4" pos="punct" morph="none" start_char="73" end_char="73">,</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="75" end_char="81">siempre</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="83" end_char="84">he</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="86" end_char="91">tenido</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="93" end_char="97">dudas</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="99" end_char="100">de</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="102" end_char="103">la</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="105" end_char="112">eficacia</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="114" end_char="115">de</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="117" end_char="120">usar</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="122" end_char="124">una</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="126" end_char="135">mascarilla</TOKEN>
<TOKEN id="token-1-16" pos="word" morph="none" start_char="137" end_char="146">quirúrgica</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="148" end_char="152">sobre</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="154" end_char="156">una</TOKEN>
<TOKEN id="token-1-19" pos="word" morph="none" start_char="158" end_char="161">FPP2</TOKEN>
<TOKEN id="token-1-20" pos="punct" morph="none" start_char="162" end_char="162">…</TOKEN>
<TOKEN id="token-1-21" pos="word" morph="none" start_char="164" end_char="164">A</TOKEN>
<TOKEN id="token-1-22" pos="word" morph="none" start_char="166" end_char="168">día</TOKEN>
<TOKEN id="token-1-23" pos="word" morph="none" start_char="170" end_char="171">de</TOKEN>
<TOKEN id="token-1-24" pos="word" morph="none" start_char="173" end_char="175">hoy</TOKEN>
<TOKEN id="token-1-25" pos="word" morph="none" start_char="177" end_char="178">me</TOKEN>
<TOKEN id="token-1-26" pos="word" morph="none" start_char="180" end_char="188">sorprende</TOKEN>
<TOKEN id="token-1-27" pos="word" morph="none" start_char="190" end_char="192">que</TOKEN>
<TOKEN id="token-1-28" pos="word" morph="none" start_char="194" end_char="197">haya</TOKEN>
<TOKEN id="token-1-29" pos="word" morph="none" start_char="199" end_char="203">tanta</TOKEN>
<TOKEN id="token-1-30" pos="word" morph="none" start_char="205" end_char="209">gente</TOKEN>
<TOKEN id="token-1-31" pos="word" morph="none" start_char="211" end_char="213">que</TOKEN>
<TOKEN id="token-1-32" pos="word" morph="none" start_char="215" end_char="217">use</TOKEN>
<TOKEN id="token-1-33" pos="word" morph="none" start_char="219" end_char="229">mascarillas</TOKEN>
<TOKEN id="token-1-34" pos="word" morph="none" start_char="231" end_char="232">de</TOKEN>
<TOKEN id="token-1-35" pos="word" morph="none" start_char="234" end_char="237">tela</TOKEN>
<TOKEN id="token-1-36" pos="word" morph="none" start_char="239" end_char="245">caseras</TOKEN>
<TOKEN id="token-1-37" pos="word" morph="none" start_char="247" end_char="249">con</TOKEN>
<TOKEN id="token-1-38" pos="word" morph="none" start_char="251" end_char="259">distintos</TOKEN>
<TOKEN id="token-1-39" pos="word" morph="none" start_char="261" end_char="270">estampados</TOKEN>
<TOKEN id="token-1-40" pos="word" morph="none" start_char="272" end_char="275">para</TOKEN>
<TOKEN id="token-1-41" pos="word" morph="none" start_char="277" end_char="278">ir</TOKEN>
<TOKEN id="token-1-42" pos="word" morph="none" start_char="280" end_char="280">a</TOKEN>
<TOKEN id="token-1-43" pos="word" morph="none" start_char="282" end_char="283">la</TOKEN>
<TOKEN id="token-1-44" pos="word" morph="none" start_char="285" end_char="288">moda</TOKEN>
<TOKEN id="token-1-45" pos="word" morph="none" start_char="290" end_char="290">o</TOKEN>
<TOKEN id="token-1-46" pos="punct" morph="none" start_char="292" end_char="292">«</TOKEN>
<TOKEN id="token-1-47" pos="word" morph="none" start_char="293" end_char="303">conjuntados</TOKEN>
<TOKEN id="token-1-48" pos="punct" morph="none" start_char="304" end_char="304">»</TOKEN>
<TOKEN id="token-1-49" pos="word" morph="none" start_char="306" end_char="308">con</TOKEN>
<TOKEN id="token-1-50" pos="word" morph="none" start_char="310" end_char="311">el</TOKEN>
<TOKEN id="token-1-51" pos="word" morph="none" start_char="313" end_char="317">resto</TOKEN>
<TOKEN id="token-1-52" pos="word" morph="none" start_char="319" end_char="320">de</TOKEN>
<TOKEN id="token-1-53" pos="word" morph="none" start_char="322" end_char="323">la</TOKEN>
<TOKEN id="token-1-54" pos="word" morph="none" start_char="325" end_char="328">ropa</TOKEN>
<TOKEN id="token-1-55" pos="punct" morph="none" start_char="329" end_char="329">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="331" end_char="369">
<ORIGINAL_TEXT>Gracias por compartir esta información!</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="331" end_char="337">Gracias</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="339" end_char="341">por</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="343" end_char="351">compartir</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="353" end_char="356">esta</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="358" end_char="368">información</TOKEN>
<TOKEN id="token-2-5" pos="punct" morph="none" start_char="369" end_char="369">!</TOKEN>
</SEG>
<SEG id="segment-3" start_char="371" end_char="378">
<ORIGINAL_TEXT>Saludos!</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="371" end_char="377">Saludos</TOKEN>
<TOKEN id="token-3-1" pos="punct" morph="none" start_char="378" end_char="378">!</TOKEN>
</SEG>
<SEG id="segment-4" start_char="382" end_char="410">
<ORIGINAL_TEXT>Y por qué lo azul para fuera?</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="382" end_char="382">Y</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="384" end_char="386">por</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="388" end_char="390">qué</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="392" end_char="393">lo</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="395" end_char="398">azul</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="400" end_char="403">para</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="405" end_char="409">fuera</TOKEN>
<TOKEN id="token-4-7" pos="punct" morph="none" start_char="410" end_char="410">?</TOKEN>
</SEG>
<SEG id="segment-5" start_char="412" end_char="459">
<ORIGINAL_TEXT>Por mera imposición o por alguna razón concreta.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="412" end_char="414">Por</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="416" end_char="419">mera</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="421" end_char="430">imposición</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="432" end_char="432">o</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="434" end_char="436">por</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="438" end_char="443">alguna</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="445" end_char="449">razón</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="451" end_char="458">concreta</TOKEN>
<TOKEN id="token-5-8" pos="punct" morph="none" start_char="459" end_char="459">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="461" end_char="609">
<ORIGINAL_TEXT>A mí especialmente estudiar teniendo que dar clase con la parte blanca por dentro hacia desgranar se la mascarilla, no obstante, al revés no ocurría.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="461" end_char="461">A</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="463" end_char="464">mí</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="466" end_char="478">especialmente</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="480" end_char="487">estudiar</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="489" end_char="496">teniendo</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="498" end_char="500">que</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="502" end_char="504">dar</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="506" end_char="510">clase</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="512" end_char="514">con</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="516" end_char="517">la</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="519" end_char="523">parte</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="525" end_char="530">blanca</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="532" end_char="534">por</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="536" end_char="541">dentro</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="543" end_char="547">hacia</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="549" end_char="557">desgranar</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="559" end_char="560">se</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="562" end_char="563">la</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="565" end_char="574">mascarilla</TOKEN>
<TOKEN id="token-6-19" pos="punct" morph="none" start_char="575" end_char="575">,</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="577" end_char="578">no</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="580" end_char="587">obstante</TOKEN>
<TOKEN id="token-6-22" pos="punct" morph="none" start_char="588" end_char="588">,</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="590" end_char="591">al</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="593" end_char="597">revés</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="599" end_char="600">no</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="602" end_char="608">ocurría</TOKEN>
<TOKEN id="token-6-27" pos="punct" morph="none" start_char="609" end_char="609">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="613" end_char="827">
<ORIGINAL_TEXT>De acuerdo con todo y añadiría que, para sanitarios, cuando no tengan ffp2 de las buenas, de las de toda la vida, no las orejeras, y con el fin de preservar las ffp2 , pero además el ajuste mejora considerablemente.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="613" end_char="614">De</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="616" end_char="622">acuerdo</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="624" end_char="626">con</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="628" end_char="631">todo</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="633" end_char="633">y</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="635" end_char="642">añadiría</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="644" end_char="646">que</TOKEN>
<TOKEN id="token-7-7" pos="punct" morph="none" start_char="647" end_char="647">,</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="649" end_char="652">para</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="654" end_char="663">sanitarios</TOKEN>
<TOKEN id="token-7-10" pos="punct" morph="none" start_char="664" end_char="664">,</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="666" end_char="671">cuando</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="673" end_char="674">no</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="676" end_char="681">tengan</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="683" end_char="686">ffp2</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="688" end_char="689">de</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="691" end_char="693">las</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="695" end_char="700">buenas</TOKEN>
<TOKEN id="token-7-18" pos="punct" morph="none" start_char="701" end_char="701">,</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="703" end_char="704">de</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="706" end_char="708">las</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="710" end_char="711">de</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="713" end_char="716">toda</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="718" end_char="719">la</TOKEN>
<TOKEN id="token-7-24" pos="word" morph="none" start_char="721" end_char="724">vida</TOKEN>
<TOKEN id="token-7-25" pos="punct" morph="none" start_char="725" end_char="725">,</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="727" end_char="728">no</TOKEN>
<TOKEN id="token-7-27" pos="word" morph="none" start_char="730" end_char="732">las</TOKEN>
<TOKEN id="token-7-28" pos="word" morph="none" start_char="734" end_char="741">orejeras</TOKEN>
<TOKEN id="token-7-29" pos="punct" morph="none" start_char="742" end_char="742">,</TOKEN>
<TOKEN id="token-7-30" pos="word" morph="none" start_char="744" end_char="744">y</TOKEN>
<TOKEN id="token-7-31" pos="word" morph="none" start_char="746" end_char="748">con</TOKEN>
<TOKEN id="token-7-32" pos="word" morph="none" start_char="750" end_char="751">el</TOKEN>
<TOKEN id="token-7-33" pos="word" morph="none" start_char="753" end_char="755">fin</TOKEN>
<TOKEN id="token-7-34" pos="word" morph="none" start_char="757" end_char="758">de</TOKEN>
<TOKEN id="token-7-35" pos="word" morph="none" start_char="760" end_char="768">preservar</TOKEN>
<TOKEN id="token-7-36" pos="word" morph="none" start_char="770" end_char="772">las</TOKEN>
<TOKEN id="token-7-37" pos="word" morph="none" start_char="774" end_char="777">ffp2</TOKEN>
<TOKEN id="token-7-38" pos="punct" morph="none" start_char="779" end_char="779">,</TOKEN>
<TOKEN id="token-7-39" pos="word" morph="none" start_char="781" end_char="784">pero</TOKEN>
<TOKEN id="token-7-40" pos="word" morph="none" start_char="786" end_char="791">además</TOKEN>
<TOKEN id="token-7-41" pos="word" morph="none" start_char="793" end_char="794">el</TOKEN>
<TOKEN id="token-7-42" pos="word" morph="none" start_char="796" end_char="801">ajuste</TOKEN>
<TOKEN id="token-7-43" pos="word" morph="none" start_char="803" end_char="808">mejora</TOKEN>
<TOKEN id="token-7-44" pos="word" morph="none" start_char="810" end_char="826">considerablemente</TOKEN>
<TOKEN id="token-7-45" pos="punct" morph="none" start_char="827" end_char="827">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="829" end_char="915">
<ORIGINAL_TEXT>En este estudio lo pudimos confirmar cuantitativamente con un test de ajuste, fit test.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="829" end_char="830">En</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="832" end_char="835">este</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="837" end_char="843">estudio</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="845" end_char="846">lo</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="848" end_char="854">pudimos</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="856" end_char="864">confirmar</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="866" end_char="882">cuantitativamente</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="884" end_char="886">con</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="888" end_char="889">un</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="891" end_char="894">test</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="896" end_char="897">de</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="899" end_char="904">ajuste</TOKEN>
<TOKEN id="token-8-12" pos="punct" morph="none" start_char="905" end_char="905">,</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="907" end_char="909">fit</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="911" end_char="914">test</TOKEN>
<TOKEN id="token-8-15" pos="punct" morph="none" start_char="915" end_char="915">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="917" end_char="1172">
<ORIGINAL_TEXT>Un articulo de este estudio se publicará próximamente en revista especializada http://www.amat.es/noticias/asepeyo_combina_la_mascarilla_autoflitrante_ffp2_con_una_mascarilla_quirurgica_que_multiplica_por_100_el_factor_de_proteccion_frente_al_covid_19.3php</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="917" end_char="918">Un</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="920" end_char="927">articulo</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="929" end_char="930">de</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="932" end_char="935">este</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="937" end_char="943">estudio</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="945" end_char="946">se</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="948" end_char="956">publicará</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="958" end_char="969">próximamente</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="971" end_char="972">en</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="974" end_char="980">revista</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="982" end_char="994">especializada</TOKEN>
<TOKEN id="token-9-11" pos="url" morph="none" start_char="996" end_char="1172">http://www.amat.es/noticias/asepeyo_combina_la_mascarilla_autoflitrante_ffp2_con_una_mascarilla_quirurgica_que_multiplica_por_100_el_factor_de_proteccion_frente_al_covid_19.3php</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1176" end_char="1337">
<ORIGINAL_TEXT>¿Puede ser eficaz, llevar una mascarilla quirúrgica, debajo de una de tela, que a lo mejor no esté homologada o se haya lavado muchas veces?, ¡¡ muchas gracias !!</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="punct" morph="none" start_char="1176" end_char="1176">¿</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1177" end_char="1181">Puede</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1183" end_char="1185">ser</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1187" end_char="1192">eficaz</TOKEN>
<TOKEN id="token-10-4" pos="punct" morph="none" start_char="1193" end_char="1193">,</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1195" end_char="1200">llevar</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1202" end_char="1204">una</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1206" end_char="1215">mascarilla</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1217" end_char="1226">quirúrgica</TOKEN>
<TOKEN id="token-10-9" pos="punct" morph="none" start_char="1227" end_char="1227">,</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1229" end_char="1234">debajo</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1236" end_char="1237">de</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1239" end_char="1241">una</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1243" end_char="1244">de</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1246" end_char="1249">tela</TOKEN>
<TOKEN id="token-10-15" pos="punct" morph="none" start_char="1250" end_char="1250">,</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1252" end_char="1254">que</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1256" end_char="1256">a</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1258" end_char="1259">lo</TOKEN>
<TOKEN id="token-10-19" pos="word" morph="none" start_char="1261" end_char="1265">mejor</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="1267" end_char="1268">no</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="1270" end_char="1273">esté</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="1275" end_char="1284">homologada</TOKEN>
<TOKEN id="token-10-23" pos="word" morph="none" start_char="1286" end_char="1286">o</TOKEN>
<TOKEN id="token-10-24" pos="word" morph="none" start_char="1288" end_char="1289">se</TOKEN>
<TOKEN id="token-10-25" pos="word" morph="none" start_char="1291" end_char="1294">haya</TOKEN>
<TOKEN id="token-10-26" pos="word" morph="none" start_char="1296" end_char="1301">lavado</TOKEN>
<TOKEN id="token-10-27" pos="word" morph="none" start_char="1303" end_char="1308">muchas</TOKEN>
<TOKEN id="token-10-28" pos="word" morph="none" start_char="1310" end_char="1314">veces</TOKEN>
<TOKEN id="token-10-29" pos="punct" morph="none" start_char="1315" end_char="1316">?,</TOKEN>
<TOKEN id="token-10-30" pos="punct" morph="none" start_char="1318" end_char="1319">¡¡</TOKEN>
<TOKEN id="token-10-31" pos="word" morph="none" start_char="1321" end_char="1326">muchas</TOKEN>
<TOKEN id="token-10-32" pos="word" morph="none" start_char="1328" end_char="1334">gracias</TOKEN>
<TOKEN id="token-10-33" pos="punct" morph="none" start_char="1336" end_char="1337">!!</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
