<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CABF" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="2915" raw_text_md5="d83a4af873ec9e1add6aefdd51973a37">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="46">
<ORIGINAL_TEXT>When was the first case of Covid in the world?</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="4">When</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="6" end_char="8">was</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="10" end_char="12">the</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="14" end_char="18">first</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="20" end_char="23">case</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="25" end_char="26">of</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="28" end_char="32">Covid</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="34" end_char="35">in</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="37" end_char="39">the</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="41" end_char="45">world</TOKEN>
<TOKEN id="token-0-10" pos="punct" morph="none" start_char="46" end_char="46">?</TOKEN>
</SEG>
<SEG id="segment-1" start_char="51" end_char="127">
<ORIGINAL_TEXT>Coronavirus may have been around longer than we think (Picture: Getty Images)</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="51" end_char="61">Coronavirus</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="63" end_char="65">may</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="67" end_char="70">have</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="72" end_char="75">been</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="77" end_char="82">around</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="84" end_char="89">longer</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="91" end_char="94">than</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="96" end_char="97">we</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="99" end_char="103">think</TOKEN>
<TOKEN id="token-1-9" pos="punct" morph="none" start_char="105" end_char="105">(</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="106" end_char="112">Picture</TOKEN>
<TOKEN id="token-1-11" pos="punct" morph="none" start_char="113" end_char="113">:</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="115" end_char="119">Getty</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="121" end_char="126">Images</TOKEN>
<TOKEN id="token-1-14" pos="punct" morph="none" start_char="127" end_char="127">)</TOKEN>
</SEG>
<SEG id="segment-2" start_char="131" end_char="277">
<ORIGINAL_TEXT>Coronavirus has become such a big part of life in 2020 it’s hard to imagine that just 12 months ago nobody had any idea what Covid-19 actually was.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="131" end_char="141">Coronavirus</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="143" end_char="145">has</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="147" end_char="152">become</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="154" end_char="157">such</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="159" end_char="159">a</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="161" end_char="163">big</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="165" end_char="168">part</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="170" end_char="171">of</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="173" end_char="176">life</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="178" end_char="179">in</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="181" end_char="184">2020</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="186" end_char="189">it’s</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="191" end_char="194">hard</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="196" end_char="197">to</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="199" end_char="205">imagine</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="207" end_char="210">that</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="212" end_char="215">just</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="217" end_char="218">12</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="220" end_char="225">months</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="227" end_char="229">ago</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="231" end_char="236">nobody</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="238" end_char="240">had</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="242" end_char="244">any</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="246" end_char="249">idea</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="251" end_char="254">what</TOKEN>
<TOKEN id="token-2-25" pos="unknown" morph="none" start_char="256" end_char="263">Covid-19</TOKEN>
<TOKEN id="token-2-26" pos="word" morph="none" start_char="265" end_char="272">actually</TOKEN>
<TOKEN id="token-2-27" pos="word" morph="none" start_char="274" end_char="276">was</TOKEN>
<TOKEN id="token-2-28" pos="punct" morph="none" start_char="277" end_char="277">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="280" end_char="419">
<ORIGINAL_TEXT>And yet it’s already been a year since what is now thought to be the first documented case – coming a month earlier than originally thought.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="280" end_char="282">And</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="284" end_char="286">yet</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="288" end_char="291">it’s</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="293" end_char="299">already</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="301" end_char="304">been</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="306" end_char="306">a</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="308" end_char="311">year</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="313" end_char="317">since</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="319" end_char="322">what</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="324" end_char="325">is</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="327" end_char="329">now</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="331" end_char="337">thought</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="339" end_char="340">to</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="342" end_char="343">be</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="345" end_char="347">the</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="349" end_char="353">first</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="355" end_char="364">documented</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="366" end_char="369">case</TOKEN>
<TOKEN id="token-3-18" pos="punct" morph="none" start_char="371" end_char="371">–</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="373" end_char="378">coming</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="380" end_char="380">a</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="382" end_char="386">month</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="388" end_char="394">earlier</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="396" end_char="399">than</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="401" end_char="410">originally</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="412" end_char="418">thought</TOKEN>
<TOKEN id="token-3-26" pos="punct" morph="none" start_char="419" end_char="419">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="422" end_char="564">
<ORIGINAL_TEXT>So just when and where was the first case of Covid-19 in the world thought to have been – and how long was it before cases first came to light?</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="422" end_char="423">So</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="425" end_char="428">just</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="430" end_char="433">when</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="435" end_char="437">and</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="439" end_char="443">where</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="445" end_char="447">was</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="449" end_char="451">the</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="453" end_char="457">first</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="459" end_char="462">case</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="464" end_char="465">of</TOKEN>
<TOKEN id="token-4-10" pos="unknown" morph="none" start_char="467" end_char="474">Covid-19</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="476" end_char="477">in</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="479" end_char="481">the</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="483" end_char="487">world</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="489" end_char="495">thought</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="497" end_char="498">to</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="500" end_char="503">have</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="505" end_char="508">been</TOKEN>
<TOKEN id="token-4-18" pos="punct" morph="none" start_char="510" end_char="510">–</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="512" end_char="514">and</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="516" end_char="518">how</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="520" end_char="523">long</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="525" end_char="527">was</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="529" end_char="530">it</TOKEN>
<TOKEN id="token-4-24" pos="word" morph="none" start_char="532" end_char="537">before</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="539" end_char="543">cases</TOKEN>
<TOKEN id="token-4-26" pos="word" morph="none" start_char="545" end_char="549">first</TOKEN>
<TOKEN id="token-4-27" pos="word" morph="none" start_char="551" end_char="554">came</TOKEN>
<TOKEN id="token-4-28" pos="word" morph="none" start_char="556" end_char="557">to</TOKEN>
<TOKEN id="token-4-29" pos="word" morph="none" start_char="559" end_char="563">light</TOKEN>
<TOKEN id="token-4-30" pos="punct" morph="none" start_char="564" end_char="564">?</TOKEN>
</SEG>
<SEG id="segment-5" start_char="567" end_char="595">
<ORIGINAL_TEXT>Here’s what you need to know…</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="567" end_char="572">Here’s</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="574" end_char="577">what</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="579" end_char="581">you</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="583" end_char="586">need</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="588" end_char="589">to</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="591" end_char="594">know</TOKEN>
<TOKEN id="token-5-6" pos="punct" morph="none" start_char="595" end_char="595">…</TOKEN>
</SEG>
<SEG id="segment-6" start_char="599" end_char="644">
<ORIGINAL_TEXT>When was the first case of Covid in the world?</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="599" end_char="602">When</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="604" end_char="606">was</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="608" end_char="610">the</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="612" end_char="616">first</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="618" end_char="621">case</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="623" end_char="624">of</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="626" end_char="630">Covid</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="632" end_char="633">in</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="635" end_char="637">the</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="639" end_char="643">world</TOKEN>
<TOKEN id="token-6-10" pos="punct" morph="none" start_char="644" end_char="644">?</TOKEN>
</SEG>
<SEG id="segment-7" start_char="648" end_char="748">
<ORIGINAL_TEXT>It’s been reported that the first known case of Covid in the world was detected on November 17, 2019.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="648" end_char="651">It’s</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="653" end_char="656">been</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="658" end_char="665">reported</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="667" end_char="670">that</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="672" end_char="674">the</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="676" end_char="680">first</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="682" end_char="686">known</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="688" end_char="691">case</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="693" end_char="694">of</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="696" end_char="700">Covid</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="702" end_char="703">in</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="705" end_char="707">the</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="709" end_char="713">world</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="715" end_char="717">was</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="719" end_char="726">detected</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="728" end_char="729">on</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="731" end_char="738">November</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="740" end_char="741">17</TOKEN>
<TOKEN id="token-7-18" pos="punct" morph="none" start_char="742" end_char="742">,</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="744" end_char="747">2019</TOKEN>
<TOKEN id="token-7-20" pos="punct" morph="none" start_char="748" end_char="748">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="751" end_char="924">
<ORIGINAL_TEXT>According to the report from the South China Morning Post, a 55-year-old from Hubei Province in China may have been the first person to contract the potentially lethal virus.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="751" end_char="759">According</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="761" end_char="762">to</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="764" end_char="766">the</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="768" end_char="773">report</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="775" end_char="778">from</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="780" end_char="782">the</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="784" end_char="788">South</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="790" end_char="794">China</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="796" end_char="802">Morning</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="804" end_char="807">Post</TOKEN>
<TOKEN id="token-8-10" pos="punct" morph="none" start_char="808" end_char="808">,</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="810" end_char="810">a</TOKEN>
<TOKEN id="token-8-12" pos="unknown" morph="none" start_char="812" end_char="822">55-year-old</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="824" end_char="827">from</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="829" end_char="833">Hubei</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="835" end_char="842">Province</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="844" end_char="845">in</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="847" end_char="851">China</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="853" end_char="855">may</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="857" end_char="860">have</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="862" end_char="865">been</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="867" end_char="869">the</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="871" end_char="875">first</TOKEN>
<TOKEN id="token-8-23" pos="word" morph="none" start_char="877" end_char="882">person</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="884" end_char="885">to</TOKEN>
<TOKEN id="token-8-25" pos="word" morph="none" start_char="887" end_char="894">contract</TOKEN>
<TOKEN id="token-8-26" pos="word" morph="none" start_char="896" end_char="898">the</TOKEN>
<TOKEN id="token-8-27" pos="word" morph="none" start_char="900" end_char="910">potentially</TOKEN>
<TOKEN id="token-8-28" pos="word" morph="none" start_char="912" end_char="917">lethal</TOKEN>
<TOKEN id="token-8-29" pos="word" morph="none" start_char="919" end_char="923">virus</TOKEN>
<TOKEN id="token-8-30" pos="punct" morph="none" start_char="924" end_char="924">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="927" end_char="1013">
<ORIGINAL_TEXT>However it was a month before reports began emerging of cases being confirmed in Wuhan.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="927" end_char="933">However</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="935" end_char="936">it</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="938" end_char="940">was</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="942" end_char="942">a</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="944" end_char="948">month</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="950" end_char="955">before</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="957" end_char="963">reports</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="965" end_char="969">began</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="971" end_char="978">emerging</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="980" end_char="981">of</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="983" end_char="987">cases</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="989" end_char="993">being</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="995" end_char="1003">confirmed</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1005" end_char="1006">in</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1008" end_char="1012">Wuhan</TOKEN>
<TOKEN id="token-9-15" pos="punct" morph="none" start_char="1013" end_char="1013">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1017" end_char="1126">
<ORIGINAL_TEXT>To view this video please enable JavaScript, and consider upgrading to a web browser that supports HTML5 video</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1017" end_char="1018">To</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1020" end_char="1023">view</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1025" end_char="1028">this</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1030" end_char="1034">video</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1036" end_char="1041">please</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1043" end_char="1048">enable</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1050" end_char="1059">JavaScript</TOKEN>
<TOKEN id="token-10-7" pos="punct" morph="none" start_char="1060" end_char="1060">,</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1062" end_char="1064">and</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1066" end_char="1073">consider</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1075" end_char="1083">upgrading</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1085" end_char="1086">to</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1088" end_char="1088">a</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1090" end_char="1092">web</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1094" end_char="1100">browser</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1102" end_char="1105">that</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1107" end_char="1114">supports</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1116" end_char="1120">HTML5</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1122" end_char="1126">video</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1130" end_char="1370">
<ORIGINAL_TEXT>A report back in March stated that between November 17 and the start of December last year, around one to five new cases were reported every day – with 27 having been reported by December 15 – and the number reaching 60 just five days later.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1130" end_char="1130">A</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1132" end_char="1137">report</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1139" end_char="1142">back</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1144" end_char="1145">in</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1147" end_char="1151">March</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1153" end_char="1158">stated</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1160" end_char="1163">that</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1165" end_char="1171">between</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1173" end_char="1180">November</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1182" end_char="1183">17</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1185" end_char="1187">and</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1189" end_char="1191">the</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1193" end_char="1197">start</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1199" end_char="1200">of</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1202" end_char="1209">December</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1211" end_char="1214">last</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1216" end_char="1219">year</TOKEN>
<TOKEN id="token-11-17" pos="punct" morph="none" start_char="1220" end_char="1220">,</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1222" end_char="1227">around</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1229" end_char="1231">one</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1233" end_char="1234">to</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1236" end_char="1239">five</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1241" end_char="1243">new</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1245" end_char="1249">cases</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1251" end_char="1254">were</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1256" end_char="1263">reported</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="1265" end_char="1269">every</TOKEN>
<TOKEN id="token-11-27" pos="word" morph="none" start_char="1271" end_char="1273">day</TOKEN>
<TOKEN id="token-11-28" pos="punct" morph="none" start_char="1275" end_char="1275">–</TOKEN>
<TOKEN id="token-11-29" pos="word" morph="none" start_char="1277" end_char="1280">with</TOKEN>
<TOKEN id="token-11-30" pos="word" morph="none" start_char="1282" end_char="1283">27</TOKEN>
<TOKEN id="token-11-31" pos="word" morph="none" start_char="1285" end_char="1290">having</TOKEN>
<TOKEN id="token-11-32" pos="word" morph="none" start_char="1292" end_char="1295">been</TOKEN>
<TOKEN id="token-11-33" pos="word" morph="none" start_char="1297" end_char="1304">reported</TOKEN>
<TOKEN id="token-11-34" pos="word" morph="none" start_char="1306" end_char="1307">by</TOKEN>
<TOKEN id="token-11-35" pos="word" morph="none" start_char="1309" end_char="1316">December</TOKEN>
<TOKEN id="token-11-36" pos="word" morph="none" start_char="1318" end_char="1319">15</TOKEN>
<TOKEN id="token-11-37" pos="punct" morph="none" start_char="1321" end_char="1321">–</TOKEN>
<TOKEN id="token-11-38" pos="word" morph="none" start_char="1323" end_char="1325">and</TOKEN>
<TOKEN id="token-11-39" pos="word" morph="none" start_char="1327" end_char="1329">the</TOKEN>
<TOKEN id="token-11-40" pos="word" morph="none" start_char="1331" end_char="1336">number</TOKEN>
<TOKEN id="token-11-41" pos="word" morph="none" start_char="1338" end_char="1345">reaching</TOKEN>
<TOKEN id="token-11-42" pos="word" morph="none" start_char="1347" end_char="1348">60</TOKEN>
<TOKEN id="token-11-43" pos="word" morph="none" start_char="1350" end_char="1353">just</TOKEN>
<TOKEN id="token-11-44" pos="word" morph="none" start_char="1355" end_char="1358">five</TOKEN>
<TOKEN id="token-11-45" pos="word" morph="none" start_char="1360" end_char="1363">days</TOKEN>
<TOKEN id="token-11-46" pos="word" morph="none" start_char="1365" end_char="1369">later</TOKEN>
<TOKEN id="token-11-47" pos="punct" morph="none" start_char="1370" end_char="1370">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1373" end_char="1550">
<ORIGINAL_TEXT>However, it’s now been suggested that Covid-19 could have been around a lot earlier than that, amid reports of the virus possibly circulating in Italy as early as last September.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1373" end_char="1379">However</TOKEN>
<TOKEN id="token-12-1" pos="punct" morph="none" start_char="1380" end_char="1380">,</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1382" end_char="1385">it’s</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1387" end_char="1389">now</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1391" end_char="1394">been</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1396" end_char="1404">suggested</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1406" end_char="1409">that</TOKEN>
<TOKEN id="token-12-7" pos="unknown" morph="none" start_char="1411" end_char="1418">Covid-19</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1420" end_char="1424">could</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1426" end_char="1429">have</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1431" end_char="1434">been</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1436" end_char="1441">around</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1443" end_char="1443">a</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1445" end_char="1447">lot</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1449" end_char="1455">earlier</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1457" end_char="1460">than</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1462" end_char="1465">that</TOKEN>
<TOKEN id="token-12-17" pos="punct" morph="none" start_char="1466" end_char="1466">,</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="1468" end_char="1471">amid</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="1473" end_char="1479">reports</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1481" end_char="1482">of</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1484" end_char="1486">the</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1488" end_char="1492">virus</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1494" end_char="1501">possibly</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1503" end_char="1513">circulating</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1515" end_char="1516">in</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="1518" end_char="1522">Italy</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="1524" end_char="1525">as</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="1527" end_char="1531">early</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="1533" end_char="1534">as</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="1536" end_char="1539">last</TOKEN>
<TOKEN id="token-12-31" pos="word" morph="none" start_char="1541" end_char="1549">September</TOKEN>
<TOKEN id="token-12-32" pos="punct" morph="none" start_char="1550" end_char="1550">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1553" end_char="1768">
<ORIGINAL_TEXT>Scientists there have said they have evidence of Covid being around at that time after checking the blood samples of patients taking part in a cancer survey – and finding that some contained antibodies for the virus.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1553" end_char="1562">Scientists</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1564" end_char="1568">there</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1570" end_char="1573">have</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1575" end_char="1578">said</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1580" end_char="1583">they</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1585" end_char="1588">have</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1590" end_char="1597">evidence</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1599" end_char="1600">of</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1602" end_char="1606">Covid</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1608" end_char="1612">being</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1614" end_char="1619">around</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1621" end_char="1622">at</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1624" end_char="1627">that</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="1629" end_char="1632">time</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="1634" end_char="1638">after</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="1640" end_char="1647">checking</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="1649" end_char="1651">the</TOKEN>
<TOKEN id="token-13-17" pos="word" morph="none" start_char="1653" end_char="1657">blood</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="1659" end_char="1665">samples</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="1667" end_char="1668">of</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="1670" end_char="1677">patients</TOKEN>
<TOKEN id="token-13-21" pos="word" morph="none" start_char="1679" end_char="1684">taking</TOKEN>
<TOKEN id="token-13-22" pos="word" morph="none" start_char="1686" end_char="1689">part</TOKEN>
<TOKEN id="token-13-23" pos="word" morph="none" start_char="1691" end_char="1692">in</TOKEN>
<TOKEN id="token-13-24" pos="word" morph="none" start_char="1694" end_char="1694">a</TOKEN>
<TOKEN id="token-13-25" pos="word" morph="none" start_char="1696" end_char="1701">cancer</TOKEN>
<TOKEN id="token-13-26" pos="word" morph="none" start_char="1703" end_char="1708">survey</TOKEN>
<TOKEN id="token-13-27" pos="punct" morph="none" start_char="1710" end_char="1710">–</TOKEN>
<TOKEN id="token-13-28" pos="word" morph="none" start_char="1712" end_char="1714">and</TOKEN>
<TOKEN id="token-13-29" pos="word" morph="none" start_char="1716" end_char="1722">finding</TOKEN>
<TOKEN id="token-13-30" pos="word" morph="none" start_char="1724" end_char="1727">that</TOKEN>
<TOKEN id="token-13-31" pos="word" morph="none" start_char="1729" end_char="1732">some</TOKEN>
<TOKEN id="token-13-32" pos="word" morph="none" start_char="1734" end_char="1742">contained</TOKEN>
<TOKEN id="token-13-33" pos="word" morph="none" start_char="1744" end_char="1753">antibodies</TOKEN>
<TOKEN id="token-13-34" pos="word" morph="none" start_char="1755" end_char="1757">for</TOKEN>
<TOKEN id="token-13-35" pos="word" morph="none" start_char="1759" end_char="1761">the</TOKEN>
<TOKEN id="token-13-36" pos="word" morph="none" start_char="1763" end_char="1767">virus</TOKEN>
<TOKEN id="token-13-37" pos="punct" morph="none" start_char="1768" end_char="1768">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1771" end_char="1817">
<ORIGINAL_TEXT>When was the UK’s first recorded case of Covid?</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1771" end_char="1774">When</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1776" end_char="1778">was</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1780" end_char="1782">the</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1784" end_char="1787">UK’s</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1789" end_char="1793">first</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1795" end_char="1802">recorded</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1804" end_char="1807">case</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1809" end_char="1810">of</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1812" end_char="1816">Covid</TOKEN>
<TOKEN id="token-14-9" pos="punct" morph="none" start_char="1817" end_char="1817">?</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1821" end_char="1963">
<ORIGINAL_TEXT>The UK recorded its first confirmed cases of coronavirus on January 29, when two Chinese nationals fell ill at the StayCity Aparthotel in York.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1821" end_char="1823">The</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1825" end_char="1826">UK</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1828" end_char="1835">recorded</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="1837" end_char="1839">its</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1841" end_char="1845">first</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1847" end_char="1855">confirmed</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="1857" end_char="1861">cases</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1863" end_char="1864">of</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1866" end_char="1876">coronavirus</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="1878" end_char="1879">on</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1881" end_char="1887">January</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="1889" end_char="1890">29</TOKEN>
<TOKEN id="token-15-12" pos="punct" morph="none" start_char="1891" end_char="1891">,</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="1893" end_char="1896">when</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="1898" end_char="1900">two</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="1902" end_char="1908">Chinese</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="1910" end_char="1918">nationals</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="1920" end_char="1923">fell</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="1925" end_char="1927">ill</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="1929" end_char="1930">at</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="1932" end_char="1934">the</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="1936" end_char="1943">StayCity</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="1945" end_char="1954">Aparthotel</TOKEN>
<TOKEN id="token-15-23" pos="word" morph="none" start_char="1956" end_char="1957">in</TOKEN>
<TOKEN id="token-15-24" pos="word" morph="none" start_char="1959" end_char="1962">York</TOKEN>
<TOKEN id="token-15-25" pos="punct" morph="none" start_char="1963" end_char="1963">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1967" end_char="2042">
<ORIGINAL_TEXT>The virus has impacted every aspect of life globally (Picture: Getty Images)</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1967" end_char="1969">The</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1971" end_char="1975">virus</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1977" end_char="1979">has</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1981" end_char="1988">impacted</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1990" end_char="1994">every</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1996" end_char="2001">aspect</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="2003" end_char="2004">of</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="2006" end_char="2009">life</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="2011" end_char="2018">globally</TOKEN>
<TOKEN id="token-16-9" pos="punct" morph="none" start_char="2020" end_char="2020">(</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="2021" end_char="2027">Picture</TOKEN>
<TOKEN id="token-16-11" pos="punct" morph="none" start_char="2028" end_char="2028">:</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="2030" end_char="2034">Getty</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2036" end_char="2041">Images</TOKEN>
<TOKEN id="token-16-14" pos="punct" morph="none" start_char="2042" end_char="2042">)</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2046" end_char="2153">
<ORIGINAL_TEXT>On February 6 a British businessman in Brighton was diagnosed with the virus after catching it in Singapore.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2046" end_char="2047">On</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2049" end_char="2056">February</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2058" end_char="2058">6</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2060" end_char="2060">a</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2062" end_char="2068">British</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2070" end_char="2080">businessman</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2082" end_char="2083">in</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="2085" end_char="2092">Brighton</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="2094" end_char="2096">was</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="2098" end_char="2106">diagnosed</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="2108" end_char="2111">with</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="2113" end_char="2115">the</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="2117" end_char="2121">virus</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="2123" end_char="2127">after</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="2129" end_char="2136">catching</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="2138" end_char="2139">it</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="2141" end_char="2142">in</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="2144" end_char="2152">Singapore</TOKEN>
<TOKEN id="token-17-18" pos="punct" morph="none" start_char="2153" end_char="2153">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2156" end_char="2251">
<ORIGINAL_TEXT>The so-called ‘super spreader’ was later linked to 11 other cases, five of which were in the UK.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2156" end_char="2158">The</TOKEN>
<TOKEN id="token-18-1" pos="unknown" morph="none" start_char="2160" end_char="2168">so-called</TOKEN>
<TOKEN id="token-18-2" pos="punct" morph="none" start_char="2170" end_char="2170">‘</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="2171" end_char="2175">super</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2177" end_char="2184">spreader</TOKEN>
<TOKEN id="token-18-5" pos="punct" morph="none" start_char="2185" end_char="2185">’</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2187" end_char="2189">was</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2191" end_char="2195">later</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2197" end_char="2202">linked</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2204" end_char="2205">to</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2207" end_char="2208">11</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="2210" end_char="2214">other</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="2216" end_char="2220">cases</TOKEN>
<TOKEN id="token-18-13" pos="punct" morph="none" start_char="2221" end_char="2221">,</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="2223" end_char="2226">five</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="2228" end_char="2229">of</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="2231" end_char="2235">which</TOKEN>
<TOKEN id="token-18-17" pos="word" morph="none" start_char="2237" end_char="2240">were</TOKEN>
<TOKEN id="token-18-18" pos="word" morph="none" start_char="2242" end_char="2243">in</TOKEN>
<TOKEN id="token-18-19" pos="word" morph="none" start_char="2245" end_char="2247">the</TOKEN>
<TOKEN id="token-18-20" pos="word" morph="none" start_char="2249" end_char="2250">UK</TOKEN>
<TOKEN id="token-18-21" pos="punct" morph="none" start_char="2251" end_char="2251">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2254" end_char="2405">
<ORIGINAL_TEXT>Later that month, on February 28, the first person to catch coronavirus in the UK was diagnosed, a man who lived in Surrey, but who had not been abroad.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2254" end_char="2258">Later</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2260" end_char="2263">that</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2265" end_char="2269">month</TOKEN>
<TOKEN id="token-19-3" pos="punct" morph="none" start_char="2270" end_char="2270">,</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="2272" end_char="2273">on</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="2275" end_char="2282">February</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2284" end_char="2285">28</TOKEN>
<TOKEN id="token-19-7" pos="punct" morph="none" start_char="2286" end_char="2286">,</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="2288" end_char="2290">the</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="2292" end_char="2296">first</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="2298" end_char="2303">person</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="2305" end_char="2306">to</TOKEN>
<TOKEN id="token-19-12" pos="word" morph="none" start_char="2308" end_char="2312">catch</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="2314" end_char="2324">coronavirus</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="2326" end_char="2327">in</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="2329" end_char="2331">the</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="2333" end_char="2334">UK</TOKEN>
<TOKEN id="token-19-17" pos="word" morph="none" start_char="2336" end_char="2338">was</TOKEN>
<TOKEN id="token-19-18" pos="word" morph="none" start_char="2340" end_char="2348">diagnosed</TOKEN>
<TOKEN id="token-19-19" pos="punct" morph="none" start_char="2349" end_char="2349">,</TOKEN>
<TOKEN id="token-19-20" pos="word" morph="none" start_char="2351" end_char="2351">a</TOKEN>
<TOKEN id="token-19-21" pos="word" morph="none" start_char="2353" end_char="2355">man</TOKEN>
<TOKEN id="token-19-22" pos="word" morph="none" start_char="2357" end_char="2359">who</TOKEN>
<TOKEN id="token-19-23" pos="word" morph="none" start_char="2361" end_char="2365">lived</TOKEN>
<TOKEN id="token-19-24" pos="word" morph="none" start_char="2367" end_char="2368">in</TOKEN>
<TOKEN id="token-19-25" pos="word" morph="none" start_char="2370" end_char="2375">Surrey</TOKEN>
<TOKEN id="token-19-26" pos="punct" morph="none" start_char="2376" end_char="2376">,</TOKEN>
<TOKEN id="token-19-27" pos="word" morph="none" start_char="2378" end_char="2380">but</TOKEN>
<TOKEN id="token-19-28" pos="word" morph="none" start_char="2382" end_char="2384">who</TOKEN>
<TOKEN id="token-19-29" pos="word" morph="none" start_char="2386" end_char="2388">had</TOKEN>
<TOKEN id="token-19-30" pos="word" morph="none" start_char="2390" end_char="2392">not</TOKEN>
<TOKEN id="token-19-31" pos="word" morph="none" start_char="2394" end_char="2397">been</TOKEN>
<TOKEN id="token-19-32" pos="word" morph="none" start_char="2399" end_char="2404">abroad</TOKEN>
<TOKEN id="token-19-33" pos="punct" morph="none" start_char="2405" end_char="2405">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2408" end_char="2636">
<ORIGINAL_TEXT>However, it’s since been claimed that the UK’s first case could also have occurred earlier, after a 66-year-old man revealed he was hospitalised with a mystery illness after returning from a family holiday in Italy 14 months ago.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2408" end_char="2414">However</TOKEN>
<TOKEN id="token-20-1" pos="punct" morph="none" start_char="2415" end_char="2415">,</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="2417" end_char="2420">it’s</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="2422" end_char="2426">since</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2428" end_char="2431">been</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2433" end_char="2439">claimed</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2441" end_char="2444">that</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="2446" end_char="2448">the</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="2450" end_char="2453">UK’s</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="2455" end_char="2459">first</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="2461" end_char="2464">case</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="2466" end_char="2470">could</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="2472" end_char="2475">also</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="2477" end_char="2480">have</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="2482" end_char="2489">occurred</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="2491" end_char="2497">earlier</TOKEN>
<TOKEN id="token-20-16" pos="punct" morph="none" start_char="2498" end_char="2498">,</TOKEN>
<TOKEN id="token-20-17" pos="word" morph="none" start_char="2500" end_char="2504">after</TOKEN>
<TOKEN id="token-20-18" pos="word" morph="none" start_char="2506" end_char="2506">a</TOKEN>
<TOKEN id="token-20-19" pos="unknown" morph="none" start_char="2508" end_char="2518">66-year-old</TOKEN>
<TOKEN id="token-20-20" pos="word" morph="none" start_char="2520" end_char="2522">man</TOKEN>
<TOKEN id="token-20-21" pos="word" morph="none" start_char="2524" end_char="2531">revealed</TOKEN>
<TOKEN id="token-20-22" pos="word" morph="none" start_char="2533" end_char="2534">he</TOKEN>
<TOKEN id="token-20-23" pos="word" morph="none" start_char="2536" end_char="2538">was</TOKEN>
<TOKEN id="token-20-24" pos="word" morph="none" start_char="2540" end_char="2551">hospitalised</TOKEN>
<TOKEN id="token-20-25" pos="word" morph="none" start_char="2553" end_char="2556">with</TOKEN>
<TOKEN id="token-20-26" pos="word" morph="none" start_char="2558" end_char="2558">a</TOKEN>
<TOKEN id="token-20-27" pos="word" morph="none" start_char="2560" end_char="2566">mystery</TOKEN>
<TOKEN id="token-20-28" pos="word" morph="none" start_char="2568" end_char="2574">illness</TOKEN>
<TOKEN id="token-20-29" pos="word" morph="none" start_char="2576" end_char="2580">after</TOKEN>
<TOKEN id="token-20-30" pos="word" morph="none" start_char="2582" end_char="2590">returning</TOKEN>
<TOKEN id="token-20-31" pos="word" morph="none" start_char="2592" end_char="2595">from</TOKEN>
<TOKEN id="token-20-32" pos="word" morph="none" start_char="2597" end_char="2597">a</TOKEN>
<TOKEN id="token-20-33" pos="word" morph="none" start_char="2599" end_char="2604">family</TOKEN>
<TOKEN id="token-20-34" pos="word" morph="none" start_char="2606" end_char="2612">holiday</TOKEN>
<TOKEN id="token-20-35" pos="word" morph="none" start_char="2614" end_char="2615">in</TOKEN>
<TOKEN id="token-20-36" pos="word" morph="none" start_char="2617" end_char="2621">Italy</TOKEN>
<TOKEN id="token-20-37" pos="word" morph="none" start_char="2623" end_char="2624">14</TOKEN>
<TOKEN id="token-20-38" pos="word" morph="none" start_char="2626" end_char="2631">months</TOKEN>
<TOKEN id="token-20-39" pos="word" morph="none" start_char="2633" end_char="2635">ago</TOKEN>
<TOKEN id="token-20-40" pos="punct" morph="none" start_char="2636" end_char="2636">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="2639" end_char="2792">
<ORIGINAL_TEXT>Brian Stoodley said that he suffered many of Covid’s most common symptoms during the illness, which left him bed-bound and gasping for oxygen in hospital.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="2639" end_char="2643">Brian</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="2645" end_char="2652">Stoodley</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="2654" end_char="2657">said</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="2659" end_char="2662">that</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="2664" end_char="2665">he</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="2667" end_char="2674">suffered</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="2676" end_char="2679">many</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="2681" end_char="2682">of</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="2684" end_char="2690">Covid’s</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="2692" end_char="2695">most</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="2697" end_char="2702">common</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="2704" end_char="2711">symptoms</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="2713" end_char="2718">during</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="2720" end_char="2722">the</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="2724" end_char="2730">illness</TOKEN>
<TOKEN id="token-21-15" pos="punct" morph="none" start_char="2731" end_char="2731">,</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="2733" end_char="2737">which</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="2739" end_char="2742">left</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="2744" end_char="2746">him</TOKEN>
<TOKEN id="token-21-19" pos="unknown" morph="none" start_char="2748" end_char="2756">bed-bound</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="2758" end_char="2760">and</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="2762" end_char="2768">gasping</TOKEN>
<TOKEN id="token-21-22" pos="word" morph="none" start_char="2770" end_char="2772">for</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="2774" end_char="2779">oxygen</TOKEN>
<TOKEN id="token-21-24" pos="word" morph="none" start_char="2781" end_char="2782">in</TOKEN>
<TOKEN id="token-21-25" pos="word" morph="none" start_char="2784" end_char="2791">hospital</TOKEN>
<TOKEN id="token-21-26" pos="punct" morph="none" start_char="2792" end_char="2792">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="2795" end_char="2870">
<ORIGINAL_TEXT>Follow Metro across our social channels, on Facebook, Twitter and Instagram.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="2795" end_char="2800">Follow</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="2802" end_char="2806">Metro</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="2808" end_char="2813">across</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="2815" end_char="2817">our</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="2819" end_char="2824">social</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="2826" end_char="2833">channels</TOKEN>
<TOKEN id="token-22-6" pos="punct" morph="none" start_char="2834" end_char="2834">,</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="2836" end_char="2837">on</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="2839" end_char="2846">Facebook</TOKEN>
<TOKEN id="token-22-9" pos="punct" morph="none" start_char="2847" end_char="2847">,</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="2849" end_char="2855">Twitter</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="2857" end_char="2859">and</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="2861" end_char="2869">Instagram</TOKEN>
<TOKEN id="token-22-13" pos="punct" morph="none" start_char="2870" end_char="2870">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="2873" end_char="2911">
<ORIGINAL_TEXT>Share your views in the comments below.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="2873" end_char="2877">Share</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="2879" end_char="2882">your</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="2884" end_char="2888">views</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="2890" end_char="2891">in</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="2893" end_char="2895">the</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="2897" end_char="2904">comments</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="2906" end_char="2910">below</TOKEN>
<TOKEN id="token-23-7" pos="punct" morph="none" start_char="2911" end_char="2911">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
