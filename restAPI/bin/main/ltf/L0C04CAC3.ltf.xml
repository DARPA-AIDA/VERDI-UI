<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CAC3" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="954" raw_text_md5="35314d2c297e38121965a7fa9769231b">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="112">
<ORIGINAL_TEXT>False: A RAI TV show from 2015 warned about Chinese research to create a new coronavirus from other virus parts.</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="5">False</TOKEN>
<TOKEN id="token-0-1" pos="punct" morph="none" start_char="6" end_char="6">:</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="8" end_char="8">A</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="10" end_char="12">RAI</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="14" end_char="15">TV</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="17" end_char="20">show</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="22" end_char="25">from</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="27" end_char="30">2015</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="32" end_char="37">warned</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="39" end_char="43">about</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="45" end_char="51">Chinese</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="53" end_char="60">research</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="62" end_char="63">to</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="65" end_char="70">create</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="72" end_char="72">a</TOKEN>
<TOKEN id="token-0-15" pos="word" morph="none" start_char="74" end_char="76">new</TOKEN>
<TOKEN id="token-0-16" pos="word" morph="none" start_char="78" end_char="88">coronavirus</TOKEN>
<TOKEN id="token-0-17" pos="word" morph="none" start_char="90" end_char="93">from</TOKEN>
<TOKEN id="token-0-18" pos="word" morph="none" start_char="95" end_char="99">other</TOKEN>
<TOKEN id="token-0-19" pos="word" morph="none" start_char="101" end_char="105">virus</TOKEN>
<TOKEN id="token-0-20" pos="word" morph="none" start_char="107" end_char="111">parts</TOKEN>
<TOKEN id="token-0-21" pos="punct" morph="none" start_char="112" end_char="112">.</TOKEN>
</SEG>
<SEG id="segment-1" start_char="114" end_char="140">
<ORIGINAL_TEXT>It all took place in Wuhan.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="114" end_char="115">It</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="117" end_char="119">all</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="121" end_char="124">took</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="126" end_char="130">place</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="132" end_char="133">in</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="135" end_char="139">Wuhan</TOKEN>
<TOKEN id="token-1-6" pos="punct" morph="none" start_char="140" end_char="140">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="144" end_char="274">
<ORIGINAL_TEXT>Explanation: A RAI TV show did run a segment on a research, that resulted in an article published by Nature, involving coronavirus.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="144" end_char="154">Explanation</TOKEN>
<TOKEN id="token-2-1" pos="punct" morph="none" start_char="155" end_char="155">:</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="157" end_char="157">A</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="159" end_char="161">RAI</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="163" end_char="164">TV</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="166" end_char="169">show</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="171" end_char="173">did</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="175" end_char="177">run</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="179" end_char="179">a</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="181" end_char="187">segment</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="189" end_char="190">on</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="192" end_char="192">a</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="194" end_char="201">research</TOKEN>
<TOKEN id="token-2-13" pos="punct" morph="none" start_char="202" end_char="202">,</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="204" end_char="207">that</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="209" end_char="216">resulted</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="218" end_char="219">in</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="221" end_char="222">an</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="224" end_char="230">article</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="232" end_char="240">published</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="242" end_char="243">by</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="245" end_char="250">Nature</TOKEN>
<TOKEN id="token-2-22" pos="punct" morph="none" start_char="251" end_char="251">,</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="253" end_char="261">involving</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="263" end_char="273">coronavirus</TOKEN>
<TOKEN id="token-2-25" pos="punct" morph="none" start_char="274" end_char="274">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="276" end_char="347">
<ORIGINAL_TEXT>Both RAI and Nature denied any links between COVID-19 and this research.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="276" end_char="279">Both</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="281" end_char="283">RAI</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="285" end_char="287">and</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="289" end_char="294">Nature</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="296" end_char="301">denied</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="303" end_char="305">any</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="307" end_char="311">links</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="313" end_char="319">between</TOKEN>
<TOKEN id="token-3-8" pos="unknown" morph="none" start_char="321" end_char="328">COVID-19</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="330" end_char="332">and</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="334" end_char="337">this</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="339" end_char="346">research</TOKEN>
<TOKEN id="token-3-12" pos="punct" morph="none" start_char="347" end_char="347">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="349" end_char="467">
<ORIGINAL_TEXT>Also, although two people involved in the research were Chinese, it was done in a lab in North Carolina, United States.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="349" end_char="352">Also</TOKEN>
<TOKEN id="token-4-1" pos="punct" morph="none" start_char="353" end_char="353">,</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="355" end_char="362">although</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="364" end_char="366">two</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="368" end_char="373">people</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="375" end_char="382">involved</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="384" end_char="385">in</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="387" end_char="389">the</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="391" end_char="398">research</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="400" end_char="403">were</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="405" end_char="411">Chinese</TOKEN>
<TOKEN id="token-4-11" pos="punct" morph="none" start_char="412" end_char="412">,</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="414" end_char="415">it</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="417" end_char="419">was</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="421" end_char="424">done</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="426" end_char="427">in</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="429" end_char="429">a</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="431" end_char="433">lab</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="435" end_char="436">in</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="438" end_char="442">North</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="444" end_char="451">Carolina</TOKEN>
<TOKEN id="token-4-21" pos="punct" morph="none" start_char="452" end_char="452">,</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="454" end_char="459">United</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="461" end_char="466">States</TOKEN>
<TOKEN id="token-4-24" pos="punct" morph="none" start_char="467" end_char="467">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="469" end_char="536">
<ORIGINAL_TEXT>All references to Wuhan were inserted after the video was published.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="469" end_char="471">All</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="473" end_char="482">references</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="484" end_char="485">to</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="487" end_char="491">Wuhan</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="493" end_char="496">were</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="498" end_char="505">inserted</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="507" end_char="511">after</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="513" end_char="515">the</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="517" end_char="521">video</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="523" end_char="525">was</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="527" end_char="535">published</TOKEN>
<TOKEN id="token-5-11" pos="punct" morph="none" start_char="536" end_char="536">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="539" end_char="574">
<ORIGINAL_TEXT>Read the Full Article (Agência Lupa)</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="539" end_char="542">Read</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="544" end_char="546">the</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="548" end_char="551">Full</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="553" end_char="559">Article</TOKEN>
<TOKEN id="token-6-4" pos="punct" morph="none" start_char="561" end_char="561">(</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="562" end_char="568">Agência</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="570" end_char="573">Lupa</TOKEN>
<TOKEN id="token-6-7" pos="punct" morph="none" start_char="574" end_char="574">)</TOKEN>
</SEG>
<SEG id="segment-7" start_char="577" end_char="618">
<ORIGINAL_TEXT>This false claim originated from: Facebook</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="577" end_char="580">This</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="582" end_char="586">false</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="588" end_char="592">claim</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="594" end_char="603">originated</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="605" end_char="608">from</TOKEN>
<TOKEN id="token-7-5" pos="punct" morph="none" start_char="609" end_char="609">:</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="611" end_char="618">Facebook</TOKEN>
</SEG>
<SEG id="segment-8" start_char="621" end_char="726">
<ORIGINAL_TEXT>The #CoronavirusFacts database records fact-checks published since the beginning of the COVID-19 outbreak.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="621" end_char="623">The</TOKEN>
<TOKEN id="token-8-1" pos="tag" morph="none" start_char="625" end_char="641">#CoronavirusFacts</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="643" end_char="650">database</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="652" end_char="658">records</TOKEN>
<TOKEN id="token-8-4" pos="unknown" morph="none" start_char="660" end_char="670">fact-checks</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="672" end_char="680">published</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="682" end_char="686">since</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="688" end_char="690">the</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="692" end_char="700">beginning</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="702" end_char="703">of</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="705" end_char="707">the</TOKEN>
<TOKEN id="token-8-11" pos="unknown" morph="none" start_char="709" end_char="716">COVID-19</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="718" end_char="725">outbreak</TOKEN>
<TOKEN id="token-8-13" pos="punct" morph="none" start_char="726" end_char="726">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="728" end_char="854">
<ORIGINAL_TEXT>The pandemic and its consequences are constantly evolving and data that was accurate weeks or even days ago might have changed.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="728" end_char="730">The</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="732" end_char="739">pandemic</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="741" end_char="743">and</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="745" end_char="747">its</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="749" end_char="760">consequences</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="762" end_char="764">are</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="766" end_char="775">constantly</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="777" end_char="784">evolving</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="786" end_char="788">and</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="790" end_char="793">data</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="795" end_char="798">that</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="800" end_char="802">was</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="804" end_char="811">accurate</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="813" end_char="817">weeks</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="819" end_char="820">or</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="822" end_char="825">even</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="827" end_char="830">days</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="832" end_char="834">ago</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="836" end_char="840">might</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="842" end_char="845">have</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="847" end_char="853">changed</TOKEN>
<TOKEN id="token-9-21" pos="punct" morph="none" start_char="854" end_char="854">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="856" end_char="950">
<ORIGINAL_TEXT>Remember to check the date when the fact-check you are reading was published before sharing it.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="856" end_char="863">Remember</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="865" end_char="866">to</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="868" end_char="872">check</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="874" end_char="876">the</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="878" end_char="881">date</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="883" end_char="886">when</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="888" end_char="890">the</TOKEN>
<TOKEN id="token-10-7" pos="unknown" morph="none" start_char="892" end_char="901">fact-check</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="903" end_char="905">you</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="907" end_char="909">are</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="911" end_char="917">reading</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="919" end_char="921">was</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="923" end_char="931">published</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="933" end_char="938">before</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="940" end_char="946">sharing</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="948" end_char="949">it</TOKEN>
<TOKEN id="token-10-16" pos="punct" morph="none" start_char="950" end_char="950">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
