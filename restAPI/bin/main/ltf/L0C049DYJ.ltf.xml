<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="eng">
<DOC id="L0C049DYJ" lang="eng" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="7577" raw_text_md5="78a258a0b7475e81ca26615715262fed">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="89">
<ORIGINAL_TEXT>Dr. Fauci Backed Controversial Wuhan Lab with U.S. Dollars for Risky Coronavirus Research</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="2">Dr</TOKEN>
<TOKEN id="token-0-1" pos="punct" morph="none" start_char="3" end_char="3">.</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="5" end_char="9">Fauci</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="11" end_char="16">Backed</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="18" end_char="30">Controversial</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="32" end_char="36">Wuhan</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="38" end_char="40">Lab</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="42" end_char="45">with</TOKEN>
<TOKEN id="token-0-8" pos="unknown" morph="none" start_char="47" end_char="49">U.S</TOKEN>
<TOKEN id="token-0-9" pos="punct" morph="none" start_char="50" end_char="50">.</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="52" end_char="58">Dollars</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="60" end_char="62">for</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="64" end_char="68">Risky</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="70" end_char="80">Coronavirus</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="82" end_char="89">Research</TOKEN>
</SEG>
<SEG id="segment-1" start_char="94" end_char="156">
<ORIGINAL_TEXT>Biomedical research ultimately protects public health, said Dr.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="94" end_char="103">Biomedical</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="105" end_char="112">research</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="114" end_char="123">ultimately</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="125" end_char="132">protects</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="134" end_char="139">public</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="141" end_char="146">health</TOKEN>
<TOKEN id="token-1-6" pos="punct" morph="none" start_char="147" end_char="147">,</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="149" end_char="152">said</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="154" end_char="155">Dr</TOKEN>
<TOKEN id="token-1-9" pos="punct" morph="none" start_char="156" end_char="156">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="158" end_char="225">
<ORIGINAL_TEXT>Anthony Fauci, in explaining his support for controversial research.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="158" end_char="164">Anthony</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="166" end_char="170">Fauci</TOKEN>
<TOKEN id="token-2-2" pos="punct" morph="none" start_char="171" end_char="171">,</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="173" end_char="174">in</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="176" end_char="185">explaining</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="187" end_char="189">his</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="191" end_char="197">support</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="199" end_char="201">for</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="203" end_char="215">controversial</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="217" end_char="224">research</TOKEN>
<TOKEN id="token-2-10" pos="punct" morph="none" start_char="225" end_char="225">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="229" end_char="231">
<ORIGINAL_TEXT>Dr.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="229" end_char="230">Dr</TOKEN>
<TOKEN id="token-3-1" pos="punct" morph="none" start_char="231" end_char="231">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="233" end_char="384">
<ORIGINAL_TEXT>Anthony Fauci is an adviser to President Donald Trump and something of an American folk hero for his steady, calm leadership during the pandemic crisis.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="233" end_char="239">Anthony</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="241" end_char="245">Fauci</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="247" end_char="248">is</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="250" end_char="251">an</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="253" end_char="259">adviser</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="261" end_char="262">to</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="264" end_char="272">President</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="274" end_char="279">Donald</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="281" end_char="285">Trump</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="287" end_char="289">and</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="291" end_char="299">something</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="301" end_char="302">of</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="304" end_char="305">an</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="307" end_char="314">American</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="316" end_char="319">folk</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="321" end_char="324">hero</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="326" end_char="328">for</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="330" end_char="332">his</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="334" end_char="339">steady</TOKEN>
<TOKEN id="token-4-19" pos="punct" morph="none" start_char="340" end_char="340">,</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="342" end_char="345">calm</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="347" end_char="356">leadership</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="358" end_char="363">during</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="365" end_char="367">the</TOKEN>
<TOKEN id="token-4-24" pos="word" morph="none" start_char="369" end_char="376">pandemic</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="378" end_char="383">crisis</TOKEN>
<TOKEN id="token-4-26" pos="punct" morph="none" start_char="384" end_char="384">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="386" end_char="532">
<ORIGINAL_TEXT>At least one poll shows that Americans trust Fauci more than Trump on the coronavirus pandemic—and few scientists are portrayed on TV by Brad Pitt.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="386" end_char="387">At</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="389" end_char="393">least</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="395" end_char="397">one</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="399" end_char="402">poll</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="404" end_char="408">shows</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="410" end_char="413">that</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="415" end_char="423">Americans</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="425" end_char="429">trust</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="431" end_char="435">Fauci</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="437" end_char="440">more</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="442" end_char="445">than</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="447" end_char="451">Trump</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="453" end_char="454">on</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="456" end_char="458">the</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="460" end_char="470">coronavirus</TOKEN>
<TOKEN id="token-5-15" pos="unknown" morph="none" start_char="472" end_char="483">pandemic—and</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="485" end_char="487">few</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="489" end_char="498">scientists</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="500" end_char="502">are</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="504" end_char="512">portrayed</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="514" end_char="515">on</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="517" end_char="518">TV</TOKEN>
<TOKEN id="token-5-22" pos="word" morph="none" start_char="520" end_char="521">by</TOKEN>
<TOKEN id="token-5-23" pos="word" morph="none" start_char="523" end_char="526">Brad</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="528" end_char="531">Pitt</TOKEN>
<TOKEN id="token-5-25" pos="punct" morph="none" start_char="532" end_char="532">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="535" end_char="784">
<ORIGINAL_TEXT>But just last year, the National Institute for Allergy and Infectious Diseases, the organization led by Dr. Fauci, funded scientists at the Wuhan Institute of Virology and other institutions for work on gain-of-function research on bat coronaviruses.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="535" end_char="537">But</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="539" end_char="542">just</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="544" end_char="547">last</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="549" end_char="552">year</TOKEN>
<TOKEN id="token-6-4" pos="punct" morph="none" start_char="553" end_char="553">,</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="555" end_char="557">the</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="559" end_char="566">National</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="568" end_char="576">Institute</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="578" end_char="580">for</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="582" end_char="588">Allergy</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="590" end_char="592">and</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="594" end_char="603">Infectious</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="605" end_char="612">Diseases</TOKEN>
<TOKEN id="token-6-13" pos="punct" morph="none" start_char="613" end_char="613">,</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="615" end_char="617">the</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="619" end_char="630">organization</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="632" end_char="634">led</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="636" end_char="637">by</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="639" end_char="640">Dr</TOKEN>
<TOKEN id="token-6-19" pos="punct" morph="none" start_char="641" end_char="641">.</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="643" end_char="647">Fauci</TOKEN>
<TOKEN id="token-6-21" pos="punct" morph="none" start_char="648" end_char="648">,</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="650" end_char="655">funded</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="657" end_char="666">scientists</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="668" end_char="669">at</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="671" end_char="673">the</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="675" end_char="679">Wuhan</TOKEN>
<TOKEN id="token-6-27" pos="word" morph="none" start_char="681" end_char="689">Institute</TOKEN>
<TOKEN id="token-6-28" pos="word" morph="none" start_char="691" end_char="692">of</TOKEN>
<TOKEN id="token-6-29" pos="word" morph="none" start_char="694" end_char="701">Virology</TOKEN>
<TOKEN id="token-6-30" pos="word" morph="none" start_char="703" end_char="705">and</TOKEN>
<TOKEN id="token-6-31" pos="word" morph="none" start_char="707" end_char="711">other</TOKEN>
<TOKEN id="token-6-32" pos="word" morph="none" start_char="713" end_char="724">institutions</TOKEN>
<TOKEN id="token-6-33" pos="word" morph="none" start_char="726" end_char="728">for</TOKEN>
<TOKEN id="token-6-34" pos="word" morph="none" start_char="730" end_char="733">work</TOKEN>
<TOKEN id="token-6-35" pos="word" morph="none" start_char="735" end_char="736">on</TOKEN>
<TOKEN id="token-6-36" pos="unknown" morph="none" start_char="738" end_char="753">gain-of-function</TOKEN>
<TOKEN id="token-6-37" pos="word" morph="none" start_char="755" end_char="762">research</TOKEN>
<TOKEN id="token-6-38" pos="word" morph="none" start_char="764" end_char="765">on</TOKEN>
<TOKEN id="token-6-39" pos="word" morph="none" start_char="767" end_char="769">bat</TOKEN>
<TOKEN id="token-6-40" pos="word" morph="none" start_char="771" end_char="783">coronaviruses</TOKEN>
<TOKEN id="token-6-41" pos="punct" morph="none" start_char="784" end_char="784">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="787" end_char="948">
<ORIGINAL_TEXT>In 2019, with the backing of NIAID, the National Institutes of Health committed $3.7 million over six years for research that included some gain-of-function work.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="787" end_char="788">In</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="790" end_char="793">2019</TOKEN>
<TOKEN id="token-7-2" pos="punct" morph="none" start_char="794" end_char="794">,</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="796" end_char="799">with</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="801" end_char="803">the</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="805" end_char="811">backing</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="813" end_char="814">of</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="816" end_char="820">NIAID</TOKEN>
<TOKEN id="token-7-8" pos="punct" morph="none" start_char="821" end_char="821">,</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="823" end_char="825">the</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="827" end_char="834">National</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="836" end_char="845">Institutes</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="847" end_char="848">of</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="850" end_char="855">Health</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="857" end_char="865">committed</TOKEN>
<TOKEN id="token-7-15" pos="unknown" morph="none" start_char="867" end_char="870">$3.7</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="872" end_char="878">million</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="880" end_char="883">over</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="885" end_char="887">six</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="889" end_char="893">years</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="895" end_char="897">for</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="899" end_char="906">research</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="908" end_char="911">that</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="913" end_char="920">included</TOKEN>
<TOKEN id="token-7-24" pos="word" morph="none" start_char="922" end_char="925">some</TOKEN>
<TOKEN id="token-7-25" pos="unknown" morph="none" start_char="927" end_char="942">gain-of-function</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="944" end_char="947">work</TOKEN>
<TOKEN id="token-7-27" pos="punct" morph="none" start_char="948" end_char="948">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="950" end_char="1110">
<ORIGINAL_TEXT>The program followed another $3.7 million, 5-year project for collecting and studying bat coronaviruses, which ended in 2019, bringing the total to $7.4 million.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="950" end_char="952">The</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="954" end_char="960">program</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="962" end_char="969">followed</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="971" end_char="977">another</TOKEN>
<TOKEN id="token-8-4" pos="unknown" morph="none" start_char="979" end_char="982">$3.7</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="984" end_char="990">million</TOKEN>
<TOKEN id="token-8-6" pos="punct" morph="none" start_char="991" end_char="991">,</TOKEN>
<TOKEN id="token-8-7" pos="unknown" morph="none" start_char="993" end_char="998">5-year</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1000" end_char="1006">project</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="1008" end_char="1010">for</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="1012" end_char="1021">collecting</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="1023" end_char="1025">and</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="1027" end_char="1034">studying</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="1036" end_char="1038">bat</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="1040" end_char="1052">coronaviruses</TOKEN>
<TOKEN id="token-8-15" pos="punct" morph="none" start_char="1053" end_char="1053">,</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="1055" end_char="1059">which</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="1061" end_char="1065">ended</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="1067" end_char="1068">in</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="1070" end_char="1073">2019</TOKEN>
<TOKEN id="token-8-20" pos="punct" morph="none" start_char="1074" end_char="1074">,</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="1076" end_char="1083">bringing</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="1085" end_char="1087">the</TOKEN>
<TOKEN id="token-8-23" pos="word" morph="none" start_char="1089" end_char="1093">total</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="1095" end_char="1096">to</TOKEN>
<TOKEN id="token-8-25" pos="unknown" morph="none" start_char="1098" end_char="1101">$7.4</TOKEN>
<TOKEN id="token-8-26" pos="word" morph="none" start_char="1103" end_char="1109">million</TOKEN>
<TOKEN id="token-8-27" pos="punct" morph="none" start_char="1110" end_char="1110">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1113" end_char="1340">
<ORIGINAL_TEXT>Many scientists have criticized gain of function research, which involves manipulating viruses in the lab to explore their potential for infecting humans, because it creates a risk of starting a pandemic from accidental release.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1113" end_char="1116">Many</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1118" end_char="1127">scientists</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="1129" end_char="1132">have</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1134" end_char="1143">criticized</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1145" end_char="1148">gain</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1150" end_char="1151">of</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1153" end_char="1160">function</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1162" end_char="1169">research</TOKEN>
<TOKEN id="token-9-8" pos="punct" morph="none" start_char="1170" end_char="1170">,</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="1172" end_char="1176">which</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1178" end_char="1185">involves</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1187" end_char="1198">manipulating</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="1200" end_char="1206">viruses</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1208" end_char="1209">in</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="1211" end_char="1213">the</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1215" end_char="1217">lab</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="1219" end_char="1220">to</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1222" end_char="1228">explore</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1230" end_char="1234">their</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1236" end_char="1244">potential</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1246" end_char="1248">for</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1250" end_char="1258">infecting</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1260" end_char="1265">humans</TOKEN>
<TOKEN id="token-9-23" pos="punct" morph="none" start_char="1266" end_char="1266">,</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1268" end_char="1274">because</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="1276" end_char="1277">it</TOKEN>
<TOKEN id="token-9-26" pos="word" morph="none" start_char="1279" end_char="1285">creates</TOKEN>
<TOKEN id="token-9-27" pos="word" morph="none" start_char="1287" end_char="1287">a</TOKEN>
<TOKEN id="token-9-28" pos="word" morph="none" start_char="1289" end_char="1292">risk</TOKEN>
<TOKEN id="token-9-29" pos="word" morph="none" start_char="1294" end_char="1295">of</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1297" end_char="1304">starting</TOKEN>
<TOKEN id="token-9-31" pos="word" morph="none" start_char="1306" end_char="1306">a</TOKEN>
<TOKEN id="token-9-32" pos="word" morph="none" start_char="1308" end_char="1315">pandemic</TOKEN>
<TOKEN id="token-9-33" pos="word" morph="none" start_char="1317" end_char="1320">from</TOKEN>
<TOKEN id="token-9-34" pos="word" morph="none" start_char="1322" end_char="1331">accidental</TOKEN>
<TOKEN id="token-9-35" pos="word" morph="none" start_char="1333" end_char="1339">release</TOKEN>
<TOKEN id="token-9-36" pos="punct" morph="none" start_char="1340" end_char="1340">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1343" end_char="1435">
<ORIGINAL_TEXT>SARS-CoV-2 , the virus now causing a global pandemic, is believed to have originated in bats.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="unknown" morph="none" start_char="1343" end_char="1352">SARS-CoV-2</TOKEN>
<TOKEN id="token-10-1" pos="punct" morph="none" start_char="1354" end_char="1354">,</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1356" end_char="1358">the</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1360" end_char="1364">virus</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1366" end_char="1368">now</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1370" end_char="1376">causing</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1378" end_char="1378">a</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1380" end_char="1385">global</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1387" end_char="1394">pandemic</TOKEN>
<TOKEN id="token-10-9" pos="punct" morph="none" start_char="1395" end_char="1395">,</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1397" end_char="1398">is</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1400" end_char="1407">believed</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1409" end_char="1410">to</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1412" end_char="1415">have</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1417" end_char="1426">originated</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1428" end_char="1429">in</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1431" end_char="1434">bats</TOKEN>
<TOKEN id="token-10-17" pos="punct" morph="none" start_char="1435" end_char="1435">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1437" end_char="1614">
<ORIGINAL_TEXT>U.S. intelligence, after originally asserting that the coronavirus had occurred naturally, conceded last month that the pandemic may have originated in a leak from the Wuhan lab.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="unknown" morph="none" start_char="1437" end_char="1439">U.S</TOKEN>
<TOKEN id="token-11-1" pos="punct" morph="none" start_char="1440" end_char="1440">.</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1442" end_char="1453">intelligence</TOKEN>
<TOKEN id="token-11-3" pos="punct" morph="none" start_char="1454" end_char="1454">,</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1456" end_char="1460">after</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1462" end_char="1471">originally</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1473" end_char="1481">asserting</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1483" end_char="1486">that</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1488" end_char="1490">the</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1492" end_char="1502">coronavirus</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1504" end_char="1506">had</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1508" end_char="1515">occurred</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1517" end_char="1525">naturally</TOKEN>
<TOKEN id="token-11-13" pos="punct" morph="none" start_char="1526" end_char="1526">,</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1528" end_char="1535">conceded</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1537" end_char="1540">last</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1542" end_char="1546">month</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1548" end_char="1551">that</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1553" end_char="1555">the</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1557" end_char="1564">pandemic</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1566" end_char="1568">may</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1570" end_char="1573">have</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1575" end_char="1584">originated</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1586" end_char="1587">in</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1589" end_char="1589">a</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1591" end_char="1594">leak</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="1596" end_char="1599">from</TOKEN>
<TOKEN id="token-11-27" pos="word" morph="none" start_char="1601" end_char="1603">the</TOKEN>
<TOKEN id="token-11-28" pos="word" morph="none" start_char="1605" end_char="1609">Wuhan</TOKEN>
<TOKEN id="token-11-29" pos="word" morph="none" start_char="1611" end_char="1613">lab</TOKEN>
<TOKEN id="token-11-30" pos="punct" morph="none" start_char="1614" end_char="1614">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1616" end_char="1734">
<ORIGINAL_TEXT>(At this point most scientists say it's possible—but not likely—that the pandemic virus was engineered or manipulated.)</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="punct" morph="none" start_char="1616" end_char="1616">(</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1617" end_char="1618">At</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1620" end_char="1623">this</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1625" end_char="1629">point</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1631" end_char="1634">most</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1636" end_char="1645">scientists</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1647" end_char="1649">say</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1651" end_char="1654">it's</TOKEN>
<TOKEN id="token-12-8" pos="unknown" morph="none" start_char="1656" end_char="1667">possible—but</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1669" end_char="1671">not</TOKEN>
<TOKEN id="token-12-10" pos="unknown" morph="none" start_char="1673" end_char="1683">likely—that</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1685" end_char="1687">the</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1689" end_char="1696">pandemic</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1698" end_char="1702">virus</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1704" end_char="1706">was</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1708" end_char="1717">engineered</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1719" end_char="1720">or</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1722" end_char="1732">manipulated</TOKEN>
<TOKEN id="token-12-18" pos="punct" morph="none" start_char="1733" end_char="1734">.)</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1737" end_char="1764">
<ORIGINAL_TEXT>Dr. Fauci did not respond to</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1737" end_char="1738">Dr</TOKEN>
<TOKEN id="token-13-1" pos="punct" morph="none" start_char="1739" end_char="1739">.</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1741" end_char="1745">Fauci</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1747" end_char="1749">did</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1751" end_char="1753">not</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1755" end_char="1761">respond</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1763" end_char="1764">to</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1767" end_char="1776">
<ORIGINAL_TEXT>Newsweek's</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1767" end_char="1776">Newsweek's</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1779" end_char="1799">
<ORIGINAL_TEXT>requests for comment.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1779" end_char="1786">requests</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1788" end_char="1790">for</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1792" end_char="1798">comment</TOKEN>
<TOKEN id="token-15-3" pos="punct" morph="none" start_char="1799" end_char="1799">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1801" end_char="2193">
<ORIGINAL_TEXT>NIH responded with a statement that said in part: "Most emerging human viruses come from wildlife, and these represent a significant threat to public health and biosecurity in the US and globally, as demonstrated by the SARS epidemic of 2002-03, and the current COVID-19 pandemic.... scientific research indicates that there is no evidence that suggests the virus was created in a laboratory."</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1801" end_char="1803">NIH</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1805" end_char="1813">responded</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1815" end_char="1818">with</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1820" end_char="1820">a</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1822" end_char="1830">statement</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1832" end_char="1835">that</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="1837" end_char="1840">said</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="1842" end_char="1843">in</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="1845" end_char="1848">part</TOKEN>
<TOKEN id="token-16-9" pos="punct" morph="none" start_char="1849" end_char="1849">:</TOKEN>
<TOKEN id="token-16-10" pos="punct" morph="none" start_char="1851" end_char="1851">"</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="1852" end_char="1855">Most</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="1857" end_char="1864">emerging</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="1866" end_char="1870">human</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="1872" end_char="1878">viruses</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="1880" end_char="1883">come</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="1885" end_char="1888">from</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="1890" end_char="1897">wildlife</TOKEN>
<TOKEN id="token-16-18" pos="punct" morph="none" start_char="1898" end_char="1898">,</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="1900" end_char="1902">and</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="1904" end_char="1908">these</TOKEN>
<TOKEN id="token-16-21" pos="word" morph="none" start_char="1910" end_char="1918">represent</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="1920" end_char="1920">a</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="1922" end_char="1932">significant</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="1934" end_char="1939">threat</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="1941" end_char="1942">to</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="1944" end_char="1949">public</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="1951" end_char="1956">health</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="1958" end_char="1960">and</TOKEN>
<TOKEN id="token-16-29" pos="word" morph="none" start_char="1962" end_char="1972">biosecurity</TOKEN>
<TOKEN id="token-16-30" pos="word" morph="none" start_char="1974" end_char="1975">in</TOKEN>
<TOKEN id="token-16-31" pos="word" morph="none" start_char="1977" end_char="1979">the</TOKEN>
<TOKEN id="token-16-32" pos="word" morph="none" start_char="1981" end_char="1982">US</TOKEN>
<TOKEN id="token-16-33" pos="word" morph="none" start_char="1984" end_char="1986">and</TOKEN>
<TOKEN id="token-16-34" pos="word" morph="none" start_char="1988" end_char="1995">globally</TOKEN>
<TOKEN id="token-16-35" pos="punct" morph="none" start_char="1996" end_char="1996">,</TOKEN>
<TOKEN id="token-16-36" pos="word" morph="none" start_char="1998" end_char="1999">as</TOKEN>
<TOKEN id="token-16-37" pos="word" morph="none" start_char="2001" end_char="2012">demonstrated</TOKEN>
<TOKEN id="token-16-38" pos="word" morph="none" start_char="2014" end_char="2015">by</TOKEN>
<TOKEN id="token-16-39" pos="word" morph="none" start_char="2017" end_char="2019">the</TOKEN>
<TOKEN id="token-16-40" pos="word" morph="none" start_char="2021" end_char="2024">SARS</TOKEN>
<TOKEN id="token-16-41" pos="word" morph="none" start_char="2026" end_char="2033">epidemic</TOKEN>
<TOKEN id="token-16-42" pos="word" morph="none" start_char="2035" end_char="2036">of</TOKEN>
<TOKEN id="token-16-43" pos="unknown" morph="none" start_char="2038" end_char="2044">2002-03</TOKEN>
<TOKEN id="token-16-44" pos="punct" morph="none" start_char="2045" end_char="2045">,</TOKEN>
<TOKEN id="token-16-45" pos="word" morph="none" start_char="2047" end_char="2049">and</TOKEN>
<TOKEN id="token-16-46" pos="word" morph="none" start_char="2051" end_char="2053">the</TOKEN>
<TOKEN id="token-16-47" pos="word" morph="none" start_char="2055" end_char="2061">current</TOKEN>
<TOKEN id="token-16-48" pos="unknown" morph="none" start_char="2063" end_char="2070">COVID-19</TOKEN>
<TOKEN id="token-16-49" pos="word" morph="none" start_char="2072" end_char="2079">pandemic</TOKEN>
<TOKEN id="token-16-50" pos="punct" morph="none" start_char="2080" end_char="2083">....</TOKEN>
<TOKEN id="token-16-51" pos="word" morph="none" start_char="2085" end_char="2094">scientific</TOKEN>
<TOKEN id="token-16-52" pos="word" morph="none" start_char="2096" end_char="2103">research</TOKEN>
<TOKEN id="token-16-53" pos="word" morph="none" start_char="2105" end_char="2113">indicates</TOKEN>
<TOKEN id="token-16-54" pos="word" morph="none" start_char="2115" end_char="2118">that</TOKEN>
<TOKEN id="token-16-55" pos="word" morph="none" start_char="2120" end_char="2124">there</TOKEN>
<TOKEN id="token-16-56" pos="word" morph="none" start_char="2126" end_char="2127">is</TOKEN>
<TOKEN id="token-16-57" pos="word" morph="none" start_char="2129" end_char="2130">no</TOKEN>
<TOKEN id="token-16-58" pos="word" morph="none" start_char="2132" end_char="2139">evidence</TOKEN>
<TOKEN id="token-16-59" pos="word" morph="none" start_char="2141" end_char="2144">that</TOKEN>
<TOKEN id="token-16-60" pos="word" morph="none" start_char="2146" end_char="2153">suggests</TOKEN>
<TOKEN id="token-16-61" pos="word" morph="none" start_char="2155" end_char="2157">the</TOKEN>
<TOKEN id="token-16-62" pos="word" morph="none" start_char="2159" end_char="2163">virus</TOKEN>
<TOKEN id="token-16-63" pos="word" morph="none" start_char="2165" end_char="2167">was</TOKEN>
<TOKEN id="token-16-64" pos="word" morph="none" start_char="2169" end_char="2175">created</TOKEN>
<TOKEN id="token-16-65" pos="word" morph="none" start_char="2177" end_char="2178">in</TOKEN>
<TOKEN id="token-16-66" pos="word" morph="none" start_char="2180" end_char="2180">a</TOKEN>
<TOKEN id="token-16-67" pos="word" morph="none" start_char="2182" end_char="2191">laboratory</TOKEN>
<TOKEN id="token-16-68" pos="punct" morph="none" start_char="2192" end_char="2193">."</TOKEN>
</SEG>
<SEG id="segment-17" start_char="2196" end_char="2235">
<ORIGINAL_TEXT>The NIH research consisted of two parts.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="2196" end_char="2198">The</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="2200" end_char="2202">NIH</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="2204" end_char="2211">research</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="2213" end_char="2221">consisted</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="2223" end_char="2224">of</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="2226" end_char="2228">two</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="2230" end_char="2234">parts</TOKEN>
<TOKEN id="token-17-7" pos="punct" morph="none" start_char="2235" end_char="2235">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2237" end_char="2346">
<ORIGINAL_TEXT>The first part began in 2014 and involved surveillance of bat coronaviruses, and had a budget of $3.7 million.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2237" end_char="2239">The</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2241" end_char="2245">first</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="2247" end_char="2250">part</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="2252" end_char="2256">began</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2258" end_char="2259">in</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="2261" end_char="2264">2014</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2266" end_char="2268">and</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2270" end_char="2277">involved</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2279" end_char="2290">surveillance</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2292" end_char="2293">of</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2295" end_char="2297">bat</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="2299" end_char="2311">coronaviruses</TOKEN>
<TOKEN id="token-18-12" pos="punct" morph="none" start_char="2312" end_char="2312">,</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="2314" end_char="2316">and</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="2318" end_char="2320">had</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="2322" end_char="2322">a</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="2324" end_char="2329">budget</TOKEN>
<TOKEN id="token-18-17" pos="word" morph="none" start_char="2331" end_char="2332">of</TOKEN>
<TOKEN id="token-18-18" pos="unknown" morph="none" start_char="2334" end_char="2337">$3.7</TOKEN>
<TOKEN id="token-18-19" pos="word" morph="none" start_char="2339" end_char="2345">million</TOKEN>
<TOKEN id="token-18-20" pos="punct" morph="none" start_char="2346" end_char="2346">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2348" end_char="2492">
<ORIGINAL_TEXT>The program funded Shi Zheng-Li, a virologist at the Wuhan lab, and other researchers to investigate and catalogue bat coronaviruses in the wild.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="2348" end_char="2350">The</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2352" end_char="2358">program</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="2360" end_char="2365">funded</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="2367" end_char="2369">Shi</TOKEN>
<TOKEN id="token-19-4" pos="unknown" morph="none" start_char="2371" end_char="2378">Zheng-Li</TOKEN>
<TOKEN id="token-19-5" pos="punct" morph="none" start_char="2379" end_char="2379">,</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="2381" end_char="2381">a</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="2383" end_char="2392">virologist</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="2394" end_char="2395">at</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="2397" end_char="2399">the</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="2401" end_char="2405">Wuhan</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="2407" end_char="2409">lab</TOKEN>
<TOKEN id="token-19-12" pos="punct" morph="none" start_char="2410" end_char="2410">,</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="2412" end_char="2414">and</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="2416" end_char="2420">other</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="2422" end_char="2432">researchers</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="2434" end_char="2435">to</TOKEN>
<TOKEN id="token-19-17" pos="word" morph="none" start_char="2437" end_char="2447">investigate</TOKEN>
<TOKEN id="token-19-18" pos="word" morph="none" start_char="2449" end_char="2451">and</TOKEN>
<TOKEN id="token-19-19" pos="word" morph="none" start_char="2453" end_char="2461">catalogue</TOKEN>
<TOKEN id="token-19-20" pos="word" morph="none" start_char="2463" end_char="2465">bat</TOKEN>
<TOKEN id="token-19-21" pos="word" morph="none" start_char="2467" end_char="2479">coronaviruses</TOKEN>
<TOKEN id="token-19-22" pos="word" morph="none" start_char="2481" end_char="2482">in</TOKEN>
<TOKEN id="token-19-23" pos="word" morph="none" start_char="2484" end_char="2486">the</TOKEN>
<TOKEN id="token-19-24" pos="word" morph="none" start_char="2488" end_char="2491">wild</TOKEN>
<TOKEN id="token-19-25" pos="punct" morph="none" start_char="2492" end_char="2492">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2494" end_char="2540">
<ORIGINAL_TEXT>This part of the project was completed in 2019.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2494" end_char="2497">This</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="2499" end_char="2502">part</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="2504" end_char="2505">of</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="2507" end_char="2509">the</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2511" end_char="2517">project</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2519" end_char="2521">was</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2523" end_char="2531">completed</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="2533" end_char="2534">in</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="2536" end_char="2539">2019</TOKEN>
<TOKEN id="token-20-9" pos="punct" morph="none" start_char="2540" end_char="2540">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="2543" end_char="2752">
<ORIGINAL_TEXT>A second phase of the project, beginning that year, included additional surveillance work but also gain-of-function research for the purpose of understanding how bat coronaviruses could mutate to attack humans.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="2543" end_char="2543">A</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="2545" end_char="2550">second</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="2552" end_char="2556">phase</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="2558" end_char="2559">of</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="2561" end_char="2563">the</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="2565" end_char="2571">project</TOKEN>
<TOKEN id="token-21-6" pos="punct" morph="none" start_char="2572" end_char="2572">,</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="2574" end_char="2582">beginning</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="2584" end_char="2587">that</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="2589" end_char="2592">year</TOKEN>
<TOKEN id="token-21-10" pos="punct" morph="none" start_char="2593" end_char="2593">,</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="2595" end_char="2602">included</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="2604" end_char="2613">additional</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="2615" end_char="2626">surveillance</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="2628" end_char="2631">work</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="2633" end_char="2635">but</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="2637" end_char="2640">also</TOKEN>
<TOKEN id="token-21-17" pos="unknown" morph="none" start_char="2642" end_char="2657">gain-of-function</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="2659" end_char="2666">research</TOKEN>
<TOKEN id="token-21-19" pos="word" morph="none" start_char="2668" end_char="2670">for</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="2672" end_char="2674">the</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="2676" end_char="2682">purpose</TOKEN>
<TOKEN id="token-21-22" pos="word" morph="none" start_char="2684" end_char="2685">of</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="2687" end_char="2699">understanding</TOKEN>
<TOKEN id="token-21-24" pos="word" morph="none" start_char="2701" end_char="2703">how</TOKEN>
<TOKEN id="token-21-25" pos="word" morph="none" start_char="2705" end_char="2707">bat</TOKEN>
<TOKEN id="token-21-26" pos="word" morph="none" start_char="2709" end_char="2721">coronaviruses</TOKEN>
<TOKEN id="token-21-27" pos="word" morph="none" start_char="2723" end_char="2727">could</TOKEN>
<TOKEN id="token-21-28" pos="word" morph="none" start_char="2729" end_char="2734">mutate</TOKEN>
<TOKEN id="token-21-29" pos="word" morph="none" start_char="2736" end_char="2737">to</TOKEN>
<TOKEN id="token-21-30" pos="word" morph="none" start_char="2739" end_char="2744">attack</TOKEN>
<TOKEN id="token-21-31" pos="word" morph="none" start_char="2746" end_char="2751">humans</TOKEN>
<TOKEN id="token-21-32" pos="punct" morph="none" start_char="2752" end_char="2752">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="2754" end_char="2901">
<ORIGINAL_TEXT>The project was run by EcoHealth Alliance, a non-profit research group, under the direction of President Peter Daszak, an expert on disease ecology.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="2754" end_char="2756">The</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="2758" end_char="2764">project</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="2766" end_char="2768">was</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="2770" end_char="2772">run</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="2774" end_char="2775">by</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="2777" end_char="2785">EcoHealth</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="2787" end_char="2794">Alliance</TOKEN>
<TOKEN id="token-22-7" pos="punct" morph="none" start_char="2795" end_char="2795">,</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="2797" end_char="2797">a</TOKEN>
<TOKEN id="token-22-9" pos="unknown" morph="none" start_char="2799" end_char="2808">non-profit</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="2810" end_char="2817">research</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="2819" end_char="2823">group</TOKEN>
<TOKEN id="token-22-12" pos="punct" morph="none" start_char="2824" end_char="2824">,</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="2826" end_char="2830">under</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="2832" end_char="2834">the</TOKEN>
<TOKEN id="token-22-15" pos="word" morph="none" start_char="2836" end_char="2844">direction</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="2846" end_char="2847">of</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="2849" end_char="2857">President</TOKEN>
<TOKEN id="token-22-18" pos="word" morph="none" start_char="2859" end_char="2863">Peter</TOKEN>
<TOKEN id="token-22-19" pos="word" morph="none" start_char="2865" end_char="2870">Daszak</TOKEN>
<TOKEN id="token-22-20" pos="punct" morph="none" start_char="2871" end_char="2871">,</TOKEN>
<TOKEN id="token-22-21" pos="word" morph="none" start_char="2873" end_char="2874">an</TOKEN>
<TOKEN id="token-22-22" pos="word" morph="none" start_char="2876" end_char="2881">expert</TOKEN>
<TOKEN id="token-22-23" pos="word" morph="none" start_char="2883" end_char="2884">on</TOKEN>
<TOKEN id="token-22-24" pos="word" morph="none" start_char="2886" end_char="2892">disease</TOKEN>
<TOKEN id="token-22-25" pos="word" morph="none" start_char="2894" end_char="2900">ecology</TOKEN>
<TOKEN id="token-22-26" pos="punct" morph="none" start_char="2901" end_char="2901">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="2903" end_char="2961">
<ORIGINAL_TEXT>NIH canceled the project just this past Friday, April 24th,</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="2903" end_char="2905">NIH</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="2907" end_char="2914">canceled</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="2916" end_char="2918">the</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="2920" end_char="2926">project</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="2928" end_char="2931">just</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="2933" end_char="2936">this</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="2938" end_char="2941">past</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="2943" end_char="2948">Friday</TOKEN>
<TOKEN id="token-23-8" pos="punct" morph="none" start_char="2949" end_char="2949">,</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="2951" end_char="2955">April</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="2957" end_char="2960">24th</TOKEN>
<TOKEN id="token-23-11" pos="punct" morph="none" start_char="2961" end_char="2961">,</TOKEN>
</SEG>
<SEG id="segment-24" start_char="2964" end_char="2971">
<ORIGINAL_TEXT>Politico</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="2964" end_char="2971">Politico</TOKEN>
</SEG>
<SEG id="segment-25" start_char="2974" end_char="2982">
<ORIGINAL_TEXT>reported.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="2974" end_char="2981">reported</TOKEN>
<TOKEN id="token-25-1" pos="punct" morph="none" start_char="2982" end_char="2982">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="2984" end_char="3020">
<ORIGINAL_TEXT>Daszak did not immediately respond to</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="2984" end_char="2989">Daszak</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="2991" end_char="2993">did</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="2995" end_char="2997">not</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="2999" end_char="3009">immediately</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="3011" end_char="3017">respond</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="3019" end_char="3020">to</TOKEN>
</SEG>
<SEG id="segment-27" start_char="3023" end_char="3030">
<ORIGINAL_TEXT>Newsweek</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="3023" end_char="3030">Newsweek</TOKEN>
</SEG>
<SEG id="segment-28" start_char="3033" end_char="3053">
<ORIGINAL_TEXT>requests for comment.</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="3033" end_char="3040">requests</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="3042" end_char="3044">for</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="3046" end_char="3052">comment</TOKEN>
<TOKEN id="token-28-3" pos="punct" morph="none" start_char="3053" end_char="3053">.</TOKEN>
</SEG>
<SEG id="segment-29" start_char="3056" end_char="3331">
<ORIGINAL_TEXT>The project proposal states: "We will use S protein sequence data, infectious clone technology, in vitro and in vivo infection experiments and analysis of receptor binding to test the hypothesis that % divergence thresholds in S protein sequences predict spillover potential."</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="3056" end_char="3058">The</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="3060" end_char="3066">project</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="3068" end_char="3075">proposal</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="3077" end_char="3082">states</TOKEN>
<TOKEN id="token-29-4" pos="punct" morph="none" start_char="3083" end_char="3083">:</TOKEN>
<TOKEN id="token-29-5" pos="punct" morph="none" start_char="3085" end_char="3085">"</TOKEN>
<TOKEN id="token-29-6" pos="word" morph="none" start_char="3086" end_char="3087">We</TOKEN>
<TOKEN id="token-29-7" pos="word" morph="none" start_char="3089" end_char="3092">will</TOKEN>
<TOKEN id="token-29-8" pos="word" morph="none" start_char="3094" end_char="3096">use</TOKEN>
<TOKEN id="token-29-9" pos="word" morph="none" start_char="3098" end_char="3098">S</TOKEN>
<TOKEN id="token-29-10" pos="word" morph="none" start_char="3100" end_char="3106">protein</TOKEN>
<TOKEN id="token-29-11" pos="word" morph="none" start_char="3108" end_char="3115">sequence</TOKEN>
<TOKEN id="token-29-12" pos="word" morph="none" start_char="3117" end_char="3120">data</TOKEN>
<TOKEN id="token-29-13" pos="punct" morph="none" start_char="3121" end_char="3121">,</TOKEN>
<TOKEN id="token-29-14" pos="word" morph="none" start_char="3123" end_char="3132">infectious</TOKEN>
<TOKEN id="token-29-15" pos="word" morph="none" start_char="3134" end_char="3138">clone</TOKEN>
<TOKEN id="token-29-16" pos="word" morph="none" start_char="3140" end_char="3149">technology</TOKEN>
<TOKEN id="token-29-17" pos="punct" morph="none" start_char="3150" end_char="3150">,</TOKEN>
<TOKEN id="token-29-18" pos="word" morph="none" start_char="3152" end_char="3153">in</TOKEN>
<TOKEN id="token-29-19" pos="word" morph="none" start_char="3155" end_char="3159">vitro</TOKEN>
<TOKEN id="token-29-20" pos="word" morph="none" start_char="3161" end_char="3163">and</TOKEN>
<TOKEN id="token-29-21" pos="word" morph="none" start_char="3165" end_char="3166">in</TOKEN>
<TOKEN id="token-29-22" pos="word" morph="none" start_char="3168" end_char="3171">vivo</TOKEN>
<TOKEN id="token-29-23" pos="word" morph="none" start_char="3173" end_char="3181">infection</TOKEN>
<TOKEN id="token-29-24" pos="word" morph="none" start_char="3183" end_char="3193">experiments</TOKEN>
<TOKEN id="token-29-25" pos="word" morph="none" start_char="3195" end_char="3197">and</TOKEN>
<TOKEN id="token-29-26" pos="word" morph="none" start_char="3199" end_char="3206">analysis</TOKEN>
<TOKEN id="token-29-27" pos="word" morph="none" start_char="3208" end_char="3209">of</TOKEN>
<TOKEN id="token-29-28" pos="word" morph="none" start_char="3211" end_char="3218">receptor</TOKEN>
<TOKEN id="token-29-29" pos="word" morph="none" start_char="3220" end_char="3226">binding</TOKEN>
<TOKEN id="token-29-30" pos="word" morph="none" start_char="3228" end_char="3229">to</TOKEN>
<TOKEN id="token-29-31" pos="word" morph="none" start_char="3231" end_char="3234">test</TOKEN>
<TOKEN id="token-29-32" pos="word" morph="none" start_char="3236" end_char="3238">the</TOKEN>
<TOKEN id="token-29-33" pos="word" morph="none" start_char="3240" end_char="3249">hypothesis</TOKEN>
<TOKEN id="token-29-34" pos="word" morph="none" start_char="3251" end_char="3254">that</TOKEN>
<TOKEN id="token-29-35" pos="punct" morph="none" start_char="3256" end_char="3256">%</TOKEN>
<TOKEN id="token-29-36" pos="word" morph="none" start_char="3258" end_char="3267">divergence</TOKEN>
<TOKEN id="token-29-37" pos="word" morph="none" start_char="3269" end_char="3278">thresholds</TOKEN>
<TOKEN id="token-29-38" pos="word" morph="none" start_char="3280" end_char="3281">in</TOKEN>
<TOKEN id="token-29-39" pos="word" morph="none" start_char="3283" end_char="3283">S</TOKEN>
<TOKEN id="token-29-40" pos="word" morph="none" start_char="3285" end_char="3291">protein</TOKEN>
<TOKEN id="token-29-41" pos="word" morph="none" start_char="3293" end_char="3301">sequences</TOKEN>
<TOKEN id="token-29-42" pos="word" morph="none" start_char="3303" end_char="3309">predict</TOKEN>
<TOKEN id="token-29-43" pos="word" morph="none" start_char="3311" end_char="3319">spillover</TOKEN>
<TOKEN id="token-29-44" pos="word" morph="none" start_char="3321" end_char="3329">potential</TOKEN>
<TOKEN id="token-29-45" pos="punct" morph="none" start_char="3330" end_char="3331">."</TOKEN>
</SEG>
<SEG id="segment-30" start_char="3334" end_char="3523">
<ORIGINAL_TEXT>In layman's terms, "spillover potential" refers to the ability of a virus to jump from animals to humans, which requires that the virus be able to attach to receptors in the cells of humans.</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="3334" end_char="3335">In</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="3337" end_char="3344">layman's</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="3346" end_char="3350">terms</TOKEN>
<TOKEN id="token-30-3" pos="punct" morph="none" start_char="3351" end_char="3351">,</TOKEN>
<TOKEN id="token-30-4" pos="punct" morph="none" start_char="3353" end_char="3353">"</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="3354" end_char="3362">spillover</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="3364" end_char="3372">potential</TOKEN>
<TOKEN id="token-30-7" pos="punct" morph="none" start_char="3373" end_char="3373">"</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="3375" end_char="3380">refers</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="3382" end_char="3383">to</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="3385" end_char="3387">the</TOKEN>
<TOKEN id="token-30-11" pos="word" morph="none" start_char="3389" end_char="3395">ability</TOKEN>
<TOKEN id="token-30-12" pos="word" morph="none" start_char="3397" end_char="3398">of</TOKEN>
<TOKEN id="token-30-13" pos="word" morph="none" start_char="3400" end_char="3400">a</TOKEN>
<TOKEN id="token-30-14" pos="word" morph="none" start_char="3402" end_char="3406">virus</TOKEN>
<TOKEN id="token-30-15" pos="word" morph="none" start_char="3408" end_char="3409">to</TOKEN>
<TOKEN id="token-30-16" pos="word" morph="none" start_char="3411" end_char="3414">jump</TOKEN>
<TOKEN id="token-30-17" pos="word" morph="none" start_char="3416" end_char="3419">from</TOKEN>
<TOKEN id="token-30-18" pos="word" morph="none" start_char="3421" end_char="3427">animals</TOKEN>
<TOKEN id="token-30-19" pos="word" morph="none" start_char="3429" end_char="3430">to</TOKEN>
<TOKEN id="token-30-20" pos="word" morph="none" start_char="3432" end_char="3437">humans</TOKEN>
<TOKEN id="token-30-21" pos="punct" morph="none" start_char="3438" end_char="3438">,</TOKEN>
<TOKEN id="token-30-22" pos="word" morph="none" start_char="3440" end_char="3444">which</TOKEN>
<TOKEN id="token-30-23" pos="word" morph="none" start_char="3446" end_char="3453">requires</TOKEN>
<TOKEN id="token-30-24" pos="word" morph="none" start_char="3455" end_char="3458">that</TOKEN>
<TOKEN id="token-30-25" pos="word" morph="none" start_char="3460" end_char="3462">the</TOKEN>
<TOKEN id="token-30-26" pos="word" morph="none" start_char="3464" end_char="3468">virus</TOKEN>
<TOKEN id="token-30-27" pos="word" morph="none" start_char="3470" end_char="3471">be</TOKEN>
<TOKEN id="token-30-28" pos="word" morph="none" start_char="3473" end_char="3476">able</TOKEN>
<TOKEN id="token-30-29" pos="word" morph="none" start_char="3478" end_char="3479">to</TOKEN>
<TOKEN id="token-30-30" pos="word" morph="none" start_char="3481" end_char="3486">attach</TOKEN>
<TOKEN id="token-30-31" pos="word" morph="none" start_char="3488" end_char="3489">to</TOKEN>
<TOKEN id="token-30-32" pos="word" morph="none" start_char="3491" end_char="3499">receptors</TOKEN>
<TOKEN id="token-30-33" pos="word" morph="none" start_char="3501" end_char="3502">in</TOKEN>
<TOKEN id="token-30-34" pos="word" morph="none" start_char="3504" end_char="3506">the</TOKEN>
<TOKEN id="token-30-35" pos="word" morph="none" start_char="3508" end_char="3512">cells</TOKEN>
<TOKEN id="token-30-36" pos="word" morph="none" start_char="3514" end_char="3515">of</TOKEN>
<TOKEN id="token-30-37" pos="word" morph="none" start_char="3517" end_char="3522">humans</TOKEN>
<TOKEN id="token-30-38" pos="punct" morph="none" start_char="3523" end_char="3523">.</TOKEN>
</SEG>
<SEG id="segment-31" start_char="3525" end_char="3623">
<ORIGINAL_TEXT>SARS-CoV-2, for instance, is adept at binding to the ACE2 receptor in human lungs and other organs.</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="unknown" morph="none" start_char="3525" end_char="3534">SARS-CoV-2</TOKEN>
<TOKEN id="token-31-1" pos="punct" morph="none" start_char="3535" end_char="3535">,</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="3537" end_char="3539">for</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="3541" end_char="3548">instance</TOKEN>
<TOKEN id="token-31-4" pos="punct" morph="none" start_char="3549" end_char="3549">,</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="3551" end_char="3552">is</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="3554" end_char="3558">adept</TOKEN>
<TOKEN id="token-31-7" pos="word" morph="none" start_char="3560" end_char="3561">at</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="3563" end_char="3569">binding</TOKEN>
<TOKEN id="token-31-9" pos="word" morph="none" start_char="3571" end_char="3572">to</TOKEN>
<TOKEN id="token-31-10" pos="word" morph="none" start_char="3574" end_char="3576">the</TOKEN>
<TOKEN id="token-31-11" pos="word" morph="none" start_char="3578" end_char="3581">ACE2</TOKEN>
<TOKEN id="token-31-12" pos="word" morph="none" start_char="3583" end_char="3590">receptor</TOKEN>
<TOKEN id="token-31-13" pos="word" morph="none" start_char="3592" end_char="3593">in</TOKEN>
<TOKEN id="token-31-14" pos="word" morph="none" start_char="3595" end_char="3599">human</TOKEN>
<TOKEN id="token-31-15" pos="word" morph="none" start_char="3601" end_char="3605">lungs</TOKEN>
<TOKEN id="token-31-16" pos="word" morph="none" start_char="3607" end_char="3609">and</TOKEN>
<TOKEN id="token-31-17" pos="word" morph="none" start_char="3611" end_char="3615">other</TOKEN>
<TOKEN id="token-31-18" pos="word" morph="none" start_char="3617" end_char="3622">organs</TOKEN>
<TOKEN id="token-31-19" pos="punct" morph="none" start_char="3623" end_char="3623">.</TOKEN>
</SEG>
<SEG id="segment-32" start_char="3626" end_char="3888">
<ORIGINAL_TEXT>According to Richard Ebright, an infectious disease expert at Rutgers University, the project description refers to experiments that would enhance the ability of bat coronavirus to infect human cells and laboratory animals using techniques of genetic engineering.</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="3626" end_char="3634">According</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="3636" end_char="3637">to</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="3639" end_char="3645">Richard</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="3647" end_char="3653">Ebright</TOKEN>
<TOKEN id="token-32-4" pos="punct" morph="none" start_char="3654" end_char="3654">,</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="3656" end_char="3657">an</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="3659" end_char="3668">infectious</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="3670" end_char="3676">disease</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="3678" end_char="3683">expert</TOKEN>
<TOKEN id="token-32-9" pos="word" morph="none" start_char="3685" end_char="3686">at</TOKEN>
<TOKEN id="token-32-10" pos="word" morph="none" start_char="3688" end_char="3694">Rutgers</TOKEN>
<TOKEN id="token-32-11" pos="word" morph="none" start_char="3696" end_char="3705">University</TOKEN>
<TOKEN id="token-32-12" pos="punct" morph="none" start_char="3706" end_char="3706">,</TOKEN>
<TOKEN id="token-32-13" pos="word" morph="none" start_char="3708" end_char="3710">the</TOKEN>
<TOKEN id="token-32-14" pos="word" morph="none" start_char="3712" end_char="3718">project</TOKEN>
<TOKEN id="token-32-15" pos="word" morph="none" start_char="3720" end_char="3730">description</TOKEN>
<TOKEN id="token-32-16" pos="word" morph="none" start_char="3732" end_char="3737">refers</TOKEN>
<TOKEN id="token-32-17" pos="word" morph="none" start_char="3739" end_char="3740">to</TOKEN>
<TOKEN id="token-32-18" pos="word" morph="none" start_char="3742" end_char="3752">experiments</TOKEN>
<TOKEN id="token-32-19" pos="word" morph="none" start_char="3754" end_char="3757">that</TOKEN>
<TOKEN id="token-32-20" pos="word" morph="none" start_char="3759" end_char="3763">would</TOKEN>
<TOKEN id="token-32-21" pos="word" morph="none" start_char="3765" end_char="3771">enhance</TOKEN>
<TOKEN id="token-32-22" pos="word" morph="none" start_char="3773" end_char="3775">the</TOKEN>
<TOKEN id="token-32-23" pos="word" morph="none" start_char="3777" end_char="3783">ability</TOKEN>
<TOKEN id="token-32-24" pos="word" morph="none" start_char="3785" end_char="3786">of</TOKEN>
<TOKEN id="token-32-25" pos="word" morph="none" start_char="3788" end_char="3790">bat</TOKEN>
<TOKEN id="token-32-26" pos="word" morph="none" start_char="3792" end_char="3802">coronavirus</TOKEN>
<TOKEN id="token-32-27" pos="word" morph="none" start_char="3804" end_char="3805">to</TOKEN>
<TOKEN id="token-32-28" pos="word" morph="none" start_char="3807" end_char="3812">infect</TOKEN>
<TOKEN id="token-32-29" pos="word" morph="none" start_char="3814" end_char="3818">human</TOKEN>
<TOKEN id="token-32-30" pos="word" morph="none" start_char="3820" end_char="3824">cells</TOKEN>
<TOKEN id="token-32-31" pos="word" morph="none" start_char="3826" end_char="3828">and</TOKEN>
<TOKEN id="token-32-32" pos="word" morph="none" start_char="3830" end_char="3839">laboratory</TOKEN>
<TOKEN id="token-32-33" pos="word" morph="none" start_char="3841" end_char="3847">animals</TOKEN>
<TOKEN id="token-32-34" pos="word" morph="none" start_char="3849" end_char="3853">using</TOKEN>
<TOKEN id="token-32-35" pos="word" morph="none" start_char="3855" end_char="3864">techniques</TOKEN>
<TOKEN id="token-32-36" pos="word" morph="none" start_char="3866" end_char="3867">of</TOKEN>
<TOKEN id="token-32-37" pos="word" morph="none" start_char="3869" end_char="3875">genetic</TOKEN>
<TOKEN id="token-32-38" pos="word" morph="none" start_char="3877" end_char="3887">engineering</TOKEN>
<TOKEN id="token-32-39" pos="punct" morph="none" start_char="3888" end_char="3888">.</TOKEN>
</SEG>
<SEG id="segment-33" start_char="3890" end_char="3946">
<ORIGINAL_TEXT>In the wake of the pandemic, that is a noteworthy detail.</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="word" morph="none" start_char="3890" end_char="3891">In</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="3893" end_char="3895">the</TOKEN>
<TOKEN id="token-33-2" pos="word" morph="none" start_char="3897" end_char="3900">wake</TOKEN>
<TOKEN id="token-33-3" pos="word" morph="none" start_char="3902" end_char="3903">of</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="3905" end_char="3907">the</TOKEN>
<TOKEN id="token-33-5" pos="word" morph="none" start_char="3909" end_char="3916">pandemic</TOKEN>
<TOKEN id="token-33-6" pos="punct" morph="none" start_char="3917" end_char="3917">,</TOKEN>
<TOKEN id="token-33-7" pos="word" morph="none" start_char="3919" end_char="3922">that</TOKEN>
<TOKEN id="token-33-8" pos="word" morph="none" start_char="3924" end_char="3925">is</TOKEN>
<TOKEN id="token-33-9" pos="word" morph="none" start_char="3927" end_char="3927">a</TOKEN>
<TOKEN id="token-33-10" pos="word" morph="none" start_char="3929" end_char="3938">noteworthy</TOKEN>
<TOKEN id="token-33-11" pos="word" morph="none" start_char="3940" end_char="3945">detail</TOKEN>
<TOKEN id="token-33-12" pos="punct" morph="none" start_char="3946" end_char="3946">.</TOKEN>
</SEG>
<SEG id="segment-34" start_char="3949" end_char="4139">
<ORIGINAL_TEXT>Ebright, along with many other scientists, has been a vocal opponent of gain-of-function research because of the risk it presents of creating a pandemic through accidental release from a lab.</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="3949" end_char="3955">Ebright</TOKEN>
<TOKEN id="token-34-1" pos="punct" morph="none" start_char="3956" end_char="3956">,</TOKEN>
<TOKEN id="token-34-2" pos="word" morph="none" start_char="3958" end_char="3962">along</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="3964" end_char="3967">with</TOKEN>
<TOKEN id="token-34-4" pos="word" morph="none" start_char="3969" end_char="3972">many</TOKEN>
<TOKEN id="token-34-5" pos="word" morph="none" start_char="3974" end_char="3978">other</TOKEN>
<TOKEN id="token-34-6" pos="word" morph="none" start_char="3980" end_char="3989">scientists</TOKEN>
<TOKEN id="token-34-7" pos="punct" morph="none" start_char="3990" end_char="3990">,</TOKEN>
<TOKEN id="token-34-8" pos="word" morph="none" start_char="3992" end_char="3994">has</TOKEN>
<TOKEN id="token-34-9" pos="word" morph="none" start_char="3996" end_char="3999">been</TOKEN>
<TOKEN id="token-34-10" pos="word" morph="none" start_char="4001" end_char="4001">a</TOKEN>
<TOKEN id="token-34-11" pos="word" morph="none" start_char="4003" end_char="4007">vocal</TOKEN>
<TOKEN id="token-34-12" pos="word" morph="none" start_char="4009" end_char="4016">opponent</TOKEN>
<TOKEN id="token-34-13" pos="word" morph="none" start_char="4018" end_char="4019">of</TOKEN>
<TOKEN id="token-34-14" pos="unknown" morph="none" start_char="4021" end_char="4036">gain-of-function</TOKEN>
<TOKEN id="token-34-15" pos="word" morph="none" start_char="4038" end_char="4045">research</TOKEN>
<TOKEN id="token-34-16" pos="word" morph="none" start_char="4047" end_char="4053">because</TOKEN>
<TOKEN id="token-34-17" pos="word" morph="none" start_char="4055" end_char="4056">of</TOKEN>
<TOKEN id="token-34-18" pos="word" morph="none" start_char="4058" end_char="4060">the</TOKEN>
<TOKEN id="token-34-19" pos="word" morph="none" start_char="4062" end_char="4065">risk</TOKEN>
<TOKEN id="token-34-20" pos="word" morph="none" start_char="4067" end_char="4068">it</TOKEN>
<TOKEN id="token-34-21" pos="word" morph="none" start_char="4070" end_char="4077">presents</TOKEN>
<TOKEN id="token-34-22" pos="word" morph="none" start_char="4079" end_char="4080">of</TOKEN>
<TOKEN id="token-34-23" pos="word" morph="none" start_char="4082" end_char="4089">creating</TOKEN>
<TOKEN id="token-34-24" pos="word" morph="none" start_char="4091" end_char="4091">a</TOKEN>
<TOKEN id="token-34-25" pos="word" morph="none" start_char="4093" end_char="4100">pandemic</TOKEN>
<TOKEN id="token-34-26" pos="word" morph="none" start_char="4102" end_char="4108">through</TOKEN>
<TOKEN id="token-34-27" pos="word" morph="none" start_char="4110" end_char="4119">accidental</TOKEN>
<TOKEN id="token-34-28" pos="word" morph="none" start_char="4121" end_char="4127">release</TOKEN>
<TOKEN id="token-34-29" pos="word" morph="none" start_char="4129" end_char="4132">from</TOKEN>
<TOKEN id="token-34-30" pos="word" morph="none" start_char="4134" end_char="4134">a</TOKEN>
<TOKEN id="token-34-31" pos="word" morph="none" start_char="4136" end_char="4138">lab</TOKEN>
<TOKEN id="token-34-32" pos="punct" morph="none" start_char="4139" end_char="4139">.</TOKEN>
</SEG>
<SEG id="segment-35" start_char="4142" end_char="4212">
<ORIGINAL_TEXT>Dr. Fauci is renowned for his work on the HIV/AIDS crisis in the 1990s.</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="4142" end_char="4143">Dr</TOKEN>
<TOKEN id="token-35-1" pos="punct" morph="none" start_char="4144" end_char="4144">.</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="4146" end_char="4150">Fauci</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="4152" end_char="4153">is</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="4155" end_char="4162">renowned</TOKEN>
<TOKEN id="token-35-5" pos="word" morph="none" start_char="4164" end_char="4166">for</TOKEN>
<TOKEN id="token-35-6" pos="word" morph="none" start_char="4168" end_char="4170">his</TOKEN>
<TOKEN id="token-35-7" pos="word" morph="none" start_char="4172" end_char="4175">work</TOKEN>
<TOKEN id="token-35-8" pos="word" morph="none" start_char="4177" end_char="4178">on</TOKEN>
<TOKEN id="token-35-9" pos="word" morph="none" start_char="4180" end_char="4182">the</TOKEN>
<TOKEN id="token-35-10" pos="unknown" morph="none" start_char="4184" end_char="4191">HIV/AIDS</TOKEN>
<TOKEN id="token-35-11" pos="word" morph="none" start_char="4193" end_char="4198">crisis</TOKEN>
<TOKEN id="token-35-12" pos="word" morph="none" start_char="4200" end_char="4201">in</TOKEN>
<TOKEN id="token-35-13" pos="word" morph="none" start_char="4203" end_char="4205">the</TOKEN>
<TOKEN id="token-35-14" pos="word" morph="none" start_char="4207" end_char="4211">1990s</TOKEN>
<TOKEN id="token-35-15" pos="punct" morph="none" start_char="4212" end_char="4212">.</TOKEN>
</SEG>
<SEG id="segment-36" start_char="4214" end_char="4311">
<ORIGINAL_TEXT>Born in Brooklyn, he graduated first in his class from Cornell University Medical College in 1966.</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="4214" end_char="4217">Born</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="4219" end_char="4220">in</TOKEN>
<TOKEN id="token-36-2" pos="word" morph="none" start_char="4222" end_char="4229">Brooklyn</TOKEN>
<TOKEN id="token-36-3" pos="punct" morph="none" start_char="4230" end_char="4230">,</TOKEN>
<TOKEN id="token-36-4" pos="word" morph="none" start_char="4232" end_char="4233">he</TOKEN>
<TOKEN id="token-36-5" pos="word" morph="none" start_char="4235" end_char="4243">graduated</TOKEN>
<TOKEN id="token-36-6" pos="word" morph="none" start_char="4245" end_char="4249">first</TOKEN>
<TOKEN id="token-36-7" pos="word" morph="none" start_char="4251" end_char="4252">in</TOKEN>
<TOKEN id="token-36-8" pos="word" morph="none" start_char="4254" end_char="4256">his</TOKEN>
<TOKEN id="token-36-9" pos="word" morph="none" start_char="4258" end_char="4262">class</TOKEN>
<TOKEN id="token-36-10" pos="word" morph="none" start_char="4264" end_char="4267">from</TOKEN>
<TOKEN id="token-36-11" pos="word" morph="none" start_char="4269" end_char="4275">Cornell</TOKEN>
<TOKEN id="token-36-12" pos="word" morph="none" start_char="4277" end_char="4286">University</TOKEN>
<TOKEN id="token-36-13" pos="word" morph="none" start_char="4288" end_char="4294">Medical</TOKEN>
<TOKEN id="token-36-14" pos="word" morph="none" start_char="4296" end_char="4302">College</TOKEN>
<TOKEN id="token-36-15" pos="word" morph="none" start_char="4304" end_char="4305">in</TOKEN>
<TOKEN id="token-36-16" pos="word" morph="none" start_char="4307" end_char="4310">1966</TOKEN>
<TOKEN id="token-36-17" pos="punct" morph="none" start_char="4311" end_char="4311">.</TOKEN>
</SEG>
<SEG id="segment-37" start_char="4313" end_char="4413">
<ORIGINAL_TEXT>As head of NIAID since 1984, he has served as an adviser to every U.S. president since Ronald Reagan.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="4313" end_char="4314">As</TOKEN>
<TOKEN id="token-37-1" pos="word" morph="none" start_char="4316" end_char="4319">head</TOKEN>
<TOKEN id="token-37-2" pos="word" morph="none" start_char="4321" end_char="4322">of</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="4324" end_char="4328">NIAID</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="4330" end_char="4334">since</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="4336" end_char="4339">1984</TOKEN>
<TOKEN id="token-37-6" pos="punct" morph="none" start_char="4340" end_char="4340">,</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="4342" end_char="4343">he</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="4345" end_char="4347">has</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="4349" end_char="4354">served</TOKEN>
<TOKEN id="token-37-10" pos="word" morph="none" start_char="4356" end_char="4357">as</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="4359" end_char="4360">an</TOKEN>
<TOKEN id="token-37-12" pos="word" morph="none" start_char="4362" end_char="4368">adviser</TOKEN>
<TOKEN id="token-37-13" pos="word" morph="none" start_char="4370" end_char="4371">to</TOKEN>
<TOKEN id="token-37-14" pos="word" morph="none" start_char="4373" end_char="4377">every</TOKEN>
<TOKEN id="token-37-15" pos="unknown" morph="none" start_char="4379" end_char="4381">U.S</TOKEN>
<TOKEN id="token-37-16" pos="punct" morph="none" start_char="4382" end_char="4382">.</TOKEN>
<TOKEN id="token-37-17" pos="word" morph="none" start_char="4384" end_char="4392">president</TOKEN>
<TOKEN id="token-37-18" pos="word" morph="none" start_char="4394" end_char="4398">since</TOKEN>
<TOKEN id="token-37-19" pos="word" morph="none" start_char="4400" end_char="4405">Ronald</TOKEN>
<TOKEN id="token-37-20" pos="word" morph="none" start_char="4407" end_char="4412">Reagan</TOKEN>
<TOKEN id="token-37-21" pos="punct" morph="none" start_char="4413" end_char="4413">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="4416" end_char="4559">
<ORIGINAL_TEXT>A decade ago, during a controversy over gain-of-function research on bird-flu viruses, Dr. Fauci played an important role in promoting the work.</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="4416" end_char="4416">A</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="4418" end_char="4423">decade</TOKEN>
<TOKEN id="token-38-2" pos="word" morph="none" start_char="4425" end_char="4427">ago</TOKEN>
<TOKEN id="token-38-3" pos="punct" morph="none" start_char="4428" end_char="4428">,</TOKEN>
<TOKEN id="token-38-4" pos="word" morph="none" start_char="4430" end_char="4435">during</TOKEN>
<TOKEN id="token-38-5" pos="word" morph="none" start_char="4437" end_char="4437">a</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="4439" end_char="4449">controversy</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="4451" end_char="4454">over</TOKEN>
<TOKEN id="token-38-8" pos="unknown" morph="none" start_char="4456" end_char="4471">gain-of-function</TOKEN>
<TOKEN id="token-38-9" pos="word" morph="none" start_char="4473" end_char="4480">research</TOKEN>
<TOKEN id="token-38-10" pos="word" morph="none" start_char="4482" end_char="4483">on</TOKEN>
<TOKEN id="token-38-11" pos="unknown" morph="none" start_char="4485" end_char="4492">bird-flu</TOKEN>
<TOKEN id="token-38-12" pos="word" morph="none" start_char="4494" end_char="4500">viruses</TOKEN>
<TOKEN id="token-38-13" pos="punct" morph="none" start_char="4501" end_char="4501">,</TOKEN>
<TOKEN id="token-38-14" pos="word" morph="none" start_char="4503" end_char="4504">Dr</TOKEN>
<TOKEN id="token-38-15" pos="punct" morph="none" start_char="4505" end_char="4505">.</TOKEN>
<TOKEN id="token-38-16" pos="word" morph="none" start_char="4507" end_char="4511">Fauci</TOKEN>
<TOKEN id="token-38-17" pos="word" morph="none" start_char="4513" end_char="4518">played</TOKEN>
<TOKEN id="token-38-18" pos="word" morph="none" start_char="4520" end_char="4521">an</TOKEN>
<TOKEN id="token-38-19" pos="word" morph="none" start_char="4523" end_char="4531">important</TOKEN>
<TOKEN id="token-38-20" pos="word" morph="none" start_char="4533" end_char="4536">role</TOKEN>
<TOKEN id="token-38-21" pos="word" morph="none" start_char="4538" end_char="4539">in</TOKEN>
<TOKEN id="token-38-22" pos="word" morph="none" start_char="4541" end_char="4549">promoting</TOKEN>
<TOKEN id="token-38-23" pos="word" morph="none" start_char="4551" end_char="4553">the</TOKEN>
<TOKEN id="token-38-24" pos="word" morph="none" start_char="4555" end_char="4558">work</TOKEN>
<TOKEN id="token-38-25" pos="punct" morph="none" start_char="4559" end_char="4559">.</TOKEN>
</SEG>
<SEG id="segment-39" start_char="4561" end_char="4779">
<ORIGINAL_TEXT>He argued that the research was worth the risk it entailed because it enables scientists to make preparations, such as investigating possible anti-viral medications, that could be useful if and when a pandemic occurred.</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="word" morph="none" start_char="4561" end_char="4562">He</TOKEN>
<TOKEN id="token-39-1" pos="word" morph="none" start_char="4564" end_char="4569">argued</TOKEN>
<TOKEN id="token-39-2" pos="word" morph="none" start_char="4571" end_char="4574">that</TOKEN>
<TOKEN id="token-39-3" pos="word" morph="none" start_char="4576" end_char="4578">the</TOKEN>
<TOKEN id="token-39-4" pos="word" morph="none" start_char="4580" end_char="4587">research</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="4589" end_char="4591">was</TOKEN>
<TOKEN id="token-39-6" pos="word" morph="none" start_char="4593" end_char="4597">worth</TOKEN>
<TOKEN id="token-39-7" pos="word" morph="none" start_char="4599" end_char="4601">the</TOKEN>
<TOKEN id="token-39-8" pos="word" morph="none" start_char="4603" end_char="4606">risk</TOKEN>
<TOKEN id="token-39-9" pos="word" morph="none" start_char="4608" end_char="4609">it</TOKEN>
<TOKEN id="token-39-10" pos="word" morph="none" start_char="4611" end_char="4618">entailed</TOKEN>
<TOKEN id="token-39-11" pos="word" morph="none" start_char="4620" end_char="4626">because</TOKEN>
<TOKEN id="token-39-12" pos="word" morph="none" start_char="4628" end_char="4629">it</TOKEN>
<TOKEN id="token-39-13" pos="word" morph="none" start_char="4631" end_char="4637">enables</TOKEN>
<TOKEN id="token-39-14" pos="word" morph="none" start_char="4639" end_char="4648">scientists</TOKEN>
<TOKEN id="token-39-15" pos="word" morph="none" start_char="4650" end_char="4651">to</TOKEN>
<TOKEN id="token-39-16" pos="word" morph="none" start_char="4653" end_char="4656">make</TOKEN>
<TOKEN id="token-39-17" pos="word" morph="none" start_char="4658" end_char="4669">preparations</TOKEN>
<TOKEN id="token-39-18" pos="punct" morph="none" start_char="4670" end_char="4670">,</TOKEN>
<TOKEN id="token-39-19" pos="word" morph="none" start_char="4672" end_char="4675">such</TOKEN>
<TOKEN id="token-39-20" pos="word" morph="none" start_char="4677" end_char="4678">as</TOKEN>
<TOKEN id="token-39-21" pos="word" morph="none" start_char="4680" end_char="4692">investigating</TOKEN>
<TOKEN id="token-39-22" pos="word" morph="none" start_char="4694" end_char="4701">possible</TOKEN>
<TOKEN id="token-39-23" pos="unknown" morph="none" start_char="4703" end_char="4712">anti-viral</TOKEN>
<TOKEN id="token-39-24" pos="word" morph="none" start_char="4714" end_char="4724">medications</TOKEN>
<TOKEN id="token-39-25" pos="punct" morph="none" start_char="4725" end_char="4725">,</TOKEN>
<TOKEN id="token-39-26" pos="word" morph="none" start_char="4727" end_char="4730">that</TOKEN>
<TOKEN id="token-39-27" pos="word" morph="none" start_char="4732" end_char="4736">could</TOKEN>
<TOKEN id="token-39-28" pos="word" morph="none" start_char="4738" end_char="4739">be</TOKEN>
<TOKEN id="token-39-29" pos="word" morph="none" start_char="4741" end_char="4746">useful</TOKEN>
<TOKEN id="token-39-30" pos="word" morph="none" start_char="4748" end_char="4749">if</TOKEN>
<TOKEN id="token-39-31" pos="word" morph="none" start_char="4751" end_char="4753">and</TOKEN>
<TOKEN id="token-39-32" pos="word" morph="none" start_char="4755" end_char="4758">when</TOKEN>
<TOKEN id="token-39-33" pos="word" morph="none" start_char="4760" end_char="4760">a</TOKEN>
<TOKEN id="token-39-34" pos="word" morph="none" start_char="4762" end_char="4769">pandemic</TOKEN>
<TOKEN id="token-39-35" pos="word" morph="none" start_char="4771" end_char="4778">occurred</TOKEN>
<TOKEN id="token-39-36" pos="punct" morph="none" start_char="4779" end_char="4779">.</TOKEN>
</SEG>
<SEG id="segment-40" start_char="4782" end_char="4978">
<ORIGINAL_TEXT>The work in question was a type of gain-of-function research that involved taking wild viruses and passing them through live animals until they mutate into a form that could pose a pandemic threat.</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="word" morph="none" start_char="4782" end_char="4784">The</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="4786" end_char="4789">work</TOKEN>
<TOKEN id="token-40-2" pos="word" morph="none" start_char="4791" end_char="4792">in</TOKEN>
<TOKEN id="token-40-3" pos="word" morph="none" start_char="4794" end_char="4801">question</TOKEN>
<TOKEN id="token-40-4" pos="word" morph="none" start_char="4803" end_char="4805">was</TOKEN>
<TOKEN id="token-40-5" pos="word" morph="none" start_char="4807" end_char="4807">a</TOKEN>
<TOKEN id="token-40-6" pos="word" morph="none" start_char="4809" end_char="4812">type</TOKEN>
<TOKEN id="token-40-7" pos="word" morph="none" start_char="4814" end_char="4815">of</TOKEN>
<TOKEN id="token-40-8" pos="unknown" morph="none" start_char="4817" end_char="4832">gain-of-function</TOKEN>
<TOKEN id="token-40-9" pos="word" morph="none" start_char="4834" end_char="4841">research</TOKEN>
<TOKEN id="token-40-10" pos="word" morph="none" start_char="4843" end_char="4846">that</TOKEN>
<TOKEN id="token-40-11" pos="word" morph="none" start_char="4848" end_char="4855">involved</TOKEN>
<TOKEN id="token-40-12" pos="word" morph="none" start_char="4857" end_char="4862">taking</TOKEN>
<TOKEN id="token-40-13" pos="word" morph="none" start_char="4864" end_char="4867">wild</TOKEN>
<TOKEN id="token-40-14" pos="word" morph="none" start_char="4869" end_char="4875">viruses</TOKEN>
<TOKEN id="token-40-15" pos="word" morph="none" start_char="4877" end_char="4879">and</TOKEN>
<TOKEN id="token-40-16" pos="word" morph="none" start_char="4881" end_char="4887">passing</TOKEN>
<TOKEN id="token-40-17" pos="word" morph="none" start_char="4889" end_char="4892">them</TOKEN>
<TOKEN id="token-40-18" pos="word" morph="none" start_char="4894" end_char="4900">through</TOKEN>
<TOKEN id="token-40-19" pos="word" morph="none" start_char="4902" end_char="4905">live</TOKEN>
<TOKEN id="token-40-20" pos="word" morph="none" start_char="4907" end_char="4913">animals</TOKEN>
<TOKEN id="token-40-21" pos="word" morph="none" start_char="4915" end_char="4919">until</TOKEN>
<TOKEN id="token-40-22" pos="word" morph="none" start_char="4921" end_char="4924">they</TOKEN>
<TOKEN id="token-40-23" pos="word" morph="none" start_char="4926" end_char="4931">mutate</TOKEN>
<TOKEN id="token-40-24" pos="word" morph="none" start_char="4933" end_char="4936">into</TOKEN>
<TOKEN id="token-40-25" pos="word" morph="none" start_char="4938" end_char="4938">a</TOKEN>
<TOKEN id="token-40-26" pos="word" morph="none" start_char="4940" end_char="4943">form</TOKEN>
<TOKEN id="token-40-27" pos="word" morph="none" start_char="4945" end_char="4948">that</TOKEN>
<TOKEN id="token-40-28" pos="word" morph="none" start_char="4950" end_char="4954">could</TOKEN>
<TOKEN id="token-40-29" pos="word" morph="none" start_char="4956" end_char="4959">pose</TOKEN>
<TOKEN id="token-40-30" pos="word" morph="none" start_char="4961" end_char="4961">a</TOKEN>
<TOKEN id="token-40-31" pos="word" morph="none" start_char="4963" end_char="4970">pandemic</TOKEN>
<TOKEN id="token-40-32" pos="word" morph="none" start_char="4972" end_char="4977">threat</TOKEN>
<TOKEN id="token-40-33" pos="punct" morph="none" start_char="4978" end_char="4978">.</TOKEN>
</SEG>
<SEG id="segment-41" start_char="4980" end_char="5137">
<ORIGINAL_TEXT>Scientists used it to take a virus that was poorly transmitted among humans and make it into one that was highly transmissible—a hallmark of a pandemic virus.</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="4980" end_char="4989">Scientists</TOKEN>
<TOKEN id="token-41-1" pos="word" morph="none" start_char="4991" end_char="4994">used</TOKEN>
<TOKEN id="token-41-2" pos="word" morph="none" start_char="4996" end_char="4997">it</TOKEN>
<TOKEN id="token-41-3" pos="word" morph="none" start_char="4999" end_char="5000">to</TOKEN>
<TOKEN id="token-41-4" pos="word" morph="none" start_char="5002" end_char="5005">take</TOKEN>
<TOKEN id="token-41-5" pos="word" morph="none" start_char="5007" end_char="5007">a</TOKEN>
<TOKEN id="token-41-6" pos="word" morph="none" start_char="5009" end_char="5013">virus</TOKEN>
<TOKEN id="token-41-7" pos="word" morph="none" start_char="5015" end_char="5018">that</TOKEN>
<TOKEN id="token-41-8" pos="word" morph="none" start_char="5020" end_char="5022">was</TOKEN>
<TOKEN id="token-41-9" pos="word" morph="none" start_char="5024" end_char="5029">poorly</TOKEN>
<TOKEN id="token-41-10" pos="word" morph="none" start_char="5031" end_char="5041">transmitted</TOKEN>
<TOKEN id="token-41-11" pos="word" morph="none" start_char="5043" end_char="5047">among</TOKEN>
<TOKEN id="token-41-12" pos="word" morph="none" start_char="5049" end_char="5054">humans</TOKEN>
<TOKEN id="token-41-13" pos="word" morph="none" start_char="5056" end_char="5058">and</TOKEN>
<TOKEN id="token-41-14" pos="word" morph="none" start_char="5060" end_char="5063">make</TOKEN>
<TOKEN id="token-41-15" pos="word" morph="none" start_char="5065" end_char="5066">it</TOKEN>
<TOKEN id="token-41-16" pos="word" morph="none" start_char="5068" end_char="5071">into</TOKEN>
<TOKEN id="token-41-17" pos="word" morph="none" start_char="5073" end_char="5075">one</TOKEN>
<TOKEN id="token-41-18" pos="word" morph="none" start_char="5077" end_char="5080">that</TOKEN>
<TOKEN id="token-41-19" pos="word" morph="none" start_char="5082" end_char="5084">was</TOKEN>
<TOKEN id="token-41-20" pos="word" morph="none" start_char="5086" end_char="5091">highly</TOKEN>
<TOKEN id="token-41-21" pos="unknown" morph="none" start_char="5093" end_char="5107">transmissible—a</TOKEN>
<TOKEN id="token-41-22" pos="word" morph="none" start_char="5109" end_char="5116">hallmark</TOKEN>
<TOKEN id="token-41-23" pos="word" morph="none" start_char="5118" end_char="5119">of</TOKEN>
<TOKEN id="token-41-24" pos="word" morph="none" start_char="5121" end_char="5121">a</TOKEN>
<TOKEN id="token-41-25" pos="word" morph="none" start_char="5123" end_char="5130">pandemic</TOKEN>
<TOKEN id="token-41-26" pos="word" morph="none" start_char="5132" end_char="5136">virus</TOKEN>
<TOKEN id="token-41-27" pos="punct" morph="none" start_char="5137" end_char="5137">.</TOKEN>
</SEG>
<SEG id="segment-42" start_char="5139" end_char="5297">
<ORIGINAL_TEXT>This work was done by infecting a series of ferrets, allowing the virus to mutate until a ferret that hadn't been deliberately infected contracted the disease.</ORIGINAL_TEXT>
<TOKEN id="token-42-0" pos="word" morph="none" start_char="5139" end_char="5142">This</TOKEN>
<TOKEN id="token-42-1" pos="word" morph="none" start_char="5144" end_char="5147">work</TOKEN>
<TOKEN id="token-42-2" pos="word" morph="none" start_char="5149" end_char="5151">was</TOKEN>
<TOKEN id="token-42-3" pos="word" morph="none" start_char="5153" end_char="5156">done</TOKEN>
<TOKEN id="token-42-4" pos="word" morph="none" start_char="5158" end_char="5159">by</TOKEN>
<TOKEN id="token-42-5" pos="word" morph="none" start_char="5161" end_char="5169">infecting</TOKEN>
<TOKEN id="token-42-6" pos="word" morph="none" start_char="5171" end_char="5171">a</TOKEN>
<TOKEN id="token-42-7" pos="word" morph="none" start_char="5173" end_char="5178">series</TOKEN>
<TOKEN id="token-42-8" pos="word" morph="none" start_char="5180" end_char="5181">of</TOKEN>
<TOKEN id="token-42-9" pos="word" morph="none" start_char="5183" end_char="5189">ferrets</TOKEN>
<TOKEN id="token-42-10" pos="punct" morph="none" start_char="5190" end_char="5190">,</TOKEN>
<TOKEN id="token-42-11" pos="word" morph="none" start_char="5192" end_char="5199">allowing</TOKEN>
<TOKEN id="token-42-12" pos="word" morph="none" start_char="5201" end_char="5203">the</TOKEN>
<TOKEN id="token-42-13" pos="word" morph="none" start_char="5205" end_char="5209">virus</TOKEN>
<TOKEN id="token-42-14" pos="word" morph="none" start_char="5211" end_char="5212">to</TOKEN>
<TOKEN id="token-42-15" pos="word" morph="none" start_char="5214" end_char="5219">mutate</TOKEN>
<TOKEN id="token-42-16" pos="word" morph="none" start_char="5221" end_char="5225">until</TOKEN>
<TOKEN id="token-42-17" pos="word" morph="none" start_char="5227" end_char="5227">a</TOKEN>
<TOKEN id="token-42-18" pos="word" morph="none" start_char="5229" end_char="5234">ferret</TOKEN>
<TOKEN id="token-42-19" pos="word" morph="none" start_char="5236" end_char="5239">that</TOKEN>
<TOKEN id="token-42-20" pos="word" morph="none" start_char="5241" end_char="5246">hadn't</TOKEN>
<TOKEN id="token-42-21" pos="word" morph="none" start_char="5248" end_char="5251">been</TOKEN>
<TOKEN id="token-42-22" pos="word" morph="none" start_char="5253" end_char="5264">deliberately</TOKEN>
<TOKEN id="token-42-23" pos="word" morph="none" start_char="5266" end_char="5273">infected</TOKEN>
<TOKEN id="token-42-24" pos="word" morph="none" start_char="5275" end_char="5284">contracted</TOKEN>
<TOKEN id="token-42-25" pos="word" morph="none" start_char="5286" end_char="5288">the</TOKEN>
<TOKEN id="token-42-26" pos="word" morph="none" start_char="5290" end_char="5296">disease</TOKEN>
<TOKEN id="token-42-27" pos="punct" morph="none" start_char="5297" end_char="5297">.</TOKEN>
</SEG>
<SEG id="segment-43" start_char="5300" end_char="5362">
<ORIGINAL_TEXT>The work entailed risks that worried even seasoned researchers.</ORIGINAL_TEXT>
<TOKEN id="token-43-0" pos="word" morph="none" start_char="5300" end_char="5302">The</TOKEN>
<TOKEN id="token-43-1" pos="word" morph="none" start_char="5304" end_char="5307">work</TOKEN>
<TOKEN id="token-43-2" pos="word" morph="none" start_char="5309" end_char="5316">entailed</TOKEN>
<TOKEN id="token-43-3" pos="word" morph="none" start_char="5318" end_char="5322">risks</TOKEN>
<TOKEN id="token-43-4" pos="word" morph="none" start_char="5324" end_char="5327">that</TOKEN>
<TOKEN id="token-43-5" pos="word" morph="none" start_char="5329" end_char="5335">worried</TOKEN>
<TOKEN id="token-43-6" pos="word" morph="none" start_char="5337" end_char="5340">even</TOKEN>
<TOKEN id="token-43-7" pos="word" morph="none" start_char="5342" end_char="5349">seasoned</TOKEN>
<TOKEN id="token-43-8" pos="word" morph="none" start_char="5351" end_char="5361">researchers</TOKEN>
<TOKEN id="token-43-9" pos="punct" morph="none" start_char="5362" end_char="5362">.</TOKEN>
</SEG>
<SEG id="segment-44" start_char="5364" end_char="5421">
<ORIGINAL_TEXT>More than 200 scientists called for the work to be halted.</ORIGINAL_TEXT>
<TOKEN id="token-44-0" pos="word" morph="none" start_char="5364" end_char="5367">More</TOKEN>
<TOKEN id="token-44-1" pos="word" morph="none" start_char="5369" end_char="5372">than</TOKEN>
<TOKEN id="token-44-2" pos="word" morph="none" start_char="5374" end_char="5376">200</TOKEN>
<TOKEN id="token-44-3" pos="word" morph="none" start_char="5378" end_char="5387">scientists</TOKEN>
<TOKEN id="token-44-4" pos="word" morph="none" start_char="5389" end_char="5394">called</TOKEN>
<TOKEN id="token-44-5" pos="word" morph="none" start_char="5396" end_char="5398">for</TOKEN>
<TOKEN id="token-44-6" pos="word" morph="none" start_char="5400" end_char="5402">the</TOKEN>
<TOKEN id="token-44-7" pos="word" morph="none" start_char="5404" end_char="5407">work</TOKEN>
<TOKEN id="token-44-8" pos="word" morph="none" start_char="5409" end_char="5410">to</TOKEN>
<TOKEN id="token-44-9" pos="word" morph="none" start_char="5412" end_char="5413">be</TOKEN>
<TOKEN id="token-44-10" pos="word" morph="none" start_char="5415" end_char="5420">halted</TOKEN>
<TOKEN id="token-44-11" pos="punct" morph="none" start_char="5421" end_char="5421">.</TOKEN>
</SEG>
<SEG id="segment-45" start_char="5423" end_char="5540">
<ORIGINAL_TEXT>The problem, they said, is that it increased the likelihood that a pandemic would occur through a laboratory accident.</ORIGINAL_TEXT>
<TOKEN id="token-45-0" pos="word" morph="none" start_char="5423" end_char="5425">The</TOKEN>
<TOKEN id="token-45-1" pos="word" morph="none" start_char="5427" end_char="5433">problem</TOKEN>
<TOKEN id="token-45-2" pos="punct" morph="none" start_char="5434" end_char="5434">,</TOKEN>
<TOKEN id="token-45-3" pos="word" morph="none" start_char="5436" end_char="5439">they</TOKEN>
<TOKEN id="token-45-4" pos="word" morph="none" start_char="5441" end_char="5444">said</TOKEN>
<TOKEN id="token-45-5" pos="punct" morph="none" start_char="5445" end_char="5445">,</TOKEN>
<TOKEN id="token-45-6" pos="word" morph="none" start_char="5447" end_char="5448">is</TOKEN>
<TOKEN id="token-45-7" pos="word" morph="none" start_char="5450" end_char="5453">that</TOKEN>
<TOKEN id="token-45-8" pos="word" morph="none" start_char="5455" end_char="5456">it</TOKEN>
<TOKEN id="token-45-9" pos="word" morph="none" start_char="5458" end_char="5466">increased</TOKEN>
<TOKEN id="token-45-10" pos="word" morph="none" start_char="5468" end_char="5470">the</TOKEN>
<TOKEN id="token-45-11" pos="word" morph="none" start_char="5472" end_char="5481">likelihood</TOKEN>
<TOKEN id="token-45-12" pos="word" morph="none" start_char="5483" end_char="5486">that</TOKEN>
<TOKEN id="token-45-13" pos="word" morph="none" start_char="5488" end_char="5488">a</TOKEN>
<TOKEN id="token-45-14" pos="word" morph="none" start_char="5490" end_char="5497">pandemic</TOKEN>
<TOKEN id="token-45-15" pos="word" morph="none" start_char="5499" end_char="5503">would</TOKEN>
<TOKEN id="token-45-16" pos="word" morph="none" start_char="5505" end_char="5509">occur</TOKEN>
<TOKEN id="token-45-17" pos="word" morph="none" start_char="5511" end_char="5517">through</TOKEN>
<TOKEN id="token-45-18" pos="word" morph="none" start_char="5519" end_char="5519">a</TOKEN>
<TOKEN id="token-45-19" pos="word" morph="none" start_char="5521" end_char="5530">laboratory</TOKEN>
<TOKEN id="token-45-20" pos="word" morph="none" start_char="5532" end_char="5539">accident</TOKEN>
<TOKEN id="token-45-21" pos="punct" morph="none" start_char="5540" end_char="5540">.</TOKEN>
</SEG>
<SEG id="segment-46" start_char="5543" end_char="5570">
<ORIGINAL_TEXT>Dr. Fauci defended the work.</ORIGINAL_TEXT>
<TOKEN id="token-46-0" pos="word" morph="none" start_char="5543" end_char="5544">Dr</TOKEN>
<TOKEN id="token-46-1" pos="punct" morph="none" start_char="5545" end_char="5545">.</TOKEN>
<TOKEN id="token-46-2" pos="word" morph="none" start_char="5547" end_char="5551">Fauci</TOKEN>
<TOKEN id="token-46-3" pos="word" morph="none" start_char="5553" end_char="5560">defended</TOKEN>
<TOKEN id="token-46-4" pos="word" morph="none" start_char="5562" end_char="5564">the</TOKEN>
<TOKEN id="token-46-5" pos="word" morph="none" start_char="5566" end_char="5569">work</TOKEN>
<TOKEN id="token-46-6" pos="punct" morph="none" start_char="5570" end_char="5570">.</TOKEN>
</SEG>
<SEG id="segment-47" start_char="5572" end_char="5835">
<ORIGINAL_TEXT>"[D]etermining the molecular Achilles' heel of these viruses can allow scientists to identify novel antiviral drug targets that could be used to prevent infection in those at risk or to better treat those who become infected," wrote Fauci and two co-authors in the</ORIGINAL_TEXT>
<TOKEN id="token-47-0" pos="punct" morph="none" start_char="5572" end_char="5573">"[</TOKEN>
<TOKEN id="token-47-1" pos="unknown" morph="none" start_char="5574" end_char="5585">D]etermining</TOKEN>
<TOKEN id="token-47-2" pos="word" morph="none" start_char="5587" end_char="5589">the</TOKEN>
<TOKEN id="token-47-3" pos="word" morph="none" start_char="5591" end_char="5599">molecular</TOKEN>
<TOKEN id="token-47-4" pos="word" morph="none" start_char="5601" end_char="5608">Achilles</TOKEN>
<TOKEN id="token-47-5" pos="punct" morph="none" start_char="5609" end_char="5609">'</TOKEN>
<TOKEN id="token-47-6" pos="word" morph="none" start_char="5611" end_char="5614">heel</TOKEN>
<TOKEN id="token-47-7" pos="word" morph="none" start_char="5616" end_char="5617">of</TOKEN>
<TOKEN id="token-47-8" pos="word" morph="none" start_char="5619" end_char="5623">these</TOKEN>
<TOKEN id="token-47-9" pos="word" morph="none" start_char="5625" end_char="5631">viruses</TOKEN>
<TOKEN id="token-47-10" pos="word" morph="none" start_char="5633" end_char="5635">can</TOKEN>
<TOKEN id="token-47-11" pos="word" morph="none" start_char="5637" end_char="5641">allow</TOKEN>
<TOKEN id="token-47-12" pos="word" morph="none" start_char="5643" end_char="5652">scientists</TOKEN>
<TOKEN id="token-47-13" pos="word" morph="none" start_char="5654" end_char="5655">to</TOKEN>
<TOKEN id="token-47-14" pos="word" morph="none" start_char="5657" end_char="5664">identify</TOKEN>
<TOKEN id="token-47-15" pos="word" morph="none" start_char="5666" end_char="5670">novel</TOKEN>
<TOKEN id="token-47-16" pos="word" morph="none" start_char="5672" end_char="5680">antiviral</TOKEN>
<TOKEN id="token-47-17" pos="word" morph="none" start_char="5682" end_char="5685">drug</TOKEN>
<TOKEN id="token-47-18" pos="word" morph="none" start_char="5687" end_char="5693">targets</TOKEN>
<TOKEN id="token-47-19" pos="word" morph="none" start_char="5695" end_char="5698">that</TOKEN>
<TOKEN id="token-47-20" pos="word" morph="none" start_char="5700" end_char="5704">could</TOKEN>
<TOKEN id="token-47-21" pos="word" morph="none" start_char="5706" end_char="5707">be</TOKEN>
<TOKEN id="token-47-22" pos="word" morph="none" start_char="5709" end_char="5712">used</TOKEN>
<TOKEN id="token-47-23" pos="word" morph="none" start_char="5714" end_char="5715">to</TOKEN>
<TOKEN id="token-47-24" pos="word" morph="none" start_char="5717" end_char="5723">prevent</TOKEN>
<TOKEN id="token-47-25" pos="word" morph="none" start_char="5725" end_char="5733">infection</TOKEN>
<TOKEN id="token-47-26" pos="word" morph="none" start_char="5735" end_char="5736">in</TOKEN>
<TOKEN id="token-47-27" pos="word" morph="none" start_char="5738" end_char="5742">those</TOKEN>
<TOKEN id="token-47-28" pos="word" morph="none" start_char="5744" end_char="5745">at</TOKEN>
<TOKEN id="token-47-29" pos="word" morph="none" start_char="5747" end_char="5750">risk</TOKEN>
<TOKEN id="token-47-30" pos="word" morph="none" start_char="5752" end_char="5753">or</TOKEN>
<TOKEN id="token-47-31" pos="word" morph="none" start_char="5755" end_char="5756">to</TOKEN>
<TOKEN id="token-47-32" pos="word" morph="none" start_char="5758" end_char="5763">better</TOKEN>
<TOKEN id="token-47-33" pos="word" morph="none" start_char="5765" end_char="5769">treat</TOKEN>
<TOKEN id="token-47-34" pos="word" morph="none" start_char="5771" end_char="5775">those</TOKEN>
<TOKEN id="token-47-35" pos="word" morph="none" start_char="5777" end_char="5779">who</TOKEN>
<TOKEN id="token-47-36" pos="word" morph="none" start_char="5781" end_char="5786">become</TOKEN>
<TOKEN id="token-47-37" pos="word" morph="none" start_char="5788" end_char="5795">infected</TOKEN>
<TOKEN id="token-47-38" pos="punct" morph="none" start_char="5796" end_char="5797">,"</TOKEN>
<TOKEN id="token-47-39" pos="word" morph="none" start_char="5799" end_char="5803">wrote</TOKEN>
<TOKEN id="token-47-40" pos="word" morph="none" start_char="5805" end_char="5809">Fauci</TOKEN>
<TOKEN id="token-47-41" pos="word" morph="none" start_char="5811" end_char="5813">and</TOKEN>
<TOKEN id="token-47-42" pos="word" morph="none" start_char="5815" end_char="5817">two</TOKEN>
<TOKEN id="token-47-43" pos="unknown" morph="none" start_char="5819" end_char="5828">co-authors</TOKEN>
<TOKEN id="token-47-44" pos="word" morph="none" start_char="5830" end_char="5831">in</TOKEN>
<TOKEN id="token-47-45" pos="word" morph="none" start_char="5833" end_char="5835">the</TOKEN>
</SEG>
<SEG id="segment-48" start_char="5838" end_char="5852">
<ORIGINAL_TEXT>Washington Post</ORIGINAL_TEXT>
<TOKEN id="token-48-0" pos="word" morph="none" start_char="5838" end_char="5847">Washington</TOKEN>
<TOKEN id="token-48-1" pos="word" morph="none" start_char="5849" end_char="5852">Post</TOKEN>
</SEG>
<SEG id="segment-49" start_char="5855" end_char="5875">
<ORIGINAL_TEXT>on December 30, 2011.</ORIGINAL_TEXT>
<TOKEN id="token-49-0" pos="word" morph="none" start_char="5855" end_char="5856">on</TOKEN>
<TOKEN id="token-49-1" pos="word" morph="none" start_char="5858" end_char="5865">December</TOKEN>
<TOKEN id="token-49-2" pos="word" morph="none" start_char="5867" end_char="5868">30</TOKEN>
<TOKEN id="token-49-3" pos="punct" morph="none" start_char="5869" end_char="5869">,</TOKEN>
<TOKEN id="token-49-4" pos="word" morph="none" start_char="5871" end_char="5874">2011</TOKEN>
<TOKEN id="token-49-5" pos="punct" morph="none" start_char="5875" end_char="5875">.</TOKEN>
</SEG>
<SEG id="segment-50" start_char="5877" end_char="6141">
<ORIGINAL_TEXT>"Decades of experience tells us that disseminating information gained through biomedical research to legitimate scientists and health officials provides a critical foundation for generating appropriate countermeasures and, ultimately, protecting the public health."</ORIGINAL_TEXT>
<TOKEN id="token-50-0" pos="punct" morph="none" start_char="5877" end_char="5877">"</TOKEN>
<TOKEN id="token-50-1" pos="word" morph="none" start_char="5878" end_char="5884">Decades</TOKEN>
<TOKEN id="token-50-2" pos="word" morph="none" start_char="5886" end_char="5887">of</TOKEN>
<TOKEN id="token-50-3" pos="word" morph="none" start_char="5889" end_char="5898">experience</TOKEN>
<TOKEN id="token-50-4" pos="word" morph="none" start_char="5900" end_char="5904">tells</TOKEN>
<TOKEN id="token-50-5" pos="word" morph="none" start_char="5906" end_char="5907">us</TOKEN>
<TOKEN id="token-50-6" pos="word" morph="none" start_char="5909" end_char="5912">that</TOKEN>
<TOKEN id="token-50-7" pos="word" morph="none" start_char="5914" end_char="5926">disseminating</TOKEN>
<TOKEN id="token-50-8" pos="word" morph="none" start_char="5928" end_char="5938">information</TOKEN>
<TOKEN id="token-50-9" pos="word" morph="none" start_char="5940" end_char="5945">gained</TOKEN>
<TOKEN id="token-50-10" pos="word" morph="none" start_char="5947" end_char="5953">through</TOKEN>
<TOKEN id="token-50-11" pos="word" morph="none" start_char="5955" end_char="5964">biomedical</TOKEN>
<TOKEN id="token-50-12" pos="word" morph="none" start_char="5966" end_char="5973">research</TOKEN>
<TOKEN id="token-50-13" pos="word" morph="none" start_char="5975" end_char="5976">to</TOKEN>
<TOKEN id="token-50-14" pos="word" morph="none" start_char="5978" end_char="5987">legitimate</TOKEN>
<TOKEN id="token-50-15" pos="word" morph="none" start_char="5989" end_char="5998">scientists</TOKEN>
<TOKEN id="token-50-16" pos="word" morph="none" start_char="6000" end_char="6002">and</TOKEN>
<TOKEN id="token-50-17" pos="word" morph="none" start_char="6004" end_char="6009">health</TOKEN>
<TOKEN id="token-50-18" pos="word" morph="none" start_char="6011" end_char="6019">officials</TOKEN>
<TOKEN id="token-50-19" pos="word" morph="none" start_char="6021" end_char="6028">provides</TOKEN>
<TOKEN id="token-50-20" pos="word" morph="none" start_char="6030" end_char="6030">a</TOKEN>
<TOKEN id="token-50-21" pos="word" morph="none" start_char="6032" end_char="6039">critical</TOKEN>
<TOKEN id="token-50-22" pos="word" morph="none" start_char="6041" end_char="6050">foundation</TOKEN>
<TOKEN id="token-50-23" pos="word" morph="none" start_char="6052" end_char="6054">for</TOKEN>
<TOKEN id="token-50-24" pos="word" morph="none" start_char="6056" end_char="6065">generating</TOKEN>
<TOKEN id="token-50-25" pos="word" morph="none" start_char="6067" end_char="6077">appropriate</TOKEN>
<TOKEN id="token-50-26" pos="word" morph="none" start_char="6079" end_char="6093">countermeasures</TOKEN>
<TOKEN id="token-50-27" pos="word" morph="none" start_char="6095" end_char="6097">and</TOKEN>
<TOKEN id="token-50-28" pos="punct" morph="none" start_char="6098" end_char="6098">,</TOKEN>
<TOKEN id="token-50-29" pos="word" morph="none" start_char="6100" end_char="6109">ultimately</TOKEN>
<TOKEN id="token-50-30" pos="punct" morph="none" start_char="6110" end_char="6110">,</TOKEN>
<TOKEN id="token-50-31" pos="word" morph="none" start_char="6112" end_char="6121">protecting</TOKEN>
<TOKEN id="token-50-32" pos="word" morph="none" start_char="6123" end_char="6125">the</TOKEN>
<TOKEN id="token-50-33" pos="word" morph="none" start_char="6127" end_char="6132">public</TOKEN>
<TOKEN id="token-50-34" pos="word" morph="none" start_char="6134" end_char="6139">health</TOKEN>
<TOKEN id="token-50-35" pos="punct" morph="none" start_char="6140" end_char="6141">."</TOKEN>
</SEG>
<SEG id="segment-51" start_char="6144" end_char="6308">
<ORIGINAL_TEXT>Nevertheless, in 2014, under pressure from the Obama administration, the National of Institutes of Health instituted a moratorium on the work, suspending 21 studies.</ORIGINAL_TEXT>
<TOKEN id="token-51-0" pos="word" morph="none" start_char="6144" end_char="6155">Nevertheless</TOKEN>
<TOKEN id="token-51-1" pos="punct" morph="none" start_char="6156" end_char="6156">,</TOKEN>
<TOKEN id="token-51-2" pos="word" morph="none" start_char="6158" end_char="6159">in</TOKEN>
<TOKEN id="token-51-3" pos="word" morph="none" start_char="6161" end_char="6164">2014</TOKEN>
<TOKEN id="token-51-4" pos="punct" morph="none" start_char="6165" end_char="6165">,</TOKEN>
<TOKEN id="token-51-5" pos="word" morph="none" start_char="6167" end_char="6171">under</TOKEN>
<TOKEN id="token-51-6" pos="word" morph="none" start_char="6173" end_char="6180">pressure</TOKEN>
<TOKEN id="token-51-7" pos="word" morph="none" start_char="6182" end_char="6185">from</TOKEN>
<TOKEN id="token-51-8" pos="word" morph="none" start_char="6187" end_char="6189">the</TOKEN>
<TOKEN id="token-51-9" pos="word" morph="none" start_char="6191" end_char="6195">Obama</TOKEN>
<TOKEN id="token-51-10" pos="word" morph="none" start_char="6197" end_char="6210">administration</TOKEN>
<TOKEN id="token-51-11" pos="punct" morph="none" start_char="6211" end_char="6211">,</TOKEN>
<TOKEN id="token-51-12" pos="word" morph="none" start_char="6213" end_char="6215">the</TOKEN>
<TOKEN id="token-51-13" pos="word" morph="none" start_char="6217" end_char="6224">National</TOKEN>
<TOKEN id="token-51-14" pos="word" morph="none" start_char="6226" end_char="6227">of</TOKEN>
<TOKEN id="token-51-15" pos="word" morph="none" start_char="6229" end_char="6238">Institutes</TOKEN>
<TOKEN id="token-51-16" pos="word" morph="none" start_char="6240" end_char="6241">of</TOKEN>
<TOKEN id="token-51-17" pos="word" morph="none" start_char="6243" end_char="6248">Health</TOKEN>
<TOKEN id="token-51-18" pos="word" morph="none" start_char="6250" end_char="6259">instituted</TOKEN>
<TOKEN id="token-51-19" pos="word" morph="none" start_char="6261" end_char="6261">a</TOKEN>
<TOKEN id="token-51-20" pos="word" morph="none" start_char="6263" end_char="6272">moratorium</TOKEN>
<TOKEN id="token-51-21" pos="word" morph="none" start_char="6274" end_char="6275">on</TOKEN>
<TOKEN id="token-51-22" pos="word" morph="none" start_char="6277" end_char="6279">the</TOKEN>
<TOKEN id="token-51-23" pos="word" morph="none" start_char="6281" end_char="6284">work</TOKEN>
<TOKEN id="token-51-24" pos="punct" morph="none" start_char="6285" end_char="6285">,</TOKEN>
<TOKEN id="token-51-25" pos="word" morph="none" start_char="6287" end_char="6296">suspending</TOKEN>
<TOKEN id="token-51-26" pos="word" morph="none" start_char="6298" end_char="6299">21</TOKEN>
<TOKEN id="token-51-27" pos="word" morph="none" start_char="6301" end_char="6307">studies</TOKEN>
<TOKEN id="token-51-28" pos="punct" morph="none" start_char="6308" end_char="6308">.</TOKEN>
</SEG>
<SEG id="segment-52" start_char="6311" end_char="6477">
<ORIGINAL_TEXT>Three years later, though—in December 2017—the NIH ended the moratorium and the second phase of the NIAID project, which included the gain-of-function research, began.</ORIGINAL_TEXT>
<TOKEN id="token-52-0" pos="word" morph="none" start_char="6311" end_char="6315">Three</TOKEN>
<TOKEN id="token-52-1" pos="word" morph="none" start_char="6317" end_char="6321">years</TOKEN>
<TOKEN id="token-52-2" pos="word" morph="none" start_char="6323" end_char="6327">later</TOKEN>
<TOKEN id="token-52-3" pos="punct" morph="none" start_char="6328" end_char="6328">,</TOKEN>
<TOKEN id="token-52-4" pos="unknown" morph="none" start_char="6330" end_char="6338">though—in</TOKEN>
<TOKEN id="token-52-5" pos="word" morph="none" start_char="6340" end_char="6347">December</TOKEN>
<TOKEN id="token-52-6" pos="unknown" morph="none" start_char="6349" end_char="6356">2017—the</TOKEN>
<TOKEN id="token-52-7" pos="word" morph="none" start_char="6358" end_char="6360">NIH</TOKEN>
<TOKEN id="token-52-8" pos="word" morph="none" start_char="6362" end_char="6366">ended</TOKEN>
<TOKEN id="token-52-9" pos="word" morph="none" start_char="6368" end_char="6370">the</TOKEN>
<TOKEN id="token-52-10" pos="word" morph="none" start_char="6372" end_char="6381">moratorium</TOKEN>
<TOKEN id="token-52-11" pos="word" morph="none" start_char="6383" end_char="6385">and</TOKEN>
<TOKEN id="token-52-12" pos="word" morph="none" start_char="6387" end_char="6389">the</TOKEN>
<TOKEN id="token-52-13" pos="word" morph="none" start_char="6391" end_char="6396">second</TOKEN>
<TOKEN id="token-52-14" pos="word" morph="none" start_char="6398" end_char="6402">phase</TOKEN>
<TOKEN id="token-52-15" pos="word" morph="none" start_char="6404" end_char="6405">of</TOKEN>
<TOKEN id="token-52-16" pos="word" morph="none" start_char="6407" end_char="6409">the</TOKEN>
<TOKEN id="token-52-17" pos="word" morph="none" start_char="6411" end_char="6415">NIAID</TOKEN>
<TOKEN id="token-52-18" pos="word" morph="none" start_char="6417" end_char="6423">project</TOKEN>
<TOKEN id="token-52-19" pos="punct" morph="none" start_char="6424" end_char="6424">,</TOKEN>
<TOKEN id="token-52-20" pos="word" morph="none" start_char="6426" end_char="6430">which</TOKEN>
<TOKEN id="token-52-21" pos="word" morph="none" start_char="6432" end_char="6439">included</TOKEN>
<TOKEN id="token-52-22" pos="word" morph="none" start_char="6441" end_char="6443">the</TOKEN>
<TOKEN id="token-52-23" pos="unknown" morph="none" start_char="6445" end_char="6460">gain-of-function</TOKEN>
<TOKEN id="token-52-24" pos="word" morph="none" start_char="6462" end_char="6469">research</TOKEN>
<TOKEN id="token-52-25" pos="punct" morph="none" start_char="6470" end_char="6470">,</TOKEN>
<TOKEN id="token-52-26" pos="word" morph="none" start_char="6472" end_char="6476">began</TOKEN>
<TOKEN id="token-52-27" pos="punct" morph="none" start_char="6477" end_char="6477">.</TOKEN>
</SEG>
<SEG id="segment-53" start_char="6479" end_char="6668">
<ORIGINAL_TEXT>The NIH established a framework for determining how the research would go forward: scientists have to get approval from a panel of experts, who would decide whether the risks were justified.</ORIGINAL_TEXT>
<TOKEN id="token-53-0" pos="word" morph="none" start_char="6479" end_char="6481">The</TOKEN>
<TOKEN id="token-53-1" pos="word" morph="none" start_char="6483" end_char="6485">NIH</TOKEN>
<TOKEN id="token-53-2" pos="word" morph="none" start_char="6487" end_char="6497">established</TOKEN>
<TOKEN id="token-53-3" pos="word" morph="none" start_char="6499" end_char="6499">a</TOKEN>
<TOKEN id="token-53-4" pos="word" morph="none" start_char="6501" end_char="6509">framework</TOKEN>
<TOKEN id="token-53-5" pos="word" morph="none" start_char="6511" end_char="6513">for</TOKEN>
<TOKEN id="token-53-6" pos="word" morph="none" start_char="6515" end_char="6525">determining</TOKEN>
<TOKEN id="token-53-7" pos="word" morph="none" start_char="6527" end_char="6529">how</TOKEN>
<TOKEN id="token-53-8" pos="word" morph="none" start_char="6531" end_char="6533">the</TOKEN>
<TOKEN id="token-53-9" pos="word" morph="none" start_char="6535" end_char="6542">research</TOKEN>
<TOKEN id="token-53-10" pos="word" morph="none" start_char="6544" end_char="6548">would</TOKEN>
<TOKEN id="token-53-11" pos="word" morph="none" start_char="6550" end_char="6551">go</TOKEN>
<TOKEN id="token-53-12" pos="word" morph="none" start_char="6553" end_char="6559">forward</TOKEN>
<TOKEN id="token-53-13" pos="punct" morph="none" start_char="6560" end_char="6560">:</TOKEN>
<TOKEN id="token-53-14" pos="word" morph="none" start_char="6562" end_char="6571">scientists</TOKEN>
<TOKEN id="token-53-15" pos="word" morph="none" start_char="6573" end_char="6576">have</TOKEN>
<TOKEN id="token-53-16" pos="word" morph="none" start_char="6578" end_char="6579">to</TOKEN>
<TOKEN id="token-53-17" pos="word" morph="none" start_char="6581" end_char="6583">get</TOKEN>
<TOKEN id="token-53-18" pos="word" morph="none" start_char="6585" end_char="6592">approval</TOKEN>
<TOKEN id="token-53-19" pos="word" morph="none" start_char="6594" end_char="6597">from</TOKEN>
<TOKEN id="token-53-20" pos="word" morph="none" start_char="6599" end_char="6599">a</TOKEN>
<TOKEN id="token-53-21" pos="word" morph="none" start_char="6601" end_char="6605">panel</TOKEN>
<TOKEN id="token-53-22" pos="word" morph="none" start_char="6607" end_char="6608">of</TOKEN>
<TOKEN id="token-53-23" pos="word" morph="none" start_char="6610" end_char="6616">experts</TOKEN>
<TOKEN id="token-53-24" pos="punct" morph="none" start_char="6617" end_char="6617">,</TOKEN>
<TOKEN id="token-53-25" pos="word" morph="none" start_char="6619" end_char="6621">who</TOKEN>
<TOKEN id="token-53-26" pos="word" morph="none" start_char="6623" end_char="6627">would</TOKEN>
<TOKEN id="token-53-27" pos="word" morph="none" start_char="6629" end_char="6634">decide</TOKEN>
<TOKEN id="token-53-28" pos="word" morph="none" start_char="6636" end_char="6642">whether</TOKEN>
<TOKEN id="token-53-29" pos="word" morph="none" start_char="6644" end_char="6646">the</TOKEN>
<TOKEN id="token-53-30" pos="word" morph="none" start_char="6648" end_char="6652">risks</TOKEN>
<TOKEN id="token-53-31" pos="word" morph="none" start_char="6654" end_char="6657">were</TOKEN>
<TOKEN id="token-53-32" pos="word" morph="none" start_char="6659" end_char="6667">justified</TOKEN>
<TOKEN id="token-53-33" pos="punct" morph="none" start_char="6668" end_char="6668">.</TOKEN>
</SEG>
<SEG id="segment-54" start_char="6671" end_char="6757">
<ORIGINAL_TEXT>The reviews were indeed conducted—but in secret, for which the NIH has drawn criticism.</ORIGINAL_TEXT>
<TOKEN id="token-54-0" pos="word" morph="none" start_char="6671" end_char="6673">The</TOKEN>
<TOKEN id="token-54-1" pos="word" morph="none" start_char="6675" end_char="6681">reviews</TOKEN>
<TOKEN id="token-54-2" pos="word" morph="none" start_char="6683" end_char="6686">were</TOKEN>
<TOKEN id="token-54-3" pos="word" morph="none" start_char="6688" end_char="6693">indeed</TOKEN>
<TOKEN id="token-54-4" pos="unknown" morph="none" start_char="6695" end_char="6707">conducted—but</TOKEN>
<TOKEN id="token-54-5" pos="word" morph="none" start_char="6709" end_char="6710">in</TOKEN>
<TOKEN id="token-54-6" pos="word" morph="none" start_char="6712" end_char="6717">secret</TOKEN>
<TOKEN id="token-54-7" pos="punct" morph="none" start_char="6718" end_char="6718">,</TOKEN>
<TOKEN id="token-54-8" pos="word" morph="none" start_char="6720" end_char="6722">for</TOKEN>
<TOKEN id="token-54-9" pos="word" morph="none" start_char="6724" end_char="6728">which</TOKEN>
<TOKEN id="token-54-10" pos="word" morph="none" start_char="6730" end_char="6732">the</TOKEN>
<TOKEN id="token-54-11" pos="word" morph="none" start_char="6734" end_char="6736">NIH</TOKEN>
<TOKEN id="token-54-12" pos="word" morph="none" start_char="6738" end_char="6740">has</TOKEN>
<TOKEN id="token-54-13" pos="word" morph="none" start_char="6742" end_char="6746">drawn</TOKEN>
<TOKEN id="token-54-14" pos="word" morph="none" start_char="6748" end_char="6756">criticism</TOKEN>
<TOKEN id="token-54-15" pos="punct" morph="none" start_char="6757" end_char="6757">.</TOKEN>
</SEG>
<SEG id="segment-55" start_char="6759" end_char="6793">
<ORIGINAL_TEXT>In early 2019, after a reporter for</ORIGINAL_TEXT>
<TOKEN id="token-55-0" pos="word" morph="none" start_char="6759" end_char="6760">In</TOKEN>
<TOKEN id="token-55-1" pos="word" morph="none" start_char="6762" end_char="6766">early</TOKEN>
<TOKEN id="token-55-2" pos="word" morph="none" start_char="6768" end_char="6771">2019</TOKEN>
<TOKEN id="token-55-3" pos="punct" morph="none" start_char="6772" end_char="6772">,</TOKEN>
<TOKEN id="token-55-4" pos="word" morph="none" start_char="6774" end_char="6778">after</TOKEN>
<TOKEN id="token-55-5" pos="word" morph="none" start_char="6780" end_char="6780">a</TOKEN>
<TOKEN id="token-55-6" pos="word" morph="none" start_char="6782" end_char="6789">reporter</TOKEN>
<TOKEN id="token-55-7" pos="word" morph="none" start_char="6791" end_char="6793">for</TOKEN>
</SEG>
<SEG id="segment-56" start_char="6796" end_char="6802">
<ORIGINAL_TEXT>Science</ORIGINAL_TEXT>
<TOKEN id="token-56-0" pos="word" morph="none" start_char="6796" end_char="6802">Science</TOKEN>
</SEG>
<SEG id="segment-57" start_char="6805" end_char="7003">
<ORIGINAL_TEXT>magazine discovered that the NIH had approved two influenza research projects that used gain of function methods, scientists who oppose this kind of research excoriated the NIH in an editorial in the</ORIGINAL_TEXT>
<TOKEN id="token-57-0" pos="word" morph="none" start_char="6805" end_char="6812">magazine</TOKEN>
<TOKEN id="token-57-1" pos="word" morph="none" start_char="6814" end_char="6823">discovered</TOKEN>
<TOKEN id="token-57-2" pos="word" morph="none" start_char="6825" end_char="6828">that</TOKEN>
<TOKEN id="token-57-3" pos="word" morph="none" start_char="6830" end_char="6832">the</TOKEN>
<TOKEN id="token-57-4" pos="word" morph="none" start_char="6834" end_char="6836">NIH</TOKEN>
<TOKEN id="token-57-5" pos="word" morph="none" start_char="6838" end_char="6840">had</TOKEN>
<TOKEN id="token-57-6" pos="word" morph="none" start_char="6842" end_char="6849">approved</TOKEN>
<TOKEN id="token-57-7" pos="word" morph="none" start_char="6851" end_char="6853">two</TOKEN>
<TOKEN id="token-57-8" pos="word" morph="none" start_char="6855" end_char="6863">influenza</TOKEN>
<TOKEN id="token-57-9" pos="word" morph="none" start_char="6865" end_char="6872">research</TOKEN>
<TOKEN id="token-57-10" pos="word" morph="none" start_char="6874" end_char="6881">projects</TOKEN>
<TOKEN id="token-57-11" pos="word" morph="none" start_char="6883" end_char="6886">that</TOKEN>
<TOKEN id="token-57-12" pos="word" morph="none" start_char="6888" end_char="6891">used</TOKEN>
<TOKEN id="token-57-13" pos="word" morph="none" start_char="6893" end_char="6896">gain</TOKEN>
<TOKEN id="token-57-14" pos="word" morph="none" start_char="6898" end_char="6899">of</TOKEN>
<TOKEN id="token-57-15" pos="word" morph="none" start_char="6901" end_char="6908">function</TOKEN>
<TOKEN id="token-57-16" pos="word" morph="none" start_char="6910" end_char="6916">methods</TOKEN>
<TOKEN id="token-57-17" pos="punct" morph="none" start_char="6917" end_char="6917">,</TOKEN>
<TOKEN id="token-57-18" pos="word" morph="none" start_char="6919" end_char="6928">scientists</TOKEN>
<TOKEN id="token-57-19" pos="word" morph="none" start_char="6930" end_char="6932">who</TOKEN>
<TOKEN id="token-57-20" pos="word" morph="none" start_char="6934" end_char="6939">oppose</TOKEN>
<TOKEN id="token-57-21" pos="word" morph="none" start_char="6941" end_char="6944">this</TOKEN>
<TOKEN id="token-57-22" pos="word" morph="none" start_char="6946" end_char="6949">kind</TOKEN>
<TOKEN id="token-57-23" pos="word" morph="none" start_char="6951" end_char="6952">of</TOKEN>
<TOKEN id="token-57-24" pos="word" morph="none" start_char="6954" end_char="6961">research</TOKEN>
<TOKEN id="token-57-25" pos="word" morph="none" start_char="6963" end_char="6972">excoriated</TOKEN>
<TOKEN id="token-57-26" pos="word" morph="none" start_char="6974" end_char="6976">the</TOKEN>
<TOKEN id="token-57-27" pos="word" morph="none" start_char="6978" end_char="6980">NIH</TOKEN>
<TOKEN id="token-57-28" pos="word" morph="none" start_char="6982" end_char="6983">in</TOKEN>
<TOKEN id="token-57-29" pos="word" morph="none" start_char="6985" end_char="6986">an</TOKEN>
<TOKEN id="token-57-30" pos="word" morph="none" start_char="6988" end_char="6996">editorial</TOKEN>
<TOKEN id="token-57-31" pos="word" morph="none" start_char="6998" end_char="6999">in</TOKEN>
<TOKEN id="token-57-32" pos="word" morph="none" start_char="7001" end_char="7003">the</TOKEN>
</SEG>
<SEG id="segment-58" start_char="7006" end_char="7020">
<ORIGINAL_TEXT>Washington Post</ORIGINAL_TEXT>
<TOKEN id="token-58-0" pos="word" morph="none" start_char="7006" end_char="7015">Washington</TOKEN>
<TOKEN id="token-58-1" pos="word" morph="none" start_char="7017" end_char="7020">Post</TOKEN>
</SEG>
<SEG id="segment-59" start_char="7023" end_char="7023">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN id="token-59-0" pos="punct" morph="none" start_char="7023" end_char="7023">.</TOKEN>
</SEG>
<SEG id="segment-60" start_char="7026" end_char="7186">
<ORIGINAL_TEXT>"We have serious doubts about whether these experiments should be conducted at all," wrote Tom Inglesby of Johns Hopkins University and Marc Lipsitch of Harvard.</ORIGINAL_TEXT>
<TOKEN id="token-60-0" pos="punct" morph="none" start_char="7026" end_char="7026">"</TOKEN>
<TOKEN id="token-60-1" pos="word" morph="none" start_char="7027" end_char="7028">We</TOKEN>
<TOKEN id="token-60-2" pos="word" morph="none" start_char="7030" end_char="7033">have</TOKEN>
<TOKEN id="token-60-3" pos="word" morph="none" start_char="7035" end_char="7041">serious</TOKEN>
<TOKEN id="token-60-4" pos="word" morph="none" start_char="7043" end_char="7048">doubts</TOKEN>
<TOKEN id="token-60-5" pos="word" morph="none" start_char="7050" end_char="7054">about</TOKEN>
<TOKEN id="token-60-6" pos="word" morph="none" start_char="7056" end_char="7062">whether</TOKEN>
<TOKEN id="token-60-7" pos="word" morph="none" start_char="7064" end_char="7068">these</TOKEN>
<TOKEN id="token-60-8" pos="word" morph="none" start_char="7070" end_char="7080">experiments</TOKEN>
<TOKEN id="token-60-9" pos="word" morph="none" start_char="7082" end_char="7087">should</TOKEN>
<TOKEN id="token-60-10" pos="word" morph="none" start_char="7089" end_char="7090">be</TOKEN>
<TOKEN id="token-60-11" pos="word" morph="none" start_char="7092" end_char="7100">conducted</TOKEN>
<TOKEN id="token-60-12" pos="word" morph="none" start_char="7102" end_char="7103">at</TOKEN>
<TOKEN id="token-60-13" pos="word" morph="none" start_char="7105" end_char="7107">all</TOKEN>
<TOKEN id="token-60-14" pos="punct" morph="none" start_char="7108" end_char="7109">,"</TOKEN>
<TOKEN id="token-60-15" pos="word" morph="none" start_char="7111" end_char="7115">wrote</TOKEN>
<TOKEN id="token-60-16" pos="word" morph="none" start_char="7117" end_char="7119">Tom</TOKEN>
<TOKEN id="token-60-17" pos="word" morph="none" start_char="7121" end_char="7128">Inglesby</TOKEN>
<TOKEN id="token-60-18" pos="word" morph="none" start_char="7130" end_char="7131">of</TOKEN>
<TOKEN id="token-60-19" pos="word" morph="none" start_char="7133" end_char="7137">Johns</TOKEN>
<TOKEN id="token-60-20" pos="word" morph="none" start_char="7139" end_char="7145">Hopkins</TOKEN>
<TOKEN id="token-60-21" pos="word" morph="none" start_char="7147" end_char="7156">University</TOKEN>
<TOKEN id="token-60-22" pos="word" morph="none" start_char="7158" end_char="7160">and</TOKEN>
<TOKEN id="token-60-23" pos="word" morph="none" start_char="7162" end_char="7165">Marc</TOKEN>
<TOKEN id="token-60-24" pos="word" morph="none" start_char="7167" end_char="7174">Lipsitch</TOKEN>
<TOKEN id="token-60-25" pos="word" morph="none" start_char="7176" end_char="7177">of</TOKEN>
<TOKEN id="token-60-26" pos="word" morph="none" start_char="7179" end_char="7185">Harvard</TOKEN>
<TOKEN id="token-60-27" pos="punct" morph="none" start_char="7186" end_char="7186">.</TOKEN>
</SEG>
<SEG id="segment-61" start_char="7188" end_char="7385">
<ORIGINAL_TEXT>"[W]ith deliberations kept behind closed doors, none of us will have the opportunity to understand how the government arrived at these decisions or to judge the rigor and integrity of that process."</ORIGINAL_TEXT>
<TOKEN id="token-61-0" pos="punct" morph="none" start_char="7188" end_char="7189">"[</TOKEN>
<TOKEN id="token-61-1" pos="unknown" morph="none" start_char="7190" end_char="7194">W]ith</TOKEN>
<TOKEN id="token-61-2" pos="word" morph="none" start_char="7196" end_char="7208">deliberations</TOKEN>
<TOKEN id="token-61-3" pos="word" morph="none" start_char="7210" end_char="7213">kept</TOKEN>
<TOKEN id="token-61-4" pos="word" morph="none" start_char="7215" end_char="7220">behind</TOKEN>
<TOKEN id="token-61-5" pos="word" morph="none" start_char="7222" end_char="7227">closed</TOKEN>
<TOKEN id="token-61-6" pos="word" morph="none" start_char="7229" end_char="7233">doors</TOKEN>
<TOKEN id="token-61-7" pos="punct" morph="none" start_char="7234" end_char="7234">,</TOKEN>
<TOKEN id="token-61-8" pos="word" morph="none" start_char="7236" end_char="7239">none</TOKEN>
<TOKEN id="token-61-9" pos="word" morph="none" start_char="7241" end_char="7242">of</TOKEN>
<TOKEN id="token-61-10" pos="word" morph="none" start_char="7244" end_char="7245">us</TOKEN>
<TOKEN id="token-61-11" pos="word" morph="none" start_char="7247" end_char="7250">will</TOKEN>
<TOKEN id="token-61-12" pos="word" morph="none" start_char="7252" end_char="7255">have</TOKEN>
<TOKEN id="token-61-13" pos="word" morph="none" start_char="7257" end_char="7259">the</TOKEN>
<TOKEN id="token-61-14" pos="word" morph="none" start_char="7261" end_char="7271">opportunity</TOKEN>
<TOKEN id="token-61-15" pos="word" morph="none" start_char="7273" end_char="7274">to</TOKEN>
<TOKEN id="token-61-16" pos="word" morph="none" start_char="7276" end_char="7285">understand</TOKEN>
<TOKEN id="token-61-17" pos="word" morph="none" start_char="7287" end_char="7289">how</TOKEN>
<TOKEN id="token-61-18" pos="word" morph="none" start_char="7291" end_char="7293">the</TOKEN>
<TOKEN id="token-61-19" pos="word" morph="none" start_char="7295" end_char="7304">government</TOKEN>
<TOKEN id="token-61-20" pos="word" morph="none" start_char="7306" end_char="7312">arrived</TOKEN>
<TOKEN id="token-61-21" pos="word" morph="none" start_char="7314" end_char="7315">at</TOKEN>
<TOKEN id="token-61-22" pos="word" morph="none" start_char="7317" end_char="7321">these</TOKEN>
<TOKEN id="token-61-23" pos="word" morph="none" start_char="7323" end_char="7331">decisions</TOKEN>
<TOKEN id="token-61-24" pos="word" morph="none" start_char="7333" end_char="7334">or</TOKEN>
<TOKEN id="token-61-25" pos="word" morph="none" start_char="7336" end_char="7337">to</TOKEN>
<TOKEN id="token-61-26" pos="word" morph="none" start_char="7339" end_char="7343">judge</TOKEN>
<TOKEN id="token-61-27" pos="word" morph="none" start_char="7345" end_char="7347">the</TOKEN>
<TOKEN id="token-61-28" pos="word" morph="none" start_char="7349" end_char="7353">rigor</TOKEN>
<TOKEN id="token-61-29" pos="word" morph="none" start_char="7355" end_char="7357">and</TOKEN>
<TOKEN id="token-61-30" pos="word" morph="none" start_char="7359" end_char="7367">integrity</TOKEN>
<TOKEN id="token-61-31" pos="word" morph="none" start_char="7369" end_char="7370">of</TOKEN>
<TOKEN id="token-61-32" pos="word" morph="none" start_char="7372" end_char="7375">that</TOKEN>
<TOKEN id="token-61-33" pos="word" morph="none" start_char="7377" end_char="7383">process</TOKEN>
<TOKEN id="token-61-34" pos="punct" morph="none" start_char="7384" end_char="7385">."</TOKEN>
</SEG>
<SEG id="segment-62" start_char="7388" end_char="7413">
<ORIGINAL_TEXT>Correction 5/5, 6:20 p.m.:</ORIGINAL_TEXT>
<TOKEN id="token-62-0" pos="word" morph="none" start_char="7388" end_char="7397">Correction</TOKEN>
<TOKEN id="token-62-1" pos="unknown" morph="none" start_char="7399" end_char="7401">5/5</TOKEN>
<TOKEN id="token-62-2" pos="punct" morph="none" start_char="7402" end_char="7402">,</TOKEN>
<TOKEN id="token-62-3" pos="unknown" morph="none" start_char="7404" end_char="7407">6:20</TOKEN>
<TOKEN id="token-62-4" pos="unknown" morph="none" start_char="7409" end_char="7411">p.m</TOKEN>
<TOKEN id="token-62-5" pos="punct" morph="none" start_char="7412" end_char="7413">.:</TOKEN>
</SEG>
<SEG id="segment-63" start_char="7416" end_char="7573">
<ORIGINAL_TEXT>The headline of this story has been corrected to reflect that the Wuhan lab received only a part of the millions of U.S. dollars allocated for virus research.</ORIGINAL_TEXT>
<TOKEN id="token-63-0" pos="word" morph="none" start_char="7416" end_char="7418">The</TOKEN>
<TOKEN id="token-63-1" pos="word" morph="none" start_char="7420" end_char="7427">headline</TOKEN>
<TOKEN id="token-63-2" pos="word" morph="none" start_char="7429" end_char="7430">of</TOKEN>
<TOKEN id="token-63-3" pos="word" morph="none" start_char="7432" end_char="7435">this</TOKEN>
<TOKEN id="token-63-4" pos="word" morph="none" start_char="7437" end_char="7441">story</TOKEN>
<TOKEN id="token-63-5" pos="word" morph="none" start_char="7443" end_char="7445">has</TOKEN>
<TOKEN id="token-63-6" pos="word" morph="none" start_char="7447" end_char="7450">been</TOKEN>
<TOKEN id="token-63-7" pos="word" morph="none" start_char="7452" end_char="7460">corrected</TOKEN>
<TOKEN id="token-63-8" pos="word" morph="none" start_char="7462" end_char="7463">to</TOKEN>
<TOKEN id="token-63-9" pos="word" morph="none" start_char="7465" end_char="7471">reflect</TOKEN>
<TOKEN id="token-63-10" pos="word" morph="none" start_char="7473" end_char="7476">that</TOKEN>
<TOKEN id="token-63-11" pos="word" morph="none" start_char="7478" end_char="7480">the</TOKEN>
<TOKEN id="token-63-12" pos="word" morph="none" start_char="7482" end_char="7486">Wuhan</TOKEN>
<TOKEN id="token-63-13" pos="word" morph="none" start_char="7488" end_char="7490">lab</TOKEN>
<TOKEN id="token-63-14" pos="word" morph="none" start_char="7492" end_char="7499">received</TOKEN>
<TOKEN id="token-63-15" pos="word" morph="none" start_char="7501" end_char="7504">only</TOKEN>
<TOKEN id="token-63-16" pos="word" morph="none" start_char="7506" end_char="7506">a</TOKEN>
<TOKEN id="token-63-17" pos="word" morph="none" start_char="7508" end_char="7511">part</TOKEN>
<TOKEN id="token-63-18" pos="word" morph="none" start_char="7513" end_char="7514">of</TOKEN>
<TOKEN id="token-63-19" pos="word" morph="none" start_char="7516" end_char="7518">the</TOKEN>
<TOKEN id="token-63-20" pos="word" morph="none" start_char="7520" end_char="7527">millions</TOKEN>
<TOKEN id="token-63-21" pos="word" morph="none" start_char="7529" end_char="7530">of</TOKEN>
<TOKEN id="token-63-22" pos="unknown" morph="none" start_char="7532" end_char="7534">U.S</TOKEN>
<TOKEN id="token-63-23" pos="punct" morph="none" start_char="7535" end_char="7535">.</TOKEN>
<TOKEN id="token-63-24" pos="word" morph="none" start_char="7537" end_char="7543">dollars</TOKEN>
<TOKEN id="token-63-25" pos="word" morph="none" start_char="7545" end_char="7553">allocated</TOKEN>
<TOKEN id="token-63-26" pos="word" morph="none" start_char="7555" end_char="7557">for</TOKEN>
<TOKEN id="token-63-27" pos="word" morph="none" start_char="7559" end_char="7563">virus</TOKEN>
<TOKEN id="token-63-28" pos="word" morph="none" start_char="7565" end_char="7572">research</TOKEN>
<TOKEN id="token-63-29" pos="punct" morph="none" start_char="7573" end_char="7573">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
