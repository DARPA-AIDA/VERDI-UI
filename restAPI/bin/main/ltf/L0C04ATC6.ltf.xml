<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATC6" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="3675" raw_text_md5="23c4df50d183e0bbf736a26f691c776c">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="68">
<ORIGINAL_TEXT>Expertos claman contra el estudio sobre perros callejeros y COVID-19</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="8">Expertos</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="10" end_char="15">claman</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="17" end_char="22">contra</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="24" end_char="25">el</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="27" end_char="33">estudio</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="35" end_char="39">sobre</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="41" end_char="46">perros</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="48" end_char="57">callejeros</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="59" end_char="59">y</TOKEN>
<TOKEN id="token-0-9" pos="unknown" morph="none" start_char="61" end_char="68">COVID-19</TOKEN>
</SEG>
<SEG id="segment-1" start_char="72" end_char="206">
<ORIGINAL_TEXT>No ven nada en el estudio que respalde esta suposición y les preocupa que este documento haya sido publicado en una revista científica.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="72" end_char="73">No</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="75" end_char="77">ven</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="79" end_char="82">nada</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="84" end_char="85">en</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="87" end_char="88">el</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="90" end_char="96">estudio</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="98" end_char="100">que</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="102" end_char="109">respalde</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="111" end_char="114">esta</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="116" end_char="125">suposición</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="127" end_char="127">y</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="129" end_char="131">les</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="133" end_char="140">preocupa</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="142" end_char="144">que</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="146" end_char="149">este</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="151" end_char="159">documento</TOKEN>
<TOKEN id="token-1-16" pos="word" morph="none" start_char="161" end_char="164">haya</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="166" end_char="169">sido</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="171" end_char="179">publicado</TOKEN>
<TOKEN id="token-1-19" pos="word" morph="none" start_char="181" end_char="182">en</TOKEN>
<TOKEN id="token-1-20" pos="word" morph="none" start_char="184" end_char="186">una</TOKEN>
<TOKEN id="token-1-21" pos="word" morph="none" start_char="188" end_char="194">revista</TOKEN>
<TOKEN id="token-1-22" pos="word" morph="none" start_char="196" end_char="205">científica</TOKEN>
<TOKEN id="token-1-23" pos="punct" morph="none" start_char="206" end_char="206">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="210" end_char="337">
<ORIGINAL_TEXT>​Numerosos expertos no entienden cómo el autor del estudio ha podido concluir que el coronavirus COVID-19 procede de los perros.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="unknown" morph="none" start_char="210" end_char="219">​Numerosos</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="221" end_char="228">expertos</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="230" end_char="231">no</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="233" end_char="241">entienden</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="243" end_char="246">cómo</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="248" end_char="249">el</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="251" end_char="255">autor</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="257" end_char="259">del</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="261" end_char="267">estudio</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="269" end_char="270">ha</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="272" end_char="277">podido</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="279" end_char="286">concluir</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="288" end_char="290">que</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="292" end_char="293">el</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="295" end_char="305">coronavirus</TOKEN>
<TOKEN id="token-2-15" pos="unknown" morph="none" start_char="307" end_char="314">COVID-19</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="316" end_char="322">procede</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="324" end_char="325">de</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="327" end_char="329">los</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="331" end_char="336">perros</TOKEN>
<TOKEN id="token-2-20" pos="punct" morph="none" start_char="337" end_char="337">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="339" end_char="398">
<ORIGINAL_TEXT>No ven nada en su investigación que respalde esta suposición</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="339" end_char="340">No</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="342" end_char="344">ven</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="346" end_char="349">nada</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="351" end_char="352">en</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="354" end_char="355">su</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="357" end_char="369">investigación</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="371" end_char="373">que</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="375" end_char="382">respalde</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="384" end_char="387">esta</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="389" end_char="398">suposición</TOKEN>
</SEG>
<SEG id="segment-4" start_char="401" end_char="610">
<ORIGINAL_TEXT>Muchos medios de comunicación se han hecho eco de un estudio publicado en Molecular Biology and Evolution que sugiere como posible origen del SARS-CoV-2, el virus que causa la COVID-19, a los perros callejeros.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="401" end_char="406">Muchos</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="408" end_char="413">medios</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="415" end_char="416">de</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="418" end_char="429">comunicación</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="431" end_char="432">se</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="434" end_char="436">han</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="438" end_char="442">hecho</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="444" end_char="446">eco</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="448" end_char="449">de</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="451" end_char="452">un</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="454" end_char="460">estudio</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="462" end_char="470">publicado</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="472" end_char="473">en</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="475" end_char="483">Molecular</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="485" end_char="491">Biology</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="493" end_char="495">and</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="497" end_char="505">Evolution</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="507" end_char="509">que</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="511" end_char="517">sugiere</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="519" end_char="522">como</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="524" end_char="530">posible</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="532" end_char="537">origen</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="539" end_char="541">del</TOKEN>
<TOKEN id="token-4-23" pos="unknown" morph="none" start_char="543" end_char="552">SARS-CoV-2</TOKEN>
<TOKEN id="token-4-24" pos="punct" morph="none" start_char="553" end_char="553">,</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="555" end_char="556">el</TOKEN>
<TOKEN id="token-4-26" pos="word" morph="none" start_char="558" end_char="562">virus</TOKEN>
<TOKEN id="token-4-27" pos="word" morph="none" start_char="564" end_char="566">que</TOKEN>
<TOKEN id="token-4-28" pos="word" morph="none" start_char="568" end_char="572">causa</TOKEN>
<TOKEN id="token-4-29" pos="word" morph="none" start_char="574" end_char="575">la</TOKEN>
<TOKEN id="token-4-30" pos="unknown" morph="none" start_char="577" end_char="584">COVID-19</TOKEN>
<TOKEN id="token-4-31" pos="punct" morph="none" start_char="585" end_char="585">,</TOKEN>
<TOKEN id="token-4-32" pos="word" morph="none" start_char="587" end_char="587">a</TOKEN>
<TOKEN id="token-4-33" pos="word" morph="none" start_char="589" end_char="591">los</TOKEN>
<TOKEN id="token-4-34" pos="word" morph="none" start_char="593" end_char="598">perros</TOKEN>
<TOKEN id="token-4-35" pos="word" morph="none" start_char="600" end_char="609">callejeros</TOKEN>
<TOKEN id="token-4-36" pos="punct" morph="none" start_char="610" end_char="610">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="613" end_char="847">
<ORIGINAL_TEXT>Una noticia que ha sorprendido a numerosos expertos, "me resulta difícil entender cómo el autor ha podido concluir de este estudio, o hacer una hipótesis, que el virus que causa COVID-19 puede haber evolucionado a través de los perros.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="613" end_char="615">Una</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="617" end_char="623">noticia</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="625" end_char="627">que</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="629" end_char="630">ha</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="632" end_char="642">sorprendido</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="644" end_char="644">a</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="646" end_char="654">numerosos</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="656" end_char="663">expertos</TOKEN>
<TOKEN id="token-5-8" pos="punct" morph="none" start_char="664" end_char="664">,</TOKEN>
<TOKEN id="token-5-9" pos="punct" morph="none" start_char="666" end_char="666">"</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="667" end_char="668">me</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="670" end_char="676">resulta</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="678" end_char="684">difícil</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="686" end_char="693">entender</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="695" end_char="698">cómo</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="700" end_char="701">el</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="703" end_char="707">autor</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="709" end_char="710">ha</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="712" end_char="717">podido</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="719" end_char="726">concluir</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="728" end_char="729">de</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="731" end_char="734">este</TOKEN>
<TOKEN id="token-5-22" pos="word" morph="none" start_char="736" end_char="742">estudio</TOKEN>
<TOKEN id="token-5-23" pos="punct" morph="none" start_char="743" end_char="743">,</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="745" end_char="745">o</TOKEN>
<TOKEN id="token-5-25" pos="word" morph="none" start_char="747" end_char="751">hacer</TOKEN>
<TOKEN id="token-5-26" pos="word" morph="none" start_char="753" end_char="755">una</TOKEN>
<TOKEN id="token-5-27" pos="word" morph="none" start_char="757" end_char="765">hipótesis</TOKEN>
<TOKEN id="token-5-28" pos="punct" morph="none" start_char="766" end_char="766">,</TOKEN>
<TOKEN id="token-5-29" pos="word" morph="none" start_char="768" end_char="770">que</TOKEN>
<TOKEN id="token-5-30" pos="word" morph="none" start_char="772" end_char="773">el</TOKEN>
<TOKEN id="token-5-31" pos="word" morph="none" start_char="775" end_char="779">virus</TOKEN>
<TOKEN id="token-5-32" pos="word" morph="none" start_char="781" end_char="783">que</TOKEN>
<TOKEN id="token-5-33" pos="word" morph="none" start_char="785" end_char="789">causa</TOKEN>
<TOKEN id="token-5-34" pos="unknown" morph="none" start_char="791" end_char="798">COVID-19</TOKEN>
<TOKEN id="token-5-35" pos="word" morph="none" start_char="800" end_char="804">puede</TOKEN>
<TOKEN id="token-5-36" pos="word" morph="none" start_char="806" end_char="810">haber</TOKEN>
<TOKEN id="token-5-37" pos="word" morph="none" start_char="812" end_char="823">evolucionado</TOKEN>
<TOKEN id="token-5-38" pos="word" morph="none" start_char="825" end_char="825">a</TOKEN>
<TOKEN id="token-5-39" pos="word" morph="none" start_char="827" end_char="832">través</TOKEN>
<TOKEN id="token-5-40" pos="word" morph="none" start_char="834" end_char="835">de</TOKEN>
<TOKEN id="token-5-41" pos="word" morph="none" start_char="837" end_char="839">los</TOKEN>
<TOKEN id="token-5-42" pos="word" morph="none" start_char="841" end_char="846">perros</TOKEN>
<TOKEN id="token-5-43" pos="punct" morph="none" start_char="847" end_char="847">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="849" end_char="900">
<ORIGINAL_TEXT>Hay demasiada inferencia y muy pocos datos directos.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="849" end_char="851">Hay</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="853" end_char="861">demasiada</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="863" end_char="872">inferencia</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="874" end_char="874">y</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="876" end_char="878">muy</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="880" end_char="884">pocos</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="886" end_char="890">datos</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="892" end_char="899">directos</TOKEN>
<TOKEN id="token-6-8" pos="punct" morph="none" start_char="900" end_char="900">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="902" end_char="1029">
<ORIGINAL_TEXT>No veo nada en este documento que respalde esta suposición y me preocupa que este documento haya sido publicado en esta revista.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="902" end_char="903">No</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="905" end_char="907">veo</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="909" end_char="912">nada</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="914" end_char="915">en</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="917" end_char="920">este</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="922" end_char="930">documento</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="932" end_char="934">que</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="936" end_char="943">respalde</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="945" end_char="948">esta</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="950" end_char="959">suposición</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="961" end_char="961">y</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="963" end_char="964">me</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="966" end_char="973">preocupa</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="975" end_char="977">que</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="979" end_char="982">este</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="984" end_char="992">documento</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="994" end_char="997">haya</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="999" end_char="1002">sido</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="1004" end_char="1012">publicado</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="1014" end_char="1015">en</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="1017" end_char="1020">esta</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="1022" end_char="1028">revista</TOKEN>
<TOKEN id="token-7-22" pos="punct" morph="none" start_char="1029" end_char="1029">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1031" end_char="1266">
<ORIGINAL_TEXT>No creo que ningún dueño de perro deba preocuparse como resultado de este trabajo", señala James Wood, jefe del Departamento de Medicina Veterinaria e investigador de infecciones y control de enfermedades de la Universidad de Cambridge.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="1031" end_char="1032">No</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="1034" end_char="1037">creo</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="1039" end_char="1041">que</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1043" end_char="1048">ningún</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1050" end_char="1054">dueño</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1056" end_char="1057">de</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="1059" end_char="1063">perro</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="1065" end_char="1068">deba</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1070" end_char="1080">preocuparse</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="1082" end_char="1085">como</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="1087" end_char="1095">resultado</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="1097" end_char="1098">de</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="1100" end_char="1103">este</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="1105" end_char="1111">trabajo</TOKEN>
<TOKEN id="token-8-14" pos="punct" morph="none" start_char="1112" end_char="1113">",</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="1115" end_char="1120">señala</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="1122" end_char="1126">James</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="1128" end_char="1131">Wood</TOKEN>
<TOKEN id="token-8-18" pos="punct" morph="none" start_char="1132" end_char="1132">,</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="1134" end_char="1137">jefe</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="1139" end_char="1141">del</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="1143" end_char="1154">Departamento</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="1156" end_char="1157">de</TOKEN>
<TOKEN id="token-8-23" pos="word" morph="none" start_char="1159" end_char="1166">Medicina</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="1168" end_char="1178">Veterinaria</TOKEN>
<TOKEN id="token-8-25" pos="word" morph="none" start_char="1180" end_char="1180">e</TOKEN>
<TOKEN id="token-8-26" pos="word" morph="none" start_char="1182" end_char="1193">investigador</TOKEN>
<TOKEN id="token-8-27" pos="word" morph="none" start_char="1195" end_char="1196">de</TOKEN>
<TOKEN id="token-8-28" pos="word" morph="none" start_char="1198" end_char="1208">infecciones</TOKEN>
<TOKEN id="token-8-29" pos="word" morph="none" start_char="1210" end_char="1210">y</TOKEN>
<TOKEN id="token-8-30" pos="word" morph="none" start_char="1212" end_char="1218">control</TOKEN>
<TOKEN id="token-8-31" pos="word" morph="none" start_char="1220" end_char="1221">de</TOKEN>
<TOKEN id="token-8-32" pos="word" morph="none" start_char="1223" end_char="1234">enfermedades</TOKEN>
<TOKEN id="token-8-33" pos="word" morph="none" start_char="1236" end_char="1237">de</TOKEN>
<TOKEN id="token-8-34" pos="word" morph="none" start_char="1239" end_char="1240">la</TOKEN>
<TOKEN id="token-8-35" pos="word" morph="none" start_char="1242" end_char="1252">Universidad</TOKEN>
<TOKEN id="token-8-36" pos="word" morph="none" start_char="1254" end_char="1255">de</TOKEN>
<TOKEN id="token-8-37" pos="word" morph="none" start_char="1257" end_char="1265">Cambridge</TOKEN>
<TOKEN id="token-8-38" pos="punct" morph="none" start_char="1266" end_char="1266">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="1269" end_char="1607">
<ORIGINAL_TEXT>Paul Digard, profesor de Virología del Roslin Institute de la Universidad de Edimburgo, apunta que "la investigación del Dr. Xia adopta un enfoque muy limitado para examinar la secuencia del SARS-CoV-2 en busca de pistas sobre su origen que no brindan un respaldo convincente para la hipótesis de que los perros fueron la fuente del virus.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="1269" end_char="1272">Paul</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="1274" end_char="1279">Digard</TOKEN>
<TOKEN id="token-9-2" pos="punct" morph="none" start_char="1280" end_char="1280">,</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="1282" end_char="1289">profesor</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="1291" end_char="1292">de</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="1294" end_char="1302">Virología</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="1304" end_char="1306">del</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="1308" end_char="1313">Roslin</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="1315" end_char="1323">Institute</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="1325" end_char="1326">de</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="1328" end_char="1329">la</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="1331" end_char="1341">Universidad</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="1343" end_char="1344">de</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1346" end_char="1354">Edimburgo</TOKEN>
<TOKEN id="token-9-14" pos="punct" morph="none" start_char="1355" end_char="1355">,</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1357" end_char="1362">apunta</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="1364" end_char="1366">que</TOKEN>
<TOKEN id="token-9-17" pos="punct" morph="none" start_char="1368" end_char="1368">"</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1369" end_char="1370">la</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1372" end_char="1384">investigación</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1386" end_char="1388">del</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1390" end_char="1391">Dr</TOKEN>
<TOKEN id="token-9-22" pos="punct" morph="none" start_char="1392" end_char="1392">.</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="1394" end_char="1396">Xia</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1398" end_char="1403">adopta</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="1405" end_char="1406">un</TOKEN>
<TOKEN id="token-9-26" pos="word" morph="none" start_char="1408" end_char="1414">enfoque</TOKEN>
<TOKEN id="token-9-27" pos="word" morph="none" start_char="1416" end_char="1418">muy</TOKEN>
<TOKEN id="token-9-28" pos="word" morph="none" start_char="1420" end_char="1427">limitado</TOKEN>
<TOKEN id="token-9-29" pos="word" morph="none" start_char="1429" end_char="1432">para</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1434" end_char="1441">examinar</TOKEN>
<TOKEN id="token-9-31" pos="word" morph="none" start_char="1443" end_char="1444">la</TOKEN>
<TOKEN id="token-9-32" pos="word" morph="none" start_char="1446" end_char="1454">secuencia</TOKEN>
<TOKEN id="token-9-33" pos="word" morph="none" start_char="1456" end_char="1458">del</TOKEN>
<TOKEN id="token-9-34" pos="unknown" morph="none" start_char="1460" end_char="1469">SARS-CoV-2</TOKEN>
<TOKEN id="token-9-35" pos="word" morph="none" start_char="1471" end_char="1472">en</TOKEN>
<TOKEN id="token-9-36" pos="word" morph="none" start_char="1474" end_char="1478">busca</TOKEN>
<TOKEN id="token-9-37" pos="word" morph="none" start_char="1480" end_char="1481">de</TOKEN>
<TOKEN id="token-9-38" pos="word" morph="none" start_char="1483" end_char="1488">pistas</TOKEN>
<TOKEN id="token-9-39" pos="word" morph="none" start_char="1490" end_char="1494">sobre</TOKEN>
<TOKEN id="token-9-40" pos="word" morph="none" start_char="1496" end_char="1497">su</TOKEN>
<TOKEN id="token-9-41" pos="word" morph="none" start_char="1499" end_char="1504">origen</TOKEN>
<TOKEN id="token-9-42" pos="word" morph="none" start_char="1506" end_char="1508">que</TOKEN>
<TOKEN id="token-9-43" pos="word" morph="none" start_char="1510" end_char="1511">no</TOKEN>
<TOKEN id="token-9-44" pos="word" morph="none" start_char="1513" end_char="1519">brindan</TOKEN>
<TOKEN id="token-9-45" pos="word" morph="none" start_char="1521" end_char="1522">un</TOKEN>
<TOKEN id="token-9-46" pos="word" morph="none" start_char="1524" end_char="1531">respaldo</TOKEN>
<TOKEN id="token-9-47" pos="word" morph="none" start_char="1533" end_char="1543">convincente</TOKEN>
<TOKEN id="token-9-48" pos="word" morph="none" start_char="1545" end_char="1548">para</TOKEN>
<TOKEN id="token-9-49" pos="word" morph="none" start_char="1550" end_char="1551">la</TOKEN>
<TOKEN id="token-9-50" pos="word" morph="none" start_char="1553" end_char="1561">hipótesis</TOKEN>
<TOKEN id="token-9-51" pos="word" morph="none" start_char="1563" end_char="1564">de</TOKEN>
<TOKEN id="token-9-52" pos="word" morph="none" start_char="1566" end_char="1568">que</TOKEN>
<TOKEN id="token-9-53" pos="word" morph="none" start_char="1570" end_char="1572">los</TOKEN>
<TOKEN id="token-9-54" pos="word" morph="none" start_char="1574" end_char="1579">perros</TOKEN>
<TOKEN id="token-9-55" pos="word" morph="none" start_char="1581" end_char="1586">fueron</TOKEN>
<TOKEN id="token-9-56" pos="word" morph="none" start_char="1588" end_char="1589">la</TOKEN>
<TOKEN id="token-9-57" pos="word" morph="none" start_char="1591" end_char="1596">fuente</TOKEN>
<TOKEN id="token-9-58" pos="word" morph="none" start_char="1598" end_char="1600">del</TOKEN>
<TOKEN id="token-9-59" pos="word" morph="none" start_char="1602" end_char="1606">virus</TOKEN>
<TOKEN id="token-9-60" pos="punct" morph="none" start_char="1607" end_char="1607">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1609" end_char="1754">
<ORIGINAL_TEXT>La investigación ciertamente no justifica el titular del comunicado de prensa ("... perros callejeros como el posible origen del SARS-CoV-2 ...")"</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1609" end_char="1610">La</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1612" end_char="1624">investigación</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1626" end_char="1636">ciertamente</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1638" end_char="1639">no</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1641" end_char="1649">justifica</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1651" end_char="1652">el</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1654" end_char="1660">titular</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1662" end_char="1664">del</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1666" end_char="1675">comunicado</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1677" end_char="1678">de</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1680" end_char="1685">prensa</TOKEN>
<TOKEN id="token-10-11" pos="punct" morph="none" start_char="1687" end_char="1691">("...</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1693" end_char="1698">perros</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1700" end_char="1709">callejeros</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1711" end_char="1714">como</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1716" end_char="1717">el</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1719" end_char="1725">posible</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1727" end_char="1732">origen</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1734" end_char="1736">del</TOKEN>
<TOKEN id="token-10-19" pos="unknown" morph="none" start_char="1738" end_char="1747">SARS-CoV-2</TOKEN>
<TOKEN id="token-10-20" pos="punct" morph="none" start_char="1749" end_char="1754">...")"</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1757" end_char="2090">
<ORIGINAL_TEXT>Por su parte, Ben Neuman, jefe del Departamento de Ciencias Biológicas en la Universidad Texas A y profesor visitante en la Universidad de Reading, asegura que "la conclusión de que los gatos o los perros estuvieron involucrados como huésped intermedio para el SARS-CoV-2 es altamente especulativo y no debe presentarse como un hecho.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1757" end_char="1759">Por</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1761" end_char="1762">su</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1764" end_char="1768">parte</TOKEN>
<TOKEN id="token-11-3" pos="punct" morph="none" start_char="1769" end_char="1769">,</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1771" end_char="1773">Ben</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1775" end_char="1780">Neuman</TOKEN>
<TOKEN id="token-11-6" pos="punct" morph="none" start_char="1781" end_char="1781">,</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1783" end_char="1786">jefe</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1788" end_char="1790">del</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1792" end_char="1803">Departamento</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1805" end_char="1806">de</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1808" end_char="1815">Ciencias</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1817" end_char="1826">Biológicas</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1828" end_char="1829">en</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1831" end_char="1832">la</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1834" end_char="1844">Universidad</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1846" end_char="1850">Texas</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1852" end_char="1852">A</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1854" end_char="1854">y</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1856" end_char="1863">profesor</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1865" end_char="1873">visitante</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1875" end_char="1876">en</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1878" end_char="1879">la</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1881" end_char="1891">Universidad</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1893" end_char="1894">de</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1896" end_char="1902">Reading</TOKEN>
<TOKEN id="token-11-26" pos="punct" morph="none" start_char="1903" end_char="1903">,</TOKEN>
<TOKEN id="token-11-27" pos="word" morph="none" start_char="1905" end_char="1911">asegura</TOKEN>
<TOKEN id="token-11-28" pos="word" morph="none" start_char="1913" end_char="1915">que</TOKEN>
<TOKEN id="token-11-29" pos="punct" morph="none" start_char="1917" end_char="1917">"</TOKEN>
<TOKEN id="token-11-30" pos="word" morph="none" start_char="1918" end_char="1919">la</TOKEN>
<TOKEN id="token-11-31" pos="word" morph="none" start_char="1921" end_char="1930">conclusión</TOKEN>
<TOKEN id="token-11-32" pos="word" morph="none" start_char="1932" end_char="1933">de</TOKEN>
<TOKEN id="token-11-33" pos="word" morph="none" start_char="1935" end_char="1937">que</TOKEN>
<TOKEN id="token-11-34" pos="word" morph="none" start_char="1939" end_char="1941">los</TOKEN>
<TOKEN id="token-11-35" pos="word" morph="none" start_char="1943" end_char="1947">gatos</TOKEN>
<TOKEN id="token-11-36" pos="word" morph="none" start_char="1949" end_char="1949">o</TOKEN>
<TOKEN id="token-11-37" pos="word" morph="none" start_char="1951" end_char="1953">los</TOKEN>
<TOKEN id="token-11-38" pos="word" morph="none" start_char="1955" end_char="1960">perros</TOKEN>
<TOKEN id="token-11-39" pos="word" morph="none" start_char="1962" end_char="1971">estuvieron</TOKEN>
<TOKEN id="token-11-40" pos="word" morph="none" start_char="1973" end_char="1984">involucrados</TOKEN>
<TOKEN id="token-11-41" pos="word" morph="none" start_char="1986" end_char="1989">como</TOKEN>
<TOKEN id="token-11-42" pos="word" morph="none" start_char="1991" end_char="1997">huésped</TOKEN>
<TOKEN id="token-11-43" pos="word" morph="none" start_char="1999" end_char="2008">intermedio</TOKEN>
<TOKEN id="token-11-44" pos="word" morph="none" start_char="2010" end_char="2013">para</TOKEN>
<TOKEN id="token-11-45" pos="word" morph="none" start_char="2015" end_char="2016">el</TOKEN>
<TOKEN id="token-11-46" pos="unknown" morph="none" start_char="2018" end_char="2027">SARS-CoV-2</TOKEN>
<TOKEN id="token-11-47" pos="word" morph="none" start_char="2029" end_char="2030">es</TOKEN>
<TOKEN id="token-11-48" pos="word" morph="none" start_char="2032" end_char="2040">altamente</TOKEN>
<TOKEN id="token-11-49" pos="word" morph="none" start_char="2042" end_char="2053">especulativo</TOKEN>
<TOKEN id="token-11-50" pos="word" morph="none" start_char="2055" end_char="2055">y</TOKEN>
<TOKEN id="token-11-51" pos="word" morph="none" start_char="2057" end_char="2058">no</TOKEN>
<TOKEN id="token-11-52" pos="word" morph="none" start_char="2060" end_char="2063">debe</TOKEN>
<TOKEN id="token-11-53" pos="word" morph="none" start_char="2065" end_char="2075">presentarse</TOKEN>
<TOKEN id="token-11-54" pos="word" morph="none" start_char="2077" end_char="2080">como</TOKEN>
<TOKEN id="token-11-55" pos="word" morph="none" start_char="2082" end_char="2083">un</TOKEN>
<TOKEN id="token-11-56" pos="word" morph="none" start_char="2085" end_char="2089">hecho</TOKEN>
<TOKEN id="token-11-57" pos="punct" morph="none" start_char="2090" end_char="2090">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="2092" end_char="2278">
<ORIGINAL_TEXT>Se necesitarán algunos datos nuevos para finalmente resolver el misterio del origen del SARS-CoV-2, pero este estudio se basa en un nuevo análisis de datos antiguos sin ningún dato nuevo.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="2092" end_char="2093">Se</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="2095" end_char="2105">necesitarán</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="2107" end_char="2113">algunos</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="2115" end_char="2119">datos</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="2121" end_char="2126">nuevos</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="2128" end_char="2131">para</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="2133" end_char="2142">finalmente</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="2144" end_char="2151">resolver</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="2153" end_char="2154">el</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="2156" end_char="2163">misterio</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="2165" end_char="2167">del</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="2169" end_char="2174">origen</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="2176" end_char="2178">del</TOKEN>
<TOKEN id="token-12-13" pos="unknown" morph="none" start_char="2180" end_char="2189">SARS-CoV-2</TOKEN>
<TOKEN id="token-12-14" pos="punct" morph="none" start_char="2190" end_char="2190">,</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="2192" end_char="2195">pero</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="2197" end_char="2200">este</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="2202" end_char="2208">estudio</TOKEN>
<TOKEN id="token-12-18" pos="word" morph="none" start_char="2210" end_char="2211">se</TOKEN>
<TOKEN id="token-12-19" pos="word" morph="none" start_char="2213" end_char="2216">basa</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="2218" end_char="2219">en</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="2221" end_char="2222">un</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="2224" end_char="2228">nuevo</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="2230" end_char="2237">análisis</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="2239" end_char="2240">de</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="2242" end_char="2246">datos</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="2248" end_char="2255">antiguos</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="2257" end_char="2259">sin</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="2261" end_char="2266">ningún</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="2268" end_char="2271">dato</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="2273" end_char="2277">nuevo</TOKEN>
<TOKEN id="token-12-31" pos="punct" morph="none" start_char="2278" end_char="2278">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="2280" end_char="2412">
<ORIGINAL_TEXT>Este estudio tiene similitudes con un artículo anterior que identificaba incorrectamente el origen del SARS-CoV-2 en las serpientes".</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="2280" end_char="2283">Este</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="2285" end_char="2291">estudio</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="2293" end_char="2297">tiene</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="2299" end_char="2309">similitudes</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="2311" end_char="2313">con</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="2315" end_char="2316">un</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="2318" end_char="2325">artículo</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="2327" end_char="2334">anterior</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="2336" end_char="2338">que</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="2340" end_char="2351">identificaba</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="2353" end_char="2367">incorrectamente</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="2369" end_char="2370">el</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="2372" end_char="2377">origen</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="2379" end_char="2381">del</TOKEN>
<TOKEN id="token-13-14" pos="unknown" morph="none" start_char="2383" end_char="2392">SARS-CoV-2</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="2394" end_char="2395">en</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="2397" end_char="2399">las</TOKEN>
<TOKEN id="token-13-17" pos="word" morph="none" start_char="2401" end_char="2410">serpientes</TOKEN>
<TOKEN id="token-13-18" pos="punct" morph="none" start_char="2411" end_char="2412">".</TOKEN>
</SEG>
<SEG id="segment-14" start_char="2415" end_char="2713">
<ORIGINAL_TEXT>La presidenta de la Asociación Británica de Veterinaria (BVA, por sus siglas en inglés), Daniella Dos Santos, manifiesta que "esta investigación es puramente teórica y el autor mismo señala que hasta la fecha no hay evidencia de que los perros puedan replicar o eliminar el virus que causa COVID-19.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="2415" end_char="2416">La</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="2418" end_char="2427">presidenta</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="2429" end_char="2430">de</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="2432" end_char="2433">la</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="2435" end_char="2444">Asociación</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="2446" end_char="2454">Británica</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="2456" end_char="2457">de</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="2459" end_char="2469">Veterinaria</TOKEN>
<TOKEN id="token-14-8" pos="punct" morph="none" start_char="2471" end_char="2471">(</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="2472" end_char="2474">BVA</TOKEN>
<TOKEN id="token-14-10" pos="punct" morph="none" start_char="2475" end_char="2475">,</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="2477" end_char="2479">por</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="2481" end_char="2483">sus</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="2485" end_char="2490">siglas</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="2492" end_char="2493">en</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="2495" end_char="2500">inglés</TOKEN>
<TOKEN id="token-14-16" pos="punct" morph="none" start_char="2501" end_char="2502">),</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="2504" end_char="2511">Daniella</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="2513" end_char="2515">Dos</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="2517" end_char="2522">Santos</TOKEN>
<TOKEN id="token-14-20" pos="punct" morph="none" start_char="2523" end_char="2523">,</TOKEN>
<TOKEN id="token-14-21" pos="word" morph="none" start_char="2525" end_char="2534">manifiesta</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="2536" end_char="2538">que</TOKEN>
<TOKEN id="token-14-23" pos="punct" morph="none" start_char="2540" end_char="2540">"</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="2541" end_char="2544">esta</TOKEN>
<TOKEN id="token-14-25" pos="word" morph="none" start_char="2546" end_char="2558">investigación</TOKEN>
<TOKEN id="token-14-26" pos="word" morph="none" start_char="2560" end_char="2561">es</TOKEN>
<TOKEN id="token-14-27" pos="word" morph="none" start_char="2563" end_char="2571">puramente</TOKEN>
<TOKEN id="token-14-28" pos="word" morph="none" start_char="2573" end_char="2579">teórica</TOKEN>
<TOKEN id="token-14-29" pos="word" morph="none" start_char="2581" end_char="2581">y</TOKEN>
<TOKEN id="token-14-30" pos="word" morph="none" start_char="2583" end_char="2584">el</TOKEN>
<TOKEN id="token-14-31" pos="word" morph="none" start_char="2586" end_char="2590">autor</TOKEN>
<TOKEN id="token-14-32" pos="word" morph="none" start_char="2592" end_char="2596">mismo</TOKEN>
<TOKEN id="token-14-33" pos="word" morph="none" start_char="2598" end_char="2603">señala</TOKEN>
<TOKEN id="token-14-34" pos="word" morph="none" start_char="2605" end_char="2607">que</TOKEN>
<TOKEN id="token-14-35" pos="word" morph="none" start_char="2609" end_char="2613">hasta</TOKEN>
<TOKEN id="token-14-36" pos="word" morph="none" start_char="2615" end_char="2616">la</TOKEN>
<TOKEN id="token-14-37" pos="word" morph="none" start_char="2618" end_char="2622">fecha</TOKEN>
<TOKEN id="token-14-38" pos="word" morph="none" start_char="2624" end_char="2625">no</TOKEN>
<TOKEN id="token-14-39" pos="word" morph="none" start_char="2627" end_char="2629">hay</TOKEN>
<TOKEN id="token-14-40" pos="word" morph="none" start_char="2631" end_char="2639">evidencia</TOKEN>
<TOKEN id="token-14-41" pos="word" morph="none" start_char="2641" end_char="2642">de</TOKEN>
<TOKEN id="token-14-42" pos="word" morph="none" start_char="2644" end_char="2646">que</TOKEN>
<TOKEN id="token-14-43" pos="word" morph="none" start_char="2648" end_char="2650">los</TOKEN>
<TOKEN id="token-14-44" pos="word" morph="none" start_char="2652" end_char="2657">perros</TOKEN>
<TOKEN id="token-14-45" pos="word" morph="none" start_char="2659" end_char="2664">puedan</TOKEN>
<TOKEN id="token-14-46" pos="word" morph="none" start_char="2666" end_char="2673">replicar</TOKEN>
<TOKEN id="token-14-47" pos="word" morph="none" start_char="2675" end_char="2675">o</TOKEN>
<TOKEN id="token-14-48" pos="word" morph="none" start_char="2677" end_char="2684">eliminar</TOKEN>
<TOKEN id="token-14-49" pos="word" morph="none" start_char="2686" end_char="2687">el</TOKEN>
<TOKEN id="token-14-50" pos="word" morph="none" start_char="2689" end_char="2693">virus</TOKEN>
<TOKEN id="token-14-51" pos="word" morph="none" start_char="2695" end_char="2697">que</TOKEN>
<TOKEN id="token-14-52" pos="word" morph="none" start_char="2699" end_char="2703">causa</TOKEN>
<TOKEN id="token-14-53" pos="unknown" morph="none" start_char="2705" end_char="2712">COVID-19</TOKEN>
<TOKEN id="token-14-54" pos="punct" morph="none" start_char="2713" end_char="2713">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="2715" end_char="2864">
<ORIGINAL_TEXT>Instamos a la extrema precaución al interpretarlo como algo más que un recordatorio de que se está trabajando para considerar los orígenes del virus".</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="2715" end_char="2722">Instamos</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="2724" end_char="2724">a</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="2726" end_char="2727">la</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="2729" end_char="2735">extrema</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="2737" end_char="2746">precaución</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="2748" end_char="2749">al</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="2751" end_char="2763">interpretarlo</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="2765" end_char="2768">como</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="2770" end_char="2773">algo</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="2775" end_char="2777">más</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="2779" end_char="2781">que</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="2783" end_char="2784">un</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="2786" end_char="2797">recordatorio</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="2799" end_char="2800">de</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="2802" end_char="2804">que</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="2806" end_char="2807">se</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="2809" end_char="2812">está</TOKEN>
<TOKEN id="token-15-17" pos="word" morph="none" start_char="2814" end_char="2823">trabajando</TOKEN>
<TOKEN id="token-15-18" pos="word" morph="none" start_char="2825" end_char="2828">para</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="2830" end_char="2839">considerar</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="2841" end_char="2843">los</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="2845" end_char="2852">orígenes</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="2854" end_char="2856">del</TOKEN>
<TOKEN id="token-15-23" pos="word" morph="none" start_char="2858" end_char="2862">virus</TOKEN>
<TOKEN id="token-15-24" pos="punct" morph="none" start_char="2863" end_char="2864">".</TOKEN>
</SEG>
<SEG id="segment-16" start_char="2867" end_char="3057">
<ORIGINAL_TEXT>Asimismo, cabe recordar que un estudio reciente en perros descubrió que el virus se replica muy mal en los caninos, lo que indica que es probable que sean un punto muerto para la transmisión.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="2867" end_char="2874">Asimismo</TOKEN>
<TOKEN id="token-16-1" pos="punct" morph="none" start_char="2875" end_char="2875">,</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="2877" end_char="2880">cabe</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="2882" end_char="2889">recordar</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="2891" end_char="2893">que</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="2895" end_char="2896">un</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="2898" end_char="2904">estudio</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="2906" end_char="2913">reciente</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="2915" end_char="2916">en</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="2918" end_char="2923">perros</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="2925" end_char="2933">descubrió</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="2935" end_char="2937">que</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="2939" end_char="2940">el</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="2942" end_char="2946">virus</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="2948" end_char="2949">se</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="2951" end_char="2957">replica</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="2959" end_char="2961">muy</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="2963" end_char="2965">mal</TOKEN>
<TOKEN id="token-16-18" pos="word" morph="none" start_char="2967" end_char="2968">en</TOKEN>
<TOKEN id="token-16-19" pos="word" morph="none" start_char="2970" end_char="2972">los</TOKEN>
<TOKEN id="token-16-20" pos="word" morph="none" start_char="2974" end_char="2980">caninos</TOKEN>
<TOKEN id="token-16-21" pos="punct" morph="none" start_char="2981" end_char="2981">,</TOKEN>
<TOKEN id="token-16-22" pos="word" morph="none" start_char="2983" end_char="2984">lo</TOKEN>
<TOKEN id="token-16-23" pos="word" morph="none" start_char="2986" end_char="2988">que</TOKEN>
<TOKEN id="token-16-24" pos="word" morph="none" start_char="2990" end_char="2995">indica</TOKEN>
<TOKEN id="token-16-25" pos="word" morph="none" start_char="2997" end_char="2999">que</TOKEN>
<TOKEN id="token-16-26" pos="word" morph="none" start_char="3001" end_char="3002">es</TOKEN>
<TOKEN id="token-16-27" pos="word" morph="none" start_char="3004" end_char="3011">probable</TOKEN>
<TOKEN id="token-16-28" pos="word" morph="none" start_char="3013" end_char="3015">que</TOKEN>
<TOKEN id="token-16-29" pos="word" morph="none" start_char="3017" end_char="3020">sean</TOKEN>
<TOKEN id="token-16-30" pos="word" morph="none" start_char="3022" end_char="3023">un</TOKEN>
<TOKEN id="token-16-31" pos="word" morph="none" start_char="3025" end_char="3029">punto</TOKEN>
<TOKEN id="token-16-32" pos="word" morph="none" start_char="3031" end_char="3036">muerto</TOKEN>
<TOKEN id="token-16-33" pos="word" morph="none" start_char="3038" end_char="3041">para</TOKEN>
<TOKEN id="token-16-34" pos="word" morph="none" start_char="3043" end_char="3044">la</TOKEN>
<TOKEN id="token-16-35" pos="word" morph="none" start_char="3046" end_char="3056">transmisión</TOKEN>
<TOKEN id="token-16-36" pos="punct" morph="none" start_char="3057" end_char="3057">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="3060" end_char="3244">
<ORIGINAL_TEXT>Por el momento, los principales organismos de la salud animal, como la OIE o la WSAVA, insisten en que no hay razón para preocuparse por la relación entre las mascotas y el coronavirus.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="3060" end_char="3062">Por</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="3064" end_char="3065">el</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="3067" end_char="3073">momento</TOKEN>
<TOKEN id="token-17-3" pos="punct" morph="none" start_char="3074" end_char="3074">,</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="3076" end_char="3078">los</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="3080" end_char="3090">principales</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="3092" end_char="3101">organismos</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="3103" end_char="3104">de</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="3106" end_char="3107">la</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="3109" end_char="3113">salud</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="3115" end_char="3120">animal</TOKEN>
<TOKEN id="token-17-11" pos="punct" morph="none" start_char="3121" end_char="3121">,</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="3123" end_char="3126">como</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="3128" end_char="3129">la</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="3131" end_char="3133">OIE</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="3135" end_char="3135">o</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="3137" end_char="3138">la</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="3140" end_char="3144">WSAVA</TOKEN>
<TOKEN id="token-17-18" pos="punct" morph="none" start_char="3145" end_char="3145">,</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="3147" end_char="3154">insisten</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="3156" end_char="3157">en</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="3159" end_char="3161">que</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="3163" end_char="3164">no</TOKEN>
<TOKEN id="token-17-23" pos="word" morph="none" start_char="3166" end_char="3168">hay</TOKEN>
<TOKEN id="token-17-24" pos="word" morph="none" start_char="3170" end_char="3174">razón</TOKEN>
<TOKEN id="token-17-25" pos="word" morph="none" start_char="3176" end_char="3179">para</TOKEN>
<TOKEN id="token-17-26" pos="word" morph="none" start_char="3181" end_char="3191">preocuparse</TOKEN>
<TOKEN id="token-17-27" pos="word" morph="none" start_char="3193" end_char="3195">por</TOKEN>
<TOKEN id="token-17-28" pos="word" morph="none" start_char="3197" end_char="3198">la</TOKEN>
<TOKEN id="token-17-29" pos="word" morph="none" start_char="3200" end_char="3207">relación</TOKEN>
<TOKEN id="token-17-30" pos="word" morph="none" start_char="3209" end_char="3213">entre</TOKEN>
<TOKEN id="token-17-31" pos="word" morph="none" start_char="3215" end_char="3217">las</TOKEN>
<TOKEN id="token-17-32" pos="word" morph="none" start_char="3219" end_char="3226">mascotas</TOKEN>
<TOKEN id="token-17-33" pos="word" morph="none" start_char="3228" end_char="3228">y</TOKEN>
<TOKEN id="token-17-34" pos="word" morph="none" start_char="3230" end_char="3231">el</TOKEN>
<TOKEN id="token-17-35" pos="word" morph="none" start_char="3233" end_char="3243">coronavirus</TOKEN>
<TOKEN id="token-17-36" pos="punct" morph="none" start_char="3244" end_char="3244">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="3247" end_char="3447">
<ORIGINAL_TEXT>Este viernes 17 tendrá lugar un webinar organizado por la Asociación Mundial de Veterinarios de Pequeños Animales (WSAVA), que contará con el apoyo de Purina, donde se debatirán todas estas cuestiones.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="3247" end_char="3250">Este</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="3252" end_char="3258">viernes</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="3260" end_char="3261">17</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="3263" end_char="3268">tendrá</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="3270" end_char="3274">lugar</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="3276" end_char="3277">un</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="3279" end_char="3285">webinar</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="3287" end_char="3296">organizado</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="3298" end_char="3300">por</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="3302" end_char="3303">la</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="3305" end_char="3314">Asociación</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="3316" end_char="3322">Mundial</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="3324" end_char="3325">de</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="3327" end_char="3338">Veterinarios</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="3340" end_char="3341">de</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="3343" end_char="3350">Pequeños</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="3352" end_char="3359">Animales</TOKEN>
<TOKEN id="token-18-17" pos="punct" morph="none" start_char="3361" end_char="3361">(</TOKEN>
<TOKEN id="token-18-18" pos="word" morph="none" start_char="3362" end_char="3366">WSAVA</TOKEN>
<TOKEN id="token-18-19" pos="punct" morph="none" start_char="3367" end_char="3368">),</TOKEN>
<TOKEN id="token-18-20" pos="word" morph="none" start_char="3370" end_char="3372">que</TOKEN>
<TOKEN id="token-18-21" pos="word" morph="none" start_char="3374" end_char="3380">contará</TOKEN>
<TOKEN id="token-18-22" pos="word" morph="none" start_char="3382" end_char="3384">con</TOKEN>
<TOKEN id="token-18-23" pos="word" morph="none" start_char="3386" end_char="3387">el</TOKEN>
<TOKEN id="token-18-24" pos="word" morph="none" start_char="3389" end_char="3393">apoyo</TOKEN>
<TOKEN id="token-18-25" pos="word" morph="none" start_char="3395" end_char="3396">de</TOKEN>
<TOKEN id="token-18-26" pos="word" morph="none" start_char="3398" end_char="3403">Purina</TOKEN>
<TOKEN id="token-18-27" pos="punct" morph="none" start_char="3404" end_char="3404">,</TOKEN>
<TOKEN id="token-18-28" pos="word" morph="none" start_char="3406" end_char="3410">donde</TOKEN>
<TOKEN id="token-18-29" pos="word" morph="none" start_char="3412" end_char="3413">se</TOKEN>
<TOKEN id="token-18-30" pos="word" morph="none" start_char="3415" end_char="3423">debatirán</TOKEN>
<TOKEN id="token-18-31" pos="word" morph="none" start_char="3425" end_char="3429">todas</TOKEN>
<TOKEN id="token-18-32" pos="word" morph="none" start_char="3431" end_char="3435">estas</TOKEN>
<TOKEN id="token-18-33" pos="word" morph="none" start_char="3437" end_char="3446">cuestiones</TOKEN>
<TOKEN id="token-18-34" pos="punct" morph="none" start_char="3447" end_char="3447">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="3450" end_char="3671">
<ORIGINAL_TEXT>Los ponentes serán la Dra Vanessa Barrs (catedrática de Salud de Animales de Compañía de la Universidad de Hong Kong), el Dr Michael Lappin (miembro del comité One Health de WSAVA) y el Dr Shane Ryan (presidente de WSAVA).</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="3450" end_char="3452">Los</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="3454" end_char="3461">ponentes</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="3463" end_char="3467">serán</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="3469" end_char="3470">la</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="3472" end_char="3474">Dra</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="3476" end_char="3482">Vanessa</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="3484" end_char="3488">Barrs</TOKEN>
<TOKEN id="token-19-7" pos="punct" morph="none" start_char="3490" end_char="3490">(</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="3491" end_char="3501">catedrática</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="3503" end_char="3504">de</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="3506" end_char="3510">Salud</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="3512" end_char="3513">de</TOKEN>
<TOKEN id="token-19-12" pos="word" morph="none" start_char="3515" end_char="3522">Animales</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="3524" end_char="3525">de</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="3527" end_char="3534">Compañía</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="3536" end_char="3537">de</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="3539" end_char="3540">la</TOKEN>
<TOKEN id="token-19-17" pos="word" morph="none" start_char="3542" end_char="3552">Universidad</TOKEN>
<TOKEN id="token-19-18" pos="word" morph="none" start_char="3554" end_char="3555">de</TOKEN>
<TOKEN id="token-19-19" pos="word" morph="none" start_char="3557" end_char="3560">Hong</TOKEN>
<TOKEN id="token-19-20" pos="word" morph="none" start_char="3562" end_char="3565">Kong</TOKEN>
<TOKEN id="token-19-21" pos="punct" morph="none" start_char="3566" end_char="3567">),</TOKEN>
<TOKEN id="token-19-22" pos="word" morph="none" start_char="3569" end_char="3570">el</TOKEN>
<TOKEN id="token-19-23" pos="word" morph="none" start_char="3572" end_char="3573">Dr</TOKEN>
<TOKEN id="token-19-24" pos="word" morph="none" start_char="3575" end_char="3581">Michael</TOKEN>
<TOKEN id="token-19-25" pos="word" morph="none" start_char="3583" end_char="3588">Lappin</TOKEN>
<TOKEN id="token-19-26" pos="punct" morph="none" start_char="3590" end_char="3590">(</TOKEN>
<TOKEN id="token-19-27" pos="word" morph="none" start_char="3591" end_char="3597">miembro</TOKEN>
<TOKEN id="token-19-28" pos="word" morph="none" start_char="3599" end_char="3601">del</TOKEN>
<TOKEN id="token-19-29" pos="word" morph="none" start_char="3603" end_char="3608">comité</TOKEN>
<TOKEN id="token-19-30" pos="word" morph="none" start_char="3610" end_char="3612">One</TOKEN>
<TOKEN id="token-19-31" pos="word" morph="none" start_char="3614" end_char="3619">Health</TOKEN>
<TOKEN id="token-19-32" pos="word" morph="none" start_char="3621" end_char="3622">de</TOKEN>
<TOKEN id="token-19-33" pos="word" morph="none" start_char="3624" end_char="3628">WSAVA</TOKEN>
<TOKEN id="token-19-34" pos="punct" morph="none" start_char="3629" end_char="3629">)</TOKEN>
<TOKEN id="token-19-35" pos="word" morph="none" start_char="3631" end_char="3631">y</TOKEN>
<TOKEN id="token-19-36" pos="word" morph="none" start_char="3633" end_char="3634">el</TOKEN>
<TOKEN id="token-19-37" pos="word" morph="none" start_char="3636" end_char="3637">Dr</TOKEN>
<TOKEN id="token-19-38" pos="word" morph="none" start_char="3639" end_char="3643">Shane</TOKEN>
<TOKEN id="token-19-39" pos="word" morph="none" start_char="3645" end_char="3648">Ryan</TOKEN>
<TOKEN id="token-19-40" pos="punct" morph="none" start_char="3650" end_char="3650">(</TOKEN>
<TOKEN id="token-19-41" pos="word" morph="none" start_char="3651" end_char="3660">presidente</TOKEN>
<TOKEN id="token-19-42" pos="word" morph="none" start_char="3662" end_char="3663">de</TOKEN>
<TOKEN id="token-19-43" pos="word" morph="none" start_char="3665" end_char="3669">WSAVA</TOKEN>
<TOKEN id="token-19-44" pos="punct" morph="none" start_char="3670" end_char="3671">).</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
