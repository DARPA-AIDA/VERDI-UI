<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATBS" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="1169" raw_text_md5="e57891dab9df288052bb5b6e14177ec2">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="19">
<ORIGINAL_TEXT>¡Pangolín inocente!</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="punct" morph="none" start_char="1" end_char="1">¡</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="2" end_char="9">Pangolín</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="11" end_char="18">inocente</TOKEN>
<TOKEN id="token-0-3" pos="punct" morph="none" start_char="19" end_char="19">!</TOKEN>
</SEG>
<SEG id="segment-1" start_char="21" end_char="55">
<ORIGINAL_TEXT>Revelan que no esparció coronavirus</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="21" end_char="27">Revelan</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="29" end_char="31">que</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="33" end_char="34">no</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="36" end_char="43">esparció</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="45" end_char="55">coronavirus</TOKEN>
</SEG>
<SEG id="segment-2" start_char="60" end_char="406">
<ORIGINAL_TEXT>Su hipótesis inicial, sostenía que un coronavirus hallado en un pangolín tenía un genoma igual en un 99% al del que causa el Covid-19 en los humanos, resultó ser errónea, pues un nuevo análisis reveló que la equivalencia entre genomas se sitúa en torno al 90%, lo que resulta insuficiente para afirmar que este animal fue el origen de la pandemia.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="60" end_char="61">Su</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="63" end_char="71">hipótesis</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="73" end_char="79">inicial</TOKEN>
<TOKEN id="token-2-3" pos="punct" morph="none" start_char="80" end_char="80">,</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="82" end_char="89">sostenía</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="91" end_char="93">que</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="95" end_char="96">un</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="98" end_char="108">coronavirus</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="110" end_char="116">hallado</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="118" end_char="119">en</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="121" end_char="122">un</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="124" end_char="131">pangolín</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="133" end_char="137">tenía</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="139" end_char="140">un</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="142" end_char="147">genoma</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="149" end_char="153">igual</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="155" end_char="156">en</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="158" end_char="159">un</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="161" end_char="162">99</TOKEN>
<TOKEN id="token-2-19" pos="punct" morph="none" start_char="163" end_char="163">%</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="165" end_char="166">al</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="168" end_char="170">del</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="172" end_char="174">que</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="176" end_char="180">causa</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="182" end_char="183">el</TOKEN>
<TOKEN id="token-2-25" pos="unknown" morph="none" start_char="185" end_char="192">Covid-19</TOKEN>
<TOKEN id="token-2-26" pos="word" morph="none" start_char="194" end_char="195">en</TOKEN>
<TOKEN id="token-2-27" pos="word" morph="none" start_char="197" end_char="199">los</TOKEN>
<TOKEN id="token-2-28" pos="word" morph="none" start_char="201" end_char="207">humanos</TOKEN>
<TOKEN id="token-2-29" pos="punct" morph="none" start_char="208" end_char="208">,</TOKEN>
<TOKEN id="token-2-30" pos="word" morph="none" start_char="210" end_char="216">resultó</TOKEN>
<TOKEN id="token-2-31" pos="word" morph="none" start_char="218" end_char="220">ser</TOKEN>
<TOKEN id="token-2-32" pos="word" morph="none" start_char="222" end_char="228">errónea</TOKEN>
<TOKEN id="token-2-33" pos="punct" morph="none" start_char="229" end_char="229">,</TOKEN>
<TOKEN id="token-2-34" pos="word" morph="none" start_char="231" end_char="234">pues</TOKEN>
<TOKEN id="token-2-35" pos="word" morph="none" start_char="236" end_char="237">un</TOKEN>
<TOKEN id="token-2-36" pos="word" morph="none" start_char="239" end_char="243">nuevo</TOKEN>
<TOKEN id="token-2-37" pos="word" morph="none" start_char="245" end_char="252">análisis</TOKEN>
<TOKEN id="token-2-38" pos="word" morph="none" start_char="254" end_char="259">reveló</TOKEN>
<TOKEN id="token-2-39" pos="word" morph="none" start_char="261" end_char="263">que</TOKEN>
<TOKEN id="token-2-40" pos="word" morph="none" start_char="265" end_char="266">la</TOKEN>
<TOKEN id="token-2-41" pos="word" morph="none" start_char="268" end_char="279">equivalencia</TOKEN>
<TOKEN id="token-2-42" pos="word" morph="none" start_char="281" end_char="285">entre</TOKEN>
<TOKEN id="token-2-43" pos="word" morph="none" start_char="287" end_char="293">genomas</TOKEN>
<TOKEN id="token-2-44" pos="word" morph="none" start_char="295" end_char="296">se</TOKEN>
<TOKEN id="token-2-45" pos="word" morph="none" start_char="298" end_char="302">sitúa</TOKEN>
<TOKEN id="token-2-46" pos="word" morph="none" start_char="304" end_char="305">en</TOKEN>
<TOKEN id="token-2-47" pos="word" morph="none" start_char="307" end_char="311">torno</TOKEN>
<TOKEN id="token-2-48" pos="word" morph="none" start_char="313" end_char="314">al</TOKEN>
<TOKEN id="token-2-49" pos="word" morph="none" start_char="316" end_char="317">90</TOKEN>
<TOKEN id="token-2-50" pos="punct" morph="none" start_char="318" end_char="319">%,</TOKEN>
<TOKEN id="token-2-51" pos="word" morph="none" start_char="321" end_char="322">lo</TOKEN>
<TOKEN id="token-2-52" pos="word" morph="none" start_char="324" end_char="326">que</TOKEN>
<TOKEN id="token-2-53" pos="word" morph="none" start_char="328" end_char="334">resulta</TOKEN>
<TOKEN id="token-2-54" pos="word" morph="none" start_char="336" end_char="347">insuficiente</TOKEN>
<TOKEN id="token-2-55" pos="word" morph="none" start_char="349" end_char="352">para</TOKEN>
<TOKEN id="token-2-56" pos="word" morph="none" start_char="354" end_char="360">afirmar</TOKEN>
<TOKEN id="token-2-57" pos="word" morph="none" start_char="362" end_char="364">que</TOKEN>
<TOKEN id="token-2-58" pos="word" morph="none" start_char="366" end_char="369">este</TOKEN>
<TOKEN id="token-2-59" pos="word" morph="none" start_char="371" end_char="376">animal</TOKEN>
<TOKEN id="token-2-60" pos="word" morph="none" start_char="378" end_char="380">fue</TOKEN>
<TOKEN id="token-2-61" pos="word" morph="none" start_char="382" end_char="383">el</TOKEN>
<TOKEN id="token-2-62" pos="word" morph="none" start_char="385" end_char="390">origen</TOKEN>
<TOKEN id="token-2-63" pos="word" morph="none" start_char="392" end_char="393">de</TOKEN>
<TOKEN id="token-2-64" pos="word" morph="none" start_char="395" end_char="396">la</TOKEN>
<TOKEN id="token-2-65" pos="word" morph="none" start_char="398" end_char="405">pandemia</TOKEN>
<TOKEN id="token-2-66" pos="punct" morph="none" start_char="406" end_char="406">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="409" end_char="684">
<ORIGINAL_TEXT>Según sostenían los científicos de la Universidad de Agricultura del sur del país asiático, los pangolines fueron los animales intermediarios entre los murciélagos y el ser humano, sin embargo, la institución advirtió desde un principio que los resultados no eran definitivos.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="409" end_char="413">Según</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="415" end_char="423">sostenían</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="425" end_char="427">los</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="429" end_char="439">científicos</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="441" end_char="442">de</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="444" end_char="445">la</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="447" end_char="457">Universidad</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="459" end_char="460">de</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="462" end_char="472">Agricultura</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="474" end_char="476">del</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="478" end_char="480">sur</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="482" end_char="484">del</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="486" end_char="489">país</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="491" end_char="498">asiático</TOKEN>
<TOKEN id="token-3-14" pos="punct" morph="none" start_char="499" end_char="499">,</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="501" end_char="503">los</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="505" end_char="514">pangolines</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="516" end_char="521">fueron</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="523" end_char="525">los</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="527" end_char="534">animales</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="536" end_char="549">intermediarios</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="551" end_char="555">entre</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="557" end_char="559">los</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="561" end_char="571">murciélagos</TOKEN>
<TOKEN id="token-3-24" pos="word" morph="none" start_char="573" end_char="573">y</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="575" end_char="576">el</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="578" end_char="580">ser</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="582" end_char="587">humano</TOKEN>
<TOKEN id="token-3-28" pos="punct" morph="none" start_char="588" end_char="588">,</TOKEN>
<TOKEN id="token-3-29" pos="word" morph="none" start_char="590" end_char="592">sin</TOKEN>
<TOKEN id="token-3-30" pos="word" morph="none" start_char="594" end_char="600">embargo</TOKEN>
<TOKEN id="token-3-31" pos="punct" morph="none" start_char="601" end_char="601">,</TOKEN>
<TOKEN id="token-3-32" pos="word" morph="none" start_char="603" end_char="604">la</TOKEN>
<TOKEN id="token-3-33" pos="word" morph="none" start_char="606" end_char="616">institución</TOKEN>
<TOKEN id="token-3-34" pos="word" morph="none" start_char="618" end_char="625">advirtió</TOKEN>
<TOKEN id="token-3-35" pos="word" morph="none" start_char="627" end_char="631">desde</TOKEN>
<TOKEN id="token-3-36" pos="word" morph="none" start_char="633" end_char="634">un</TOKEN>
<TOKEN id="token-3-37" pos="word" morph="none" start_char="636" end_char="644">principio</TOKEN>
<TOKEN id="token-3-38" pos="word" morph="none" start_char="646" end_char="648">que</TOKEN>
<TOKEN id="token-3-39" pos="word" morph="none" start_char="650" end_char="652">los</TOKEN>
<TOKEN id="token-3-40" pos="word" morph="none" start_char="654" end_char="663">resultados</TOKEN>
<TOKEN id="token-3-41" pos="word" morph="none" start_char="665" end_char="666">no</TOKEN>
<TOKEN id="token-3-42" pos="word" morph="none" start_char="668" end_char="671">eran</TOKEN>
<TOKEN id="token-3-43" pos="word" morph="none" start_char="673" end_char="683">definitivos</TOKEN>
<TOKEN id="token-3-44" pos="punct" morph="none" start_char="684" end_char="684">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="687" end_char="1059">
<ORIGINAL_TEXT>El error, explicó Xiao Linhua, coautor del estudio, a la revista Nature, se debió a un error de comunicación entre el grupo de bioinformática y el grupo de laboratorio, pues la equivalencia del 99% del genoma se refería únicamente a una pequeña parte del genoma del coronavirus, es decir, sólo a la de una proteína que el virus utiliza para entrar a las células que afecta.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="687" end_char="688">El</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="690" end_char="694">error</TOKEN>
<TOKEN id="token-4-2" pos="punct" morph="none" start_char="695" end_char="695">,</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="697" end_char="703">explicó</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="705" end_char="708">Xiao</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="710" end_char="715">Linhua</TOKEN>
<TOKEN id="token-4-6" pos="punct" morph="none" start_char="716" end_char="716">,</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="718" end_char="724">coautor</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="726" end_char="728">del</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="730" end_char="736">estudio</TOKEN>
<TOKEN id="token-4-10" pos="punct" morph="none" start_char="737" end_char="737">,</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="739" end_char="739">a</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="741" end_char="742">la</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="744" end_char="750">revista</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="752" end_char="757">Nature</TOKEN>
<TOKEN id="token-4-15" pos="punct" morph="none" start_char="758" end_char="758">,</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="760" end_char="761">se</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="763" end_char="767">debió</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="769" end_char="769">a</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="771" end_char="772">un</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="774" end_char="778">error</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="780" end_char="781">de</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="783" end_char="794">comunicación</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="796" end_char="800">entre</TOKEN>
<TOKEN id="token-4-24" pos="word" morph="none" start_char="802" end_char="803">el</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="805" end_char="809">grupo</TOKEN>
<TOKEN id="token-4-26" pos="word" morph="none" start_char="811" end_char="812">de</TOKEN>
<TOKEN id="token-4-27" pos="word" morph="none" start_char="814" end_char="827">bioinformática</TOKEN>
<TOKEN id="token-4-28" pos="word" morph="none" start_char="829" end_char="829">y</TOKEN>
<TOKEN id="token-4-29" pos="word" morph="none" start_char="831" end_char="832">el</TOKEN>
<TOKEN id="token-4-30" pos="word" morph="none" start_char="834" end_char="838">grupo</TOKEN>
<TOKEN id="token-4-31" pos="word" morph="none" start_char="840" end_char="841">de</TOKEN>
<TOKEN id="token-4-32" pos="word" morph="none" start_char="843" end_char="853">laboratorio</TOKEN>
<TOKEN id="token-4-33" pos="punct" morph="none" start_char="854" end_char="854">,</TOKEN>
<TOKEN id="token-4-34" pos="word" morph="none" start_char="856" end_char="859">pues</TOKEN>
<TOKEN id="token-4-35" pos="word" morph="none" start_char="861" end_char="862">la</TOKEN>
<TOKEN id="token-4-36" pos="word" morph="none" start_char="864" end_char="875">equivalencia</TOKEN>
<TOKEN id="token-4-37" pos="word" morph="none" start_char="877" end_char="879">del</TOKEN>
<TOKEN id="token-4-38" pos="word" morph="none" start_char="881" end_char="882">99</TOKEN>
<TOKEN id="token-4-39" pos="punct" morph="none" start_char="883" end_char="883">%</TOKEN>
<TOKEN id="token-4-40" pos="word" morph="none" start_char="885" end_char="887">del</TOKEN>
<TOKEN id="token-4-41" pos="word" morph="none" start_char="889" end_char="894">genoma</TOKEN>
<TOKEN id="token-4-42" pos="word" morph="none" start_char="896" end_char="897">se</TOKEN>
<TOKEN id="token-4-43" pos="word" morph="none" start_char="899" end_char="905">refería</TOKEN>
<TOKEN id="token-4-44" pos="word" morph="none" start_char="907" end_char="916">únicamente</TOKEN>
<TOKEN id="token-4-45" pos="word" morph="none" start_char="918" end_char="918">a</TOKEN>
<TOKEN id="token-4-46" pos="word" morph="none" start_char="920" end_char="922">una</TOKEN>
<TOKEN id="token-4-47" pos="word" morph="none" start_char="924" end_char="930">pequeña</TOKEN>
<TOKEN id="token-4-48" pos="word" morph="none" start_char="932" end_char="936">parte</TOKEN>
<TOKEN id="token-4-49" pos="word" morph="none" start_char="938" end_char="940">del</TOKEN>
<TOKEN id="token-4-50" pos="word" morph="none" start_char="942" end_char="947">genoma</TOKEN>
<TOKEN id="token-4-51" pos="word" morph="none" start_char="949" end_char="951">del</TOKEN>
<TOKEN id="token-4-52" pos="word" morph="none" start_char="953" end_char="963">coronavirus</TOKEN>
<TOKEN id="token-4-53" pos="punct" morph="none" start_char="964" end_char="964">,</TOKEN>
<TOKEN id="token-4-54" pos="word" morph="none" start_char="966" end_char="967">es</TOKEN>
<TOKEN id="token-4-55" pos="word" morph="none" start_char="969" end_char="973">decir</TOKEN>
<TOKEN id="token-4-56" pos="punct" morph="none" start_char="974" end_char="974">,</TOKEN>
<TOKEN id="token-4-57" pos="word" morph="none" start_char="976" end_char="979">sólo</TOKEN>
<TOKEN id="token-4-58" pos="word" morph="none" start_char="981" end_char="981">a</TOKEN>
<TOKEN id="token-4-59" pos="word" morph="none" start_char="983" end_char="984">la</TOKEN>
<TOKEN id="token-4-60" pos="word" morph="none" start_char="986" end_char="987">de</TOKEN>
<TOKEN id="token-4-61" pos="word" morph="none" start_char="989" end_char="991">una</TOKEN>
<TOKEN id="token-4-62" pos="word" morph="none" start_char="993" end_char="1000">proteína</TOKEN>
<TOKEN id="token-4-63" pos="word" morph="none" start_char="1002" end_char="1004">que</TOKEN>
<TOKEN id="token-4-64" pos="word" morph="none" start_char="1006" end_char="1007">el</TOKEN>
<TOKEN id="token-4-65" pos="word" morph="none" start_char="1009" end_char="1013">virus</TOKEN>
<TOKEN id="token-4-66" pos="word" morph="none" start_char="1015" end_char="1021">utiliza</TOKEN>
<TOKEN id="token-4-67" pos="word" morph="none" start_char="1023" end_char="1026">para</TOKEN>
<TOKEN id="token-4-68" pos="word" morph="none" start_char="1028" end_char="1033">entrar</TOKEN>
<TOKEN id="token-4-69" pos="word" morph="none" start_char="1035" end_char="1035">a</TOKEN>
<TOKEN id="token-4-70" pos="word" morph="none" start_char="1037" end_char="1039">las</TOKEN>
<TOKEN id="token-4-71" pos="word" morph="none" start_char="1041" end_char="1047">células</TOKEN>
<TOKEN id="token-4-72" pos="word" morph="none" start_char="1049" end_char="1051">que</TOKEN>
<TOKEN id="token-4-73" pos="word" morph="none" start_char="1053" end_char="1058">afecta</TOKEN>
<TOKEN id="token-4-74" pos="punct" morph="none" start_char="1059" end_char="1059">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="1062" end_char="1165">
<ORIGINAL_TEXT>Sin embargo, si se tiene en cuenta la secuencia completa del genoma, la equivalencia se reduce al 90.3%.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="1062" end_char="1064">Sin</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="1066" end_char="1072">embargo</TOKEN>
<TOKEN id="token-5-2" pos="punct" morph="none" start_char="1073" end_char="1073">,</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="1075" end_char="1076">si</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="1078" end_char="1079">se</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="1081" end_char="1085">tiene</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="1087" end_char="1088">en</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="1090" end_char="1095">cuenta</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="1097" end_char="1098">la</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="1100" end_char="1108">secuencia</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="1110" end_char="1117">completa</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="1119" end_char="1121">del</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="1123" end_char="1128">genoma</TOKEN>
<TOKEN id="token-5-13" pos="punct" morph="none" start_char="1129" end_char="1129">,</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="1131" end_char="1132">la</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="1134" end_char="1145">equivalencia</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="1147" end_char="1148">se</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="1150" end_char="1155">reduce</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="1157" end_char="1158">al</TOKEN>
<TOKEN id="token-5-19" pos="unknown" morph="none" start_char="1160" end_char="1163">90.3</TOKEN>
<TOKEN id="token-5-20" pos="punct" morph="none" start_char="1164" end_char="1165">%.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
