<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CA2J" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="4313" raw_text_md5="ec6657d26bcba451156489bd8172980e">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="92">
<ORIGINAL_TEXT>No, this 2015 Italian TV news does not prove that the Covid-19 was created in the laboratory</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="2">No</TOKEN>
<TOKEN id="token-0-1" pos="punct" morph="none" start_char="3" end_char="3">,</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="5" end_char="8">this</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="10" end_char="13">2015</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="15" end_char="21">Italian</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="23" end_char="24">TV</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="26" end_char="29">news</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="31" end_char="34">does</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="36" end_char="38">not</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="40" end_char="44">prove</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="46" end_char="49">that</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="51" end_char="53">the</TOKEN>
<TOKEN id="token-0-12" pos="unknown" morph="none" start_char="55" end_char="62">Covid-19</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="64" end_char="66">was</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="68" end_char="74">created</TOKEN>
<TOKEN id="token-0-15" pos="word" morph="none" start_char="76" end_char="77">in</TOKEN>
<TOKEN id="token-0-16" pos="word" morph="none" start_char="79" end_char="81">the</TOKEN>
<TOKEN id="token-0-17" pos="word" morph="none" start_char="83" end_char="92">laboratory</TOKEN>
</SEG>
<SEG id="segment-1" start_char="97" end_char="143">
<ORIGINAL_TEXT>A sample for a screening test in Nice, April 2.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="97" end_char="97">A</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="99" end_char="104">sample</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="106" end_char="108">for</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="110" end_char="110">a</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="112" end_char="120">screening</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="122" end_char="125">test</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="127" end_char="128">in</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="130" end_char="133">Nice</TOKEN>
<TOKEN id="token-1-8" pos="punct" morph="none" start_char="134" end_char="134">,</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="136" end_char="140">April</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="142" end_char="142">2</TOKEN>
<TOKEN id="token-1-11" pos="punct" morph="none" start_char="143" end_char="143">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="145" end_char="145">
<ORIGINAL_TEXT>–</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="punct" morph="none" start_char="145" end_char="145">–</TOKEN>
</SEG>
<SEG id="segment-3" start_char="148" end_char="160">
<ORIGINAL_TEXT>SYSPEO / SIPA</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="148" end_char="153">SYSPEO</TOKEN>
<TOKEN id="token-3-1" pos="punct" morph="none" start_char="155" end_char="155">/</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="157" end_char="160">SIPA</TOKEN>
</SEG>
<SEG id="segment-4" start_char="166" end_char="276">
<ORIGINAL_TEXT>In 2015, the Leonardo program, broadcast on RAI, devoted a report to an experiment carried out in a laboratory.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="166" end_char="167">In</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="169" end_char="172">2015</TOKEN>
<TOKEN id="token-4-2" pos="punct" morph="none" start_char="173" end_char="173">,</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="175" end_char="177">the</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="179" end_char="186">Leonardo</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="188" end_char="194">program</TOKEN>
<TOKEN id="token-4-6" pos="punct" morph="none" start_char="195" end_char="195">,</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="197" end_char="205">broadcast</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="207" end_char="208">on</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="210" end_char="212">RAI</TOKEN>
<TOKEN id="token-4-10" pos="punct" morph="none" start_char="213" end_char="213">,</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="215" end_char="221">devoted</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="223" end_char="223">a</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="225" end_char="230">report</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="232" end_char="233">to</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="235" end_char="236">an</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="238" end_char="247">experiment</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="249" end_char="255">carried</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="257" end_char="259">out</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="261" end_char="262">in</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="264" end_char="264">a</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="266" end_char="275">laboratory</TOKEN>
<TOKEN id="token-4-22" pos="punct" morph="none" start_char="276" end_char="276">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="280" end_char="335">
<ORIGINAL_TEXT>The experiment involved a virus different from Covid-19.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="280" end_char="282">The</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="284" end_char="293">experiment</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="295" end_char="302">involved</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="304" end_char="304">a</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="306" end_char="310">virus</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="312" end_char="320">different</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="322" end_char="325">from</TOKEN>
<TOKEN id="token-5-7" pos="unknown" morph="none" start_char="327" end_char="334">Covid-19</TOKEN>
<TOKEN id="token-5-8" pos="punct" morph="none" start_char="335" end_char="335">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="339" end_char="391">
<ORIGINAL_TEXT>This verification request was sent to us by a reader.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="339" end_char="342">This</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="344" end_char="355">verification</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="357" end_char="363">request</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="365" end_char="367">was</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="369" end_char="372">sent</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="374" end_char="375">to</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="377" end_char="378">us</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="380" end_char="381">by</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="383" end_char="383">a</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="385" end_char="390">reader</TOKEN>
<TOKEN id="token-6-10" pos="punct" morph="none" start_char="391" end_char="391">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="396" end_char="474">
<ORIGINAL_TEXT>Why does a report broadcast in 2015 on Italian television resurface on YouTube?</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="396" end_char="398">Why</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="400" end_char="403">does</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="405" end_char="405">a</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="407" end_char="412">report</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="414" end_char="422">broadcast</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="424" end_char="425">in</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="427" end_char="430">2015</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="432" end_char="433">on</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="435" end_char="441">Italian</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="443" end_char="452">television</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="454" end_char="462">resurface</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="464" end_char="465">on</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="467" end_char="473">YouTube</TOKEN>
<TOKEN id="token-7-13" pos="punct" morph="none" start_char="474" end_char="474">?</TOKEN>
</SEG>
<SEG id="segment-8" start_char="476" end_char="627">
<ORIGINAL_TEXT>Posted on March 27 on a French-language channel with the title "A coronavirus created in the laboratory", it has accumulated nearly 110,000 views since.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="476" end_char="481">Posted</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="483" end_char="484">on</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="486" end_char="490">March</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="492" end_char="493">27</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="495" end_char="496">on</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="498" end_char="498">a</TOKEN>
<TOKEN id="token-8-6" pos="unknown" morph="none" start_char="500" end_char="514">French-language</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="516" end_char="522">channel</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="524" end_char="527">with</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="529" end_char="531">the</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="533" end_char="537">title</TOKEN>
<TOKEN id="token-8-11" pos="punct" morph="none" start_char="539" end_char="539">"</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="540" end_char="540">A</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="542" end_char="552">coronavirus</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="554" end_char="560">created</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="562" end_char="563">in</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="565" end_char="567">the</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="569" end_char="578">laboratory</TOKEN>
<TOKEN id="token-8-18" pos="punct" morph="none" start_char="579" end_char="580">",</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="582" end_char="583">it</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="585" end_char="587">has</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="589" end_char="599">accumulated</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="601" end_char="606">nearly</TOKEN>
<TOKEN id="token-8-23" pos="unknown" morph="none" start_char="608" end_char="614">110,000</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="616" end_char="620">views</TOKEN>
<TOKEN id="token-8-25" pos="word" morph="none" start_char="622" end_char="626">since</TOKEN>
<TOKEN id="token-8-26" pos="punct" morph="none" start_char="627" end_char="627">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="630" end_char="780">
<ORIGINAL_TEXT>The report deals with an experiment carried out on the SARS virus, which raged in 2003 and which is part of the family of coronaviruses, like Covid-19.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="630" end_char="632">The</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="634" end_char="639">report</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="641" end_char="645">deals</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="647" end_char="650">with</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="652" end_char="653">an</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="655" end_char="664">experiment</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="666" end_char="672">carried</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="674" end_char="676">out</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="678" end_char="679">on</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="681" end_char="683">the</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="685" end_char="688">SARS</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="690" end_char="694">virus</TOKEN>
<TOKEN id="token-9-12" pos="punct" morph="none" start_char="695" end_char="695">,</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="697" end_char="701">which</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="703" end_char="707">raged</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="709" end_char="710">in</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="712" end_char="715">2003</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="717" end_char="719">and</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="721" end_char="725">which</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="727" end_char="728">is</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="730" end_char="733">part</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="735" end_char="736">of</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="738" end_char="740">the</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="742" end_char="747">family</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="749" end_char="750">of</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="752" end_char="764">coronaviruses</TOKEN>
<TOKEN id="token-9-26" pos="punct" morph="none" start_char="765" end_char="765">,</TOKEN>
<TOKEN id="token-9-27" pos="word" morph="none" start_char="767" end_char="770">like</TOKEN>
<TOKEN id="token-9-28" pos="unknown" morph="none" start_char="772" end_char="779">Covid-19</TOKEN>
<TOKEN id="token-9-29" pos="punct" morph="none" start_char="780" end_char="780">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="782" end_char="945">
<ORIGINAL_TEXT>"It’s just an experiment, but it raises a lot of concerns," said the presenter of Leonardo, a science show broadcast on RAI, the Italian public broadcasting system.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="punct" morph="none" start_char="782" end_char="782">"</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="783" end_char="786">It’s</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="788" end_char="791">just</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="793" end_char="794">an</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="796" end_char="805">experiment</TOKEN>
<TOKEN id="token-10-5" pos="punct" morph="none" start_char="806" end_char="806">,</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="808" end_char="810">but</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="812" end_char="813">it</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="815" end_char="820">raises</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="822" end_char="822">a</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="824" end_char="826">lot</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="828" end_char="829">of</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="831" end_char="838">concerns</TOKEN>
<TOKEN id="token-10-13" pos="punct" morph="none" start_char="839" end_char="840">,"</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="842" end_char="845">said</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="847" end_char="849">the</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="851" end_char="859">presenter</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="861" end_char="862">of</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="864" end_char="871">Leonardo</TOKEN>
<TOKEN id="token-10-19" pos="punct" morph="none" start_char="872" end_char="872">,</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="874" end_char="874">a</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="876" end_char="882">science</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="884" end_char="887">show</TOKEN>
<TOKEN id="token-10-23" pos="word" morph="none" start_char="889" end_char="897">broadcast</TOKEN>
<TOKEN id="token-10-24" pos="word" morph="none" start_char="899" end_char="900">on</TOKEN>
<TOKEN id="token-10-25" pos="word" morph="none" start_char="902" end_char="904">RAI</TOKEN>
<TOKEN id="token-10-26" pos="punct" morph="none" start_char="905" end_char="905">,</TOKEN>
<TOKEN id="token-10-27" pos="word" morph="none" start_char="907" end_char="909">the</TOKEN>
<TOKEN id="token-10-28" pos="word" morph="none" start_char="911" end_char="917">Italian</TOKEN>
<TOKEN id="token-10-29" pos="word" morph="none" start_char="919" end_char="924">public</TOKEN>
<TOKEN id="token-10-30" pos="word" morph="none" start_char="926" end_char="937">broadcasting</TOKEN>
<TOKEN id="token-10-31" pos="word" morph="none" start_char="939" end_char="944">system</TOKEN>
<TOKEN id="token-10-32" pos="punct" morph="none" start_char="945" end_char="945">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="947" end_char="1065">
<ORIGINAL_TEXT>A group of Chinese researchers has grafted a protein from bats into the Sars virus – acute pneumonia – taken from mice.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="947" end_char="947">A</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="949" end_char="953">group</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="955" end_char="956">of</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="958" end_char="964">Chinese</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="966" end_char="976">researchers</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="978" end_char="980">has</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="982" end_char="988">grafted</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="990" end_char="990">a</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="992" end_char="998">protein</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1000" end_char="1003">from</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1005" end_char="1008">bats</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1010" end_char="1013">into</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1015" end_char="1017">the</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1019" end_char="1022">Sars</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1024" end_char="1028">virus</TOKEN>
<TOKEN id="token-11-15" pos="punct" morph="none" start_char="1030" end_char="1030">–</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1032" end_char="1036">acute</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1038" end_char="1046">pneumonia</TOKEN>
<TOKEN id="token-11-18" pos="punct" morph="none" start_char="1048" end_char="1048">–</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1050" end_char="1054">taken</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1056" end_char="1059">from</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1061" end_char="1064">mice</TOKEN>
<TOKEN id="token-11-22" pos="punct" morph="none" start_char="1065" end_char="1065">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1067" end_char="1119">
<ORIGINAL_TEXT>The result is a super virus that could strike humans.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1067" end_char="1069">The</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1071" end_char="1076">result</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1078" end_char="1079">is</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1081" end_char="1081">a</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1083" end_char="1087">super</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1089" end_char="1093">virus</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1095" end_char="1098">that</TOKEN>
<TOKEN id="token-12-7" pos="word" morph="none" start_char="1100" end_char="1104">could</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1106" end_char="1111">strike</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1113" end_char="1118">humans</TOKEN>
<TOKEN id="token-12-10" pos="punct" morph="none" start_char="1119" end_char="1119">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1121" end_char="1197">
<ORIGINAL_TEXT>The presenter immediately added that the virus "is confined in a laboratory".</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1121" end_char="1123">The</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1125" end_char="1133">presenter</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1135" end_char="1145">immediately</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1147" end_char="1151">added</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1153" end_char="1156">that</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1158" end_char="1160">the</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1162" end_char="1166">virus</TOKEN>
<TOKEN id="token-13-7" pos="punct" morph="none" start_char="1168" end_char="1168">"</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1169" end_char="1170">is</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1172" end_char="1179">confined</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1181" end_char="1182">in</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1184" end_char="1184">a</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1186" end_char="1195">laboratory</TOKEN>
<TOKEN id="token-13-13" pos="punct" morph="none" start_char="1196" end_char="1197">".</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1200" end_char="1304">
<ORIGINAL_TEXT>The report was relayed by Matteo Salvini, former interior minister and member of the League, on March 25.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1200" end_char="1202">The</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1204" end_char="1209">report</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1211" end_char="1213">was</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1215" end_char="1221">relayed</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1223" end_char="1224">by</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1226" end_char="1231">Matteo</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1233" end_char="1239">Salvini</TOKEN>
<TOKEN id="token-14-7" pos="punct" morph="none" start_char="1240" end_char="1240">,</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1242" end_char="1247">former</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1249" end_char="1256">interior</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1258" end_char="1265">minister</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1267" end_char="1269">and</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="1271" end_char="1276">member</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="1278" end_char="1279">of</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="1281" end_char="1283">the</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="1285" end_char="1290">League</TOKEN>
<TOKEN id="token-14-16" pos="punct" morph="none" start_char="1291" end_char="1291">,</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="1293" end_char="1294">on</TOKEN>
<TOKEN id="token-14-18" pos="word" morph="none" start_char="1296" end_char="1300">March</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="1302" end_char="1303">25</TOKEN>
<TOKEN id="token-14-20" pos="punct" morph="none" start_char="1304" end_char="1304">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1307" end_char="1310">
<ORIGINAL_TEXT>????</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="punct" morph="none" start_char="1307" end_char="1310">????</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1312" end_char="1330">
<ORIGINAL_TEXT>INCREDIBILE !!! ???</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1312" end_char="1322">INCREDIBILE</TOKEN>
<TOKEN id="token-16-1" pos="punct" morph="none" start_char="1324" end_char="1326">!!!</TOKEN>
<TOKEN id="token-16-2" pos="punct" morph="none" start_char="1328" end_char="1330">???</TOKEN>
</SEG>
<SEG id="segment-17" start_char="1332" end_char="1519">
<ORIGINAL_TEXT>?Da Tgr Leonardo (Rai Tre) del 16.11.2015 servizio su un supervirus polmonare Coronavirus creato dai cinesi con pipistrelli e topi, pericolosissimo per l’uomo (con annesse preoccupazioni).</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="punct" morph="none" start_char="1332" end_char="1332">?</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="1333" end_char="1334">Da</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="1336" end_char="1338">Tgr</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="1340" end_char="1347">Leonardo</TOKEN>
<TOKEN id="token-17-4" pos="punct" morph="none" start_char="1349" end_char="1349">(</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="1350" end_char="1352">Rai</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="1354" end_char="1356">Tre</TOKEN>
<TOKEN id="token-17-7" pos="punct" morph="none" start_char="1357" end_char="1357">)</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="1359" end_char="1361">del</TOKEN>
<TOKEN id="token-17-9" pos="unknown" morph="none" start_char="1363" end_char="1372">16.11.2015</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="1374" end_char="1381">servizio</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="1383" end_char="1384">su</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="1386" end_char="1387">un</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="1389" end_char="1398">supervirus</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="1400" end_char="1408">polmonare</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="1410" end_char="1420">Coronavirus</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="1422" end_char="1427">creato</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="1429" end_char="1431">dai</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="1433" end_char="1438">cinesi</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="1440" end_char="1442">con</TOKEN>
<TOKEN id="token-17-20" pos="word" morph="none" start_char="1444" end_char="1454">pipistrelli</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="1456" end_char="1456">e</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="1458" end_char="1461">topi</TOKEN>
<TOKEN id="token-17-23" pos="punct" morph="none" start_char="1462" end_char="1462">,</TOKEN>
<TOKEN id="token-17-24" pos="word" morph="none" start_char="1464" end_char="1478">pericolosissimo</TOKEN>
<TOKEN id="token-17-25" pos="word" morph="none" start_char="1480" end_char="1482">per</TOKEN>
<TOKEN id="token-17-26" pos="word" morph="none" start_char="1484" end_char="1489">l’uomo</TOKEN>
<TOKEN id="token-17-27" pos="punct" morph="none" start_char="1491" end_char="1491">(</TOKEN>
<TOKEN id="token-17-28" pos="word" morph="none" start_char="1492" end_char="1494">con</TOKEN>
<TOKEN id="token-17-29" pos="word" morph="none" start_char="1496" end_char="1502">annesse</TOKEN>
<TOKEN id="token-17-30" pos="word" morph="none" start_char="1504" end_char="1517">preoccupazioni</TOKEN>
<TOKEN id="token-17-31" pos="punct" morph="none" start_char="1518" end_char="1519">).</TOKEN>
</SEG>
<SEG id="segment-18" start_char="1521" end_char="1603">
<ORIGINAL_TEXT>(1/2) pic.twitter.com/QuLG07XdAt – Matteo Salvini (@matteosalvinimi) March 25, 2020</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="punct" morph="none" start_char="1521" end_char="1521">(</TOKEN>
<TOKEN id="token-18-1" pos="unknown" morph="none" start_char="1522" end_char="1524">1/2</TOKEN>
<TOKEN id="token-18-2" pos="punct" morph="none" start_char="1525" end_char="1525">)</TOKEN>
<TOKEN id="token-18-3" pos="unknown" morph="none" start_char="1527" end_char="1552">pic.twitter.com/QuLG07XdAt</TOKEN>
<TOKEN id="token-18-4" pos="punct" morph="none" start_char="1554" end_char="1554">–</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="1556" end_char="1561">Matteo</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="1563" end_char="1569">Salvini</TOKEN>
<TOKEN id="token-18-7" pos="punct" morph="none" start_char="1571" end_char="1572">(@</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="1573" end_char="1587">matteosalvinimi</TOKEN>
<TOKEN id="token-18-9" pos="punct" morph="none" start_char="1588" end_char="1588">)</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="1590" end_char="1594">March</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="1596" end_char="1597">25</TOKEN>
<TOKEN id="token-18-12" pos="punct" morph="none" start_char="1598" end_char="1598">,</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="1600" end_char="1603">2020</TOKEN>
</SEG>
<SEG id="segment-19" start_char="1606" end_char="1613">
<ORIGINAL_TEXT>FAKE OFF</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="1606" end_char="1609">FAKE</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="1611" end_char="1613">OFF</TOKEN>
</SEG>
<SEG id="segment-20" start_char="1616" end_char="1668">
<ORIGINAL_TEXT>RAI report alluded to 2015 study published in journal</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="1616" end_char="1618">RAI</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="1620" end_char="1625">report</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="1627" end_char="1633">alluded</TOKEN>
<TOKEN id="token-20-3" pos="word" morph="none" start_char="1635" end_char="1636">to</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="1638" end_char="1641">2015</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="1643" end_char="1647">study</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="1649" end_char="1657">published</TOKEN>
<TOKEN id="token-20-7" pos="word" morph="none" start_char="1659" end_char="1660">in</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="1662" end_char="1668">journal</TOKEN>
</SEG>
<SEG id="segment-21" start_char="1671" end_char="1685">
<ORIGINAL_TEXT>Nature Medicine</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="1671" end_char="1676">Nature</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="1678" end_char="1685">Medicine</TOKEN>
</SEG>
<SEG id="segment-22" start_char="1688" end_char="1688">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="punct" morph="none" start_char="1688" end_char="1688">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="1690" end_char="1905">
<ORIGINAL_TEXT>Carried out by researchers from the University of North Carolina, the Harvard School of Medicine and the Wuhan Institute of Virology, the study looked well at a virus created using the method described in the report.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="1690" end_char="1696">Carried</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="1698" end_char="1700">out</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="1702" end_char="1703">by</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="1705" end_char="1715">researchers</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="1717" end_char="1720">from</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="1722" end_char="1724">the</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="1726" end_char="1735">University</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="1737" end_char="1738">of</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="1740" end_char="1744">North</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="1746" end_char="1753">Carolina</TOKEN>
<TOKEN id="token-23-10" pos="punct" morph="none" start_char="1754" end_char="1754">,</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="1756" end_char="1758">the</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="1760" end_char="1766">Harvard</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="1768" end_char="1773">School</TOKEN>
<TOKEN id="token-23-14" pos="word" morph="none" start_char="1775" end_char="1776">of</TOKEN>
<TOKEN id="token-23-15" pos="word" morph="none" start_char="1778" end_char="1785">Medicine</TOKEN>
<TOKEN id="token-23-16" pos="word" morph="none" start_char="1787" end_char="1789">and</TOKEN>
<TOKEN id="token-23-17" pos="word" morph="none" start_char="1791" end_char="1793">the</TOKEN>
<TOKEN id="token-23-18" pos="word" morph="none" start_char="1795" end_char="1799">Wuhan</TOKEN>
<TOKEN id="token-23-19" pos="word" morph="none" start_char="1801" end_char="1809">Institute</TOKEN>
<TOKEN id="token-23-20" pos="word" morph="none" start_char="1811" end_char="1812">of</TOKEN>
<TOKEN id="token-23-21" pos="word" morph="none" start_char="1814" end_char="1821">Virology</TOKEN>
<TOKEN id="token-23-22" pos="punct" morph="none" start_char="1822" end_char="1822">,</TOKEN>
<TOKEN id="token-23-23" pos="word" morph="none" start_char="1824" end_char="1826">the</TOKEN>
<TOKEN id="token-23-24" pos="word" morph="none" start_char="1828" end_char="1832">study</TOKEN>
<TOKEN id="token-23-25" pos="word" morph="none" start_char="1834" end_char="1839">looked</TOKEN>
<TOKEN id="token-23-26" pos="word" morph="none" start_char="1841" end_char="1844">well</TOKEN>
<TOKEN id="token-23-27" pos="word" morph="none" start_char="1846" end_char="1847">at</TOKEN>
<TOKEN id="token-23-28" pos="word" morph="none" start_char="1849" end_char="1849">a</TOKEN>
<TOKEN id="token-23-29" pos="word" morph="none" start_char="1851" end_char="1855">virus</TOKEN>
<TOKEN id="token-23-30" pos="word" morph="none" start_char="1857" end_char="1863">created</TOKEN>
<TOKEN id="token-23-31" pos="word" morph="none" start_char="1865" end_char="1869">using</TOKEN>
<TOKEN id="token-23-32" pos="word" morph="none" start_char="1871" end_char="1873">the</TOKEN>
<TOKEN id="token-23-33" pos="word" morph="none" start_char="1875" end_char="1880">method</TOKEN>
<TOKEN id="token-23-34" pos="word" morph="none" start_char="1882" end_char="1890">described</TOKEN>
<TOKEN id="token-23-35" pos="word" morph="none" start_char="1892" end_char="1893">in</TOKEN>
<TOKEN id="token-23-36" pos="word" morph="none" start_char="1895" end_char="1897">the</TOKEN>
<TOKEN id="token-23-37" pos="word" morph="none" start_char="1899" end_char="1904">report</TOKEN>
<TOKEN id="token-23-38" pos="punct" morph="none" start_char="1905" end_char="1905">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="1908" end_char="2135">
<ORIGINAL_TEXT>"The current virus is completely different from the Sars virus [qui a sévi en 2003] and the study virus, "Antonio Lanzavecchia, one of the researchers who participated in the study, said on March 26 in the same Leonardo program.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="punct" morph="none" start_char="1908" end_char="1908">"</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="1909" end_char="1911">The</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="1913" end_char="1919">current</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="1921" end_char="1925">virus</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="1927" end_char="1928">is</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="1930" end_char="1939">completely</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="1941" end_char="1949">different</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="1951" end_char="1954">from</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="1956" end_char="1958">the</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="1960" end_char="1963">Sars</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="1965" end_char="1969">virus</TOKEN>
<TOKEN id="token-24-11" pos="punct" morph="none" start_char="1971" end_char="1971">[</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="1972" end_char="1974">qui</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="1976" end_char="1976">a</TOKEN>
<TOKEN id="token-24-14" pos="word" morph="none" start_char="1978" end_char="1981">sévi</TOKEN>
<TOKEN id="token-24-15" pos="word" morph="none" start_char="1983" end_char="1984">en</TOKEN>
<TOKEN id="token-24-16" pos="word" morph="none" start_char="1986" end_char="1989">2003</TOKEN>
<TOKEN id="token-24-17" pos="punct" morph="none" start_char="1990" end_char="1990">]</TOKEN>
<TOKEN id="token-24-18" pos="word" morph="none" start_char="1992" end_char="1994">and</TOKEN>
<TOKEN id="token-24-19" pos="word" morph="none" start_char="1996" end_char="1998">the</TOKEN>
<TOKEN id="token-24-20" pos="word" morph="none" start_char="2000" end_char="2004">study</TOKEN>
<TOKEN id="token-24-21" pos="word" morph="none" start_char="2006" end_char="2010">virus</TOKEN>
<TOKEN id="token-24-22" pos="punct" morph="none" start_char="2011" end_char="2011">,</TOKEN>
<TOKEN id="token-24-23" pos="punct" morph="none" start_char="2013" end_char="2013">"</TOKEN>
<TOKEN id="token-24-24" pos="word" morph="none" start_char="2014" end_char="2020">Antonio</TOKEN>
<TOKEN id="token-24-25" pos="word" morph="none" start_char="2022" end_char="2033">Lanzavecchia</TOKEN>
<TOKEN id="token-24-26" pos="punct" morph="none" start_char="2034" end_char="2034">,</TOKEN>
<TOKEN id="token-24-27" pos="word" morph="none" start_char="2036" end_char="2038">one</TOKEN>
<TOKEN id="token-24-28" pos="word" morph="none" start_char="2040" end_char="2041">of</TOKEN>
<TOKEN id="token-24-29" pos="word" morph="none" start_char="2043" end_char="2045">the</TOKEN>
<TOKEN id="token-24-30" pos="word" morph="none" start_char="2047" end_char="2057">researchers</TOKEN>
<TOKEN id="token-24-31" pos="word" morph="none" start_char="2059" end_char="2061">who</TOKEN>
<TOKEN id="token-24-32" pos="word" morph="none" start_char="2063" end_char="2074">participated</TOKEN>
<TOKEN id="token-24-33" pos="word" morph="none" start_char="2076" end_char="2077">in</TOKEN>
<TOKEN id="token-24-34" pos="word" morph="none" start_char="2079" end_char="2081">the</TOKEN>
<TOKEN id="token-24-35" pos="word" morph="none" start_char="2083" end_char="2087">study</TOKEN>
<TOKEN id="token-24-36" pos="punct" morph="none" start_char="2088" end_char="2088">,</TOKEN>
<TOKEN id="token-24-37" pos="word" morph="none" start_char="2090" end_char="2093">said</TOKEN>
<TOKEN id="token-24-38" pos="word" morph="none" start_char="2095" end_char="2096">on</TOKEN>
<TOKEN id="token-24-39" pos="word" morph="none" start_char="2098" end_char="2102">March</TOKEN>
<TOKEN id="token-24-40" pos="word" morph="none" start_char="2104" end_char="2105">26</TOKEN>
<TOKEN id="token-24-41" pos="word" morph="none" start_char="2107" end_char="2108">in</TOKEN>
<TOKEN id="token-24-42" pos="word" morph="none" start_char="2110" end_char="2112">the</TOKEN>
<TOKEN id="token-24-43" pos="word" morph="none" start_char="2114" end_char="2117">same</TOKEN>
<TOKEN id="token-24-44" pos="word" morph="none" start_char="2119" end_char="2126">Leonardo</TOKEN>
<TOKEN id="token-24-45" pos="word" morph="none" start_char="2128" end_char="2134">program</TOKEN>
<TOKEN id="token-24-46" pos="punct" morph="none" start_char="2135" end_char="2135">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="2137" end_char="2236">
<ORIGINAL_TEXT>The Covid-19 and the 2015 virus "are completely different," he says, particularly in their sequence.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="2137" end_char="2139">The</TOKEN>
<TOKEN id="token-25-1" pos="unknown" morph="none" start_char="2141" end_char="2148">Covid-19</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="2150" end_char="2152">and</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="2154" end_char="2156">the</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="2158" end_char="2161">2015</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="2163" end_char="2167">virus</TOKEN>
<TOKEN id="token-25-6" pos="punct" morph="none" start_char="2169" end_char="2169">"</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="2170" end_char="2172">are</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="2174" end_char="2183">completely</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="2185" end_char="2193">different</TOKEN>
<TOKEN id="token-25-10" pos="punct" morph="none" start_char="2194" end_char="2195">,"</TOKEN>
<TOKEN id="token-25-11" pos="word" morph="none" start_char="2197" end_char="2198">he</TOKEN>
<TOKEN id="token-25-12" pos="word" morph="none" start_char="2200" end_char="2203">says</TOKEN>
<TOKEN id="token-25-13" pos="punct" morph="none" start_char="2204" end_char="2204">,</TOKEN>
<TOKEN id="token-25-14" pos="word" morph="none" start_char="2206" end_char="2217">particularly</TOKEN>
<TOKEN id="token-25-15" pos="word" morph="none" start_char="2219" end_char="2220">in</TOKEN>
<TOKEN id="token-25-16" pos="word" morph="none" start_char="2222" end_char="2226">their</TOKEN>
<TOKEN id="token-25-17" pos="word" morph="none" start_char="2228" end_char="2235">sequence</TOKEN>
<TOKEN id="token-25-18" pos="punct" morph="none" start_char="2236" end_char="2236">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="2239" end_char="2311">
<ORIGINAL_TEXT>Covid-19 "is not a laboratory construct or a purposely manipulated virus"</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="unknown" morph="none" start_char="2239" end_char="2246">Covid-19</TOKEN>
<TOKEN id="token-26-1" pos="punct" morph="none" start_char="2248" end_char="2248">"</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="2249" end_char="2250">is</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="2252" end_char="2254">not</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="2256" end_char="2256">a</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="2258" end_char="2267">laboratory</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="2269" end_char="2277">construct</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="2279" end_char="2280">or</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="2282" end_char="2282">a</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="2284" end_char="2292">purposely</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="2294" end_char="2304">manipulated</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="2306" end_char="2310">virus</TOKEN>
<TOKEN id="token-26-12" pos="punct" morph="none" start_char="2311" end_char="2311">"</TOKEN>
</SEG>
<SEG id="segment-27" start_char="2315" end_char="2518">
<ORIGINAL_TEXT>In a note added to the 2015 study on March 30, the publisher said, "We are aware that this article is used as the basis for unverified theories that the new coronavirus that causes Covid-19 has been made.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="2315" end_char="2316">In</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="2318" end_char="2318">a</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="2320" end_char="2323">note</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="2325" end_char="2329">added</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="2331" end_char="2332">to</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="2334" end_char="2336">the</TOKEN>
<TOKEN id="token-27-6" pos="word" morph="none" start_char="2338" end_char="2341">2015</TOKEN>
<TOKEN id="token-27-7" pos="word" morph="none" start_char="2343" end_char="2347">study</TOKEN>
<TOKEN id="token-27-8" pos="word" morph="none" start_char="2349" end_char="2350">on</TOKEN>
<TOKEN id="token-27-9" pos="word" morph="none" start_char="2352" end_char="2356">March</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="2358" end_char="2359">30</TOKEN>
<TOKEN id="token-27-11" pos="punct" morph="none" start_char="2360" end_char="2360">,</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="2362" end_char="2364">the</TOKEN>
<TOKEN id="token-27-13" pos="word" morph="none" start_char="2366" end_char="2374">publisher</TOKEN>
<TOKEN id="token-27-14" pos="word" morph="none" start_char="2376" end_char="2379">said</TOKEN>
<TOKEN id="token-27-15" pos="punct" morph="none" start_char="2380" end_char="2380">,</TOKEN>
<TOKEN id="token-27-16" pos="punct" morph="none" start_char="2382" end_char="2382">"</TOKEN>
<TOKEN id="token-27-17" pos="word" morph="none" start_char="2383" end_char="2384">We</TOKEN>
<TOKEN id="token-27-18" pos="word" morph="none" start_char="2386" end_char="2388">are</TOKEN>
<TOKEN id="token-27-19" pos="word" morph="none" start_char="2390" end_char="2394">aware</TOKEN>
<TOKEN id="token-27-20" pos="word" morph="none" start_char="2396" end_char="2399">that</TOKEN>
<TOKEN id="token-27-21" pos="word" morph="none" start_char="2401" end_char="2404">this</TOKEN>
<TOKEN id="token-27-22" pos="word" morph="none" start_char="2406" end_char="2412">article</TOKEN>
<TOKEN id="token-27-23" pos="word" morph="none" start_char="2414" end_char="2415">is</TOKEN>
<TOKEN id="token-27-24" pos="word" morph="none" start_char="2417" end_char="2420">used</TOKEN>
<TOKEN id="token-27-25" pos="word" morph="none" start_char="2422" end_char="2423">as</TOKEN>
<TOKEN id="token-27-26" pos="word" morph="none" start_char="2425" end_char="2427">the</TOKEN>
<TOKEN id="token-27-27" pos="word" morph="none" start_char="2429" end_char="2433">basis</TOKEN>
<TOKEN id="token-27-28" pos="word" morph="none" start_char="2435" end_char="2437">for</TOKEN>
<TOKEN id="token-27-29" pos="word" morph="none" start_char="2439" end_char="2448">unverified</TOKEN>
<TOKEN id="token-27-30" pos="word" morph="none" start_char="2450" end_char="2457">theories</TOKEN>
<TOKEN id="token-27-31" pos="word" morph="none" start_char="2459" end_char="2462">that</TOKEN>
<TOKEN id="token-27-32" pos="word" morph="none" start_char="2464" end_char="2466">the</TOKEN>
<TOKEN id="token-27-33" pos="word" morph="none" start_char="2468" end_char="2470">new</TOKEN>
<TOKEN id="token-27-34" pos="word" morph="none" start_char="2472" end_char="2482">coronavirus</TOKEN>
<TOKEN id="token-27-35" pos="word" morph="none" start_char="2484" end_char="2487">that</TOKEN>
<TOKEN id="token-27-36" pos="word" morph="none" start_char="2489" end_char="2494">causes</TOKEN>
<TOKEN id="token-27-37" pos="unknown" morph="none" start_char="2496" end_char="2503">Covid-19</TOKEN>
<TOKEN id="token-27-38" pos="word" morph="none" start_char="2505" end_char="2507">has</TOKEN>
<TOKEN id="token-27-39" pos="word" morph="none" start_char="2509" end_char="2512">been</TOKEN>
<TOKEN id="token-27-40" pos="word" morph="none" start_char="2514" end_char="2517">made</TOKEN>
<TOKEN id="token-27-41" pos="punct" morph="none" start_char="2518" end_char="2518">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="2520" end_char="2636">
<ORIGINAL_TEXT>There is no evidence that this is true; scientists believe that an animal is the likely source of this coronavirus. "</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="2520" end_char="2524">There</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="2526" end_char="2527">is</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="2529" end_char="2530">no</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="2532" end_char="2539">evidence</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="2541" end_char="2544">that</TOKEN>
<TOKEN id="token-28-5" pos="word" morph="none" start_char="2546" end_char="2549">this</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="2551" end_char="2552">is</TOKEN>
<TOKEN id="token-28-7" pos="word" morph="none" start_char="2554" end_char="2557">true</TOKEN>
<TOKEN id="token-28-8" pos="punct" morph="none" start_char="2558" end_char="2558">;</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="2560" end_char="2569">scientists</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="2571" end_char="2577">believe</TOKEN>
<TOKEN id="token-28-11" pos="word" morph="none" start_char="2579" end_char="2582">that</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="2584" end_char="2585">an</TOKEN>
<TOKEN id="token-28-13" pos="word" morph="none" start_char="2587" end_char="2592">animal</TOKEN>
<TOKEN id="token-28-14" pos="word" morph="none" start_char="2594" end_char="2595">is</TOKEN>
<TOKEN id="token-28-15" pos="word" morph="none" start_char="2597" end_char="2599">the</TOKEN>
<TOKEN id="token-28-16" pos="word" morph="none" start_char="2601" end_char="2606">likely</TOKEN>
<TOKEN id="token-28-17" pos="word" morph="none" start_char="2608" end_char="2613">source</TOKEN>
<TOKEN id="token-28-18" pos="word" morph="none" start_char="2615" end_char="2616">of</TOKEN>
<TOKEN id="token-28-19" pos="word" morph="none" start_char="2618" end_char="2621">this</TOKEN>
<TOKEN id="token-28-20" pos="word" morph="none" start_char="2623" end_char="2633">coronavirus</TOKEN>
<TOKEN id="token-28-21" pos="punct" morph="none" start_char="2634" end_char="2634">.</TOKEN>
<TOKEN id="token-28-22" pos="punct" morph="none" start_char="2636" end_char="2636">"</TOKEN>
</SEG>
<SEG id="segment-29" start_char="2639" end_char="2817">
<ORIGINAL_TEXT>A study on the origin of Covid-19 published on March 17, 2020 in the same journal concluded that this new virus "is not a laboratory construct or a virus manipulated with design".</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="2639" end_char="2639">A</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="2641" end_char="2645">study</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="2647" end_char="2648">on</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="2650" end_char="2652">the</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="2654" end_char="2659">origin</TOKEN>
<TOKEN id="token-29-5" pos="word" morph="none" start_char="2661" end_char="2662">of</TOKEN>
<TOKEN id="token-29-6" pos="unknown" morph="none" start_char="2664" end_char="2671">Covid-19</TOKEN>
<TOKEN id="token-29-7" pos="word" morph="none" start_char="2673" end_char="2681">published</TOKEN>
<TOKEN id="token-29-8" pos="word" morph="none" start_char="2683" end_char="2684">on</TOKEN>
<TOKEN id="token-29-9" pos="word" morph="none" start_char="2686" end_char="2690">March</TOKEN>
<TOKEN id="token-29-10" pos="word" morph="none" start_char="2692" end_char="2693">17</TOKEN>
<TOKEN id="token-29-11" pos="punct" morph="none" start_char="2694" end_char="2694">,</TOKEN>
<TOKEN id="token-29-12" pos="word" morph="none" start_char="2696" end_char="2699">2020</TOKEN>
<TOKEN id="token-29-13" pos="word" morph="none" start_char="2701" end_char="2702">in</TOKEN>
<TOKEN id="token-29-14" pos="word" morph="none" start_char="2704" end_char="2706">the</TOKEN>
<TOKEN id="token-29-15" pos="word" morph="none" start_char="2708" end_char="2711">same</TOKEN>
<TOKEN id="token-29-16" pos="word" morph="none" start_char="2713" end_char="2719">journal</TOKEN>
<TOKEN id="token-29-17" pos="word" morph="none" start_char="2721" end_char="2729">concluded</TOKEN>
<TOKEN id="token-29-18" pos="word" morph="none" start_char="2731" end_char="2734">that</TOKEN>
<TOKEN id="token-29-19" pos="word" morph="none" start_char="2736" end_char="2739">this</TOKEN>
<TOKEN id="token-29-20" pos="word" morph="none" start_char="2741" end_char="2743">new</TOKEN>
<TOKEN id="token-29-21" pos="word" morph="none" start_char="2745" end_char="2749">virus</TOKEN>
<TOKEN id="token-29-22" pos="punct" morph="none" start_char="2751" end_char="2751">"</TOKEN>
<TOKEN id="token-29-23" pos="word" morph="none" start_char="2752" end_char="2753">is</TOKEN>
<TOKEN id="token-29-24" pos="word" morph="none" start_char="2755" end_char="2757">not</TOKEN>
<TOKEN id="token-29-25" pos="word" morph="none" start_char="2759" end_char="2759">a</TOKEN>
<TOKEN id="token-29-26" pos="word" morph="none" start_char="2761" end_char="2770">laboratory</TOKEN>
<TOKEN id="token-29-27" pos="word" morph="none" start_char="2772" end_char="2780">construct</TOKEN>
<TOKEN id="token-29-28" pos="word" morph="none" start_char="2782" end_char="2783">or</TOKEN>
<TOKEN id="token-29-29" pos="word" morph="none" start_char="2785" end_char="2785">a</TOKEN>
<TOKEN id="token-29-30" pos="word" morph="none" start_char="2787" end_char="2791">virus</TOKEN>
<TOKEN id="token-29-31" pos="word" morph="none" start_char="2793" end_char="2803">manipulated</TOKEN>
<TOKEN id="token-29-32" pos="word" morph="none" start_char="2805" end_char="2808">with</TOKEN>
<TOKEN id="token-29-33" pos="word" morph="none" start_char="2810" end_char="2815">design</TOKEN>
<TOKEN id="token-29-34" pos="punct" morph="none" start_char="2816" end_char="2817">".</TOKEN>
</SEG>
<SEG id="segment-30" start_char="2820" end_char="3028">
<ORIGINAL_TEXT>Olivier Schwartz, head of the virus and immunity unit at the Institut Pasteur, also recalled that scientists are able to distinguish if a virus has been manipulated in the laboratory by analyzing its sequence.</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="2820" end_char="2826">Olivier</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="2828" end_char="2835">Schwartz</TOKEN>
<TOKEN id="token-30-2" pos="punct" morph="none" start_char="2836" end_char="2836">,</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="2838" end_char="2841">head</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="2843" end_char="2844">of</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="2846" end_char="2848">the</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="2850" end_char="2854">virus</TOKEN>
<TOKEN id="token-30-7" pos="word" morph="none" start_char="2856" end_char="2858">and</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="2860" end_char="2867">immunity</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="2869" end_char="2872">unit</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="2874" end_char="2875">at</TOKEN>
<TOKEN id="token-30-11" pos="word" morph="none" start_char="2877" end_char="2879">the</TOKEN>
<TOKEN id="token-30-12" pos="word" morph="none" start_char="2881" end_char="2888">Institut</TOKEN>
<TOKEN id="token-30-13" pos="word" morph="none" start_char="2890" end_char="2896">Pasteur</TOKEN>
<TOKEN id="token-30-14" pos="punct" morph="none" start_char="2897" end_char="2897">,</TOKEN>
<TOKEN id="token-30-15" pos="word" morph="none" start_char="2899" end_char="2902">also</TOKEN>
<TOKEN id="token-30-16" pos="word" morph="none" start_char="2904" end_char="2911">recalled</TOKEN>
<TOKEN id="token-30-17" pos="word" morph="none" start_char="2913" end_char="2916">that</TOKEN>
<TOKEN id="token-30-18" pos="word" morph="none" start_char="2918" end_char="2927">scientists</TOKEN>
<TOKEN id="token-30-19" pos="word" morph="none" start_char="2929" end_char="2931">are</TOKEN>
<TOKEN id="token-30-20" pos="word" morph="none" start_char="2933" end_char="2936">able</TOKEN>
<TOKEN id="token-30-21" pos="word" morph="none" start_char="2938" end_char="2939">to</TOKEN>
<TOKEN id="token-30-22" pos="word" morph="none" start_char="2941" end_char="2951">distinguish</TOKEN>
<TOKEN id="token-30-23" pos="word" morph="none" start_char="2953" end_char="2954">if</TOKEN>
<TOKEN id="token-30-24" pos="word" morph="none" start_char="2956" end_char="2956">a</TOKEN>
<TOKEN id="token-30-25" pos="word" morph="none" start_char="2958" end_char="2962">virus</TOKEN>
<TOKEN id="token-30-26" pos="word" morph="none" start_char="2964" end_char="2966">has</TOKEN>
<TOKEN id="token-30-27" pos="word" morph="none" start_char="2968" end_char="2971">been</TOKEN>
<TOKEN id="token-30-28" pos="word" morph="none" start_char="2973" end_char="2983">manipulated</TOKEN>
<TOKEN id="token-30-29" pos="word" morph="none" start_char="2985" end_char="2986">in</TOKEN>
<TOKEN id="token-30-30" pos="word" morph="none" start_char="2988" end_char="2990">the</TOKEN>
<TOKEN id="token-30-31" pos="word" morph="none" start_char="2992" end_char="3001">laboratory</TOKEN>
<TOKEN id="token-30-32" pos="word" morph="none" start_char="3003" end_char="3004">by</TOKEN>
<TOKEN id="token-30-33" pos="word" morph="none" start_char="3006" end_char="3014">analyzing</TOKEN>
<TOKEN id="token-30-34" pos="word" morph="none" start_char="3016" end_char="3018">its</TOKEN>
<TOKEN id="token-30-35" pos="word" morph="none" start_char="3020" end_char="3027">sequence</TOKEN>
<TOKEN id="token-30-36" pos="punct" morph="none" start_char="3028" end_char="3028">.</TOKEN>
</SEG>
<SEG id="segment-31" start_char="3030" end_char="3163">
<ORIGINAL_TEXT>"We now know how to analyze very precisely [les séquences], by creating phylogenetic trees, he developed on March 11 with France Info.</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="punct" morph="none" start_char="3030" end_char="3030">"</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="3031" end_char="3032">We</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="3034" end_char="3036">now</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="3038" end_char="3041">know</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="3043" end_char="3045">how</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="3047" end_char="3048">to</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="3050" end_char="3056">analyze</TOKEN>
<TOKEN id="token-31-7" pos="word" morph="none" start_char="3058" end_char="3061">very</TOKEN>
<TOKEN id="token-31-8" pos="word" morph="none" start_char="3063" end_char="3071">precisely</TOKEN>
<TOKEN id="token-31-9" pos="punct" morph="none" start_char="3073" end_char="3073">[</TOKEN>
<TOKEN id="token-31-10" pos="word" morph="none" start_char="3074" end_char="3076">les</TOKEN>
<TOKEN id="token-31-11" pos="word" morph="none" start_char="3078" end_char="3086">séquences</TOKEN>
<TOKEN id="token-31-12" pos="punct" morph="none" start_char="3087" end_char="3088">],</TOKEN>
<TOKEN id="token-31-13" pos="word" morph="none" start_char="3090" end_char="3091">by</TOKEN>
<TOKEN id="token-31-14" pos="word" morph="none" start_char="3093" end_char="3100">creating</TOKEN>
<TOKEN id="token-31-15" pos="word" morph="none" start_char="3102" end_char="3113">phylogenetic</TOKEN>
<TOKEN id="token-31-16" pos="word" morph="none" start_char="3115" end_char="3119">trees</TOKEN>
<TOKEN id="token-31-17" pos="punct" morph="none" start_char="3120" end_char="3120">,</TOKEN>
<TOKEN id="token-31-18" pos="word" morph="none" start_char="3122" end_char="3123">he</TOKEN>
<TOKEN id="token-31-19" pos="word" morph="none" start_char="3125" end_char="3133">developed</TOKEN>
<TOKEN id="token-31-20" pos="word" morph="none" start_char="3135" end_char="3136">on</TOKEN>
<TOKEN id="token-31-21" pos="word" morph="none" start_char="3138" end_char="3142">March</TOKEN>
<TOKEN id="token-31-22" pos="word" morph="none" start_char="3144" end_char="3145">11</TOKEN>
<TOKEN id="token-31-23" pos="word" morph="none" start_char="3147" end_char="3150">with</TOKEN>
<TOKEN id="token-31-24" pos="word" morph="none" start_char="3152" end_char="3157">France</TOKEN>
<TOKEN id="token-31-25" pos="word" morph="none" start_char="3159" end_char="3162">Info</TOKEN>
<TOKEN id="token-31-26" pos="punct" morph="none" start_char="3163" end_char="3163">.</TOKEN>
</SEG>
<SEG id="segment-32" start_char="3165" end_char="3360">
<ORIGINAL_TEXT>We can trace the origin of a virus and we know, in the case of this coronavirus [le Covid-19], that it comes from a virus that occurs naturally in some animals, in some bats, and also in pangolin.</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="3165" end_char="3166">We</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="3168" end_char="3170">can</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="3172" end_char="3176">trace</TOKEN>
<TOKEN id="token-32-3" pos="word" morph="none" start_char="3178" end_char="3180">the</TOKEN>
<TOKEN id="token-32-4" pos="word" morph="none" start_char="3182" end_char="3187">origin</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="3189" end_char="3190">of</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="3192" end_char="3192">a</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="3194" end_char="3198">virus</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="3200" end_char="3202">and</TOKEN>
<TOKEN id="token-32-9" pos="word" morph="none" start_char="3204" end_char="3205">we</TOKEN>
<TOKEN id="token-32-10" pos="word" morph="none" start_char="3207" end_char="3210">know</TOKEN>
<TOKEN id="token-32-11" pos="punct" morph="none" start_char="3211" end_char="3211">,</TOKEN>
<TOKEN id="token-32-12" pos="word" morph="none" start_char="3213" end_char="3214">in</TOKEN>
<TOKEN id="token-32-13" pos="word" morph="none" start_char="3216" end_char="3218">the</TOKEN>
<TOKEN id="token-32-14" pos="word" morph="none" start_char="3220" end_char="3223">case</TOKEN>
<TOKEN id="token-32-15" pos="word" morph="none" start_char="3225" end_char="3226">of</TOKEN>
<TOKEN id="token-32-16" pos="word" morph="none" start_char="3228" end_char="3231">this</TOKEN>
<TOKEN id="token-32-17" pos="word" morph="none" start_char="3233" end_char="3243">coronavirus</TOKEN>
<TOKEN id="token-32-18" pos="punct" morph="none" start_char="3245" end_char="3245">[</TOKEN>
<TOKEN id="token-32-19" pos="word" morph="none" start_char="3246" end_char="3247">le</TOKEN>
<TOKEN id="token-32-20" pos="unknown" morph="none" start_char="3249" end_char="3256">Covid-19</TOKEN>
<TOKEN id="token-32-21" pos="punct" morph="none" start_char="3257" end_char="3258">],</TOKEN>
<TOKEN id="token-32-22" pos="word" morph="none" start_char="3260" end_char="3263">that</TOKEN>
<TOKEN id="token-32-23" pos="word" morph="none" start_char="3265" end_char="3266">it</TOKEN>
<TOKEN id="token-32-24" pos="word" morph="none" start_char="3268" end_char="3272">comes</TOKEN>
<TOKEN id="token-32-25" pos="word" morph="none" start_char="3274" end_char="3277">from</TOKEN>
<TOKEN id="token-32-26" pos="word" morph="none" start_char="3279" end_char="3279">a</TOKEN>
<TOKEN id="token-32-27" pos="word" morph="none" start_char="3281" end_char="3285">virus</TOKEN>
<TOKEN id="token-32-28" pos="word" morph="none" start_char="3287" end_char="3290">that</TOKEN>
<TOKEN id="token-32-29" pos="word" morph="none" start_char="3292" end_char="3297">occurs</TOKEN>
<TOKEN id="token-32-30" pos="word" morph="none" start_char="3299" end_char="3307">naturally</TOKEN>
<TOKEN id="token-32-31" pos="word" morph="none" start_char="3309" end_char="3310">in</TOKEN>
<TOKEN id="token-32-32" pos="word" morph="none" start_char="3312" end_char="3315">some</TOKEN>
<TOKEN id="token-32-33" pos="word" morph="none" start_char="3317" end_char="3323">animals</TOKEN>
<TOKEN id="token-32-34" pos="punct" morph="none" start_char="3324" end_char="3324">,</TOKEN>
<TOKEN id="token-32-35" pos="word" morph="none" start_char="3326" end_char="3327">in</TOKEN>
<TOKEN id="token-32-36" pos="word" morph="none" start_char="3329" end_char="3332">some</TOKEN>
<TOKEN id="token-32-37" pos="word" morph="none" start_char="3334" end_char="3337">bats</TOKEN>
<TOKEN id="token-32-38" pos="punct" morph="none" start_char="3338" end_char="3338">,</TOKEN>
<TOKEN id="token-32-39" pos="word" morph="none" start_char="3340" end_char="3342">and</TOKEN>
<TOKEN id="token-32-40" pos="word" morph="none" start_char="3344" end_char="3347">also</TOKEN>
<TOKEN id="token-32-41" pos="word" morph="none" start_char="3349" end_char="3350">in</TOKEN>
<TOKEN id="token-32-42" pos="word" morph="none" start_char="3352" end_char="3359">pangolin</TOKEN>
<TOKEN id="token-32-43" pos="punct" morph="none" start_char="3360" end_char="3360">.</TOKEN>
</SEG>
<SEG id="segment-33" start_char="3362" end_char="3441">
<ORIGINAL_TEXT>In these animals, the virus can be found identical to 90 or almost 98% homology.</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="word" morph="none" start_char="3362" end_char="3363">In</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="3365" end_char="3369">these</TOKEN>
<TOKEN id="token-33-2" pos="word" morph="none" start_char="3371" end_char="3377">animals</TOKEN>
<TOKEN id="token-33-3" pos="punct" morph="none" start_char="3378" end_char="3378">,</TOKEN>
<TOKEN id="token-33-4" pos="word" morph="none" start_char="3380" end_char="3382">the</TOKEN>
<TOKEN id="token-33-5" pos="word" morph="none" start_char="3384" end_char="3388">virus</TOKEN>
<TOKEN id="token-33-6" pos="word" morph="none" start_char="3390" end_char="3392">can</TOKEN>
<TOKEN id="token-33-7" pos="word" morph="none" start_char="3394" end_char="3395">be</TOKEN>
<TOKEN id="token-33-8" pos="word" morph="none" start_char="3397" end_char="3401">found</TOKEN>
<TOKEN id="token-33-9" pos="word" morph="none" start_char="3403" end_char="3411">identical</TOKEN>
<TOKEN id="token-33-10" pos="word" morph="none" start_char="3413" end_char="3414">to</TOKEN>
<TOKEN id="token-33-11" pos="word" morph="none" start_char="3416" end_char="3417">90</TOKEN>
<TOKEN id="token-33-12" pos="word" morph="none" start_char="3419" end_char="3420">or</TOKEN>
<TOKEN id="token-33-13" pos="word" morph="none" start_char="3422" end_char="3427">almost</TOKEN>
<TOKEN id="token-33-14" pos="word" morph="none" start_char="3429" end_char="3430">98</TOKEN>
<TOKEN id="token-33-15" pos="punct" morph="none" start_char="3431" end_char="3431">%</TOKEN>
<TOKEN id="token-33-16" pos="word" morph="none" start_char="3433" end_char="3440">homology</TOKEN>
<TOKEN id="token-33-17" pos="punct" morph="none" start_char="3441" end_char="3441">.</TOKEN>
</SEG>
<SEG id="segment-34" start_char="3443" end_char="3529">
<ORIGINAL_TEXT>There, we know that the virus has passed directly from the bat or pangolin to humans. "</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="3443" end_char="3447">There</TOKEN>
<TOKEN id="token-34-1" pos="punct" morph="none" start_char="3448" end_char="3448">,</TOKEN>
<TOKEN id="token-34-2" pos="word" morph="none" start_char="3450" end_char="3451">we</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="3453" end_char="3456">know</TOKEN>
<TOKEN id="token-34-4" pos="word" morph="none" start_char="3458" end_char="3461">that</TOKEN>
<TOKEN id="token-34-5" pos="word" morph="none" start_char="3463" end_char="3465">the</TOKEN>
<TOKEN id="token-34-6" pos="word" morph="none" start_char="3467" end_char="3471">virus</TOKEN>
<TOKEN id="token-34-7" pos="word" morph="none" start_char="3473" end_char="3475">has</TOKEN>
<TOKEN id="token-34-8" pos="word" morph="none" start_char="3477" end_char="3482">passed</TOKEN>
<TOKEN id="token-34-9" pos="word" morph="none" start_char="3484" end_char="3491">directly</TOKEN>
<TOKEN id="token-34-10" pos="word" morph="none" start_char="3493" end_char="3496">from</TOKEN>
<TOKEN id="token-34-11" pos="word" morph="none" start_char="3498" end_char="3500">the</TOKEN>
<TOKEN id="token-34-12" pos="word" morph="none" start_char="3502" end_char="3504">bat</TOKEN>
<TOKEN id="token-34-13" pos="word" morph="none" start_char="3506" end_char="3507">or</TOKEN>
<TOKEN id="token-34-14" pos="word" morph="none" start_char="3509" end_char="3516">pangolin</TOKEN>
<TOKEN id="token-34-15" pos="word" morph="none" start_char="3518" end_char="3519">to</TOKEN>
<TOKEN id="token-34-16" pos="word" morph="none" start_char="3521" end_char="3526">humans</TOKEN>
<TOKEN id="token-34-17" pos="punct" morph="none" start_char="3527" end_char="3527">.</TOKEN>
<TOKEN id="token-34-18" pos="punct" morph="none" start_char="3529" end_char="3529">"</TOKEN>
</SEG>
<SEG id="segment-35" start_char="3532" end_char="3578">
<ORIGINAL_TEXT>This is what was also recalled on a daily basis</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="word" morph="none" start_char="3532" end_char="3535">This</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="3537" end_char="3538">is</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="3540" end_char="3543">what</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="3545" end_char="3547">was</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="3549" end_char="3552">also</TOKEN>
<TOKEN id="token-35-5" pos="word" morph="none" start_char="3554" end_char="3561">recalled</TOKEN>
<TOKEN id="token-35-6" pos="word" morph="none" start_char="3563" end_char="3564">on</TOKEN>
<TOKEN id="token-35-7" pos="word" morph="none" start_char="3566" end_char="3566">a</TOKEN>
<TOKEN id="token-35-8" pos="word" morph="none" start_char="3568" end_char="3572">daily</TOKEN>
<TOKEN id="token-35-9" pos="word" morph="none" start_char="3574" end_char="3578">basis</TOKEN>
</SEG>
<SEG id="segment-36" start_char="3581" end_char="3593">
<ORIGINAL_TEXT>La Repubblica</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="3581" end_char="3582">La</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="3584" end_char="3593">Repubblica</TOKEN>
</SEG>
<SEG id="segment-37" start_char="3596" end_char="3800">
<ORIGINAL_TEXT>Fausto Baldanti, professor at the University of Pavia: "A natural virus and a virus created in the laboratory stand out perfectly", before adding: "The experiment of 2015 took place before everyone’s eyes.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="3596" end_char="3601">Fausto</TOKEN>
<TOKEN id="token-37-1" pos="word" morph="none" start_char="3603" end_char="3610">Baldanti</TOKEN>
<TOKEN id="token-37-2" pos="punct" morph="none" start_char="3611" end_char="3611">,</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="3613" end_char="3621">professor</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="3623" end_char="3624">at</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="3626" end_char="3628">the</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="3630" end_char="3639">University</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="3641" end_char="3642">of</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="3644" end_char="3648">Pavia</TOKEN>
<TOKEN id="token-37-9" pos="punct" morph="none" start_char="3649" end_char="3649">:</TOKEN>
<TOKEN id="token-37-10" pos="punct" morph="none" start_char="3651" end_char="3651">"</TOKEN>
<TOKEN id="token-37-11" pos="word" morph="none" start_char="3652" end_char="3652">A</TOKEN>
<TOKEN id="token-37-12" pos="word" morph="none" start_char="3654" end_char="3660">natural</TOKEN>
<TOKEN id="token-37-13" pos="word" morph="none" start_char="3662" end_char="3666">virus</TOKEN>
<TOKEN id="token-37-14" pos="word" morph="none" start_char="3668" end_char="3670">and</TOKEN>
<TOKEN id="token-37-15" pos="word" morph="none" start_char="3672" end_char="3672">a</TOKEN>
<TOKEN id="token-37-16" pos="word" morph="none" start_char="3674" end_char="3678">virus</TOKEN>
<TOKEN id="token-37-17" pos="word" morph="none" start_char="3680" end_char="3686">created</TOKEN>
<TOKEN id="token-37-18" pos="word" morph="none" start_char="3688" end_char="3689">in</TOKEN>
<TOKEN id="token-37-19" pos="word" morph="none" start_char="3691" end_char="3693">the</TOKEN>
<TOKEN id="token-37-20" pos="word" morph="none" start_char="3695" end_char="3704">laboratory</TOKEN>
<TOKEN id="token-37-21" pos="word" morph="none" start_char="3706" end_char="3710">stand</TOKEN>
<TOKEN id="token-37-22" pos="word" morph="none" start_char="3712" end_char="3714">out</TOKEN>
<TOKEN id="token-37-23" pos="word" morph="none" start_char="3716" end_char="3724">perfectly</TOKEN>
<TOKEN id="token-37-24" pos="punct" morph="none" start_char="3725" end_char="3726">",</TOKEN>
<TOKEN id="token-37-25" pos="word" morph="none" start_char="3728" end_char="3733">before</TOKEN>
<TOKEN id="token-37-26" pos="word" morph="none" start_char="3735" end_char="3740">adding</TOKEN>
<TOKEN id="token-37-27" pos="punct" morph="none" start_char="3741" end_char="3741">:</TOKEN>
<TOKEN id="token-37-28" pos="punct" morph="none" start_char="3743" end_char="3743">"</TOKEN>
<TOKEN id="token-37-29" pos="word" morph="none" start_char="3744" end_char="3746">The</TOKEN>
<TOKEN id="token-37-30" pos="word" morph="none" start_char="3748" end_char="3757">experiment</TOKEN>
<TOKEN id="token-37-31" pos="word" morph="none" start_char="3759" end_char="3760">of</TOKEN>
<TOKEN id="token-37-32" pos="word" morph="none" start_char="3762" end_char="3765">2015</TOKEN>
<TOKEN id="token-37-33" pos="word" morph="none" start_char="3767" end_char="3770">took</TOKEN>
<TOKEN id="token-37-34" pos="word" morph="none" start_char="3772" end_char="3776">place</TOKEN>
<TOKEN id="token-37-35" pos="word" morph="none" start_char="3778" end_char="3783">before</TOKEN>
<TOKEN id="token-37-36" pos="word" morph="none" start_char="3785" end_char="3794">everyone’s</TOKEN>
<TOKEN id="token-37-37" pos="word" morph="none" start_char="3796" end_char="3799">eyes</TOKEN>
<TOKEN id="token-37-38" pos="punct" morph="none" start_char="3800" end_char="3800">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="3802" end_char="3869">
<ORIGINAL_TEXT>The genome of this microorganism has been published in its entirety.</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="3802" end_char="3804">The</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="3806" end_char="3811">genome</TOKEN>
<TOKEN id="token-38-2" pos="word" morph="none" start_char="3813" end_char="3814">of</TOKEN>
<TOKEN id="token-38-3" pos="word" morph="none" start_char="3816" end_char="3819">this</TOKEN>
<TOKEN id="token-38-4" pos="word" morph="none" start_char="3821" end_char="3833">microorganism</TOKEN>
<TOKEN id="token-38-5" pos="word" morph="none" start_char="3835" end_char="3837">has</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="3839" end_char="3842">been</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="3844" end_char="3852">published</TOKEN>
<TOKEN id="token-38-8" pos="word" morph="none" start_char="3854" end_char="3855">in</TOKEN>
<TOKEN id="token-38-9" pos="word" morph="none" start_char="3857" end_char="3859">its</TOKEN>
<TOKEN id="token-38-10" pos="word" morph="none" start_char="3861" end_char="3868">entirety</TOKEN>
<TOKEN id="token-38-11" pos="punct" morph="none" start_char="3869" end_char="3869">.</TOKEN>
</SEG>
<SEG id="segment-39" start_char="3871" end_char="3928">
<ORIGINAL_TEXT>And this one is not the same as the current coronavirus. "</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="word" morph="none" start_char="3871" end_char="3873">And</TOKEN>
<TOKEN id="token-39-1" pos="word" morph="none" start_char="3875" end_char="3878">this</TOKEN>
<TOKEN id="token-39-2" pos="word" morph="none" start_char="3880" end_char="3882">one</TOKEN>
<TOKEN id="token-39-3" pos="word" morph="none" start_char="3884" end_char="3885">is</TOKEN>
<TOKEN id="token-39-4" pos="word" morph="none" start_char="3887" end_char="3889">not</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="3891" end_char="3893">the</TOKEN>
<TOKEN id="token-39-6" pos="word" morph="none" start_char="3895" end_char="3898">same</TOKEN>
<TOKEN id="token-39-7" pos="word" morph="none" start_char="3900" end_char="3901">as</TOKEN>
<TOKEN id="token-39-8" pos="word" morph="none" start_char="3903" end_char="3905">the</TOKEN>
<TOKEN id="token-39-9" pos="word" morph="none" start_char="3907" end_char="3913">current</TOKEN>
<TOKEN id="token-39-10" pos="word" morph="none" start_char="3915" end_char="3925">coronavirus</TOKEN>
<TOKEN id="token-39-11" pos="punct" morph="none" start_char="3926" end_char="3926">.</TOKEN>
<TOKEN id="token-39-12" pos="punct" morph="none" start_char="3928" end_char="3928">"</TOKEN>
</SEG>
<SEG id="segment-40" start_char="3931" end_char="3987">
<ORIGINAL_TEXT>This request for verifications was sent to us by readers.</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="word" morph="none" start_char="3931" end_char="3934">This</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="3936" end_char="3942">request</TOKEN>
<TOKEN id="token-40-2" pos="word" morph="none" start_char="3944" end_char="3946">for</TOKEN>
<TOKEN id="token-40-3" pos="word" morph="none" start_char="3948" end_char="3960">verifications</TOKEN>
<TOKEN id="token-40-4" pos="word" morph="none" start_char="3962" end_char="3964">was</TOKEN>
<TOKEN id="token-40-5" pos="word" morph="none" start_char="3966" end_char="3969">sent</TOKEN>
<TOKEN id="token-40-6" pos="word" morph="none" start_char="3971" end_char="3972">to</TOKEN>
<TOKEN id="token-40-7" pos="word" morph="none" start_char="3974" end_char="3975">us</TOKEN>
<TOKEN id="token-40-8" pos="word" morph="none" start_char="3977" end_char="3978">by</TOKEN>
<TOKEN id="token-40-9" pos="word" morph="none" start_char="3980" end_char="3986">readers</TOKEN>
<TOKEN id="token-40-10" pos="punct" morph="none" start_char="3987" end_char="3987">.</TOKEN>
</SEG>
<SEG id="segment-41" start_char="3989" end_char="4056">
<ORIGINAL_TEXT>Do you also want the Fake off team to check an info, photo or video?</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="3989" end_char="3990">Do</TOKEN>
<TOKEN id="token-41-1" pos="word" morph="none" start_char="3992" end_char="3994">you</TOKEN>
<TOKEN id="token-41-2" pos="word" morph="none" start_char="3996" end_char="3999">also</TOKEN>
<TOKEN id="token-41-3" pos="word" morph="none" start_char="4001" end_char="4004">want</TOKEN>
<TOKEN id="token-41-4" pos="word" morph="none" start_char="4006" end_char="4008">the</TOKEN>
<TOKEN id="token-41-5" pos="word" morph="none" start_char="4010" end_char="4013">Fake</TOKEN>
<TOKEN id="token-41-6" pos="word" morph="none" start_char="4015" end_char="4017">off</TOKEN>
<TOKEN id="token-41-7" pos="word" morph="none" start_char="4019" end_char="4022">team</TOKEN>
<TOKEN id="token-41-8" pos="word" morph="none" start_char="4024" end_char="4025">to</TOKEN>
<TOKEN id="token-41-9" pos="word" morph="none" start_char="4027" end_char="4031">check</TOKEN>
<TOKEN id="token-41-10" pos="word" morph="none" start_char="4033" end_char="4034">an</TOKEN>
<TOKEN id="token-41-11" pos="word" morph="none" start_char="4036" end_char="4039">info</TOKEN>
<TOKEN id="token-41-12" pos="punct" morph="none" start_char="4040" end_char="4040">,</TOKEN>
<TOKEN id="token-41-13" pos="word" morph="none" start_char="4042" end_char="4046">photo</TOKEN>
<TOKEN id="token-41-14" pos="word" morph="none" start_char="4048" end_char="4049">or</TOKEN>
<TOKEN id="token-41-15" pos="word" morph="none" start_char="4051" end_char="4055">video</TOKEN>
<TOKEN id="token-41-16" pos="punct" morph="none" start_char="4056" end_char="4056">?</TOKEN>
</SEG>
<SEG id="segment-42" start_char="4058" end_char="4140">
<ORIGINAL_TEXT>Fill out the form below or write to us on Twitter: https://twitter.com/20minFakeOff</ORIGINAL_TEXT>
<TOKEN id="token-42-0" pos="word" morph="none" start_char="4058" end_char="4061">Fill</TOKEN>
<TOKEN id="token-42-1" pos="word" morph="none" start_char="4063" end_char="4065">out</TOKEN>
<TOKEN id="token-42-2" pos="word" morph="none" start_char="4067" end_char="4069">the</TOKEN>
<TOKEN id="token-42-3" pos="word" morph="none" start_char="4071" end_char="4074">form</TOKEN>
<TOKEN id="token-42-4" pos="word" morph="none" start_char="4076" end_char="4080">below</TOKEN>
<TOKEN id="token-42-5" pos="word" morph="none" start_char="4082" end_char="4083">or</TOKEN>
<TOKEN id="token-42-6" pos="word" morph="none" start_char="4085" end_char="4089">write</TOKEN>
<TOKEN id="token-42-7" pos="word" morph="none" start_char="4091" end_char="4092">to</TOKEN>
<TOKEN id="token-42-8" pos="word" morph="none" start_char="4094" end_char="4095">us</TOKEN>
<TOKEN id="token-42-9" pos="word" morph="none" start_char="4097" end_char="4098">on</TOKEN>
<TOKEN id="token-42-10" pos="word" morph="none" start_char="4100" end_char="4106">Twitter</TOKEN>
<TOKEN id="token-42-11" pos="punct" morph="none" start_char="4107" end_char="4107">:</TOKEN>
<TOKEN id="token-42-12" pos="url" morph="none" start_char="4109" end_char="4140">https://twitter.com/20minFakeOff</TOKEN>
</SEG>
<SEG id="segment-43" start_char="4143" end_char="4152">
<ORIGINAL_TEXT>20 minutes</ORIGINAL_TEXT>
<TOKEN id="token-43-0" pos="word" morph="none" start_char="4143" end_char="4144">20</TOKEN>
<TOKEN id="token-43-1" pos="word" morph="none" start_char="4146" end_char="4152">minutes</TOKEN>
</SEG>
<SEG id="segment-44" start_char="4155" end_char="4204">
<ORIGINAL_TEXT>East Facebook partner to fight against false news.</ORIGINAL_TEXT>
<TOKEN id="token-44-0" pos="word" morph="none" start_char="4155" end_char="4158">East</TOKEN>
<TOKEN id="token-44-1" pos="word" morph="none" start_char="4160" end_char="4167">Facebook</TOKEN>
<TOKEN id="token-44-2" pos="word" morph="none" start_char="4169" end_char="4175">partner</TOKEN>
<TOKEN id="token-44-3" pos="word" morph="none" start_char="4177" end_char="4178">to</TOKEN>
<TOKEN id="token-44-4" pos="word" morph="none" start_char="4180" end_char="4184">fight</TOKEN>
<TOKEN id="token-44-5" pos="word" morph="none" start_char="4186" end_char="4192">against</TOKEN>
<TOKEN id="token-44-6" pos="word" morph="none" start_char="4194" end_char="4198">false</TOKEN>
<TOKEN id="token-44-7" pos="word" morph="none" start_char="4200" end_char="4203">news</TOKEN>
<TOKEN id="token-44-8" pos="punct" morph="none" start_char="4204" end_char="4204">.</TOKEN>
</SEG>
<SEG id="segment-45" start_char="4206" end_char="4309">
<ORIGINAL_TEXT>Thanks to this device, users of the social network may report information that they believe to be false.</ORIGINAL_TEXT>
<TOKEN id="token-45-0" pos="word" morph="none" start_char="4206" end_char="4211">Thanks</TOKEN>
<TOKEN id="token-45-1" pos="word" morph="none" start_char="4213" end_char="4214">to</TOKEN>
<TOKEN id="token-45-2" pos="word" morph="none" start_char="4216" end_char="4219">this</TOKEN>
<TOKEN id="token-45-3" pos="word" morph="none" start_char="4221" end_char="4226">device</TOKEN>
<TOKEN id="token-45-4" pos="punct" morph="none" start_char="4227" end_char="4227">,</TOKEN>
<TOKEN id="token-45-5" pos="word" morph="none" start_char="4229" end_char="4233">users</TOKEN>
<TOKEN id="token-45-6" pos="word" morph="none" start_char="4235" end_char="4236">of</TOKEN>
<TOKEN id="token-45-7" pos="word" morph="none" start_char="4238" end_char="4240">the</TOKEN>
<TOKEN id="token-45-8" pos="word" morph="none" start_char="4242" end_char="4247">social</TOKEN>
<TOKEN id="token-45-9" pos="word" morph="none" start_char="4249" end_char="4255">network</TOKEN>
<TOKEN id="token-45-10" pos="word" morph="none" start_char="4257" end_char="4259">may</TOKEN>
<TOKEN id="token-45-11" pos="word" morph="none" start_char="4261" end_char="4266">report</TOKEN>
<TOKEN id="token-45-12" pos="word" morph="none" start_char="4268" end_char="4278">information</TOKEN>
<TOKEN id="token-45-13" pos="word" morph="none" start_char="4280" end_char="4283">that</TOKEN>
<TOKEN id="token-45-14" pos="word" morph="none" start_char="4285" end_char="4288">they</TOKEN>
<TOKEN id="token-45-15" pos="word" morph="none" start_char="4290" end_char="4296">believe</TOKEN>
<TOKEN id="token-45-16" pos="word" morph="none" start_char="4298" end_char="4299">to</TOKEN>
<TOKEN id="token-45-17" pos="word" morph="none" start_char="4301" end_char="4302">be</TOKEN>
<TOKEN id="token-45-18" pos="word" morph="none" start_char="4304" end_char="4308">false</TOKEN>
<TOKEN id="token-45-19" pos="punct" morph="none" start_char="4309" end_char="4309">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
