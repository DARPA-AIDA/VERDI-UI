<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04ATCE" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="2610" raw_text_md5="a311b01ba75fffbc06cd2ce27ee163fa">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="62">
<ORIGINAL_TEXT>La teoría ‘conspiranoica’ de Frank Cuesta sobre el coronavirus</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="2">La</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="4" end_char="9">teoría</TOKEN>
<TOKEN id="token-0-2" pos="punct" morph="none" start_char="11" end_char="11">‘</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="12" end_char="24">conspiranoica</TOKEN>
<TOKEN id="token-0-4" pos="punct" morph="none" start_char="25" end_char="25">’</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="27" end_char="28">de</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="30" end_char="34">Frank</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="36" end_char="41">Cuesta</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="43" end_char="47">sobre</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="49" end_char="50">el</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="52" end_char="62">coronavirus</TOKEN>
</SEG>
<SEG id="segment-1" start_char="66" end_char="77">
<ORIGINAL_TEXT>Frank Cuesta</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="66" end_char="70">Frank</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="72" end_char="77">Cuesta</TOKEN>
</SEG>
<SEG id="segment-2" start_char="81" end_char="148">
<ORIGINAL_TEXT>Frank Cuesta sigue dejando titulares cada vez que da una entrevista.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="81" end_char="85">Frank</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="87" end_char="92">Cuesta</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="94" end_char="98">sigue</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="100" end_char="106">dejando</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="108" end_char="116">titulares</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="118" end_char="121">cada</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="123" end_char="125">vez</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="127" end_char="129">que</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="131" end_char="132">da</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="134" end_char="136">una</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="138" end_char="147">entrevista</TOKEN>
<TOKEN id="token-2-11" pos="punct" morph="none" start_char="148" end_char="148">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="150" end_char="397">
<ORIGINAL_TEXT>En esta ocasión, y preguntado sobre el coronavirus Covid-19 , no ha dudado en lanzar una teoría sobre los contagios y su propagación, que según el animalista se remontaría al mes de agosto y no a principios de este año como reza la versión oficial.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="150" end_char="151">En</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="153" end_char="156">esta</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="158" end_char="164">ocasión</TOKEN>
<TOKEN id="token-3-3" pos="punct" morph="none" start_char="165" end_char="165">,</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="167" end_char="167">y</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="169" end_char="178">preguntado</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="180" end_char="184">sobre</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="186" end_char="187">el</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="189" end_char="199">coronavirus</TOKEN>
<TOKEN id="token-3-9" pos="unknown" morph="none" start_char="201" end_char="208">Covid-19</TOKEN>
<TOKEN id="token-3-10" pos="punct" morph="none" start_char="210" end_char="210">,</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="212" end_char="213">no</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="215" end_char="216">ha</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="218" end_char="223">dudado</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="225" end_char="226">en</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="228" end_char="233">lanzar</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="235" end_char="237">una</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="239" end_char="244">teoría</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="246" end_char="250">sobre</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="252" end_char="254">los</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="256" end_char="264">contagios</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="266" end_char="266">y</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="268" end_char="269">su</TOKEN>
<TOKEN id="token-3-23" pos="word" morph="none" start_char="271" end_char="281">propagación</TOKEN>
<TOKEN id="token-3-24" pos="punct" morph="none" start_char="282" end_char="282">,</TOKEN>
<TOKEN id="token-3-25" pos="word" morph="none" start_char="284" end_char="286">que</TOKEN>
<TOKEN id="token-3-26" pos="word" morph="none" start_char="288" end_char="292">según</TOKEN>
<TOKEN id="token-3-27" pos="word" morph="none" start_char="294" end_char="295">el</TOKEN>
<TOKEN id="token-3-28" pos="word" morph="none" start_char="297" end_char="306">animalista</TOKEN>
<TOKEN id="token-3-29" pos="word" morph="none" start_char="308" end_char="309">se</TOKEN>
<TOKEN id="token-3-30" pos="word" morph="none" start_char="311" end_char="320">remontaría</TOKEN>
<TOKEN id="token-3-31" pos="word" morph="none" start_char="322" end_char="323">al</TOKEN>
<TOKEN id="token-3-32" pos="word" morph="none" start_char="325" end_char="327">mes</TOKEN>
<TOKEN id="token-3-33" pos="word" morph="none" start_char="329" end_char="330">de</TOKEN>
<TOKEN id="token-3-34" pos="word" morph="none" start_char="332" end_char="337">agosto</TOKEN>
<TOKEN id="token-3-35" pos="word" morph="none" start_char="339" end_char="339">y</TOKEN>
<TOKEN id="token-3-36" pos="word" morph="none" start_char="341" end_char="342">no</TOKEN>
<TOKEN id="token-3-37" pos="word" morph="none" start_char="344" end_char="344">a</TOKEN>
<TOKEN id="token-3-38" pos="word" morph="none" start_char="346" end_char="355">principios</TOKEN>
<TOKEN id="token-3-39" pos="word" morph="none" start_char="357" end_char="358">de</TOKEN>
<TOKEN id="token-3-40" pos="word" morph="none" start_char="360" end_char="363">este</TOKEN>
<TOKEN id="token-3-41" pos="word" morph="none" start_char="365" end_char="367">año</TOKEN>
<TOKEN id="token-3-42" pos="word" morph="none" start_char="369" end_char="372">como</TOKEN>
<TOKEN id="token-3-43" pos="word" morph="none" start_char="374" end_char="377">reza</TOKEN>
<TOKEN id="token-3-44" pos="word" morph="none" start_char="379" end_char="380">la</TOKEN>
<TOKEN id="token-3-45" pos="word" morph="none" start_char="382" end_char="388">versión</TOKEN>
<TOKEN id="token-3-46" pos="word" morph="none" start_char="390" end_char="396">oficial</TOKEN>
<TOKEN id="token-3-47" pos="punct" morph="none" start_char="397" end_char="397">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="400" end_char="566">
<ORIGINAL_TEXT>Lo ha hecho en una entrevista a Federico Jiménez Losantos, donde Cuesta ha abordado la "histeria" por la actual epidemia de coronavirus en España y el resto del mundo.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="400" end_char="401">Lo</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="403" end_char="404">ha</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="406" end_char="410">hecho</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="412" end_char="413">en</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="415" end_char="417">una</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="419" end_char="428">entrevista</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="430" end_char="430">a</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="432" end_char="439">Federico</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="441" end_char="447">Jiménez</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="449" end_char="456">Losantos</TOKEN>
<TOKEN id="token-4-10" pos="punct" morph="none" start_char="457" end_char="457">,</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="459" end_char="463">donde</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="465" end_char="470">Cuesta</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="472" end_char="473">ha</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="475" end_char="482">abordado</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="484" end_char="485">la</TOKEN>
<TOKEN id="token-4-16" pos="punct" morph="none" start_char="487" end_char="487">"</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="488" end_char="495">histeria</TOKEN>
<TOKEN id="token-4-18" pos="punct" morph="none" start_char="496" end_char="496">"</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="498" end_char="500">por</TOKEN>
<TOKEN id="token-4-20" pos="word" morph="none" start_char="502" end_char="503">la</TOKEN>
<TOKEN id="token-4-21" pos="word" morph="none" start_char="505" end_char="510">actual</TOKEN>
<TOKEN id="token-4-22" pos="word" morph="none" start_char="512" end_char="519">epidemia</TOKEN>
<TOKEN id="token-4-23" pos="word" morph="none" start_char="521" end_char="522">de</TOKEN>
<TOKEN id="token-4-24" pos="word" morph="none" start_char="524" end_char="534">coronavirus</TOKEN>
<TOKEN id="token-4-25" pos="word" morph="none" start_char="536" end_char="537">en</TOKEN>
<TOKEN id="token-4-26" pos="word" morph="none" start_char="539" end_char="544">España</TOKEN>
<TOKEN id="token-4-27" pos="word" morph="none" start_char="546" end_char="546">y</TOKEN>
<TOKEN id="token-4-28" pos="word" morph="none" start_char="548" end_char="549">el</TOKEN>
<TOKEN id="token-4-29" pos="word" morph="none" start_char="551" end_char="555">resto</TOKEN>
<TOKEN id="token-4-30" pos="word" morph="none" start_char="557" end_char="559">del</TOKEN>
<TOKEN id="token-4-31" pos="word" morph="none" start_char="561" end_char="565">mundo</TOKEN>
<TOKEN id="token-4-32" pos="punct" morph="none" start_char="566" end_char="566">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="568" end_char="735">
<ORIGINAL_TEXT>Sin embargo, y contrariamente a lo que se ha publicado hasta ahora, el presentador asegura haber presenciado sus inicios en Tailandia hace ya unos meses, en pleno 2019.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="568" end_char="570">Sin</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="572" end_char="578">embargo</TOKEN>
<TOKEN id="token-5-2" pos="punct" morph="none" start_char="579" end_char="579">,</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="581" end_char="581">y</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="583" end_char="596">contrariamente</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="598" end_char="598">a</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="600" end_char="601">lo</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="603" end_char="605">que</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="607" end_char="608">se</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="610" end_char="611">ha</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="613" end_char="621">publicado</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="623" end_char="627">hasta</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="629" end_char="633">ahora</TOKEN>
<TOKEN id="token-5-13" pos="punct" morph="none" start_char="634" end_char="634">,</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="636" end_char="637">el</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="639" end_char="649">presentador</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="651" end_char="657">asegura</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="659" end_char="663">haber</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="665" end_char="675">presenciado</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="677" end_char="679">sus</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="681" end_char="687">inicios</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="689" end_char="690">en</TOKEN>
<TOKEN id="token-5-22" pos="word" morph="none" start_char="692" end_char="700">Tailandia</TOKEN>
<TOKEN id="token-5-23" pos="word" morph="none" start_char="702" end_char="705">hace</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="707" end_char="708">ya</TOKEN>
<TOKEN id="token-5-25" pos="word" morph="none" start_char="710" end_char="713">unos</TOKEN>
<TOKEN id="token-5-26" pos="word" morph="none" start_char="715" end_char="719">meses</TOKEN>
<TOKEN id="token-5-27" pos="punct" morph="none" start_char="720" end_char="720">,</TOKEN>
<TOKEN id="token-5-28" pos="word" morph="none" start_char="722" end_char="723">en</TOKEN>
<TOKEN id="token-5-29" pos="word" morph="none" start_char="725" end_char="729">pleno</TOKEN>
<TOKEN id="token-5-30" pos="word" morph="none" start_char="731" end_char="734">2019</TOKEN>
<TOKEN id="token-5-31" pos="punct" morph="none" start_char="735" end_char="735">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="738" end_char="752">
<ORIGINAL_TEXT>Frank Cuesta FB</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="738" end_char="742">Frank</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="744" end_char="749">Cuesta</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="751" end_char="752">FB</TOKEN>
</SEG>
<SEG id="segment-7" start_char="756" end_char="775">
<ORIGINAL_TEXT>Frank Cuesta Twitter</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="756" end_char="760">Frank</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="762" end_char="767">Cuesta</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="769" end_char="775">Twitter</TOKEN>
</SEG>
<SEG id="segment-8" start_char="779" end_char="925">
<ORIGINAL_TEXT>Su teoría se basa en el pangolín, un animal en el que se había puesto el foco del primer contagio entre animales y humanos pero que fue descartado.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="779" end_char="780">Su</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="782" end_char="787">teoría</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="789" end_char="790">se</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="792" end_char="795">basa</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="797" end_char="798">en</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="800" end_char="801">el</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="803" end_char="810">pangolín</TOKEN>
<TOKEN id="token-8-7" pos="punct" morph="none" start_char="811" end_char="811">,</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="813" end_char="814">un</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="816" end_char="821">animal</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="823" end_char="824">en</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="826" end_char="827">el</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="829" end_char="831">que</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="833" end_char="834">se</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="836" end_char="840">había</TOKEN>
<TOKEN id="token-8-15" pos="word" morph="none" start_char="842" end_char="847">puesto</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="849" end_char="850">el</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="852" end_char="855">foco</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="857" end_char="859">del</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="861" end_char="866">primer</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="868" end_char="875">contagio</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="877" end_char="881">entre</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="883" end_char="890">animales</TOKEN>
<TOKEN id="token-8-23" pos="word" morph="none" start_char="892" end_char="892">y</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="894" end_char="900">humanos</TOKEN>
<TOKEN id="token-8-25" pos="word" morph="none" start_char="902" end_char="905">pero</TOKEN>
<TOKEN id="token-8-26" pos="word" morph="none" start_char="907" end_char="909">que</TOKEN>
<TOKEN id="token-8-27" pos="word" morph="none" start_char="911" end_char="913">fue</TOKEN>
<TOKEN id="token-8-28" pos="word" morph="none" start_char="915" end_char="924">descartado</TOKEN>
<TOKEN id="token-8-29" pos="punct" morph="none" start_char="925" end_char="925">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="927" end_char="1116">
<ORIGINAL_TEXT>Aún así, Cuesta sigue apuntando a esta posibilidad y la argumenta diciendo que "sabemos que se pillan una media de cincuenta a cien pangolines todas las semanas que salen a Vietnam o China".</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="927" end_char="929">Aún</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="931" end_char="933">así</TOKEN>
<TOKEN id="token-9-2" pos="punct" morph="none" start_char="934" end_char="934">,</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="936" end_char="941">Cuesta</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="943" end_char="947">sigue</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="949" end_char="957">apuntando</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="959" end_char="959">a</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="961" end_char="964">esta</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="966" end_char="976">posibilidad</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="978" end_char="978">y</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="980" end_char="981">la</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="983" end_char="991">argumenta</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="993" end_char="1000">diciendo</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="1002" end_char="1004">que</TOKEN>
<TOKEN id="token-9-14" pos="punct" morph="none" start_char="1006" end_char="1006">"</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="1007" end_char="1013">sabemos</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="1015" end_char="1017">que</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1019" end_char="1020">se</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="1022" end_char="1027">pillan</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1029" end_char="1031">una</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1033" end_char="1037">media</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1039" end_char="1040">de</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1042" end_char="1050">cincuenta</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="1052" end_char="1052">a</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1054" end_char="1057">cien</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="1059" end_char="1068">pangolines</TOKEN>
<TOKEN id="token-9-26" pos="word" morph="none" start_char="1070" end_char="1074">todas</TOKEN>
<TOKEN id="token-9-27" pos="word" morph="none" start_char="1076" end_char="1078">las</TOKEN>
<TOKEN id="token-9-28" pos="word" morph="none" start_char="1080" end_char="1086">semanas</TOKEN>
<TOKEN id="token-9-29" pos="word" morph="none" start_char="1088" end_char="1090">que</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1092" end_char="1096">salen</TOKEN>
<TOKEN id="token-9-31" pos="word" morph="none" start_char="1098" end_char="1098">a</TOKEN>
<TOKEN id="token-9-32" pos="word" morph="none" start_char="1100" end_char="1106">Vietnam</TOKEN>
<TOKEN id="token-9-33" pos="word" morph="none" start_char="1108" end_char="1108">o</TOKEN>
<TOKEN id="token-9-34" pos="word" morph="none" start_char="1110" end_char="1114">China</TOKEN>
<TOKEN id="token-9-35" pos="punct" morph="none" start_char="1115" end_char="1116">".</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1119" end_char="1204">
<ORIGINAL_TEXT>"La segunda semana de agosto se dejó de pillar, y también de murciélago y de musaraña.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="punct" morph="none" start_char="1119" end_char="1119">"</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1120" end_char="1121">La</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1123" end_char="1129">segunda</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1131" end_char="1136">semana</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1138" end_char="1139">de</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1141" end_char="1146">agosto</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1148" end_char="1149">se</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1151" end_char="1154">dejó</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1156" end_char="1157">de</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1159" end_char="1164">pillar</TOKEN>
<TOKEN id="token-10-10" pos="punct" morph="none" start_char="1165" end_char="1165">,</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1167" end_char="1167">y</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1169" end_char="1175">también</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1177" end_char="1178">de</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1180" end_char="1189">murciélago</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1191" end_char="1191">y</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1193" end_char="1194">de</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1196" end_char="1203">musaraña</TOKEN>
<TOKEN id="token-10-18" pos="punct" morph="none" start_char="1204" end_char="1204">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1206" end_char="1253">
<ORIGINAL_TEXT>No hemos encontrado ni un cargamento, cosa rara.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1206" end_char="1207">No</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1209" end_char="1213">hemos</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1215" end_char="1224">encontrado</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1226" end_char="1227">ni</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1229" end_char="1230">un</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1232" end_char="1241">cargamento</TOKEN>
<TOKEN id="token-11-6" pos="punct" morph="none" start_char="1242" end_char="1242">,</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1244" end_char="1247">cosa</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1249" end_char="1252">rara</TOKEN>
<TOKEN id="token-11-9" pos="punct" morph="none" start_char="1253" end_char="1253">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1255" end_char="1454">
<ORIGINAL_TEXT>Te estoy hablando del mes de agosto", ha asegurado antes de plantear la conclusión de lo ocurrido: "¿Cómo puedes coger cien pangolines a la semana y salir de las mismas zonas y luego dejar de hacerlo?</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1255" end_char="1256">Te</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1258" end_char="1262">estoy</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1264" end_char="1271">hablando</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1273" end_char="1275">del</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1277" end_char="1279">mes</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1281" end_char="1282">de</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1284" end_char="1289">agosto</TOKEN>
<TOKEN id="token-12-7" pos="punct" morph="none" start_char="1290" end_char="1291">",</TOKEN>
<TOKEN id="token-12-8" pos="word" morph="none" start_char="1293" end_char="1294">ha</TOKEN>
<TOKEN id="token-12-9" pos="word" morph="none" start_char="1296" end_char="1304">asegurado</TOKEN>
<TOKEN id="token-12-10" pos="word" morph="none" start_char="1306" end_char="1310">antes</TOKEN>
<TOKEN id="token-12-11" pos="word" morph="none" start_char="1312" end_char="1313">de</TOKEN>
<TOKEN id="token-12-12" pos="word" morph="none" start_char="1315" end_char="1322">plantear</TOKEN>
<TOKEN id="token-12-13" pos="word" morph="none" start_char="1324" end_char="1325">la</TOKEN>
<TOKEN id="token-12-14" pos="word" morph="none" start_char="1327" end_char="1336">conclusión</TOKEN>
<TOKEN id="token-12-15" pos="word" morph="none" start_char="1338" end_char="1339">de</TOKEN>
<TOKEN id="token-12-16" pos="word" morph="none" start_char="1341" end_char="1342">lo</TOKEN>
<TOKEN id="token-12-17" pos="word" morph="none" start_char="1344" end_char="1351">ocurrido</TOKEN>
<TOKEN id="token-12-18" pos="punct" morph="none" start_char="1352" end_char="1352">:</TOKEN>
<TOKEN id="token-12-19" pos="punct" morph="none" start_char="1354" end_char="1355">"¿</TOKEN>
<TOKEN id="token-12-20" pos="word" morph="none" start_char="1356" end_char="1359">Cómo</TOKEN>
<TOKEN id="token-12-21" pos="word" morph="none" start_char="1361" end_char="1366">puedes</TOKEN>
<TOKEN id="token-12-22" pos="word" morph="none" start_char="1368" end_char="1372">coger</TOKEN>
<TOKEN id="token-12-23" pos="word" morph="none" start_char="1374" end_char="1377">cien</TOKEN>
<TOKEN id="token-12-24" pos="word" morph="none" start_char="1379" end_char="1388">pangolines</TOKEN>
<TOKEN id="token-12-25" pos="word" morph="none" start_char="1390" end_char="1390">a</TOKEN>
<TOKEN id="token-12-26" pos="word" morph="none" start_char="1392" end_char="1393">la</TOKEN>
<TOKEN id="token-12-27" pos="word" morph="none" start_char="1395" end_char="1400">semana</TOKEN>
<TOKEN id="token-12-28" pos="word" morph="none" start_char="1402" end_char="1402">y</TOKEN>
<TOKEN id="token-12-29" pos="word" morph="none" start_char="1404" end_char="1408">salir</TOKEN>
<TOKEN id="token-12-30" pos="word" morph="none" start_char="1410" end_char="1411">de</TOKEN>
<TOKEN id="token-12-31" pos="word" morph="none" start_char="1413" end_char="1415">las</TOKEN>
<TOKEN id="token-12-32" pos="word" morph="none" start_char="1417" end_char="1422">mismas</TOKEN>
<TOKEN id="token-12-33" pos="word" morph="none" start_char="1424" end_char="1428">zonas</TOKEN>
<TOKEN id="token-12-34" pos="word" morph="none" start_char="1430" end_char="1430">y</TOKEN>
<TOKEN id="token-12-35" pos="word" morph="none" start_char="1432" end_char="1436">luego</TOKEN>
<TOKEN id="token-12-36" pos="word" morph="none" start_char="1438" end_char="1442">dejar</TOKEN>
<TOKEN id="token-12-37" pos="word" morph="none" start_char="1444" end_char="1445">de</TOKEN>
<TOKEN id="token-12-38" pos="word" morph="none" start_char="1447" end_char="1453">hacerlo</TOKEN>
<TOKEN id="token-12-39" pos="punct" morph="none" start_char="1454" end_char="1454">?</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1456" end_char="1457">
<ORIGINAL_TEXT>".</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="punct" morph="none" start_char="1456" end_char="1457">".</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1460" end_char="1472">
<ORIGINAL_TEXT>Frank Cuesta.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1460" end_char="1464">Frank</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1466" end_char="1471">Cuesta</TOKEN>
<TOKEN id="token-14-2" pos="punct" morph="none" start_char="1472" end_char="1472">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1474" end_char="1474">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="punct" morph="none" start_char="1474" end_char="1474">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1478" end_char="1490">
<ORIGINAL_TEXT>Frank Cuesta.</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1478" end_char="1482">Frank</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1484" end_char="1489">Cuesta</TOKEN>
<TOKEN id="token-16-2" pos="punct" morph="none" start_char="1490" end_char="1490">.</TOKEN>
</SEG>
<SEG id="segment-17" start_char="1492" end_char="1492">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="punct" morph="none" start_char="1492" end_char="1492">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="1496" end_char="1610">
<ORIGINAL_TEXT>La conclusión para Cuesta es clara: "En agosto ya pasaba algo ahí" pero no se hizo público hasta medio año después.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="1496" end_char="1497">La</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="1499" end_char="1508">conclusión</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="1510" end_char="1513">para</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="1515" end_char="1520">Cuesta</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="1522" end_char="1523">es</TOKEN>
<TOKEN id="token-18-5" pos="word" morph="none" start_char="1525" end_char="1529">clara</TOKEN>
<TOKEN id="token-18-6" pos="punct" morph="none" start_char="1530" end_char="1530">:</TOKEN>
<TOKEN id="token-18-7" pos="punct" morph="none" start_char="1532" end_char="1532">"</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="1533" end_char="1534">En</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="1536" end_char="1541">agosto</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="1543" end_char="1544">ya</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="1546" end_char="1551">pasaba</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="1553" end_char="1556">algo</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="1558" end_char="1560">ahí</TOKEN>
<TOKEN id="token-18-14" pos="punct" morph="none" start_char="1561" end_char="1561">"</TOKEN>
<TOKEN id="token-18-15" pos="word" morph="none" start_char="1563" end_char="1566">pero</TOKEN>
<TOKEN id="token-18-16" pos="word" morph="none" start_char="1568" end_char="1569">no</TOKEN>
<TOKEN id="token-18-17" pos="word" morph="none" start_char="1571" end_char="1572">se</TOKEN>
<TOKEN id="token-18-18" pos="word" morph="none" start_char="1574" end_char="1577">hizo</TOKEN>
<TOKEN id="token-18-19" pos="word" morph="none" start_char="1579" end_char="1585">público</TOKEN>
<TOKEN id="token-18-20" pos="word" morph="none" start_char="1587" end_char="1591">hasta</TOKEN>
<TOKEN id="token-18-21" pos="word" morph="none" start_char="1593" end_char="1597">medio</TOKEN>
<TOKEN id="token-18-22" pos="word" morph="none" start_char="1599" end_char="1601">año</TOKEN>
<TOKEN id="token-18-23" pos="word" morph="none" start_char="1603" end_char="1609">después</TOKEN>
<TOKEN id="token-18-24" pos="punct" morph="none" start_char="1610" end_char="1610">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="1612" end_char="1781">
<ORIGINAL_TEXT>Jiménez Losantos se ha apuntado al carro conspiranoico y ha asegurado que "hubo un médico que sí lo denunció y lo metieron en la cárcel, y después lo sacaron para morir".</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="1612" end_char="1618">Jiménez</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="1620" end_char="1627">Losantos</TOKEN>
<TOKEN id="token-19-2" pos="word" morph="none" start_char="1629" end_char="1630">se</TOKEN>
<TOKEN id="token-19-3" pos="word" morph="none" start_char="1632" end_char="1633">ha</TOKEN>
<TOKEN id="token-19-4" pos="word" morph="none" start_char="1635" end_char="1642">apuntado</TOKEN>
<TOKEN id="token-19-5" pos="word" morph="none" start_char="1644" end_char="1645">al</TOKEN>
<TOKEN id="token-19-6" pos="word" morph="none" start_char="1647" end_char="1651">carro</TOKEN>
<TOKEN id="token-19-7" pos="word" morph="none" start_char="1653" end_char="1665">conspiranoico</TOKEN>
<TOKEN id="token-19-8" pos="word" morph="none" start_char="1667" end_char="1667">y</TOKEN>
<TOKEN id="token-19-9" pos="word" morph="none" start_char="1669" end_char="1670">ha</TOKEN>
<TOKEN id="token-19-10" pos="word" morph="none" start_char="1672" end_char="1680">asegurado</TOKEN>
<TOKEN id="token-19-11" pos="word" morph="none" start_char="1682" end_char="1684">que</TOKEN>
<TOKEN id="token-19-12" pos="punct" morph="none" start_char="1686" end_char="1686">"</TOKEN>
<TOKEN id="token-19-13" pos="word" morph="none" start_char="1687" end_char="1690">hubo</TOKEN>
<TOKEN id="token-19-14" pos="word" morph="none" start_char="1692" end_char="1693">un</TOKEN>
<TOKEN id="token-19-15" pos="word" morph="none" start_char="1695" end_char="1700">médico</TOKEN>
<TOKEN id="token-19-16" pos="word" morph="none" start_char="1702" end_char="1704">que</TOKEN>
<TOKEN id="token-19-17" pos="word" morph="none" start_char="1706" end_char="1707">sí</TOKEN>
<TOKEN id="token-19-18" pos="word" morph="none" start_char="1709" end_char="1710">lo</TOKEN>
<TOKEN id="token-19-19" pos="word" morph="none" start_char="1712" end_char="1719">denunció</TOKEN>
<TOKEN id="token-19-20" pos="word" morph="none" start_char="1721" end_char="1721">y</TOKEN>
<TOKEN id="token-19-21" pos="word" morph="none" start_char="1723" end_char="1724">lo</TOKEN>
<TOKEN id="token-19-22" pos="word" morph="none" start_char="1726" end_char="1733">metieron</TOKEN>
<TOKEN id="token-19-23" pos="word" morph="none" start_char="1735" end_char="1736">en</TOKEN>
<TOKEN id="token-19-24" pos="word" morph="none" start_char="1738" end_char="1739">la</TOKEN>
<TOKEN id="token-19-25" pos="word" morph="none" start_char="1741" end_char="1746">cárcel</TOKEN>
<TOKEN id="token-19-26" pos="punct" morph="none" start_char="1747" end_char="1747">,</TOKEN>
<TOKEN id="token-19-27" pos="word" morph="none" start_char="1749" end_char="1749">y</TOKEN>
<TOKEN id="token-19-28" pos="word" morph="none" start_char="1751" end_char="1757">después</TOKEN>
<TOKEN id="token-19-29" pos="word" morph="none" start_char="1759" end_char="1760">lo</TOKEN>
<TOKEN id="token-19-30" pos="word" morph="none" start_char="1762" end_char="1768">sacaron</TOKEN>
<TOKEN id="token-19-31" pos="word" morph="none" start_char="1770" end_char="1773">para</TOKEN>
<TOKEN id="token-19-32" pos="word" morph="none" start_char="1775" end_char="1779">morir</TOKEN>
<TOKEN id="token-19-33" pos="punct" morph="none" start_char="1780" end_char="1781">".</TOKEN>
</SEG>
<SEG id="segment-20" start_char="1784" end_char="1948">
<ORIGINAL_TEXT>Sobre el pangolín, Cuesta explicaba que "se usan hasta las escamas para ponerselas en las heridas… Y que se lo comen casi crudo, lo meten unos segundos en agua y ya.</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="1784" end_char="1788">Sobre</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="1790" end_char="1791">el</TOKEN>
<TOKEN id="token-20-2" pos="word" morph="none" start_char="1793" end_char="1800">pangolín</TOKEN>
<TOKEN id="token-20-3" pos="punct" morph="none" start_char="1801" end_char="1801">,</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="1803" end_char="1808">Cuesta</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="1810" end_char="1818">explicaba</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="1820" end_char="1822">que</TOKEN>
<TOKEN id="token-20-7" pos="punct" morph="none" start_char="1824" end_char="1824">"</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="1825" end_char="1826">se</TOKEN>
<TOKEN id="token-20-9" pos="word" morph="none" start_char="1828" end_char="1831">usan</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="1833" end_char="1837">hasta</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="1839" end_char="1841">las</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="1843" end_char="1849">escamas</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="1851" end_char="1854">para</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="1856" end_char="1865">ponerselas</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="1867" end_char="1868">en</TOKEN>
<TOKEN id="token-20-16" pos="word" morph="none" start_char="1870" end_char="1872">las</TOKEN>
<TOKEN id="token-20-17" pos="word" morph="none" start_char="1874" end_char="1880">heridas</TOKEN>
<TOKEN id="token-20-18" pos="punct" morph="none" start_char="1881" end_char="1881">…</TOKEN>
<TOKEN id="token-20-19" pos="word" morph="none" start_char="1883" end_char="1883">Y</TOKEN>
<TOKEN id="token-20-20" pos="word" morph="none" start_char="1885" end_char="1887">que</TOKEN>
<TOKEN id="token-20-21" pos="word" morph="none" start_char="1889" end_char="1890">se</TOKEN>
<TOKEN id="token-20-22" pos="word" morph="none" start_char="1892" end_char="1893">lo</TOKEN>
<TOKEN id="token-20-23" pos="word" morph="none" start_char="1895" end_char="1899">comen</TOKEN>
<TOKEN id="token-20-24" pos="word" morph="none" start_char="1901" end_char="1904">casi</TOKEN>
<TOKEN id="token-20-25" pos="word" morph="none" start_char="1906" end_char="1910">crudo</TOKEN>
<TOKEN id="token-20-26" pos="punct" morph="none" start_char="1911" end_char="1911">,</TOKEN>
<TOKEN id="token-20-27" pos="word" morph="none" start_char="1913" end_char="1914">lo</TOKEN>
<TOKEN id="token-20-28" pos="word" morph="none" start_char="1916" end_char="1920">meten</TOKEN>
<TOKEN id="token-20-29" pos="word" morph="none" start_char="1922" end_char="1925">unos</TOKEN>
<TOKEN id="token-20-30" pos="word" morph="none" start_char="1927" end_char="1934">segundos</TOKEN>
<TOKEN id="token-20-31" pos="word" morph="none" start_char="1936" end_char="1937">en</TOKEN>
<TOKEN id="token-20-32" pos="word" morph="none" start_char="1939" end_char="1942">agua</TOKEN>
<TOKEN id="token-20-33" pos="word" morph="none" start_char="1944" end_char="1944">y</TOKEN>
<TOKEN id="token-20-34" pos="word" morph="none" start_char="1946" end_char="1947">ya</TOKEN>
<TOKEN id="token-20-35" pos="punct" morph="none" start_char="1948" end_char="1948">.</TOKEN>
</SEG>
<SEG id="segment-21" start_char="1950" end_char="1966">
<ORIGINAL_TEXT>Es una especie de</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="1950" end_char="1951">Es</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="1953" end_char="1955">una</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="1957" end_char="1963">especie</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="1965" end_char="1966">de</TOKEN>
</SEG>
<SEG id="segment-22" start_char="1969" end_char="1973">
<ORIGINAL_TEXT>sushi</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="1969" end_char="1973">sushi</TOKEN>
</SEG>
<SEG id="segment-23" start_char="1976" end_char="1976">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="punct" morph="none" start_char="1976" end_char="1976">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="1978" end_char="2038">
<ORIGINAL_TEXT>Es el mamífero mas castigado del mundo, han acabado con él...</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="1978" end_char="1979">Es</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="1981" end_char="1982">el</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="1984" end_char="1991">mamífero</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="1993" end_char="1995">mas</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="1997" end_char="2005">castigado</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="2007" end_char="2009">del</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="2011" end_char="2015">mundo</TOKEN>
<TOKEN id="token-24-7" pos="punct" morph="none" start_char="2016" end_char="2016">,</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="2018" end_char="2020">han</TOKEN>
<TOKEN id="token-24-9" pos="word" morph="none" start_char="2022" end_char="2028">acabado</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="2030" end_char="2032">con</TOKEN>
<TOKEN id="token-24-11" pos="word" morph="none" start_char="2034" end_char="2035">él</TOKEN>
<TOKEN id="token-24-12" pos="punct" morph="none" start_char="2036" end_char="2038">...</TOKEN>
</SEG>
<SEG id="segment-25" start_char="2040" end_char="2132">
<ORIGINAL_TEXT>El problema no es el pangolín: es que se lo han comido y no han dicho lo que estaba pasando".</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="2040" end_char="2041">El</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="2043" end_char="2050">problema</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="2052" end_char="2053">no</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="2055" end_char="2056">es</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="2058" end_char="2059">el</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="2061" end_char="2068">pangolín</TOKEN>
<TOKEN id="token-25-6" pos="punct" morph="none" start_char="2069" end_char="2069">:</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="2071" end_char="2072">es</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="2074" end_char="2076">que</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="2078" end_char="2079">se</TOKEN>
<TOKEN id="token-25-10" pos="word" morph="none" start_char="2081" end_char="2082">lo</TOKEN>
<TOKEN id="token-25-11" pos="word" morph="none" start_char="2084" end_char="2086">han</TOKEN>
<TOKEN id="token-25-12" pos="word" morph="none" start_char="2088" end_char="2093">comido</TOKEN>
<TOKEN id="token-25-13" pos="word" morph="none" start_char="2095" end_char="2095">y</TOKEN>
<TOKEN id="token-25-14" pos="word" morph="none" start_char="2097" end_char="2098">no</TOKEN>
<TOKEN id="token-25-15" pos="word" morph="none" start_char="2100" end_char="2102">han</TOKEN>
<TOKEN id="token-25-16" pos="word" morph="none" start_char="2104" end_char="2108">dicho</TOKEN>
<TOKEN id="token-25-17" pos="word" morph="none" start_char="2110" end_char="2111">lo</TOKEN>
<TOKEN id="token-25-18" pos="word" morph="none" start_char="2113" end_char="2115">que</TOKEN>
<TOKEN id="token-25-19" pos="word" morph="none" start_char="2117" end_char="2122">estaba</TOKEN>
<TOKEN id="token-25-20" pos="word" morph="none" start_char="2124" end_char="2130">pasando</TOKEN>
<TOKEN id="token-25-21" pos="punct" morph="none" start_char="2131" end_char="2132">".</TOKEN>
</SEG>
<SEG id="segment-26" start_char="2135" end_char="2375">
<ORIGINAL_TEXT>Para rematar el tema, Cuesta considera que en Tailandia, en realidad casi todo el mundo ya ha pasado en los meses de agosto, septiembre u octubre de 2019 el coronavirus, apuntando a que la epidemia lleva en realidad varios meses sin control.</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="word" morph="none" start_char="2135" end_char="2138">Para</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="2140" end_char="2146">rematar</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="2148" end_char="2149">el</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="2151" end_char="2154">tema</TOKEN>
<TOKEN id="token-26-4" pos="punct" morph="none" start_char="2155" end_char="2155">,</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="2157" end_char="2162">Cuesta</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="2164" end_char="2172">considera</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="2174" end_char="2176">que</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="2178" end_char="2179">en</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="2181" end_char="2189">Tailandia</TOKEN>
<TOKEN id="token-26-10" pos="punct" morph="none" start_char="2190" end_char="2190">,</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="2192" end_char="2193">en</TOKEN>
<TOKEN id="token-26-12" pos="word" morph="none" start_char="2195" end_char="2202">realidad</TOKEN>
<TOKEN id="token-26-13" pos="word" morph="none" start_char="2204" end_char="2207">casi</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="2209" end_char="2212">todo</TOKEN>
<TOKEN id="token-26-15" pos="word" morph="none" start_char="2214" end_char="2215">el</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="2217" end_char="2221">mundo</TOKEN>
<TOKEN id="token-26-17" pos="word" morph="none" start_char="2223" end_char="2224">ya</TOKEN>
<TOKEN id="token-26-18" pos="word" morph="none" start_char="2226" end_char="2227">ha</TOKEN>
<TOKEN id="token-26-19" pos="word" morph="none" start_char="2229" end_char="2234">pasado</TOKEN>
<TOKEN id="token-26-20" pos="word" morph="none" start_char="2236" end_char="2237">en</TOKEN>
<TOKEN id="token-26-21" pos="word" morph="none" start_char="2239" end_char="2241">los</TOKEN>
<TOKEN id="token-26-22" pos="word" morph="none" start_char="2243" end_char="2247">meses</TOKEN>
<TOKEN id="token-26-23" pos="word" morph="none" start_char="2249" end_char="2250">de</TOKEN>
<TOKEN id="token-26-24" pos="word" morph="none" start_char="2252" end_char="2257">agosto</TOKEN>
<TOKEN id="token-26-25" pos="punct" morph="none" start_char="2258" end_char="2258">,</TOKEN>
<TOKEN id="token-26-26" pos="word" morph="none" start_char="2260" end_char="2269">septiembre</TOKEN>
<TOKEN id="token-26-27" pos="word" morph="none" start_char="2271" end_char="2271">u</TOKEN>
<TOKEN id="token-26-28" pos="word" morph="none" start_char="2273" end_char="2279">octubre</TOKEN>
<TOKEN id="token-26-29" pos="word" morph="none" start_char="2281" end_char="2282">de</TOKEN>
<TOKEN id="token-26-30" pos="word" morph="none" start_char="2284" end_char="2287">2019</TOKEN>
<TOKEN id="token-26-31" pos="word" morph="none" start_char="2289" end_char="2290">el</TOKEN>
<TOKEN id="token-26-32" pos="word" morph="none" start_char="2292" end_char="2302">coronavirus</TOKEN>
<TOKEN id="token-26-33" pos="punct" morph="none" start_char="2303" end_char="2303">,</TOKEN>
<TOKEN id="token-26-34" pos="word" morph="none" start_char="2305" end_char="2313">apuntando</TOKEN>
<TOKEN id="token-26-35" pos="word" morph="none" start_char="2315" end_char="2315">a</TOKEN>
<TOKEN id="token-26-36" pos="word" morph="none" start_char="2317" end_char="2319">que</TOKEN>
<TOKEN id="token-26-37" pos="word" morph="none" start_char="2321" end_char="2322">la</TOKEN>
<TOKEN id="token-26-38" pos="word" morph="none" start_char="2324" end_char="2331">epidemia</TOKEN>
<TOKEN id="token-26-39" pos="word" morph="none" start_char="2333" end_char="2337">lleva</TOKEN>
<TOKEN id="token-26-40" pos="word" morph="none" start_char="2339" end_char="2340">en</TOKEN>
<TOKEN id="token-26-41" pos="word" morph="none" start_char="2342" end_char="2349">realidad</TOKEN>
<TOKEN id="token-26-42" pos="word" morph="none" start_char="2351" end_char="2356">varios</TOKEN>
<TOKEN id="token-26-43" pos="word" morph="none" start_char="2358" end_char="2362">meses</TOKEN>
<TOKEN id="token-26-44" pos="word" morph="none" start_char="2364" end_char="2366">sin</TOKEN>
<TOKEN id="token-26-45" pos="word" morph="none" start_char="2368" end_char="2374">control</TOKEN>
<TOKEN id="token-26-46" pos="punct" morph="none" start_char="2375" end_char="2375">.</TOKEN>
</SEG>
<SEG id="segment-27" start_char="2377" end_char="2429">
<ORIGINAL_TEXT>"En agosto, por allí, ya hemos pasado todos el virus.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="punct" morph="none" start_char="2377" end_char="2377">"</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="2378" end_char="2379">En</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="2381" end_char="2386">agosto</TOKEN>
<TOKEN id="token-27-3" pos="punct" morph="none" start_char="2387" end_char="2387">,</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="2389" end_char="2391">por</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="2393" end_char="2396">allí</TOKEN>
<TOKEN id="token-27-6" pos="punct" morph="none" start_char="2397" end_char="2397">,</TOKEN>
<TOKEN id="token-27-7" pos="word" morph="none" start_char="2399" end_char="2400">ya</TOKEN>
<TOKEN id="token-27-8" pos="word" morph="none" start_char="2402" end_char="2406">hemos</TOKEN>
<TOKEN id="token-27-9" pos="word" morph="none" start_char="2408" end_char="2413">pasado</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="2415" end_char="2419">todos</TOKEN>
<TOKEN id="token-27-11" pos="word" morph="none" start_char="2421" end_char="2422">el</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="2424" end_char="2428">virus</TOKEN>
<TOKEN id="token-27-13" pos="punct" morph="none" start_char="2429" end_char="2429">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="2431" end_char="2451">
<ORIGINAL_TEXT>Hemos pasado catarro.</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="2431" end_char="2435">Hemos</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="2437" end_char="2442">pasado</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="2444" end_char="2450">catarro</TOKEN>
<TOKEN id="token-28-3" pos="punct" morph="none" start_char="2451" end_char="2451">.</TOKEN>
</SEG>
<SEG id="segment-29" start_char="2453" end_char="2477">
<ORIGINAL_TEXT>Lo cogimos en casa todos.</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="2453" end_char="2454">Lo</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="2456" end_char="2462">cogimos</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="2464" end_char="2465">en</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="2467" end_char="2470">casa</TOKEN>
<TOKEN id="token-29-4" pos="word" morph="none" start_char="2472" end_char="2476">todos</TOKEN>
<TOKEN id="token-29-5" pos="punct" morph="none" start_char="2477" end_char="2477">.</TOKEN>
</SEG>
<SEG id="segment-30" start_char="2479" end_char="2606">
<ORIGINAL_TEXT>Y ahora, empiezas a dar marchas atrás y piensas qué ha estado pasando desde septiembre, octubre y nadie entonces se dio cuenta".</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="2479" end_char="2479">Y</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="2481" end_char="2485">ahora</TOKEN>
<TOKEN id="token-30-2" pos="punct" morph="none" start_char="2486" end_char="2486">,</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="2488" end_char="2495">empiezas</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="2497" end_char="2497">a</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="2499" end_char="2501">dar</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="2503" end_char="2509">marchas</TOKEN>
<TOKEN id="token-30-7" pos="word" morph="none" start_char="2511" end_char="2515">atrás</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="2517" end_char="2517">y</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="2519" end_char="2525">piensas</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="2527" end_char="2529">qué</TOKEN>
<TOKEN id="token-30-11" pos="word" morph="none" start_char="2531" end_char="2532">ha</TOKEN>
<TOKEN id="token-30-12" pos="word" morph="none" start_char="2534" end_char="2539">estado</TOKEN>
<TOKEN id="token-30-13" pos="word" morph="none" start_char="2541" end_char="2547">pasando</TOKEN>
<TOKEN id="token-30-14" pos="word" morph="none" start_char="2549" end_char="2553">desde</TOKEN>
<TOKEN id="token-30-15" pos="word" morph="none" start_char="2555" end_char="2564">septiembre</TOKEN>
<TOKEN id="token-30-16" pos="punct" morph="none" start_char="2565" end_char="2565">,</TOKEN>
<TOKEN id="token-30-17" pos="word" morph="none" start_char="2567" end_char="2573">octubre</TOKEN>
<TOKEN id="token-30-18" pos="word" morph="none" start_char="2575" end_char="2575">y</TOKEN>
<TOKEN id="token-30-19" pos="word" morph="none" start_char="2577" end_char="2581">nadie</TOKEN>
<TOKEN id="token-30-20" pos="word" morph="none" start_char="2583" end_char="2590">entonces</TOKEN>
<TOKEN id="token-30-21" pos="word" morph="none" start_char="2592" end_char="2593">se</TOKEN>
<TOKEN id="token-30-22" pos="word" morph="none" start_char="2595" end_char="2597">dio</TOKEN>
<TOKEN id="token-30-23" pos="word" morph="none" start_char="2599" end_char="2604">cuenta</TOKEN>
<TOKEN id="token-30-24" pos="punct" morph="none" start_char="2605" end_char="2606">".</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
