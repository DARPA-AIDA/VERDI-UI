<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C049DV5" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="5671" raw_text_md5="3cbacc850a7647031b7b144d58a3d4f4">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="86">
<ORIGINAL_TEXT>Por qué llevar mascarilla no aumenta la probabilidad de dar positivo en una prueba PCR</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="3">Por</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="5" end_char="7">qué</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="9" end_char="14">llevar</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="16" end_char="25">mascarilla</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="27" end_char="28">no</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="30" end_char="36">aumenta</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="38" end_char="39">la</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="41" end_char="52">probabilidad</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="54" end_char="55">de</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="57" end_char="59">dar</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="61" end_char="68">positivo</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="70" end_char="71">en</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="73" end_char="75">una</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="77" end_char="82">prueba</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="84" end_char="86">PCR</TOKEN>
</SEG>
<SEG id="segment-1" start_char="91" end_char="313">
<ORIGINAL_TEXT>Nos habéis preguntado por publicaciones como esta o esta en las que se afirma que al llevar mascarilla todos los gérmenes que exhalamos quedan "atrapados" en ella y aumenta la probabilidad de dar positivo en una prueba PCR.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="91" end_char="93">Nos</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="95" end_char="100">habéis</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="102" end_char="111">preguntado</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="113" end_char="115">por</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="117" end_char="129">publicaciones</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="131" end_char="134">como</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="136" end_char="139">esta</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="141" end_char="141">o</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="143" end_char="146">esta</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="148" end_char="149">en</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="151" end_char="153">las</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="155" end_char="157">que</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="159" end_char="160">se</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="162" end_char="167">afirma</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="169" end_char="171">que</TOKEN>
<TOKEN id="token-1-15" pos="word" morph="none" start_char="173" end_char="174">al</TOKEN>
<TOKEN id="token-1-16" pos="word" morph="none" start_char="176" end_char="181">llevar</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="183" end_char="192">mascarilla</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="194" end_char="198">todos</TOKEN>
<TOKEN id="token-1-19" pos="word" morph="none" start_char="200" end_char="202">los</TOKEN>
<TOKEN id="token-1-20" pos="word" morph="none" start_char="204" end_char="211">gérmenes</TOKEN>
<TOKEN id="token-1-21" pos="word" morph="none" start_char="213" end_char="215">que</TOKEN>
<TOKEN id="token-1-22" pos="word" morph="none" start_char="217" end_char="225">exhalamos</TOKEN>
<TOKEN id="token-1-23" pos="word" morph="none" start_char="227" end_char="232">quedan</TOKEN>
<TOKEN id="token-1-24" pos="punct" morph="none" start_char="234" end_char="234">"</TOKEN>
<TOKEN id="token-1-25" pos="word" morph="none" start_char="235" end_char="243">atrapados</TOKEN>
<TOKEN id="token-1-26" pos="punct" morph="none" start_char="244" end_char="244">"</TOKEN>
<TOKEN id="token-1-27" pos="word" morph="none" start_char="246" end_char="247">en</TOKEN>
<TOKEN id="token-1-28" pos="word" morph="none" start_char="249" end_char="252">ella</TOKEN>
<TOKEN id="token-1-29" pos="word" morph="none" start_char="254" end_char="254">y</TOKEN>
<TOKEN id="token-1-30" pos="word" morph="none" start_char="256" end_char="262">aumenta</TOKEN>
<TOKEN id="token-1-31" pos="word" morph="none" start_char="264" end_char="265">la</TOKEN>
<TOKEN id="token-1-32" pos="word" morph="none" start_char="267" end_char="278">probabilidad</TOKEN>
<TOKEN id="token-1-33" pos="word" morph="none" start_char="280" end_char="281">de</TOKEN>
<TOKEN id="token-1-34" pos="word" morph="none" start_char="283" end_char="285">dar</TOKEN>
<TOKEN id="token-1-35" pos="word" morph="none" start_char="287" end_char="294">positivo</TOKEN>
<TOKEN id="token-1-36" pos="word" morph="none" start_char="296" end_char="297">en</TOKEN>
<TOKEN id="token-1-37" pos="word" morph="none" start_char="299" end_char="301">una</TOKEN>
<TOKEN id="token-1-38" pos="word" morph="none" start_char="303" end_char="308">prueba</TOKEN>
<TOKEN id="token-1-39" pos="word" morph="none" start_char="310" end_char="312">PCR</TOKEN>
<TOKEN id="token-1-40" pos="punct" morph="none" start_char="313" end_char="313">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="315" end_char="327">
<ORIGINAL_TEXT>No es cierto.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="315" end_char="316">No</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="318" end_char="319">es</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="321" end_char="326">cierto</TOKEN>
<TOKEN id="token-2-3" pos="punct" morph="none" start_char="327" end_char="327">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="329" end_char="393">
<ORIGINAL_TEXT>Pepe Alcamí, virólogo del Instituto de Salud Carlos III, indica a</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="329" end_char="332">Pepe</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="334" end_char="339">Alcamí</TOKEN>
<TOKEN id="token-3-2" pos="punct" morph="none" start_char="340" end_char="340">,</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="342" end_char="349">virólogo</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="351" end_char="353">del</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="355" end_char="363">Instituto</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="365" end_char="366">de</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="368" end_char="372">Salud</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="374" end_char="379">Carlos</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="381" end_char="383">III</TOKEN>
<TOKEN id="token-3-10" pos="punct" morph="none" start_char="384" end_char="384">,</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="386" end_char="391">indica</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="393" end_char="393">a</TOKEN>
</SEG>
<SEG id="segment-4" start_char="396" end_char="410">
<ORIGINAL_TEXT>Maldita Ciencia</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="396" end_char="402">Maldita</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="404" end_char="410">Ciencia</TOKEN>
</SEG>
<SEG id="segment-5" start_char="413" end_char="502">
<ORIGINAL_TEXT>que si el virus está en la parte interna de tu mascarilla es que lo tienes en tu garganta.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="413" end_char="415">que</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="417" end_char="418">si</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="420" end_char="421">el</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="423" end_char="427">virus</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="429" end_char="432">está</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="434" end_char="435">en</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="437" end_char="438">la</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="440" end_char="444">parte</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="446" end_char="452">interna</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="454" end_char="455">de</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="457" end_char="458">tu</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="460" end_char="469">mascarilla</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="471" end_char="472">es</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="474" end_char="476">que</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="478" end_char="479">lo</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="481" end_char="486">tienes</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="488" end_char="489">en</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="491" end_char="492">tu</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="494" end_char="501">garganta</TOKEN>
<TOKEN id="token-5-19" pos="punct" morph="none" start_char="502" end_char="502">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="504" end_char="683">
<ORIGINAL_TEXT>Si no, la mascarilla podrá tener otros microbios, bacterias que estén en la garganta o incluso otros virus pero estos "no serán amplificados por una PCR específica de coronavirus".</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="504" end_char="505">Si</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="507" end_char="508">no</TOKEN>
<TOKEN id="token-6-2" pos="punct" morph="none" start_char="509" end_char="509">,</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="511" end_char="512">la</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="514" end_char="523">mascarilla</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="525" end_char="529">podrá</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="531" end_char="535">tener</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="537" end_char="541">otros</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="543" end_char="551">microbios</TOKEN>
<TOKEN id="token-6-9" pos="punct" morph="none" start_char="552" end_char="552">,</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="554" end_char="562">bacterias</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="564" end_char="566">que</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="568" end_char="572">estén</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="574" end_char="575">en</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="577" end_char="578">la</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="580" end_char="587">garganta</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="589" end_char="589">o</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="591" end_char="597">incluso</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="599" end_char="603">otros</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="605" end_char="609">virus</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="611" end_char="614">pero</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="616" end_char="620">estos</TOKEN>
<TOKEN id="token-6-22" pos="punct" morph="none" start_char="622" end_char="622">"</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="623" end_char="624">no</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="626" end_char="630">serán</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="632" end_char="643">amplificados</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="645" end_char="647">por</TOKEN>
<TOKEN id="token-6-27" pos="word" morph="none" start_char="649" end_char="651">una</TOKEN>
<TOKEN id="token-6-28" pos="word" morph="none" start_char="653" end_char="655">PCR</TOKEN>
<TOKEN id="token-6-29" pos="word" morph="none" start_char="657" end_char="666">específica</TOKEN>
<TOKEN id="token-6-30" pos="word" morph="none" start_char="668" end_char="669">de</TOKEN>
<TOKEN id="token-6-31" pos="word" morph="none" start_char="671" end_char="681">coronavirus</TOKEN>
<TOKEN id="token-6-32" pos="punct" morph="none" start_char="682" end_char="683">".</TOKEN>
</SEG>
<SEG id="segment-7" start_char="685" end_char="701">
<ORIGINAL_TEXT>Os lo explicamos.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="685" end_char="686">Os</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="688" end_char="689">lo</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="691" end_char="700">explicamos</TOKEN>
<TOKEN id="token-7-3" pos="punct" morph="none" start_char="701" end_char="701">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="704" end_char="762">
<ORIGINAL_TEXT>Las pruebas PCR para detectar el SARS-CoV-2 son específicas</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="704" end_char="706">Las</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="708" end_char="714">pruebas</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="716" end_char="718">PCR</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="720" end_char="723">para</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="725" end_char="732">detectar</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="734" end_char="735">el</TOKEN>
<TOKEN id="token-8-6" pos="unknown" morph="none" start_char="737" end_char="746">SARS-CoV-2</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="748" end_char="750">son</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="752" end_char="762">específicas</TOKEN>
</SEG>
<SEG id="segment-9" start_char="766" end_char="877">
<ORIGINAL_TEXT>El mensaje afirma que "a mayor uso de mascarillas, mayor probabilidades de dar positivo en una prueba de Covid".</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="766" end_char="767">El</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="769" end_char="775">mensaje</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="777" end_char="782">afirma</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="784" end_char="786">que</TOKEN>
<TOKEN id="token-9-4" pos="punct" morph="none" start_char="788" end_char="788">"</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="789" end_char="789">a</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="791" end_char="795">mayor</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="797" end_char="799">uso</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="801" end_char="802">de</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="804" end_char="814">mascarillas</TOKEN>
<TOKEN id="token-9-10" pos="punct" morph="none" start_char="815" end_char="815">,</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="817" end_char="821">mayor</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="823" end_char="836">probabilidades</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="838" end_char="839">de</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="841" end_char="843">dar</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="845" end_char="852">positivo</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="854" end_char="855">en</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="857" end_char="859">una</TOKEN>
<TOKEN id="token-9-18" pos="word" morph="none" start_char="861" end_char="866">prueba</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="868" end_char="869">de</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="871" end_char="875">Covid</TOKEN>
<TOKEN id="token-9-21" pos="punct" morph="none" start_char="876" end_char="877">".</TOKEN>
</SEG>
<SEG id="segment-10" start_char="879" end_char="1036">
<ORIGINAL_TEXT>"Todos los gérmenes que exhalas, se quedan atrapados en la mascarilla y retornan hacia dentro, los cuales se estancan en el conducto nasal que filtra el aire.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="punct" morph="none" start_char="879" end_char="879">"</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="880" end_char="884">Todos</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="886" end_char="888">los</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="890" end_char="897">gérmenes</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="899" end_char="901">que</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="903" end_char="909">exhalas</TOKEN>
<TOKEN id="token-10-6" pos="punct" morph="none" start_char="910" end_char="910">,</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="912" end_char="913">se</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="915" end_char="920">quedan</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="922" end_char="930">atrapados</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="932" end_char="933">en</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="935" end_char="936">la</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="938" end_char="947">mascarilla</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="949" end_char="949">y</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="951" end_char="958">retornan</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="960" end_char="964">hacia</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="966" end_char="971">dentro</TOKEN>
<TOKEN id="token-10-17" pos="punct" morph="none" start_char="972" end_char="972">,</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="974" end_char="976">los</TOKEN>
<TOKEN id="token-10-19" pos="word" morph="none" start_char="978" end_char="983">cuales</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="985" end_char="986">se</TOKEN>
<TOKEN id="token-10-21" pos="word" morph="none" start_char="988" end_char="995">estancan</TOKEN>
<TOKEN id="token-10-22" pos="word" morph="none" start_char="997" end_char="998">en</TOKEN>
<TOKEN id="token-10-23" pos="word" morph="none" start_char="1000" end_char="1001">el</TOKEN>
<TOKEN id="token-10-24" pos="word" morph="none" start_char="1003" end_char="1010">conducto</TOKEN>
<TOKEN id="token-10-25" pos="word" morph="none" start_char="1012" end_char="1016">nasal</TOKEN>
<TOKEN id="token-10-26" pos="word" morph="none" start_char="1018" end_char="1020">que</TOKEN>
<TOKEN id="token-10-27" pos="word" morph="none" start_char="1022" end_char="1027">filtra</TOKEN>
<TOKEN id="token-10-28" pos="word" morph="none" start_char="1029" end_char="1030">el</TOKEN>
<TOKEN id="token-10-29" pos="word" morph="none" start_char="1032" end_char="1035">aire</TOKEN>
<TOKEN id="token-10-30" pos="punct" morph="none" start_char="1036" end_char="1036">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1038" end_char="1177">
<ORIGINAL_TEXT>Luego meten el hisopo en esta cavidad, para agarrar, cuántos más gérmenes mejor, el resultado será un positivo asintomático seguro", indica.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1038" end_char="1042">Luego</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1044" end_char="1048">meten</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1050" end_char="1051">el</TOKEN>
<TOKEN id="token-11-3" pos="word" morph="none" start_char="1053" end_char="1058">hisopo</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1060" end_char="1061">en</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1063" end_char="1066">esta</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1068" end_char="1074">cavidad</TOKEN>
<TOKEN id="token-11-7" pos="punct" morph="none" start_char="1075" end_char="1075">,</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1077" end_char="1080">para</TOKEN>
<TOKEN id="token-11-9" pos="word" morph="none" start_char="1082" end_char="1088">agarrar</TOKEN>
<TOKEN id="token-11-10" pos="punct" morph="none" start_char="1089" end_char="1089">,</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1091" end_char="1097">cuántos</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1099" end_char="1101">más</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1103" end_char="1110">gérmenes</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1112" end_char="1116">mejor</TOKEN>
<TOKEN id="token-11-15" pos="punct" morph="none" start_char="1117" end_char="1117">,</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1119" end_char="1120">el</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1122" end_char="1130">resultado</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1132" end_char="1135">será</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1137" end_char="1138">un</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1140" end_char="1147">positivo</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1149" end_char="1160">asintomático</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1162" end_char="1167">seguro</TOKEN>
<TOKEN id="token-11-23" pos="punct" morph="none" start_char="1168" end_char="1169">",</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1171" end_char="1176">indica</TOKEN>
<TOKEN id="token-11-25" pos="punct" morph="none" start_char="1177" end_char="1177">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1180" end_char="1212">
<ORIGINAL_TEXT>No hay evidencias de que sea así.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1180" end_char="1181">No</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1183" end_char="1185">hay</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1187" end_char="1196">evidencias</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1198" end_char="1199">de</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1201" end_char="1203">que</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1205" end_char="1207">sea</TOKEN>
<TOKEN id="token-12-6" pos="word" morph="none" start_char="1209" end_char="1211">así</TOKEN>
<TOKEN id="token-12-7" pos="punct" morph="none" start_char="1212" end_char="1212">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1214" end_char="1421">
<ORIGINAL_TEXT>Si no se está infectado, por muchos gérmenes que acumule la mascarilla eso no provocaría un positivo en la PCR, ya que esta prueba solo detecta el SARS-Cov-2 y no da falsos positivos por cualquier otro virus.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1214" end_char="1215">Si</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1217" end_char="1218">no</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1220" end_char="1221">se</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1223" end_char="1226">está</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1228" end_char="1236">infectado</TOKEN>
<TOKEN id="token-13-5" pos="punct" morph="none" start_char="1237" end_char="1237">,</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1239" end_char="1241">por</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1243" end_char="1248">muchos</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1250" end_char="1257">gérmenes</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1259" end_char="1261">que</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1263" end_char="1269">acumule</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1271" end_char="1272">la</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1274" end_char="1283">mascarilla</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="1285" end_char="1287">eso</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="1289" end_char="1290">no</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="1292" end_char="1301">provocaría</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="1303" end_char="1304">un</TOKEN>
<TOKEN id="token-13-17" pos="word" morph="none" start_char="1306" end_char="1313">positivo</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="1315" end_char="1316">en</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="1318" end_char="1319">la</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="1321" end_char="1323">PCR</TOKEN>
<TOKEN id="token-13-21" pos="punct" morph="none" start_char="1324" end_char="1324">,</TOKEN>
<TOKEN id="token-13-22" pos="word" morph="none" start_char="1326" end_char="1327">ya</TOKEN>
<TOKEN id="token-13-23" pos="word" morph="none" start_char="1329" end_char="1331">que</TOKEN>
<TOKEN id="token-13-24" pos="word" morph="none" start_char="1333" end_char="1336">esta</TOKEN>
<TOKEN id="token-13-25" pos="word" morph="none" start_char="1338" end_char="1343">prueba</TOKEN>
<TOKEN id="token-13-26" pos="word" morph="none" start_char="1345" end_char="1348">solo</TOKEN>
<TOKEN id="token-13-27" pos="word" morph="none" start_char="1350" end_char="1356">detecta</TOKEN>
<TOKEN id="token-13-28" pos="word" morph="none" start_char="1358" end_char="1359">el</TOKEN>
<TOKEN id="token-13-29" pos="unknown" morph="none" start_char="1361" end_char="1370">SARS-Cov-2</TOKEN>
<TOKEN id="token-13-30" pos="word" morph="none" start_char="1372" end_char="1372">y</TOKEN>
<TOKEN id="token-13-31" pos="word" morph="none" start_char="1374" end_char="1375">no</TOKEN>
<TOKEN id="token-13-32" pos="word" morph="none" start_char="1377" end_char="1378">da</TOKEN>
<TOKEN id="token-13-33" pos="word" morph="none" start_char="1380" end_char="1385">falsos</TOKEN>
<TOKEN id="token-13-34" pos="word" morph="none" start_char="1387" end_char="1395">positivos</TOKEN>
<TOKEN id="token-13-35" pos="word" morph="none" start_char="1397" end_char="1399">por</TOKEN>
<TOKEN id="token-13-36" pos="word" morph="none" start_char="1401" end_char="1409">cualquier</TOKEN>
<TOKEN id="token-13-37" pos="word" morph="none" start_char="1411" end_char="1414">otro</TOKEN>
<TOKEN id="token-13-38" pos="word" morph="none" start_char="1416" end_char="1420">virus</TOKEN>
<TOKEN id="token-13-39" pos="punct" morph="none" start_char="1421" end_char="1421">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1424" end_char="1657">
<ORIGINAL_TEXT>En las publicaciones se afirma que el test PCR "solo detecta una secuencia de 200 letras Génicas [sic] de algún germen X" y que "basta con rascar esta zona, donde hay muchos gérmenes acumulados por el uso de la infecciosa mascarilla".</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1424" end_char="1425">En</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1427" end_char="1429">las</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1431" end_char="1443">publicaciones</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1445" end_char="1446">se</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1448" end_char="1453">afirma</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1455" end_char="1457">que</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1459" end_char="1460">el</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1462" end_char="1465">test</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1467" end_char="1469">PCR</TOKEN>
<TOKEN id="token-14-9" pos="punct" morph="none" start_char="1471" end_char="1471">"</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1472" end_char="1475">solo</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1477" end_char="1483">detecta</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="1485" end_char="1487">una</TOKEN>
<TOKEN id="token-14-13" pos="word" morph="none" start_char="1489" end_char="1497">secuencia</TOKEN>
<TOKEN id="token-14-14" pos="word" morph="none" start_char="1499" end_char="1500">de</TOKEN>
<TOKEN id="token-14-15" pos="word" morph="none" start_char="1502" end_char="1504">200</TOKEN>
<TOKEN id="token-14-16" pos="word" morph="none" start_char="1506" end_char="1511">letras</TOKEN>
<TOKEN id="token-14-17" pos="word" morph="none" start_char="1513" end_char="1519">Génicas</TOKEN>
<TOKEN id="token-14-18" pos="punct" morph="none" start_char="1521" end_char="1521">[</TOKEN>
<TOKEN id="token-14-19" pos="word" morph="none" start_char="1522" end_char="1524">sic</TOKEN>
<TOKEN id="token-14-20" pos="punct" morph="none" start_char="1525" end_char="1525">]</TOKEN>
<TOKEN id="token-14-21" pos="word" morph="none" start_char="1527" end_char="1528">de</TOKEN>
<TOKEN id="token-14-22" pos="word" morph="none" start_char="1530" end_char="1534">algún</TOKEN>
<TOKEN id="token-14-23" pos="word" morph="none" start_char="1536" end_char="1541">germen</TOKEN>
<TOKEN id="token-14-24" pos="word" morph="none" start_char="1543" end_char="1543">X</TOKEN>
<TOKEN id="token-14-25" pos="punct" morph="none" start_char="1544" end_char="1544">"</TOKEN>
<TOKEN id="token-14-26" pos="word" morph="none" start_char="1546" end_char="1546">y</TOKEN>
<TOKEN id="token-14-27" pos="word" morph="none" start_char="1548" end_char="1550">que</TOKEN>
<TOKEN id="token-14-28" pos="punct" morph="none" start_char="1552" end_char="1552">"</TOKEN>
<TOKEN id="token-14-29" pos="word" morph="none" start_char="1553" end_char="1557">basta</TOKEN>
<TOKEN id="token-14-30" pos="word" morph="none" start_char="1559" end_char="1561">con</TOKEN>
<TOKEN id="token-14-31" pos="word" morph="none" start_char="1563" end_char="1568">rascar</TOKEN>
<TOKEN id="token-14-32" pos="word" morph="none" start_char="1570" end_char="1573">esta</TOKEN>
<TOKEN id="token-14-33" pos="word" morph="none" start_char="1575" end_char="1578">zona</TOKEN>
<TOKEN id="token-14-34" pos="punct" morph="none" start_char="1579" end_char="1579">,</TOKEN>
<TOKEN id="token-14-35" pos="word" morph="none" start_char="1581" end_char="1585">donde</TOKEN>
<TOKEN id="token-14-36" pos="word" morph="none" start_char="1587" end_char="1589">hay</TOKEN>
<TOKEN id="token-14-37" pos="word" morph="none" start_char="1591" end_char="1596">muchos</TOKEN>
<TOKEN id="token-14-38" pos="word" morph="none" start_char="1598" end_char="1605">gérmenes</TOKEN>
<TOKEN id="token-14-39" pos="word" morph="none" start_char="1607" end_char="1616">acumulados</TOKEN>
<TOKEN id="token-14-40" pos="word" morph="none" start_char="1618" end_char="1620">por</TOKEN>
<TOKEN id="token-14-41" pos="word" morph="none" start_char="1622" end_char="1623">el</TOKEN>
<TOKEN id="token-14-42" pos="word" morph="none" start_char="1625" end_char="1627">uso</TOKEN>
<TOKEN id="token-14-43" pos="word" morph="none" start_char="1629" end_char="1630">de</TOKEN>
<TOKEN id="token-14-44" pos="word" morph="none" start_char="1632" end_char="1633">la</TOKEN>
<TOKEN id="token-14-45" pos="word" morph="none" start_char="1635" end_char="1644">infecciosa</TOKEN>
<TOKEN id="token-14-46" pos="word" morph="none" start_char="1646" end_char="1655">mascarilla</TOKEN>
<TOKEN id="token-14-47" pos="punct" morph="none" start_char="1656" end_char="1657">".</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1660" end_char="1741">
<ORIGINAL_TEXT>Alcamí sostiene que "el mensaje es erróneo porque la prueba de PCR es específica".</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1660" end_char="1665">Alcamí</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1667" end_char="1674">sostiene</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1676" end_char="1678">que</TOKEN>
<TOKEN id="token-15-3" pos="punct" morph="none" start_char="1680" end_char="1680">"</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1681" end_char="1682">el</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1684" end_char="1690">mensaje</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="1692" end_char="1693">es</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1695" end_char="1701">erróneo</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1703" end_char="1708">porque</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="1710" end_char="1711">la</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1713" end_char="1718">prueba</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="1720" end_char="1721">de</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="1723" end_char="1725">PCR</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="1727" end_char="1728">es</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="1730" end_char="1739">específica</TOKEN>
<TOKEN id="token-15-15" pos="punct" morph="none" start_char="1740" end_char="1741">".</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1743" end_char="1820">
<ORIGINAL_TEXT>Según cuenta, no es cierto que detecte "200 letras génicas de algún germen X".</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1743" end_char="1747">Según</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1749" end_char="1754">cuenta</TOKEN>
<TOKEN id="token-16-2" pos="punct" morph="none" start_char="1755" end_char="1755">,</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1757" end_char="1758">no</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1760" end_char="1761">es</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1763" end_char="1768">cierto</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="1770" end_char="1772">que</TOKEN>
<TOKEN id="token-16-7" pos="word" morph="none" start_char="1774" end_char="1780">detecte</TOKEN>
<TOKEN id="token-16-8" pos="punct" morph="none" start_char="1782" end_char="1782">"</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="1783" end_char="1785">200</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="1787" end_char="1792">letras</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="1794" end_char="1800">génicas</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="1802" end_char="1803">de</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="1805" end_char="1809">algún</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="1811" end_char="1816">germen</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="1818" end_char="1818">X</TOKEN>
<TOKEN id="token-16-16" pos="punct" morph="none" start_char="1819" end_char="1820">".</TOKEN>
</SEG>
<SEG id="segment-17" start_char="1823" end_char="2089">
<ORIGINAL_TEXT>"El método de PCR se caracteriza porque utiliza para amplificar la secuencia genética de un germen cebadores o primers (sustancias necesarias en la reacción en que se basa las PCR) que corresponden a un fragmento del código de ese germen que es único en ese microbio.</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="punct" morph="none" start_char="1823" end_char="1823">"</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="1824" end_char="1825">El</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="1827" end_char="1832">método</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="1834" end_char="1835">de</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="1837" end_char="1839">PCR</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="1841" end_char="1842">se</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="1844" end_char="1854">caracteriza</TOKEN>
<TOKEN id="token-17-7" pos="word" morph="none" start_char="1856" end_char="1861">porque</TOKEN>
<TOKEN id="token-17-8" pos="word" morph="none" start_char="1863" end_char="1869">utiliza</TOKEN>
<TOKEN id="token-17-9" pos="word" morph="none" start_char="1871" end_char="1874">para</TOKEN>
<TOKEN id="token-17-10" pos="word" morph="none" start_char="1876" end_char="1885">amplificar</TOKEN>
<TOKEN id="token-17-11" pos="word" morph="none" start_char="1887" end_char="1888">la</TOKEN>
<TOKEN id="token-17-12" pos="word" morph="none" start_char="1890" end_char="1898">secuencia</TOKEN>
<TOKEN id="token-17-13" pos="word" morph="none" start_char="1900" end_char="1907">genética</TOKEN>
<TOKEN id="token-17-14" pos="word" morph="none" start_char="1909" end_char="1910">de</TOKEN>
<TOKEN id="token-17-15" pos="word" morph="none" start_char="1912" end_char="1913">un</TOKEN>
<TOKEN id="token-17-16" pos="word" morph="none" start_char="1915" end_char="1920">germen</TOKEN>
<TOKEN id="token-17-17" pos="word" morph="none" start_char="1922" end_char="1930">cebadores</TOKEN>
<TOKEN id="token-17-18" pos="word" morph="none" start_char="1932" end_char="1932">o</TOKEN>
<TOKEN id="token-17-19" pos="word" morph="none" start_char="1934" end_char="1940">primers</TOKEN>
<TOKEN id="token-17-20" pos="punct" morph="none" start_char="1942" end_char="1942">(</TOKEN>
<TOKEN id="token-17-21" pos="word" morph="none" start_char="1943" end_char="1952">sustancias</TOKEN>
<TOKEN id="token-17-22" pos="word" morph="none" start_char="1954" end_char="1963">necesarias</TOKEN>
<TOKEN id="token-17-23" pos="word" morph="none" start_char="1965" end_char="1966">en</TOKEN>
<TOKEN id="token-17-24" pos="word" morph="none" start_char="1968" end_char="1969">la</TOKEN>
<TOKEN id="token-17-25" pos="word" morph="none" start_char="1971" end_char="1978">reacción</TOKEN>
<TOKEN id="token-17-26" pos="word" morph="none" start_char="1980" end_char="1981">en</TOKEN>
<TOKEN id="token-17-27" pos="word" morph="none" start_char="1983" end_char="1985">que</TOKEN>
<TOKEN id="token-17-28" pos="word" morph="none" start_char="1987" end_char="1988">se</TOKEN>
<TOKEN id="token-17-29" pos="word" morph="none" start_char="1990" end_char="1993">basa</TOKEN>
<TOKEN id="token-17-30" pos="word" morph="none" start_char="1995" end_char="1997">las</TOKEN>
<TOKEN id="token-17-31" pos="word" morph="none" start_char="1999" end_char="2001">PCR</TOKEN>
<TOKEN id="token-17-32" pos="punct" morph="none" start_char="2002" end_char="2002">)</TOKEN>
<TOKEN id="token-17-33" pos="word" morph="none" start_char="2004" end_char="2006">que</TOKEN>
<TOKEN id="token-17-34" pos="word" morph="none" start_char="2008" end_char="2019">corresponden</TOKEN>
<TOKEN id="token-17-35" pos="word" morph="none" start_char="2021" end_char="2021">a</TOKEN>
<TOKEN id="token-17-36" pos="word" morph="none" start_char="2023" end_char="2024">un</TOKEN>
<TOKEN id="token-17-37" pos="word" morph="none" start_char="2026" end_char="2034">fragmento</TOKEN>
<TOKEN id="token-17-38" pos="word" morph="none" start_char="2036" end_char="2038">del</TOKEN>
<TOKEN id="token-17-39" pos="word" morph="none" start_char="2040" end_char="2045">código</TOKEN>
<TOKEN id="token-17-40" pos="word" morph="none" start_char="2047" end_char="2048">de</TOKEN>
<TOKEN id="token-17-41" pos="word" morph="none" start_char="2050" end_char="2052">ese</TOKEN>
<TOKEN id="token-17-42" pos="word" morph="none" start_char="2054" end_char="2059">germen</TOKEN>
<TOKEN id="token-17-43" pos="word" morph="none" start_char="2061" end_char="2063">que</TOKEN>
<TOKEN id="token-17-44" pos="word" morph="none" start_char="2065" end_char="2066">es</TOKEN>
<TOKEN id="token-17-45" pos="word" morph="none" start_char="2068" end_char="2072">único</TOKEN>
<TOKEN id="token-17-46" pos="word" morph="none" start_char="2074" end_char="2075">en</TOKEN>
<TOKEN id="token-17-47" pos="word" morph="none" start_char="2077" end_char="2079">ese</TOKEN>
<TOKEN id="token-17-48" pos="word" morph="none" start_char="2081" end_char="2088">microbio</TOKEN>
<TOKEN id="token-17-49" pos="punct" morph="none" start_char="2089" end_char="2089">.</TOKEN>
</SEG>
<SEG id="segment-18" start_char="2091" end_char="2232">
<ORIGINAL_TEXT>En el caso del coronavirus, la PCR amplifica sólo si el virus es el SARS-CoV-2, no amplifica el SARS-CoV-1 ni otros coronavirus respiratorios.</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="2091" end_char="2092">En</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="2094" end_char="2095">el</TOKEN>
<TOKEN id="token-18-2" pos="word" morph="none" start_char="2097" end_char="2100">caso</TOKEN>
<TOKEN id="token-18-3" pos="word" morph="none" start_char="2102" end_char="2104">del</TOKEN>
<TOKEN id="token-18-4" pos="word" morph="none" start_char="2106" end_char="2116">coronavirus</TOKEN>
<TOKEN id="token-18-5" pos="punct" morph="none" start_char="2117" end_char="2117">,</TOKEN>
<TOKEN id="token-18-6" pos="word" morph="none" start_char="2119" end_char="2120">la</TOKEN>
<TOKEN id="token-18-7" pos="word" morph="none" start_char="2122" end_char="2124">PCR</TOKEN>
<TOKEN id="token-18-8" pos="word" morph="none" start_char="2126" end_char="2134">amplifica</TOKEN>
<TOKEN id="token-18-9" pos="word" morph="none" start_char="2136" end_char="2139">sólo</TOKEN>
<TOKEN id="token-18-10" pos="word" morph="none" start_char="2141" end_char="2142">si</TOKEN>
<TOKEN id="token-18-11" pos="word" morph="none" start_char="2144" end_char="2145">el</TOKEN>
<TOKEN id="token-18-12" pos="word" morph="none" start_char="2147" end_char="2151">virus</TOKEN>
<TOKEN id="token-18-13" pos="word" morph="none" start_char="2153" end_char="2154">es</TOKEN>
<TOKEN id="token-18-14" pos="word" morph="none" start_char="2156" end_char="2157">el</TOKEN>
<TOKEN id="token-18-15" pos="unknown" morph="none" start_char="2159" end_char="2168">SARS-CoV-2</TOKEN>
<TOKEN id="token-18-16" pos="punct" morph="none" start_char="2169" end_char="2169">,</TOKEN>
<TOKEN id="token-18-17" pos="word" morph="none" start_char="2171" end_char="2172">no</TOKEN>
<TOKEN id="token-18-18" pos="word" morph="none" start_char="2174" end_char="2182">amplifica</TOKEN>
<TOKEN id="token-18-19" pos="word" morph="none" start_char="2184" end_char="2185">el</TOKEN>
<TOKEN id="token-18-20" pos="unknown" morph="none" start_char="2187" end_char="2196">SARS-CoV-1</TOKEN>
<TOKEN id="token-18-21" pos="word" morph="none" start_char="2198" end_char="2199">ni</TOKEN>
<TOKEN id="token-18-22" pos="word" morph="none" start_char="2201" end_char="2205">otros</TOKEN>
<TOKEN id="token-18-23" pos="word" morph="none" start_char="2207" end_char="2217">coronavirus</TOKEN>
<TOKEN id="token-18-24" pos="word" morph="none" start_char="2219" end_char="2231">respiratorios</TOKEN>
<TOKEN id="token-18-25" pos="punct" morph="none" start_char="2232" end_char="2232">.</TOKEN>
</SEG>
<SEG id="segment-19" start_char="2234" end_char="2243">
<ORIGINAL_TEXT>", afirma.</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="punct" morph="none" start_char="2234" end_char="2235">",</TOKEN>
<TOKEN id="token-19-1" pos="word" morph="none" start_char="2237" end_char="2242">afirma</TOKEN>
<TOKEN id="token-19-2" pos="punct" morph="none" start_char="2243" end_char="2243">.</TOKEN>
</SEG>
<SEG id="segment-20" start_char="2245" end_char="2328">
<ORIGINAL_TEXT>Es decir, "se eligen fragmentos ‘únicos’ del virus que no están presentes en otros".</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="2245" end_char="2246">Es</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="2248" end_char="2252">decir</TOKEN>
<TOKEN id="token-20-2" pos="punct" morph="none" start_char="2253" end_char="2253">,</TOKEN>
<TOKEN id="token-20-3" pos="punct" morph="none" start_char="2255" end_char="2255">"</TOKEN>
<TOKEN id="token-20-4" pos="word" morph="none" start_char="2256" end_char="2257">se</TOKEN>
<TOKEN id="token-20-5" pos="word" morph="none" start_char="2259" end_char="2264">eligen</TOKEN>
<TOKEN id="token-20-6" pos="word" morph="none" start_char="2266" end_char="2275">fragmentos</TOKEN>
<TOKEN id="token-20-7" pos="punct" morph="none" start_char="2277" end_char="2277">‘</TOKEN>
<TOKEN id="token-20-8" pos="word" morph="none" start_char="2278" end_char="2283">únicos</TOKEN>
<TOKEN id="token-20-9" pos="punct" morph="none" start_char="2284" end_char="2284">’</TOKEN>
<TOKEN id="token-20-10" pos="word" morph="none" start_char="2286" end_char="2288">del</TOKEN>
<TOKEN id="token-20-11" pos="word" morph="none" start_char="2290" end_char="2294">virus</TOKEN>
<TOKEN id="token-20-12" pos="word" morph="none" start_char="2296" end_char="2298">que</TOKEN>
<TOKEN id="token-20-13" pos="word" morph="none" start_char="2300" end_char="2301">no</TOKEN>
<TOKEN id="token-20-14" pos="word" morph="none" start_char="2303" end_char="2307">están</TOKEN>
<TOKEN id="token-20-15" pos="word" morph="none" start_char="2309" end_char="2317">presentes</TOKEN>
<TOKEN id="token-20-16" pos="word" morph="none" start_char="2319" end_char="2320">en</TOKEN>
<TOKEN id="token-20-17" pos="word" morph="none" start_char="2322" end_char="2326">otros</TOKEN>
<TOKEN id="token-20-18" pos="punct" morph="none" start_char="2327" end_char="2328">".</TOKEN>
</SEG>
<SEG id="segment-21" start_char="2331" end_char="2384">
<ORIGINAL_TEXT>Para explicarlo, pone como ejemplo los relatos cortos.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="2331" end_char="2334">Para</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="2336" end_char="2345">explicarlo</TOKEN>
<TOKEN id="token-21-2" pos="punct" morph="none" start_char="2346" end_char="2346">,</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="2348" end_char="2351">pone</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="2353" end_char="2356">como</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="2358" end_char="2364">ejemplo</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="2366" end_char="2368">los</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="2370" end_char="2376">relatos</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="2378" end_char="2383">cortos</TOKEN>
<TOKEN id="token-21-9" pos="punct" morph="none" start_char="2384" end_char="2384">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="2386" end_char="2494">
<ORIGINAL_TEXT>Cuenta que entre diferentes relatos, sobre todo si están escritos en el mismo idioma, hay letras compartidas.</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="2386" end_char="2391">Cuenta</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="2393" end_char="2395">que</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="2397" end_char="2401">entre</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="2403" end_char="2412">diferentes</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="2414" end_char="2420">relatos</TOKEN>
<TOKEN id="token-22-5" pos="punct" morph="none" start_char="2421" end_char="2421">,</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="2423" end_char="2427">sobre</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="2429" end_char="2432">todo</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="2434" end_char="2435">si</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="2437" end_char="2441">están</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="2443" end_char="2450">escritos</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="2452" end_char="2453">en</TOKEN>
<TOKEN id="token-22-12" pos="word" morph="none" start_char="2455" end_char="2456">el</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="2458" end_char="2462">mismo</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="2464" end_char="2469">idioma</TOKEN>
<TOKEN id="token-22-15" pos="punct" morph="none" start_char="2470" end_char="2470">,</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="2472" end_char="2474">hay</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="2476" end_char="2481">letras</TOKEN>
<TOKEN id="token-22-18" pos="word" morph="none" start_char="2483" end_char="2493">compartidas</TOKEN>
<TOKEN id="token-22-19" pos="punct" morph="none" start_char="2494" end_char="2494">.</TOKEN>
</SEG>
<SEG id="segment-23" start_char="2496" end_char="2576">
<ORIGINAL_TEXT>Pero en todo relato hay una serie de palabras que no están en ningún otro relato.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="2496" end_char="2499">Pero</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="2501" end_char="2502">en</TOKEN>
<TOKEN id="token-23-2" pos="word" morph="none" start_char="2504" end_char="2507">todo</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="2509" end_char="2514">relato</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="2516" end_char="2518">hay</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="2520" end_char="2522">una</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="2524" end_char="2528">serie</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="2530" end_char="2531">de</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="2533" end_char="2540">palabras</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="2542" end_char="2544">que</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="2546" end_char="2547">no</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="2549" end_char="2553">están</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="2555" end_char="2556">en</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="2558" end_char="2563">ningún</TOKEN>
<TOKEN id="token-23-14" pos="word" morph="none" start_char="2565" end_char="2568">otro</TOKEN>
<TOKEN id="token-23-15" pos="word" morph="none" start_char="2570" end_char="2575">relato</TOKEN>
<TOKEN id="token-23-16" pos="punct" morph="none" start_char="2576" end_char="2576">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="2578" end_char="2655">
<ORIGINAL_TEXT>Por ejemplo, afirma que puede aparecer la palabra "canguro" en muchos relatos.</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="2578" end_char="2580">Por</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="2582" end_char="2588">ejemplo</TOKEN>
<TOKEN id="token-24-2" pos="punct" morph="none" start_char="2589" end_char="2589">,</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="2591" end_char="2596">afirma</TOKEN>
<TOKEN id="token-24-4" pos="word" morph="none" start_char="2598" end_char="2600">que</TOKEN>
<TOKEN id="token-24-5" pos="word" morph="none" start_char="2602" end_char="2606">puede</TOKEN>
<TOKEN id="token-24-6" pos="word" morph="none" start_char="2608" end_char="2615">aparecer</TOKEN>
<TOKEN id="token-24-7" pos="word" morph="none" start_char="2617" end_char="2618">la</TOKEN>
<TOKEN id="token-24-8" pos="word" morph="none" start_char="2620" end_char="2626">palabra</TOKEN>
<TOKEN id="token-24-9" pos="punct" morph="none" start_char="2628" end_char="2628">"</TOKEN>
<TOKEN id="token-24-10" pos="word" morph="none" start_char="2629" end_char="2635">canguro</TOKEN>
<TOKEN id="token-24-11" pos="punct" morph="none" start_char="2636" end_char="2636">"</TOKEN>
<TOKEN id="token-24-12" pos="word" morph="none" start_char="2638" end_char="2639">en</TOKEN>
<TOKEN id="token-24-13" pos="word" morph="none" start_char="2641" end_char="2646">muchos</TOKEN>
<TOKEN id="token-24-14" pos="word" morph="none" start_char="2648" end_char="2654">relatos</TOKEN>
<TOKEN id="token-24-15" pos="punct" morph="none" start_char="2655" end_char="2655">.</TOKEN>
</SEG>
<SEG id="segment-25" start_char="2657" end_char="2767">
<ORIGINAL_TEXT>Pero "el canguro se llamaba Willy y lo localicé en la sabana de Melbourne" solo existirá en un relato concreto.</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="2657" end_char="2660">Pero</TOKEN>
<TOKEN id="token-25-1" pos="punct" morph="none" start_char="2662" end_char="2662">"</TOKEN>
<TOKEN id="token-25-2" pos="word" morph="none" start_char="2663" end_char="2664">el</TOKEN>
<TOKEN id="token-25-3" pos="word" morph="none" start_char="2666" end_char="2672">canguro</TOKEN>
<TOKEN id="token-25-4" pos="word" morph="none" start_char="2674" end_char="2675">se</TOKEN>
<TOKEN id="token-25-5" pos="word" morph="none" start_char="2677" end_char="2683">llamaba</TOKEN>
<TOKEN id="token-25-6" pos="word" morph="none" start_char="2685" end_char="2689">Willy</TOKEN>
<TOKEN id="token-25-7" pos="word" morph="none" start_char="2691" end_char="2691">y</TOKEN>
<TOKEN id="token-25-8" pos="word" morph="none" start_char="2693" end_char="2694">lo</TOKEN>
<TOKEN id="token-25-9" pos="word" morph="none" start_char="2696" end_char="2703">localicé</TOKEN>
<TOKEN id="token-25-10" pos="word" morph="none" start_char="2705" end_char="2706">en</TOKEN>
<TOKEN id="token-25-11" pos="word" morph="none" start_char="2708" end_char="2709">la</TOKEN>
<TOKEN id="token-25-12" pos="word" morph="none" start_char="2711" end_char="2716">sabana</TOKEN>
<TOKEN id="token-25-13" pos="word" morph="none" start_char="2718" end_char="2719">de</TOKEN>
<TOKEN id="token-25-14" pos="word" morph="none" start_char="2721" end_char="2729">Melbourne</TOKEN>
<TOKEN id="token-25-15" pos="punct" morph="none" start_char="2730" end_char="2730">"</TOKEN>
<TOKEN id="token-25-16" pos="word" morph="none" start_char="2732" end_char="2735">solo</TOKEN>
<TOKEN id="token-25-17" pos="word" morph="none" start_char="2737" end_char="2744">existirá</TOKEN>
<TOKEN id="token-25-18" pos="word" morph="none" start_char="2746" end_char="2747">en</TOKEN>
<TOKEN id="token-25-19" pos="word" morph="none" start_char="2749" end_char="2750">un</TOKEN>
<TOKEN id="token-25-20" pos="word" morph="none" start_char="2752" end_char="2757">relato</TOKEN>
<TOKEN id="token-25-21" pos="word" morph="none" start_char="2759" end_char="2766">concreto</TOKEN>
<TOKEN id="token-25-22" pos="punct" morph="none" start_char="2767" end_char="2767">.</TOKEN>
</SEG>
<SEG id="segment-26" start_char="2769" end_char="2832">
<ORIGINAL_TEXT>"Si buscamos esa frase entre mil relatos solo aparecerá en ese".</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="punct" morph="none" start_char="2769" end_char="2769">"</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="2770" end_char="2771">Si</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="2773" end_char="2780">buscamos</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="2782" end_char="2784">esa</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="2786" end_char="2790">frase</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="2792" end_char="2796">entre</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="2798" end_char="2800">mil</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="2802" end_char="2808">relatos</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="2810" end_char="2813">solo</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="2815" end_char="2823">aparecerá</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="2825" end_char="2826">en</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="2828" end_char="2830">ese</TOKEN>
<TOKEN id="token-26-12" pos="punct" morph="none" start_char="2831" end_char="2832">".</TOKEN>
</SEG>
<SEG id="segment-27" start_char="2835" end_char="2986">
<ORIGINAL_TEXT>Lo mismo ocurre en este caso: "Lo importante para tener una buena PCR es que seleccione un fragmento ‘original’ del virus que nos interesa diagnosticar.</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="word" morph="none" start_char="2835" end_char="2836">Lo</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="2838" end_char="2842">mismo</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="2844" end_char="2849">ocurre</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="2851" end_char="2852">en</TOKEN>
<TOKEN id="token-27-4" pos="word" morph="none" start_char="2854" end_char="2857">este</TOKEN>
<TOKEN id="token-27-5" pos="word" morph="none" start_char="2859" end_char="2862">caso</TOKEN>
<TOKEN id="token-27-6" pos="punct" morph="none" start_char="2863" end_char="2863">:</TOKEN>
<TOKEN id="token-27-7" pos="punct" morph="none" start_char="2865" end_char="2865">"</TOKEN>
<TOKEN id="token-27-8" pos="word" morph="none" start_char="2866" end_char="2867">Lo</TOKEN>
<TOKEN id="token-27-9" pos="word" morph="none" start_char="2869" end_char="2878">importante</TOKEN>
<TOKEN id="token-27-10" pos="word" morph="none" start_char="2880" end_char="2883">para</TOKEN>
<TOKEN id="token-27-11" pos="word" morph="none" start_char="2885" end_char="2889">tener</TOKEN>
<TOKEN id="token-27-12" pos="word" morph="none" start_char="2891" end_char="2893">una</TOKEN>
<TOKEN id="token-27-13" pos="word" morph="none" start_char="2895" end_char="2899">buena</TOKEN>
<TOKEN id="token-27-14" pos="word" morph="none" start_char="2901" end_char="2903">PCR</TOKEN>
<TOKEN id="token-27-15" pos="word" morph="none" start_char="2905" end_char="2906">es</TOKEN>
<TOKEN id="token-27-16" pos="word" morph="none" start_char="2908" end_char="2910">que</TOKEN>
<TOKEN id="token-27-17" pos="word" morph="none" start_char="2912" end_char="2921">seleccione</TOKEN>
<TOKEN id="token-27-18" pos="word" morph="none" start_char="2923" end_char="2924">un</TOKEN>
<TOKEN id="token-27-19" pos="word" morph="none" start_char="2926" end_char="2934">fragmento</TOKEN>
<TOKEN id="token-27-20" pos="punct" morph="none" start_char="2936" end_char="2936">‘</TOKEN>
<TOKEN id="token-27-21" pos="word" morph="none" start_char="2937" end_char="2944">original</TOKEN>
<TOKEN id="token-27-22" pos="punct" morph="none" start_char="2945" end_char="2945">’</TOKEN>
<TOKEN id="token-27-23" pos="word" morph="none" start_char="2947" end_char="2949">del</TOKEN>
<TOKEN id="token-27-24" pos="word" morph="none" start_char="2951" end_char="2955">virus</TOKEN>
<TOKEN id="token-27-25" pos="word" morph="none" start_char="2957" end_char="2959">que</TOKEN>
<TOKEN id="token-27-26" pos="word" morph="none" start_char="2961" end_char="2963">nos</TOKEN>
<TOKEN id="token-27-27" pos="word" morph="none" start_char="2965" end_char="2972">interesa</TOKEN>
<TOKEN id="token-27-28" pos="word" morph="none" start_char="2974" end_char="2985">diagnosticar</TOKEN>
<TOKEN id="token-27-29" pos="punct" morph="none" start_char="2986" end_char="2986">.</TOKEN>
</SEG>
<SEG id="segment-28" start_char="2988" end_char="3298">
<ORIGINAL_TEXT>Si elegimos un fragmento compartido por otros coronavirus no podremos diferenciarlo, pero los tests de PCR que se utilizan diferencian perfectamente entre distintos coronavirus y más todavía entre virus de familias diferentes o microbios tan distintos y distantes genéticamente como las bacterias o los hongos".</ORIGINAL_TEXT>
<TOKEN id="token-28-0" pos="word" morph="none" start_char="2988" end_char="2989">Si</TOKEN>
<TOKEN id="token-28-1" pos="word" morph="none" start_char="2991" end_char="2998">elegimos</TOKEN>
<TOKEN id="token-28-2" pos="word" morph="none" start_char="3000" end_char="3001">un</TOKEN>
<TOKEN id="token-28-3" pos="word" morph="none" start_char="3003" end_char="3011">fragmento</TOKEN>
<TOKEN id="token-28-4" pos="word" morph="none" start_char="3013" end_char="3022">compartido</TOKEN>
<TOKEN id="token-28-5" pos="word" morph="none" start_char="3024" end_char="3026">por</TOKEN>
<TOKEN id="token-28-6" pos="word" morph="none" start_char="3028" end_char="3032">otros</TOKEN>
<TOKEN id="token-28-7" pos="word" morph="none" start_char="3034" end_char="3044">coronavirus</TOKEN>
<TOKEN id="token-28-8" pos="word" morph="none" start_char="3046" end_char="3047">no</TOKEN>
<TOKEN id="token-28-9" pos="word" morph="none" start_char="3049" end_char="3056">podremos</TOKEN>
<TOKEN id="token-28-10" pos="word" morph="none" start_char="3058" end_char="3070">diferenciarlo</TOKEN>
<TOKEN id="token-28-11" pos="punct" morph="none" start_char="3071" end_char="3071">,</TOKEN>
<TOKEN id="token-28-12" pos="word" morph="none" start_char="3073" end_char="3076">pero</TOKEN>
<TOKEN id="token-28-13" pos="word" morph="none" start_char="3078" end_char="3080">los</TOKEN>
<TOKEN id="token-28-14" pos="word" morph="none" start_char="3082" end_char="3086">tests</TOKEN>
<TOKEN id="token-28-15" pos="word" morph="none" start_char="3088" end_char="3089">de</TOKEN>
<TOKEN id="token-28-16" pos="word" morph="none" start_char="3091" end_char="3093">PCR</TOKEN>
<TOKEN id="token-28-17" pos="word" morph="none" start_char="3095" end_char="3097">que</TOKEN>
<TOKEN id="token-28-18" pos="word" morph="none" start_char="3099" end_char="3100">se</TOKEN>
<TOKEN id="token-28-19" pos="word" morph="none" start_char="3102" end_char="3109">utilizan</TOKEN>
<TOKEN id="token-28-20" pos="word" morph="none" start_char="3111" end_char="3121">diferencian</TOKEN>
<TOKEN id="token-28-21" pos="word" morph="none" start_char="3123" end_char="3135">perfectamente</TOKEN>
<TOKEN id="token-28-22" pos="word" morph="none" start_char="3137" end_char="3141">entre</TOKEN>
<TOKEN id="token-28-23" pos="word" morph="none" start_char="3143" end_char="3151">distintos</TOKEN>
<TOKEN id="token-28-24" pos="word" morph="none" start_char="3153" end_char="3163">coronavirus</TOKEN>
<TOKEN id="token-28-25" pos="word" morph="none" start_char="3165" end_char="3165">y</TOKEN>
<TOKEN id="token-28-26" pos="word" morph="none" start_char="3167" end_char="3169">más</TOKEN>
<TOKEN id="token-28-27" pos="word" morph="none" start_char="3171" end_char="3177">todavía</TOKEN>
<TOKEN id="token-28-28" pos="word" morph="none" start_char="3179" end_char="3183">entre</TOKEN>
<TOKEN id="token-28-29" pos="word" morph="none" start_char="3185" end_char="3189">virus</TOKEN>
<TOKEN id="token-28-30" pos="word" morph="none" start_char="3191" end_char="3192">de</TOKEN>
<TOKEN id="token-28-31" pos="word" morph="none" start_char="3194" end_char="3201">familias</TOKEN>
<TOKEN id="token-28-32" pos="word" morph="none" start_char="3203" end_char="3212">diferentes</TOKEN>
<TOKEN id="token-28-33" pos="word" morph="none" start_char="3214" end_char="3214">o</TOKEN>
<TOKEN id="token-28-34" pos="word" morph="none" start_char="3216" end_char="3224">microbios</TOKEN>
<TOKEN id="token-28-35" pos="word" morph="none" start_char="3226" end_char="3228">tan</TOKEN>
<TOKEN id="token-28-36" pos="word" morph="none" start_char="3230" end_char="3238">distintos</TOKEN>
<TOKEN id="token-28-37" pos="word" morph="none" start_char="3240" end_char="3240">y</TOKEN>
<TOKEN id="token-28-38" pos="word" morph="none" start_char="3242" end_char="3250">distantes</TOKEN>
<TOKEN id="token-28-39" pos="word" morph="none" start_char="3252" end_char="3264">genéticamente</TOKEN>
<TOKEN id="token-28-40" pos="word" morph="none" start_char="3266" end_char="3269">como</TOKEN>
<TOKEN id="token-28-41" pos="word" morph="none" start_char="3271" end_char="3273">las</TOKEN>
<TOKEN id="token-28-42" pos="word" morph="none" start_char="3275" end_char="3283">bacterias</TOKEN>
<TOKEN id="token-28-43" pos="word" morph="none" start_char="3285" end_char="3285">o</TOKEN>
<TOKEN id="token-28-44" pos="word" morph="none" start_char="3287" end_char="3289">los</TOKEN>
<TOKEN id="token-28-45" pos="word" morph="none" start_char="3291" end_char="3296">hongos</TOKEN>
<TOKEN id="token-28-46" pos="punct" morph="none" start_char="3297" end_char="3298">".</TOKEN>
</SEG>
<SEG id="segment-29" start_char="3300" end_char="3321">
<ORIGINAL_TEXT>Aquí os lo explicamos.</ORIGINAL_TEXT>
<TOKEN id="token-29-0" pos="word" morph="none" start_char="3300" end_char="3303">Aquí</TOKEN>
<TOKEN id="token-29-1" pos="word" morph="none" start_char="3305" end_char="3306">os</TOKEN>
<TOKEN id="token-29-2" pos="word" morph="none" start_char="3308" end_char="3309">lo</TOKEN>
<TOKEN id="token-29-3" pos="word" morph="none" start_char="3311" end_char="3320">explicamos</TOKEN>
<TOKEN id="token-29-4" pos="punct" morph="none" start_char="3321" end_char="3321">.</TOKEN>
</SEG>
<SEG id="segment-30" start_char="3324" end_char="3438">
<ORIGINAL_TEXT>Los gérmenes que exhalamos ya están en nuestro tracto respiratorio y normalmente no suponen un riesgo para la salud</ORIGINAL_TEXT>
<TOKEN id="token-30-0" pos="word" morph="none" start_char="3324" end_char="3326">Los</TOKEN>
<TOKEN id="token-30-1" pos="word" morph="none" start_char="3328" end_char="3335">gérmenes</TOKEN>
<TOKEN id="token-30-2" pos="word" morph="none" start_char="3337" end_char="3339">que</TOKEN>
<TOKEN id="token-30-3" pos="word" morph="none" start_char="3341" end_char="3349">exhalamos</TOKEN>
<TOKEN id="token-30-4" pos="word" morph="none" start_char="3351" end_char="3352">ya</TOKEN>
<TOKEN id="token-30-5" pos="word" morph="none" start_char="3354" end_char="3358">están</TOKEN>
<TOKEN id="token-30-6" pos="word" morph="none" start_char="3360" end_char="3361">en</TOKEN>
<TOKEN id="token-30-7" pos="word" morph="none" start_char="3363" end_char="3369">nuestro</TOKEN>
<TOKEN id="token-30-8" pos="word" morph="none" start_char="3371" end_char="3376">tracto</TOKEN>
<TOKEN id="token-30-9" pos="word" morph="none" start_char="3378" end_char="3389">respiratorio</TOKEN>
<TOKEN id="token-30-10" pos="word" morph="none" start_char="3391" end_char="3391">y</TOKEN>
<TOKEN id="token-30-11" pos="word" morph="none" start_char="3393" end_char="3403">normalmente</TOKEN>
<TOKEN id="token-30-12" pos="word" morph="none" start_char="3405" end_char="3406">no</TOKEN>
<TOKEN id="token-30-13" pos="word" morph="none" start_char="3408" end_char="3414">suponen</TOKEN>
<TOKEN id="token-30-14" pos="word" morph="none" start_char="3416" end_char="3417">un</TOKEN>
<TOKEN id="token-30-15" pos="word" morph="none" start_char="3419" end_char="3424">riesgo</TOKEN>
<TOKEN id="token-30-16" pos="word" morph="none" start_char="3426" end_char="3429">para</TOKEN>
<TOKEN id="token-30-17" pos="word" morph="none" start_char="3431" end_char="3432">la</TOKEN>
<TOKEN id="token-30-18" pos="word" morph="none" start_char="3434" end_char="3438">salud</TOKEN>
</SEG>
<SEG id="segment-31" start_char="3442" end_char="3493">
<ORIGINAL_TEXT>¿Qué ocurre exactamente cuando usamos la mascarilla?</ORIGINAL_TEXT>
<TOKEN id="token-31-0" pos="punct" morph="none" start_char="3442" end_char="3442">¿</TOKEN>
<TOKEN id="token-31-1" pos="word" morph="none" start_char="3443" end_char="3445">Qué</TOKEN>
<TOKEN id="token-31-2" pos="word" morph="none" start_char="3447" end_char="3452">ocurre</TOKEN>
<TOKEN id="token-31-3" pos="word" morph="none" start_char="3454" end_char="3464">exactamente</TOKEN>
<TOKEN id="token-31-4" pos="word" morph="none" start_char="3466" end_char="3471">cuando</TOKEN>
<TOKEN id="token-31-5" pos="word" morph="none" start_char="3473" end_char="3478">usamos</TOKEN>
<TOKEN id="token-31-6" pos="word" morph="none" start_char="3480" end_char="3481">la</TOKEN>
<TOKEN id="token-31-7" pos="word" morph="none" start_char="3483" end_char="3492">mascarilla</TOKEN>
<TOKEN id="token-31-8" pos="punct" morph="none" start_char="3493" end_char="3493">?</TOKEN>
</SEG>
<SEG id="segment-32" start_char="3495" end_char="3623">
<ORIGINAL_TEXT>Magdalena Martínez Cañamero, microbióloga de la Universidad de Jaén y miembro de la Sociedad Española de Microbiología, explica a</ORIGINAL_TEXT>
<TOKEN id="token-32-0" pos="word" morph="none" start_char="3495" end_char="3503">Magdalena</TOKEN>
<TOKEN id="token-32-1" pos="word" morph="none" start_char="3505" end_char="3512">Martínez</TOKEN>
<TOKEN id="token-32-2" pos="word" morph="none" start_char="3514" end_char="3521">Cañamero</TOKEN>
<TOKEN id="token-32-3" pos="punct" morph="none" start_char="3522" end_char="3522">,</TOKEN>
<TOKEN id="token-32-4" pos="word" morph="none" start_char="3524" end_char="3535">microbióloga</TOKEN>
<TOKEN id="token-32-5" pos="word" morph="none" start_char="3537" end_char="3538">de</TOKEN>
<TOKEN id="token-32-6" pos="word" morph="none" start_char="3540" end_char="3541">la</TOKEN>
<TOKEN id="token-32-7" pos="word" morph="none" start_char="3543" end_char="3553">Universidad</TOKEN>
<TOKEN id="token-32-8" pos="word" morph="none" start_char="3555" end_char="3556">de</TOKEN>
<TOKEN id="token-32-9" pos="word" morph="none" start_char="3558" end_char="3561">Jaén</TOKEN>
<TOKEN id="token-32-10" pos="word" morph="none" start_char="3563" end_char="3563">y</TOKEN>
<TOKEN id="token-32-11" pos="word" morph="none" start_char="3565" end_char="3571">miembro</TOKEN>
<TOKEN id="token-32-12" pos="word" morph="none" start_char="3573" end_char="3574">de</TOKEN>
<TOKEN id="token-32-13" pos="word" morph="none" start_char="3576" end_char="3577">la</TOKEN>
<TOKEN id="token-32-14" pos="word" morph="none" start_char="3579" end_char="3586">Sociedad</TOKEN>
<TOKEN id="token-32-15" pos="word" morph="none" start_char="3588" end_char="3595">Española</TOKEN>
<TOKEN id="token-32-16" pos="word" morph="none" start_char="3597" end_char="3598">de</TOKEN>
<TOKEN id="token-32-17" pos="word" morph="none" start_char="3600" end_char="3612">Microbiología</TOKEN>
<TOKEN id="token-32-18" pos="punct" morph="none" start_char="3613" end_char="3613">,</TOKEN>
<TOKEN id="token-32-19" pos="word" morph="none" start_char="3615" end_char="3621">explica</TOKEN>
<TOKEN id="token-32-20" pos="word" morph="none" start_char="3623" end_char="3623">a</TOKEN>
</SEG>
<SEG id="segment-33" start_char="3626" end_char="3640">
<ORIGINAL_TEXT>Maldita Ciencia</ORIGINAL_TEXT>
<TOKEN id="token-33-0" pos="word" morph="none" start_char="3626" end_char="3632">Maldita</TOKEN>
<TOKEN id="token-33-1" pos="word" morph="none" start_char="3634" end_char="3640">Ciencia</TOKEN>
</SEG>
<SEG id="segment-34" start_char="3643" end_char="3734">
<ORIGINAL_TEXT>que principalmente se acumulan bacterias nuestras de la piel y de la boca que "son inocuas".</ORIGINAL_TEXT>
<TOKEN id="token-34-0" pos="word" morph="none" start_char="3643" end_char="3645">que</TOKEN>
<TOKEN id="token-34-1" pos="word" morph="none" start_char="3647" end_char="3660">principalmente</TOKEN>
<TOKEN id="token-34-2" pos="word" morph="none" start_char="3662" end_char="3663">se</TOKEN>
<TOKEN id="token-34-3" pos="word" morph="none" start_char="3665" end_char="3672">acumulan</TOKEN>
<TOKEN id="token-34-4" pos="word" morph="none" start_char="3674" end_char="3682">bacterias</TOKEN>
<TOKEN id="token-34-5" pos="word" morph="none" start_char="3684" end_char="3691">nuestras</TOKEN>
<TOKEN id="token-34-6" pos="word" morph="none" start_char="3693" end_char="3694">de</TOKEN>
<TOKEN id="token-34-7" pos="word" morph="none" start_char="3696" end_char="3697">la</TOKEN>
<TOKEN id="token-34-8" pos="word" morph="none" start_char="3699" end_char="3702">piel</TOKEN>
<TOKEN id="token-34-9" pos="word" morph="none" start_char="3704" end_char="3704">y</TOKEN>
<TOKEN id="token-34-10" pos="word" morph="none" start_char="3706" end_char="3707">de</TOKEN>
<TOKEN id="token-34-11" pos="word" morph="none" start_char="3709" end_char="3710">la</TOKEN>
<TOKEN id="token-34-12" pos="word" morph="none" start_char="3712" end_char="3715">boca</TOKEN>
<TOKEN id="token-34-13" pos="word" morph="none" start_char="3717" end_char="3719">que</TOKEN>
<TOKEN id="token-34-14" pos="punct" morph="none" start_char="3721" end_char="3721">"</TOKEN>
<TOKEN id="token-34-15" pos="word" morph="none" start_char="3722" end_char="3724">son</TOKEN>
<TOKEN id="token-34-16" pos="word" morph="none" start_char="3726" end_char="3732">inocuas</TOKEN>
<TOKEN id="token-34-17" pos="punct" morph="none" start_char="3733" end_char="3734">".</TOKEN>
</SEG>
<SEG id="segment-35" start_char="3736" end_char="3856">
<ORIGINAL_TEXT>"Una mascarilla reutilizada pierde la capacidad filtrante, ese es su peor peligro para el que la usa (y para los demás)".</ORIGINAL_TEXT>
<TOKEN id="token-35-0" pos="punct" morph="none" start_char="3736" end_char="3736">"</TOKEN>
<TOKEN id="token-35-1" pos="word" morph="none" start_char="3737" end_char="3739">Una</TOKEN>
<TOKEN id="token-35-2" pos="word" morph="none" start_char="3741" end_char="3750">mascarilla</TOKEN>
<TOKEN id="token-35-3" pos="word" morph="none" start_char="3752" end_char="3762">reutilizada</TOKEN>
<TOKEN id="token-35-4" pos="word" morph="none" start_char="3764" end_char="3769">pierde</TOKEN>
<TOKEN id="token-35-5" pos="word" morph="none" start_char="3771" end_char="3772">la</TOKEN>
<TOKEN id="token-35-6" pos="word" morph="none" start_char="3774" end_char="3782">capacidad</TOKEN>
<TOKEN id="token-35-7" pos="word" morph="none" start_char="3784" end_char="3792">filtrante</TOKEN>
<TOKEN id="token-35-8" pos="punct" morph="none" start_char="3793" end_char="3793">,</TOKEN>
<TOKEN id="token-35-9" pos="word" morph="none" start_char="3795" end_char="3797">ese</TOKEN>
<TOKEN id="token-35-10" pos="word" morph="none" start_char="3799" end_char="3800">es</TOKEN>
<TOKEN id="token-35-11" pos="word" morph="none" start_char="3802" end_char="3803">su</TOKEN>
<TOKEN id="token-35-12" pos="word" morph="none" start_char="3805" end_char="3808">peor</TOKEN>
<TOKEN id="token-35-13" pos="word" morph="none" start_char="3810" end_char="3816">peligro</TOKEN>
<TOKEN id="token-35-14" pos="word" morph="none" start_char="3818" end_char="3821">para</TOKEN>
<TOKEN id="token-35-15" pos="word" morph="none" start_char="3823" end_char="3824">el</TOKEN>
<TOKEN id="token-35-16" pos="word" morph="none" start_char="3826" end_char="3828">que</TOKEN>
<TOKEN id="token-35-17" pos="word" morph="none" start_char="3830" end_char="3831">la</TOKEN>
<TOKEN id="token-35-18" pos="word" morph="none" start_char="3833" end_char="3835">usa</TOKEN>
<TOKEN id="token-35-19" pos="punct" morph="none" start_char="3837" end_char="3837">(</TOKEN>
<TOKEN id="token-35-20" pos="word" morph="none" start_char="3838" end_char="3838">y</TOKEN>
<TOKEN id="token-35-21" pos="word" morph="none" start_char="3840" end_char="3843">para</TOKEN>
<TOKEN id="token-35-22" pos="word" morph="none" start_char="3845" end_char="3847">los</TOKEN>
<TOKEN id="token-35-23" pos="word" morph="none" start_char="3849" end_char="3853">demás</TOKEN>
<TOKEN id="token-35-24" pos="punct" morph="none" start_char="3854" end_char="3856">)".</TOKEN>
</SEG>
<SEG id="segment-36" start_char="3859" end_char="3992">
<ORIGINAL_TEXT>Sí que se puede correr el riesgo de contagiarse y padecer la COVID-19 si no se usa la mascarilla de manera adecuada, según la experta.</ORIGINAL_TEXT>
<TOKEN id="token-36-0" pos="word" morph="none" start_char="3859" end_char="3860">Sí</TOKEN>
<TOKEN id="token-36-1" pos="word" morph="none" start_char="3862" end_char="3864">que</TOKEN>
<TOKEN id="token-36-2" pos="word" morph="none" start_char="3866" end_char="3867">se</TOKEN>
<TOKEN id="token-36-3" pos="word" morph="none" start_char="3869" end_char="3873">puede</TOKEN>
<TOKEN id="token-36-4" pos="word" morph="none" start_char="3875" end_char="3880">correr</TOKEN>
<TOKEN id="token-36-5" pos="word" morph="none" start_char="3882" end_char="3883">el</TOKEN>
<TOKEN id="token-36-6" pos="word" morph="none" start_char="3885" end_char="3890">riesgo</TOKEN>
<TOKEN id="token-36-7" pos="word" morph="none" start_char="3892" end_char="3893">de</TOKEN>
<TOKEN id="token-36-8" pos="word" morph="none" start_char="3895" end_char="3905">contagiarse</TOKEN>
<TOKEN id="token-36-9" pos="word" morph="none" start_char="3907" end_char="3907">y</TOKEN>
<TOKEN id="token-36-10" pos="word" morph="none" start_char="3909" end_char="3915">padecer</TOKEN>
<TOKEN id="token-36-11" pos="word" morph="none" start_char="3917" end_char="3918">la</TOKEN>
<TOKEN id="token-36-12" pos="unknown" morph="none" start_char="3920" end_char="3927">COVID-19</TOKEN>
<TOKEN id="token-36-13" pos="word" morph="none" start_char="3929" end_char="3930">si</TOKEN>
<TOKEN id="token-36-14" pos="word" morph="none" start_char="3932" end_char="3933">no</TOKEN>
<TOKEN id="token-36-15" pos="word" morph="none" start_char="3935" end_char="3936">se</TOKEN>
<TOKEN id="token-36-16" pos="word" morph="none" start_char="3938" end_char="3940">usa</TOKEN>
<TOKEN id="token-36-17" pos="word" morph="none" start_char="3942" end_char="3943">la</TOKEN>
<TOKEN id="token-36-18" pos="word" morph="none" start_char="3945" end_char="3954">mascarilla</TOKEN>
<TOKEN id="token-36-19" pos="word" morph="none" start_char="3956" end_char="3957">de</TOKEN>
<TOKEN id="token-36-20" pos="word" morph="none" start_char="3959" end_char="3964">manera</TOKEN>
<TOKEN id="token-36-21" pos="word" morph="none" start_char="3966" end_char="3973">adecuada</TOKEN>
<TOKEN id="token-36-22" pos="punct" morph="none" start_char="3974" end_char="3974">,</TOKEN>
<TOKEN id="token-36-23" pos="word" morph="none" start_char="3976" end_char="3980">según</TOKEN>
<TOKEN id="token-36-24" pos="word" morph="none" start_char="3982" end_char="3983">la</TOKEN>
<TOKEN id="token-36-25" pos="word" morph="none" start_char="3985" end_char="3991">experta</TOKEN>
<TOKEN id="token-36-26" pos="punct" morph="none" start_char="3992" end_char="3992">.</TOKEN>
</SEG>
<SEG id="segment-37" start_char="3994" end_char="4039">
<ORIGINAL_TEXT>Por ejemplo, si la tocamos o la guardamos mal.</ORIGINAL_TEXT>
<TOKEN id="token-37-0" pos="word" morph="none" start_char="3994" end_char="3996">Por</TOKEN>
<TOKEN id="token-37-1" pos="word" morph="none" start_char="3998" end_char="4004">ejemplo</TOKEN>
<TOKEN id="token-37-2" pos="punct" morph="none" start_char="4005" end_char="4005">,</TOKEN>
<TOKEN id="token-37-3" pos="word" morph="none" start_char="4007" end_char="4008">si</TOKEN>
<TOKEN id="token-37-4" pos="word" morph="none" start_char="4010" end_char="4011">la</TOKEN>
<TOKEN id="token-37-5" pos="word" morph="none" start_char="4013" end_char="4019">tocamos</TOKEN>
<TOKEN id="token-37-6" pos="word" morph="none" start_char="4021" end_char="4021">o</TOKEN>
<TOKEN id="token-37-7" pos="word" morph="none" start_char="4023" end_char="4024">la</TOKEN>
<TOKEN id="token-37-8" pos="word" morph="none" start_char="4026" end_char="4034">guardamos</TOKEN>
<TOKEN id="token-37-9" pos="word" morph="none" start_char="4036" end_char="4038">mal</TOKEN>
<TOKEN id="token-37-10" pos="punct" morph="none" start_char="4039" end_char="4039">.</TOKEN>
</SEG>
<SEG id="segment-38" start_char="4041" end_char="4173">
<ORIGINAL_TEXT>Aquí ya os explicamos cuál es la manera idónea de guardar la mascarilla (en un sobre de papel, una tela o un estuche de gafas viejo).</ORIGINAL_TEXT>
<TOKEN id="token-38-0" pos="word" morph="none" start_char="4041" end_char="4044">Aquí</TOKEN>
<TOKEN id="token-38-1" pos="word" morph="none" start_char="4046" end_char="4047">ya</TOKEN>
<TOKEN id="token-38-2" pos="word" morph="none" start_char="4049" end_char="4050">os</TOKEN>
<TOKEN id="token-38-3" pos="word" morph="none" start_char="4052" end_char="4061">explicamos</TOKEN>
<TOKEN id="token-38-4" pos="word" morph="none" start_char="4063" end_char="4066">cuál</TOKEN>
<TOKEN id="token-38-5" pos="word" morph="none" start_char="4068" end_char="4069">es</TOKEN>
<TOKEN id="token-38-6" pos="word" morph="none" start_char="4071" end_char="4072">la</TOKEN>
<TOKEN id="token-38-7" pos="word" morph="none" start_char="4074" end_char="4079">manera</TOKEN>
<TOKEN id="token-38-8" pos="word" morph="none" start_char="4081" end_char="4086">idónea</TOKEN>
<TOKEN id="token-38-9" pos="word" morph="none" start_char="4088" end_char="4089">de</TOKEN>
<TOKEN id="token-38-10" pos="word" morph="none" start_char="4091" end_char="4097">guardar</TOKEN>
<TOKEN id="token-38-11" pos="word" morph="none" start_char="4099" end_char="4100">la</TOKEN>
<TOKEN id="token-38-12" pos="word" morph="none" start_char="4102" end_char="4111">mascarilla</TOKEN>
<TOKEN id="token-38-13" pos="punct" morph="none" start_char="4113" end_char="4113">(</TOKEN>
<TOKEN id="token-38-14" pos="word" morph="none" start_char="4114" end_char="4115">en</TOKEN>
<TOKEN id="token-38-15" pos="word" morph="none" start_char="4117" end_char="4118">un</TOKEN>
<TOKEN id="token-38-16" pos="word" morph="none" start_char="4120" end_char="4124">sobre</TOKEN>
<TOKEN id="token-38-17" pos="word" morph="none" start_char="4126" end_char="4127">de</TOKEN>
<TOKEN id="token-38-18" pos="word" morph="none" start_char="4129" end_char="4133">papel</TOKEN>
<TOKEN id="token-38-19" pos="punct" morph="none" start_char="4134" end_char="4134">,</TOKEN>
<TOKEN id="token-38-20" pos="word" morph="none" start_char="4136" end_char="4138">una</TOKEN>
<TOKEN id="token-38-21" pos="word" morph="none" start_char="4140" end_char="4143">tela</TOKEN>
<TOKEN id="token-38-22" pos="word" morph="none" start_char="4145" end_char="4145">o</TOKEN>
<TOKEN id="token-38-23" pos="word" morph="none" start_char="4147" end_char="4148">un</TOKEN>
<TOKEN id="token-38-24" pos="word" morph="none" start_char="4150" end_char="4156">estuche</TOKEN>
<TOKEN id="token-38-25" pos="word" morph="none" start_char="4158" end_char="4159">de</TOKEN>
<TOKEN id="token-38-26" pos="word" morph="none" start_char="4161" end_char="4165">gafas</TOKEN>
<TOKEN id="token-38-27" pos="word" morph="none" start_char="4167" end_char="4171">viejo</TOKEN>
<TOKEN id="token-38-28" pos="punct" morph="none" start_char="4172" end_char="4173">).</TOKEN>
</SEG>
<SEG id="segment-39" start_char="4176" end_char="4394">
<ORIGINAL_TEXT>En la misma línea se posiciona Víctor Jiménez Cid, catedrático de Microbiología de la Universidad Complutense de Madrid y miembro de la Sociedad Española de Microbiología, que compara la mascarilla con la ropa interior.</ORIGINAL_TEXT>
<TOKEN id="token-39-0" pos="word" morph="none" start_char="4176" end_char="4177">En</TOKEN>
<TOKEN id="token-39-1" pos="word" morph="none" start_char="4179" end_char="4180">la</TOKEN>
<TOKEN id="token-39-2" pos="word" morph="none" start_char="4182" end_char="4186">misma</TOKEN>
<TOKEN id="token-39-3" pos="word" morph="none" start_char="4188" end_char="4192">línea</TOKEN>
<TOKEN id="token-39-4" pos="word" morph="none" start_char="4194" end_char="4195">se</TOKEN>
<TOKEN id="token-39-5" pos="word" morph="none" start_char="4197" end_char="4205">posiciona</TOKEN>
<TOKEN id="token-39-6" pos="word" morph="none" start_char="4207" end_char="4212">Víctor</TOKEN>
<TOKEN id="token-39-7" pos="word" morph="none" start_char="4214" end_char="4220">Jiménez</TOKEN>
<TOKEN id="token-39-8" pos="word" morph="none" start_char="4222" end_char="4224">Cid</TOKEN>
<TOKEN id="token-39-9" pos="punct" morph="none" start_char="4225" end_char="4225">,</TOKEN>
<TOKEN id="token-39-10" pos="word" morph="none" start_char="4227" end_char="4237">catedrático</TOKEN>
<TOKEN id="token-39-11" pos="word" morph="none" start_char="4239" end_char="4240">de</TOKEN>
<TOKEN id="token-39-12" pos="word" morph="none" start_char="4242" end_char="4254">Microbiología</TOKEN>
<TOKEN id="token-39-13" pos="word" morph="none" start_char="4256" end_char="4257">de</TOKEN>
<TOKEN id="token-39-14" pos="word" morph="none" start_char="4259" end_char="4260">la</TOKEN>
<TOKEN id="token-39-15" pos="word" morph="none" start_char="4262" end_char="4272">Universidad</TOKEN>
<TOKEN id="token-39-16" pos="word" morph="none" start_char="4274" end_char="4284">Complutense</TOKEN>
<TOKEN id="token-39-17" pos="word" morph="none" start_char="4286" end_char="4287">de</TOKEN>
<TOKEN id="token-39-18" pos="word" morph="none" start_char="4289" end_char="4294">Madrid</TOKEN>
<TOKEN id="token-39-19" pos="word" morph="none" start_char="4296" end_char="4296">y</TOKEN>
<TOKEN id="token-39-20" pos="word" morph="none" start_char="4298" end_char="4304">miembro</TOKEN>
<TOKEN id="token-39-21" pos="word" morph="none" start_char="4306" end_char="4307">de</TOKEN>
<TOKEN id="token-39-22" pos="word" morph="none" start_char="4309" end_char="4310">la</TOKEN>
<TOKEN id="token-39-23" pos="word" morph="none" start_char="4312" end_char="4319">Sociedad</TOKEN>
<TOKEN id="token-39-24" pos="word" morph="none" start_char="4321" end_char="4328">Española</TOKEN>
<TOKEN id="token-39-25" pos="word" morph="none" start_char="4330" end_char="4331">de</TOKEN>
<TOKEN id="token-39-26" pos="word" morph="none" start_char="4333" end_char="4345">Microbiología</TOKEN>
<TOKEN id="token-39-27" pos="punct" morph="none" start_char="4346" end_char="4346">,</TOKEN>
<TOKEN id="token-39-28" pos="word" morph="none" start_char="4348" end_char="4350">que</TOKEN>
<TOKEN id="token-39-29" pos="word" morph="none" start_char="4352" end_char="4358">compara</TOKEN>
<TOKEN id="token-39-30" pos="word" morph="none" start_char="4360" end_char="4361">la</TOKEN>
<TOKEN id="token-39-31" pos="word" morph="none" start_char="4363" end_char="4372">mascarilla</TOKEN>
<TOKEN id="token-39-32" pos="word" morph="none" start_char="4374" end_char="4376">con</TOKEN>
<TOKEN id="token-39-33" pos="word" morph="none" start_char="4378" end_char="4379">la</TOKEN>
<TOKEN id="token-39-34" pos="word" morph="none" start_char="4381" end_char="4384">ropa</TOKEN>
<TOKEN id="token-39-35" pos="word" morph="none" start_char="4386" end_char="4393">interior</TOKEN>
<TOKEN id="token-39-36" pos="punct" morph="none" start_char="4394" end_char="4394">.</TOKEN>
</SEG>
<SEG id="segment-40" start_char="4396" end_char="4579">
<ORIGINAL_TEXT>"En su cara interna va a acumular tras su uso todo tipo de microorganismos procedentes de la microbiota de la piel y mucosas y, por la cara externa, microorganismos del medio ambiente.</ORIGINAL_TEXT>
<TOKEN id="token-40-0" pos="punct" morph="none" start_char="4396" end_char="4396">"</TOKEN>
<TOKEN id="token-40-1" pos="word" morph="none" start_char="4397" end_char="4398">En</TOKEN>
<TOKEN id="token-40-2" pos="word" morph="none" start_char="4400" end_char="4401">su</TOKEN>
<TOKEN id="token-40-3" pos="word" morph="none" start_char="4403" end_char="4406">cara</TOKEN>
<TOKEN id="token-40-4" pos="word" morph="none" start_char="4408" end_char="4414">interna</TOKEN>
<TOKEN id="token-40-5" pos="word" morph="none" start_char="4416" end_char="4417">va</TOKEN>
<TOKEN id="token-40-6" pos="word" morph="none" start_char="4419" end_char="4419">a</TOKEN>
<TOKEN id="token-40-7" pos="word" morph="none" start_char="4421" end_char="4428">acumular</TOKEN>
<TOKEN id="token-40-8" pos="word" morph="none" start_char="4430" end_char="4433">tras</TOKEN>
<TOKEN id="token-40-9" pos="word" morph="none" start_char="4435" end_char="4436">su</TOKEN>
<TOKEN id="token-40-10" pos="word" morph="none" start_char="4438" end_char="4440">uso</TOKEN>
<TOKEN id="token-40-11" pos="word" morph="none" start_char="4442" end_char="4445">todo</TOKEN>
<TOKEN id="token-40-12" pos="word" morph="none" start_char="4447" end_char="4450">tipo</TOKEN>
<TOKEN id="token-40-13" pos="word" morph="none" start_char="4452" end_char="4453">de</TOKEN>
<TOKEN id="token-40-14" pos="word" morph="none" start_char="4455" end_char="4469">microorganismos</TOKEN>
<TOKEN id="token-40-15" pos="word" morph="none" start_char="4471" end_char="4481">procedentes</TOKEN>
<TOKEN id="token-40-16" pos="word" morph="none" start_char="4483" end_char="4484">de</TOKEN>
<TOKEN id="token-40-17" pos="word" morph="none" start_char="4486" end_char="4487">la</TOKEN>
<TOKEN id="token-40-18" pos="word" morph="none" start_char="4489" end_char="4498">microbiota</TOKEN>
<TOKEN id="token-40-19" pos="word" morph="none" start_char="4500" end_char="4501">de</TOKEN>
<TOKEN id="token-40-20" pos="word" morph="none" start_char="4503" end_char="4504">la</TOKEN>
<TOKEN id="token-40-21" pos="word" morph="none" start_char="4506" end_char="4509">piel</TOKEN>
<TOKEN id="token-40-22" pos="word" morph="none" start_char="4511" end_char="4511">y</TOKEN>
<TOKEN id="token-40-23" pos="word" morph="none" start_char="4513" end_char="4519">mucosas</TOKEN>
<TOKEN id="token-40-24" pos="word" morph="none" start_char="4521" end_char="4521">y</TOKEN>
<TOKEN id="token-40-25" pos="punct" morph="none" start_char="4522" end_char="4522">,</TOKEN>
<TOKEN id="token-40-26" pos="word" morph="none" start_char="4524" end_char="4526">por</TOKEN>
<TOKEN id="token-40-27" pos="word" morph="none" start_char="4528" end_char="4529">la</TOKEN>
<TOKEN id="token-40-28" pos="word" morph="none" start_char="4531" end_char="4534">cara</TOKEN>
<TOKEN id="token-40-29" pos="word" morph="none" start_char="4536" end_char="4542">externa</TOKEN>
<TOKEN id="token-40-30" pos="punct" morph="none" start_char="4543" end_char="4543">,</TOKEN>
<TOKEN id="token-40-31" pos="word" morph="none" start_char="4545" end_char="4559">microorganismos</TOKEN>
<TOKEN id="token-40-32" pos="word" morph="none" start_char="4561" end_char="4563">del</TOKEN>
<TOKEN id="token-40-33" pos="word" morph="none" start_char="4565" end_char="4569">medio</TOKEN>
<TOKEN id="token-40-34" pos="word" morph="none" start_char="4571" end_char="4578">ambiente</TOKEN>
<TOKEN id="token-40-35" pos="punct" morph="none" start_char="4579" end_char="4579">.</TOKEN>
</SEG>
<SEG id="segment-41" start_char="4581" end_char="4766">
<ORIGINAL_TEXT>Los primeros no suponen ningún peligro para el usuario y los segundos, con las debidas medidas de higiene, tampoco (de hecho las mascarilla está protegiéndonos frente a ellos)", afirma a</ORIGINAL_TEXT>
<TOKEN id="token-41-0" pos="word" morph="none" start_char="4581" end_char="4583">Los</TOKEN>
<TOKEN id="token-41-1" pos="word" morph="none" start_char="4585" end_char="4592">primeros</TOKEN>
<TOKEN id="token-41-2" pos="word" morph="none" start_char="4594" end_char="4595">no</TOKEN>
<TOKEN id="token-41-3" pos="word" morph="none" start_char="4597" end_char="4603">suponen</TOKEN>
<TOKEN id="token-41-4" pos="word" morph="none" start_char="4605" end_char="4610">ningún</TOKEN>
<TOKEN id="token-41-5" pos="word" morph="none" start_char="4612" end_char="4618">peligro</TOKEN>
<TOKEN id="token-41-6" pos="word" morph="none" start_char="4620" end_char="4623">para</TOKEN>
<TOKEN id="token-41-7" pos="word" morph="none" start_char="4625" end_char="4626">el</TOKEN>
<TOKEN id="token-41-8" pos="word" morph="none" start_char="4628" end_char="4634">usuario</TOKEN>
<TOKEN id="token-41-9" pos="word" morph="none" start_char="4636" end_char="4636">y</TOKEN>
<TOKEN id="token-41-10" pos="word" morph="none" start_char="4638" end_char="4640">los</TOKEN>
<TOKEN id="token-41-11" pos="word" morph="none" start_char="4642" end_char="4649">segundos</TOKEN>
<TOKEN id="token-41-12" pos="punct" morph="none" start_char="4650" end_char="4650">,</TOKEN>
<TOKEN id="token-41-13" pos="word" morph="none" start_char="4652" end_char="4654">con</TOKEN>
<TOKEN id="token-41-14" pos="word" morph="none" start_char="4656" end_char="4658">las</TOKEN>
<TOKEN id="token-41-15" pos="word" morph="none" start_char="4660" end_char="4666">debidas</TOKEN>
<TOKEN id="token-41-16" pos="word" morph="none" start_char="4668" end_char="4674">medidas</TOKEN>
<TOKEN id="token-41-17" pos="word" morph="none" start_char="4676" end_char="4677">de</TOKEN>
<TOKEN id="token-41-18" pos="word" morph="none" start_char="4679" end_char="4685">higiene</TOKEN>
<TOKEN id="token-41-19" pos="punct" morph="none" start_char="4686" end_char="4686">,</TOKEN>
<TOKEN id="token-41-20" pos="word" morph="none" start_char="4688" end_char="4694">tampoco</TOKEN>
<TOKEN id="token-41-21" pos="punct" morph="none" start_char="4696" end_char="4696">(</TOKEN>
<TOKEN id="token-41-22" pos="word" morph="none" start_char="4697" end_char="4698">de</TOKEN>
<TOKEN id="token-41-23" pos="word" morph="none" start_char="4700" end_char="4704">hecho</TOKEN>
<TOKEN id="token-41-24" pos="word" morph="none" start_char="4706" end_char="4708">las</TOKEN>
<TOKEN id="token-41-25" pos="word" morph="none" start_char="4710" end_char="4719">mascarilla</TOKEN>
<TOKEN id="token-41-26" pos="word" morph="none" start_char="4721" end_char="4724">está</TOKEN>
<TOKEN id="token-41-27" pos="word" morph="none" start_char="4726" end_char="4739">protegiéndonos</TOKEN>
<TOKEN id="token-41-28" pos="word" morph="none" start_char="4741" end_char="4746">frente</TOKEN>
<TOKEN id="token-41-29" pos="word" morph="none" start_char="4748" end_char="4748">a</TOKEN>
<TOKEN id="token-41-30" pos="word" morph="none" start_char="4750" end_char="4754">ellos</TOKEN>
<TOKEN id="token-41-31" pos="punct" morph="none" start_char="4755" end_char="4757">)",</TOKEN>
<TOKEN id="token-41-32" pos="word" morph="none" start_char="4759" end_char="4764">afirma</TOKEN>
<TOKEN id="token-41-33" pos="word" morph="none" start_char="4766" end_char="4766">a</TOKEN>
</SEG>
<SEG id="segment-42" start_char="4769" end_char="4784">
<ORIGINAL_TEXT>Maldita Ciencia.</ORIGINAL_TEXT>
<TOKEN id="token-42-0" pos="word" morph="none" start_char="4769" end_char="4775">Maldita</TOKEN>
<TOKEN id="token-42-1" pos="word" morph="none" start_char="4777" end_char="4783">Ciencia</TOKEN>
<TOKEN id="token-42-2" pos="punct" morph="none" start_char="4784" end_char="4784">.</TOKEN>
</SEG>
<SEG id="segment-43" start_char="4788" end_char="5039">
<ORIGINAL_TEXT>Juan Sabatté, médico y doctor en microbiología e investigador del Consejo Nacional de Investigaciones Científicas y Técnicas (CONICET) de Argentina, explica que retener algunos de los gérmenes que exhalamos es justamente la utilidad de las mascarillas.</ORIGINAL_TEXT>
<TOKEN id="token-43-0" pos="word" morph="none" start_char="4788" end_char="4791">Juan</TOKEN>
<TOKEN id="token-43-1" pos="word" morph="none" start_char="4793" end_char="4799">Sabatté</TOKEN>
<TOKEN id="token-43-2" pos="punct" morph="none" start_char="4800" end_char="4800">,</TOKEN>
<TOKEN id="token-43-3" pos="word" morph="none" start_char="4802" end_char="4807">médico</TOKEN>
<TOKEN id="token-43-4" pos="word" morph="none" start_char="4809" end_char="4809">y</TOKEN>
<TOKEN id="token-43-5" pos="word" morph="none" start_char="4811" end_char="4816">doctor</TOKEN>
<TOKEN id="token-43-6" pos="word" morph="none" start_char="4818" end_char="4819">en</TOKEN>
<TOKEN id="token-43-7" pos="word" morph="none" start_char="4821" end_char="4833">microbiología</TOKEN>
<TOKEN id="token-43-8" pos="word" morph="none" start_char="4835" end_char="4835">e</TOKEN>
<TOKEN id="token-43-9" pos="word" morph="none" start_char="4837" end_char="4848">investigador</TOKEN>
<TOKEN id="token-43-10" pos="word" morph="none" start_char="4850" end_char="4852">del</TOKEN>
<TOKEN id="token-43-11" pos="word" morph="none" start_char="4854" end_char="4860">Consejo</TOKEN>
<TOKEN id="token-43-12" pos="word" morph="none" start_char="4862" end_char="4869">Nacional</TOKEN>
<TOKEN id="token-43-13" pos="word" morph="none" start_char="4871" end_char="4872">de</TOKEN>
<TOKEN id="token-43-14" pos="word" morph="none" start_char="4874" end_char="4888">Investigaciones</TOKEN>
<TOKEN id="token-43-15" pos="word" morph="none" start_char="4890" end_char="4900">Científicas</TOKEN>
<TOKEN id="token-43-16" pos="word" morph="none" start_char="4902" end_char="4902">y</TOKEN>
<TOKEN id="token-43-17" pos="word" morph="none" start_char="4904" end_char="4911">Técnicas</TOKEN>
<TOKEN id="token-43-18" pos="punct" morph="none" start_char="4913" end_char="4913">(</TOKEN>
<TOKEN id="token-43-19" pos="word" morph="none" start_char="4914" end_char="4920">CONICET</TOKEN>
<TOKEN id="token-43-20" pos="punct" morph="none" start_char="4921" end_char="4921">)</TOKEN>
<TOKEN id="token-43-21" pos="word" morph="none" start_char="4923" end_char="4924">de</TOKEN>
<TOKEN id="token-43-22" pos="word" morph="none" start_char="4926" end_char="4934">Argentina</TOKEN>
<TOKEN id="token-43-23" pos="punct" morph="none" start_char="4935" end_char="4935">,</TOKEN>
<TOKEN id="token-43-24" pos="word" morph="none" start_char="4937" end_char="4943">explica</TOKEN>
<TOKEN id="token-43-25" pos="word" morph="none" start_char="4945" end_char="4947">que</TOKEN>
<TOKEN id="token-43-26" pos="word" morph="none" start_char="4949" end_char="4955">retener</TOKEN>
<TOKEN id="token-43-27" pos="word" morph="none" start_char="4957" end_char="4963">algunos</TOKEN>
<TOKEN id="token-43-28" pos="word" morph="none" start_char="4965" end_char="4966">de</TOKEN>
<TOKEN id="token-43-29" pos="word" morph="none" start_char="4968" end_char="4970">los</TOKEN>
<TOKEN id="token-43-30" pos="word" morph="none" start_char="4972" end_char="4979">gérmenes</TOKEN>
<TOKEN id="token-43-31" pos="word" morph="none" start_char="4981" end_char="4983">que</TOKEN>
<TOKEN id="token-43-32" pos="word" morph="none" start_char="4985" end_char="4993">exhalamos</TOKEN>
<TOKEN id="token-43-33" pos="word" morph="none" start_char="4995" end_char="4996">es</TOKEN>
<TOKEN id="token-43-34" pos="word" morph="none" start_char="4998" end_char="5007">justamente</TOKEN>
<TOKEN id="token-43-35" pos="word" morph="none" start_char="5009" end_char="5010">la</TOKEN>
<TOKEN id="token-43-36" pos="word" morph="none" start_char="5012" end_char="5019">utilidad</TOKEN>
<TOKEN id="token-43-37" pos="word" morph="none" start_char="5021" end_char="5022">de</TOKEN>
<TOKEN id="token-43-38" pos="word" morph="none" start_char="5024" end_char="5026">las</TOKEN>
<TOKEN id="token-43-39" pos="word" morph="none" start_char="5028" end_char="5038">mascarillas</TOKEN>
<TOKEN id="token-43-40" pos="punct" morph="none" start_char="5039" end_char="5039">.</TOKEN>
</SEG>
<SEG id="segment-44" start_char="5041" end_char="5117">
<ORIGINAL_TEXT>"No dejar pasar gérmenes o al menos disminuir notablemente su paso", cuenta a</ORIGINAL_TEXT>
<TOKEN id="token-44-0" pos="punct" morph="none" start_char="5041" end_char="5041">"</TOKEN>
<TOKEN id="token-44-1" pos="word" morph="none" start_char="5042" end_char="5043">No</TOKEN>
<TOKEN id="token-44-2" pos="word" morph="none" start_char="5045" end_char="5049">dejar</TOKEN>
<TOKEN id="token-44-3" pos="word" morph="none" start_char="5051" end_char="5055">pasar</TOKEN>
<TOKEN id="token-44-4" pos="word" morph="none" start_char="5057" end_char="5064">gérmenes</TOKEN>
<TOKEN id="token-44-5" pos="word" morph="none" start_char="5066" end_char="5066">o</TOKEN>
<TOKEN id="token-44-6" pos="word" morph="none" start_char="5068" end_char="5069">al</TOKEN>
<TOKEN id="token-44-7" pos="word" morph="none" start_char="5071" end_char="5075">menos</TOKEN>
<TOKEN id="token-44-8" pos="word" morph="none" start_char="5077" end_char="5085">disminuir</TOKEN>
<TOKEN id="token-44-9" pos="word" morph="none" start_char="5087" end_char="5098">notablemente</TOKEN>
<TOKEN id="token-44-10" pos="word" morph="none" start_char="5100" end_char="5101">su</TOKEN>
<TOKEN id="token-44-11" pos="word" morph="none" start_char="5103" end_char="5106">paso</TOKEN>
<TOKEN id="token-44-12" pos="punct" morph="none" start_char="5107" end_char="5108">",</TOKEN>
<TOKEN id="token-44-13" pos="word" morph="none" start_char="5110" end_char="5115">cuenta</TOKEN>
<TOKEN id="token-44-14" pos="word" morph="none" start_char="5117" end_char="5117">a</TOKEN>
</SEG>
<SEG id="segment-45" start_char="5120" end_char="5122">
<ORIGINAL_TEXT>AFP</ORIGINAL_TEXT>
<TOKEN id="token-45-0" pos="word" morph="none" start_char="5120" end_char="5122">AFP</TOKEN>
</SEG>
<SEG id="segment-46" start_char="5125" end_char="5165">
<ORIGINAL_TEXT>, medio que ha desmentido la información.</ORIGINAL_TEXT>
<TOKEN id="token-46-0" pos="punct" morph="none" start_char="5125" end_char="5125">,</TOKEN>
<TOKEN id="token-46-1" pos="word" morph="none" start_char="5127" end_char="5131">medio</TOKEN>
<TOKEN id="token-46-2" pos="word" morph="none" start_char="5133" end_char="5135">que</TOKEN>
<TOKEN id="token-46-3" pos="word" morph="none" start_char="5137" end_char="5138">ha</TOKEN>
<TOKEN id="token-46-4" pos="word" morph="none" start_char="5140" end_char="5149">desmentido</TOKEN>
<TOKEN id="token-46-5" pos="word" morph="none" start_char="5151" end_char="5152">la</TOKEN>
<TOKEN id="token-46-6" pos="word" morph="none" start_char="5154" end_char="5164">información</TOKEN>
<TOKEN id="token-46-7" pos="punct" morph="none" start_char="5165" end_char="5165">.</TOKEN>
</SEG>
<SEG id="segment-47" start_char="5168" end_char="5362">
<ORIGINAL_TEXT>Explica que esto no tiene consecuencias negativas para la salud y que la posibilidad de que los microorganismos atrapados en la mascarilla reingresen al cuerpo "no ha sido debidamente estudiada".</ORIGINAL_TEXT>
<TOKEN id="token-47-0" pos="word" morph="none" start_char="5168" end_char="5174">Explica</TOKEN>
<TOKEN id="token-47-1" pos="word" morph="none" start_char="5176" end_char="5178">que</TOKEN>
<TOKEN id="token-47-2" pos="word" morph="none" start_char="5180" end_char="5183">esto</TOKEN>
<TOKEN id="token-47-3" pos="word" morph="none" start_char="5185" end_char="5186">no</TOKEN>
<TOKEN id="token-47-4" pos="word" morph="none" start_char="5188" end_char="5192">tiene</TOKEN>
<TOKEN id="token-47-5" pos="word" morph="none" start_char="5194" end_char="5206">consecuencias</TOKEN>
<TOKEN id="token-47-6" pos="word" morph="none" start_char="5208" end_char="5216">negativas</TOKEN>
<TOKEN id="token-47-7" pos="word" morph="none" start_char="5218" end_char="5221">para</TOKEN>
<TOKEN id="token-47-8" pos="word" morph="none" start_char="5223" end_char="5224">la</TOKEN>
<TOKEN id="token-47-9" pos="word" morph="none" start_char="5226" end_char="5230">salud</TOKEN>
<TOKEN id="token-47-10" pos="word" morph="none" start_char="5232" end_char="5232">y</TOKEN>
<TOKEN id="token-47-11" pos="word" morph="none" start_char="5234" end_char="5236">que</TOKEN>
<TOKEN id="token-47-12" pos="word" morph="none" start_char="5238" end_char="5239">la</TOKEN>
<TOKEN id="token-47-13" pos="word" morph="none" start_char="5241" end_char="5251">posibilidad</TOKEN>
<TOKEN id="token-47-14" pos="word" morph="none" start_char="5253" end_char="5254">de</TOKEN>
<TOKEN id="token-47-15" pos="word" morph="none" start_char="5256" end_char="5258">que</TOKEN>
<TOKEN id="token-47-16" pos="word" morph="none" start_char="5260" end_char="5262">los</TOKEN>
<TOKEN id="token-47-17" pos="word" morph="none" start_char="5264" end_char="5278">microorganismos</TOKEN>
<TOKEN id="token-47-18" pos="word" morph="none" start_char="5280" end_char="5288">atrapados</TOKEN>
<TOKEN id="token-47-19" pos="word" morph="none" start_char="5290" end_char="5291">en</TOKEN>
<TOKEN id="token-47-20" pos="word" morph="none" start_char="5293" end_char="5294">la</TOKEN>
<TOKEN id="token-47-21" pos="word" morph="none" start_char="5296" end_char="5305">mascarilla</TOKEN>
<TOKEN id="token-47-22" pos="word" morph="none" start_char="5307" end_char="5316">reingresen</TOKEN>
<TOKEN id="token-47-23" pos="word" morph="none" start_char="5318" end_char="5319">al</TOKEN>
<TOKEN id="token-47-24" pos="word" morph="none" start_char="5321" end_char="5326">cuerpo</TOKEN>
<TOKEN id="token-47-25" pos="punct" morph="none" start_char="5328" end_char="5328">"</TOKEN>
<TOKEN id="token-47-26" pos="word" morph="none" start_char="5329" end_char="5330">no</TOKEN>
<TOKEN id="token-47-27" pos="word" morph="none" start_char="5332" end_char="5333">ha</TOKEN>
<TOKEN id="token-47-28" pos="word" morph="none" start_char="5335" end_char="5338">sido</TOKEN>
<TOKEN id="token-47-29" pos="word" morph="none" start_char="5340" end_char="5350">debidamente</TOKEN>
<TOKEN id="token-47-30" pos="word" morph="none" start_char="5352" end_char="5360">estudiada</TOKEN>
<TOKEN id="token-47-31" pos="punct" morph="none" start_char="5361" end_char="5362">".</TOKEN>
</SEG>
<SEG id="segment-48" start_char="5364" end_char="5552">
<ORIGINAL_TEXT>Aunque así fuera, considera que esto no tendría gran relevancia: "Esos gérmenes ya están en nuestro tracto respiratorio y la exhalación de ninguna manera los elimina en forma considerable".</ORIGINAL_TEXT>
<TOKEN id="token-48-0" pos="word" morph="none" start_char="5364" end_char="5369">Aunque</TOKEN>
<TOKEN id="token-48-1" pos="word" morph="none" start_char="5371" end_char="5373">así</TOKEN>
<TOKEN id="token-48-2" pos="word" morph="none" start_char="5375" end_char="5379">fuera</TOKEN>
<TOKEN id="token-48-3" pos="punct" morph="none" start_char="5380" end_char="5380">,</TOKEN>
<TOKEN id="token-48-4" pos="word" morph="none" start_char="5382" end_char="5390">considera</TOKEN>
<TOKEN id="token-48-5" pos="word" morph="none" start_char="5392" end_char="5394">que</TOKEN>
<TOKEN id="token-48-6" pos="word" morph="none" start_char="5396" end_char="5399">esto</TOKEN>
<TOKEN id="token-48-7" pos="word" morph="none" start_char="5401" end_char="5402">no</TOKEN>
<TOKEN id="token-48-8" pos="word" morph="none" start_char="5404" end_char="5410">tendría</TOKEN>
<TOKEN id="token-48-9" pos="word" morph="none" start_char="5412" end_char="5415">gran</TOKEN>
<TOKEN id="token-48-10" pos="word" morph="none" start_char="5417" end_char="5426">relevancia</TOKEN>
<TOKEN id="token-48-11" pos="punct" morph="none" start_char="5427" end_char="5427">:</TOKEN>
<TOKEN id="token-48-12" pos="punct" morph="none" start_char="5429" end_char="5429">"</TOKEN>
<TOKEN id="token-48-13" pos="word" morph="none" start_char="5430" end_char="5433">Esos</TOKEN>
<TOKEN id="token-48-14" pos="word" morph="none" start_char="5435" end_char="5442">gérmenes</TOKEN>
<TOKEN id="token-48-15" pos="word" morph="none" start_char="5444" end_char="5445">ya</TOKEN>
<TOKEN id="token-48-16" pos="word" morph="none" start_char="5447" end_char="5451">están</TOKEN>
<TOKEN id="token-48-17" pos="word" morph="none" start_char="5453" end_char="5454">en</TOKEN>
<TOKEN id="token-48-18" pos="word" morph="none" start_char="5456" end_char="5462">nuestro</TOKEN>
<TOKEN id="token-48-19" pos="word" morph="none" start_char="5464" end_char="5469">tracto</TOKEN>
<TOKEN id="token-48-20" pos="word" morph="none" start_char="5471" end_char="5482">respiratorio</TOKEN>
<TOKEN id="token-48-21" pos="word" morph="none" start_char="5484" end_char="5484">y</TOKEN>
<TOKEN id="token-48-22" pos="word" morph="none" start_char="5486" end_char="5487">la</TOKEN>
<TOKEN id="token-48-23" pos="word" morph="none" start_char="5489" end_char="5498">exhalación</TOKEN>
<TOKEN id="token-48-24" pos="word" morph="none" start_char="5500" end_char="5501">de</TOKEN>
<TOKEN id="token-48-25" pos="word" morph="none" start_char="5503" end_char="5509">ninguna</TOKEN>
<TOKEN id="token-48-26" pos="word" morph="none" start_char="5511" end_char="5516">manera</TOKEN>
<TOKEN id="token-48-27" pos="word" morph="none" start_char="5518" end_char="5520">los</TOKEN>
<TOKEN id="token-48-28" pos="word" morph="none" start_char="5522" end_char="5528">elimina</TOKEN>
<TOKEN id="token-48-29" pos="word" morph="none" start_char="5530" end_char="5531">en</TOKEN>
<TOKEN id="token-48-30" pos="word" morph="none" start_char="5533" end_char="5537">forma</TOKEN>
<TOKEN id="token-48-31" pos="word" morph="none" start_char="5539" end_char="5550">considerable</TOKEN>
<TOKEN id="token-48-32" pos="punct" morph="none" start_char="5551" end_char="5552">".</TOKEN>
</SEG>
<SEG id="segment-49" start_char="5555" end_char="5608">
<ORIGINAL_TEXT>Primera fecha de publicación del artículo: 21/08/2020.</ORIGINAL_TEXT>
<TOKEN id="token-49-0" pos="word" morph="none" start_char="5555" end_char="5561">Primera</TOKEN>
<TOKEN id="token-49-1" pos="word" morph="none" start_char="5563" end_char="5567">fecha</TOKEN>
<TOKEN id="token-49-2" pos="word" morph="none" start_char="5569" end_char="5570">de</TOKEN>
<TOKEN id="token-49-3" pos="word" morph="none" start_char="5572" end_char="5582">publicación</TOKEN>
<TOKEN id="token-49-4" pos="word" morph="none" start_char="5584" end_char="5586">del</TOKEN>
<TOKEN id="token-49-5" pos="word" morph="none" start_char="5588" end_char="5595">artículo</TOKEN>
<TOKEN id="token-49-6" pos="punct" morph="none" start_char="5596" end_char="5596">:</TOKEN>
<TOKEN id="token-49-7" pos="unknown" morph="none" start_char="5598" end_char="5607">21/08/2020</TOKEN>
<TOKEN id="token-49-8" pos="punct" morph="none" start_char="5608" end_char="5608">.</TOKEN>
</SEG>
<SEG id="segment-50" start_char="5611" end_char="5667">
<ORIGINAL_TEXT>Primera fecha de publicación de este artículo: 21/08/2020</ORIGINAL_TEXT>
<TOKEN id="token-50-0" pos="word" morph="none" start_char="5611" end_char="5617">Primera</TOKEN>
<TOKEN id="token-50-1" pos="word" morph="none" start_char="5619" end_char="5623">fecha</TOKEN>
<TOKEN id="token-50-2" pos="word" morph="none" start_char="5625" end_char="5626">de</TOKEN>
<TOKEN id="token-50-3" pos="word" morph="none" start_char="5628" end_char="5638">publicación</TOKEN>
<TOKEN id="token-50-4" pos="word" morph="none" start_char="5640" end_char="5641">de</TOKEN>
<TOKEN id="token-50-5" pos="word" morph="none" start_char="5643" end_char="5646">este</TOKEN>
<TOKEN id="token-50-6" pos="word" morph="none" start_char="5648" end_char="5655">artículo</TOKEN>
<TOKEN id="token-50-7" pos="punct" morph="none" start_char="5656" end_char="5656">:</TOKEN>
<TOKEN id="token-50-8" pos="unknown" morph="none" start_char="5658" end_char="5667">21/08/2020</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
