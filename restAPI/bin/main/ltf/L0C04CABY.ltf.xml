<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CABY" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="1643" raw_text_md5="202eec721025c26be98d5a2223560aaf">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="30">
<ORIGINAL_TEXT>When Did Covid Actually Begin?</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="4">When</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="6" end_char="8">Did</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="10" end_char="14">Covid</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="16" end_char="23">Actually</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="25" end_char="29">Begin</TOKEN>
<TOKEN id="token-0-5" pos="punct" morph="none" start_char="30" end_char="30">?</TOKEN>
</SEG>
<SEG id="segment-1" start_char="34" end_char="124">
<ORIGINAL_TEXT>Scientists have been trying to piece together a timeline for the Covid outbreak for months.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="34" end_char="43">Scientists</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="45" end_char="48">have</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="50" end_char="53">been</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="55" end_char="60">trying</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="62" end_char="63">to</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="65" end_char="69">piece</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="71" end_char="78">together</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="80" end_char="80">a</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="82" end_char="89">timeline</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="91" end_char="93">for</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="95" end_char="97">the</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="99" end_char="103">Covid</TOKEN>
<TOKEN id="token-1-12" pos="word" morph="none" start_char="105" end_char="112">outbreak</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="114" end_char="116">for</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="118" end_char="123">months</TOKEN>
<TOKEN id="token-1-15" pos="punct" morph="none" start_char="124" end_char="124">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="126" end_char="300">
<ORIGINAL_TEXT>The first official warning about the danger the infection posed was in mid February, but evidence is surfacing that Covid had expanded the the western hemisphere much earlier.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="126" end_char="128">The</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="130" end_char="134">first</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="136" end_char="143">official</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="145" end_char="151">warning</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="153" end_char="157">about</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="159" end_char="161">the</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="163" end_char="168">danger</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="170" end_char="172">the</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="174" end_char="182">infection</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="184" end_char="188">posed</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="190" end_char="192">was</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="194" end_char="195">in</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="197" end_char="199">mid</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="201" end_char="208">February</TOKEN>
<TOKEN id="token-2-14" pos="punct" morph="none" start_char="209" end_char="209">,</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="211" end_char="213">but</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="215" end_char="222">evidence</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="224" end_char="225">is</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="227" end_char="235">surfacing</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="237" end_char="240">that</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="242" end_char="246">Covid</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="248" end_char="250">had</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="252" end_char="259">expanded</TOKEN>
<TOKEN id="token-2-23" pos="word" morph="none" start_char="261" end_char="263">the</TOKEN>
<TOKEN id="token-2-24" pos="word" morph="none" start_char="265" end_char="267">the</TOKEN>
<TOKEN id="token-2-25" pos="word" morph="none" start_char="269" end_char="275">western</TOKEN>
<TOKEN id="token-2-26" pos="word" morph="none" start_char="277" end_char="286">hemisphere</TOKEN>
<TOKEN id="token-2-27" pos="word" morph="none" start_char="288" end_char="291">much</TOKEN>
<TOKEN id="token-2-28" pos="word" morph="none" start_char="293" end_char="299">earlier</TOKEN>
<TOKEN id="token-2-29" pos="punct" morph="none" start_char="300" end_char="300">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="303" end_char="427">
<ORIGINAL_TEXT>Blood test analyses have found infections in the US as early as December in California, Oregon, Washington, and other states.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="303" end_char="307">Blood</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="309" end_char="312">test</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="314" end_char="321">analyses</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="323" end_char="326">have</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="328" end_char="332">found</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="334" end_char="343">infections</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="345" end_char="346">in</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="348" end_char="350">the</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="352" end_char="353">US</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="355" end_char="356">as</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="358" end_char="362">early</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="364" end_char="365">as</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="367" end_char="374">December</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="376" end_char="377">in</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="379" end_char="388">California</TOKEN>
<TOKEN id="token-3-15" pos="punct" morph="none" start_char="389" end_char="389">,</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="391" end_char="396">Oregon</TOKEN>
<TOKEN id="token-3-17" pos="punct" morph="none" start_char="397" end_char="397">,</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="399" end_char="408">Washington</TOKEN>
<TOKEN id="token-3-19" pos="punct" morph="none" start_char="409" end_char="409">,</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="411" end_char="413">and</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="415" end_char="419">other</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="421" end_char="426">states</TOKEN>
<TOKEN id="token-3-23" pos="punct" morph="none" start_char="427" end_char="427">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="429" end_char="551">
<ORIGINAL_TEXT>Similar analyses conducted in Europe show similar findings, claiming that Italy had Covid cases as early as September 2019.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="429" end_char="435">Similar</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="437" end_char="444">analyses</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="446" end_char="454">conducted</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="456" end_char="457">in</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="459" end_char="464">Europe</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="466" end_char="469">show</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="471" end_char="477">similar</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="479" end_char="486">findings</TOKEN>
<TOKEN id="token-4-8" pos="punct" morph="none" start_char="487" end_char="487">,</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="489" end_char="496">claiming</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="498" end_char="501">that</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="503" end_char="507">Italy</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="509" end_char="511">had</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="513" end_char="517">Covid</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="519" end_char="523">cases</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="525" end_char="526">as</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="528" end_char="532">early</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="534" end_char="535">as</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="537" end_char="545">September</TOKEN>
<TOKEN id="token-4-19" pos="word" morph="none" start_char="547" end_char="550">2019</TOKEN>
<TOKEN id="token-4-20" pos="punct" morph="none" start_char="551" end_char="551">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="553" end_char="617">
<ORIGINAL_TEXT>France had a cases in December, and Spain found traces in sewage.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="553" end_char="558">France</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="560" end_char="562">had</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="564" end_char="564">a</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="566" end_char="570">cases</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="572" end_char="573">in</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="575" end_char="582">December</TOKEN>
<TOKEN id="token-5-6" pos="punct" morph="none" start_char="583" end_char="583">,</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="585" end_char="587">and</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="589" end_char="593">Spain</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="595" end_char="599">found</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="601" end_char="606">traces</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="608" end_char="609">in</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="611" end_char="616">sewage</TOKEN>
<TOKEN id="token-5-13" pos="punct" morph="none" start_char="617" end_char="617">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="620" end_char="763">
<ORIGINAL_TEXT>To make all of this worse, leaked documents to CNN prove that the Chinese covered up thousands of cases at the beginning of the global outbreak.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="620" end_char="621">To</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="623" end_char="626">make</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="628" end_char="630">all</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="632" end_char="633">of</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="635" end_char="638">this</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="640" end_char="644">worse</TOKEN>
<TOKEN id="token-6-6" pos="punct" morph="none" start_char="645" end_char="645">,</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="647" end_char="652">leaked</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="654" end_char="662">documents</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="664" end_char="665">to</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="667" end_char="669">CNN</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="671" end_char="675">prove</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="677" end_char="680">that</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="682" end_char="684">the</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="686" end_char="692">Chinese</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="694" end_char="700">covered</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="702" end_char="703">up</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="705" end_char="713">thousands</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="715" end_char="716">of</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="718" end_char="722">cases</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="724" end_char="725">at</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="727" end_char="729">the</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="731" end_char="739">beginning</TOKEN>
<TOKEN id="token-6-23" pos="word" morph="none" start_char="741" end_char="742">of</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="744" end_char="746">the</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="748" end_char="753">global</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="755" end_char="762">outbreak</TOKEN>
<TOKEN id="token-6-27" pos="punct" morph="none" start_char="763" end_char="763">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="765" end_char="832">
<ORIGINAL_TEXT>This information could have been vital to saving thousands of lives.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="765" end_char="768">This</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="770" end_char="780">information</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="782" end_char="786">could</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="788" end_char="791">have</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="793" end_char="796">been</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="798" end_char="802">vital</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="804" end_char="805">to</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="807" end_char="812">saving</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="814" end_char="822">thousands</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="824" end_char="825">of</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="827" end_char="831">lives</TOKEN>
<TOKEN id="token-7-11" pos="punct" morph="none" start_char="832" end_char="832">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="834" end_char="920">
<ORIGINAL_TEXT>China claimed to have almost one hundred thousand cases and almost half that in deaths.</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="834" end_char="838">China</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="840" end_char="846">claimed</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="848" end_char="849">to</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="851" end_char="854">have</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="856" end_char="861">almost</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="863" end_char="865">one</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="867" end_char="873">hundred</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="875" end_char="882">thousand</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="884" end_char="888">cases</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="890" end_char="892">and</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="894" end_char="899">almost</TOKEN>
<TOKEN id="token-8-11" pos="word" morph="none" start_char="901" end_char="904">half</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="906" end_char="909">that</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="911" end_char="912">in</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="914" end_char="919">deaths</TOKEN>
<TOKEN id="token-8-15" pos="punct" morph="none" start_char="920" end_char="920">.</TOKEN>
</SEG>
<SEG id="segment-9" start_char="922" end_char="1088">
<ORIGINAL_TEXT>This, compared to a much better ratio of cases to deaths in all of the western world, should indicate that China has been less than truthful about their Covid numbers.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="922" end_char="925">This</TOKEN>
<TOKEN id="token-9-1" pos="punct" morph="none" start_char="926" end_char="926">,</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="928" end_char="935">compared</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="937" end_char="938">to</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="940" end_char="940">a</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="942" end_char="945">much</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="947" end_char="952">better</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="954" end_char="958">ratio</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="960" end_char="961">of</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="963" end_char="967">cases</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="969" end_char="970">to</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="972" end_char="977">deaths</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="979" end_char="980">in</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="982" end_char="984">all</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="986" end_char="987">of</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="989" end_char="991">the</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="993" end_char="999">western</TOKEN>
<TOKEN id="token-9-17" pos="word" morph="none" start_char="1001" end_char="1005">world</TOKEN>
<TOKEN id="token-9-18" pos="punct" morph="none" start_char="1006" end_char="1006">,</TOKEN>
<TOKEN id="token-9-19" pos="word" morph="none" start_char="1008" end_char="1013">should</TOKEN>
<TOKEN id="token-9-20" pos="word" morph="none" start_char="1015" end_char="1022">indicate</TOKEN>
<TOKEN id="token-9-21" pos="word" morph="none" start_char="1024" end_char="1027">that</TOKEN>
<TOKEN id="token-9-22" pos="word" morph="none" start_char="1029" end_char="1033">China</TOKEN>
<TOKEN id="token-9-23" pos="word" morph="none" start_char="1035" end_char="1037">has</TOKEN>
<TOKEN id="token-9-24" pos="word" morph="none" start_char="1039" end_char="1042">been</TOKEN>
<TOKEN id="token-9-25" pos="word" morph="none" start_char="1044" end_char="1047">less</TOKEN>
<TOKEN id="token-9-26" pos="word" morph="none" start_char="1049" end_char="1052">than</TOKEN>
<TOKEN id="token-9-27" pos="word" morph="none" start_char="1054" end_char="1061">truthful</TOKEN>
<TOKEN id="token-9-28" pos="word" morph="none" start_char="1063" end_char="1067">about</TOKEN>
<TOKEN id="token-9-29" pos="word" morph="none" start_char="1069" end_char="1073">their</TOKEN>
<TOKEN id="token-9-30" pos="word" morph="none" start_char="1075" end_char="1079">Covid</TOKEN>
<TOKEN id="token-9-31" pos="word" morph="none" start_char="1081" end_char="1087">numbers</TOKEN>
<TOKEN id="token-9-32" pos="punct" morph="none" start_char="1088" end_char="1088">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="1091" end_char="1189">
<ORIGINAL_TEXT>What this should indicate to the rest of the world is that China is a bad actor on the world stage.</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="1091" end_char="1094">What</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="1096" end_char="1099">this</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="1101" end_char="1106">should</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="1108" end_char="1115">indicate</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="1117" end_char="1118">to</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="1120" end_char="1122">the</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="1124" end_char="1127">rest</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="1129" end_char="1130">of</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="1132" end_char="1134">the</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1136" end_char="1140">world</TOKEN>
<TOKEN id="token-10-10" pos="word" morph="none" start_char="1142" end_char="1143">is</TOKEN>
<TOKEN id="token-10-11" pos="word" morph="none" start_char="1145" end_char="1148">that</TOKEN>
<TOKEN id="token-10-12" pos="word" morph="none" start_char="1150" end_char="1154">China</TOKEN>
<TOKEN id="token-10-13" pos="word" morph="none" start_char="1156" end_char="1157">is</TOKEN>
<TOKEN id="token-10-14" pos="word" morph="none" start_char="1159" end_char="1159">a</TOKEN>
<TOKEN id="token-10-15" pos="word" morph="none" start_char="1161" end_char="1163">bad</TOKEN>
<TOKEN id="token-10-16" pos="word" morph="none" start_char="1165" end_char="1169">actor</TOKEN>
<TOKEN id="token-10-17" pos="word" morph="none" start_char="1171" end_char="1172">on</TOKEN>
<TOKEN id="token-10-18" pos="word" morph="none" start_char="1174" end_char="1176">the</TOKEN>
<TOKEN id="token-10-19" pos="word" morph="none" start_char="1178" end_char="1182">world</TOKEN>
<TOKEN id="token-10-20" pos="word" morph="none" start_char="1184" end_char="1188">stage</TOKEN>
<TOKEN id="token-10-21" pos="punct" morph="none" start_char="1189" end_char="1189">.</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1191" end_char="1347">
<ORIGINAL_TEXT>It is clear, based only on Covid response, that China can not be trusted and should show that China is not ready to be recognized as an ally in any capacity.</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1191" end_char="1192">It</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1194" end_char="1195">is</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1197" end_char="1201">clear</TOKEN>
<TOKEN id="token-11-3" pos="punct" morph="none" start_char="1202" end_char="1202">,</TOKEN>
<TOKEN id="token-11-4" pos="word" morph="none" start_char="1204" end_char="1208">based</TOKEN>
<TOKEN id="token-11-5" pos="word" morph="none" start_char="1210" end_char="1213">only</TOKEN>
<TOKEN id="token-11-6" pos="word" morph="none" start_char="1215" end_char="1216">on</TOKEN>
<TOKEN id="token-11-7" pos="word" morph="none" start_char="1218" end_char="1222">Covid</TOKEN>
<TOKEN id="token-11-8" pos="word" morph="none" start_char="1224" end_char="1231">response</TOKEN>
<TOKEN id="token-11-9" pos="punct" morph="none" start_char="1232" end_char="1232">,</TOKEN>
<TOKEN id="token-11-10" pos="word" morph="none" start_char="1234" end_char="1237">that</TOKEN>
<TOKEN id="token-11-11" pos="word" morph="none" start_char="1239" end_char="1243">China</TOKEN>
<TOKEN id="token-11-12" pos="word" morph="none" start_char="1245" end_char="1247">can</TOKEN>
<TOKEN id="token-11-13" pos="word" morph="none" start_char="1249" end_char="1251">not</TOKEN>
<TOKEN id="token-11-14" pos="word" morph="none" start_char="1253" end_char="1254">be</TOKEN>
<TOKEN id="token-11-15" pos="word" morph="none" start_char="1256" end_char="1262">trusted</TOKEN>
<TOKEN id="token-11-16" pos="word" morph="none" start_char="1264" end_char="1266">and</TOKEN>
<TOKEN id="token-11-17" pos="word" morph="none" start_char="1268" end_char="1273">should</TOKEN>
<TOKEN id="token-11-18" pos="word" morph="none" start_char="1275" end_char="1278">show</TOKEN>
<TOKEN id="token-11-19" pos="word" morph="none" start_char="1280" end_char="1283">that</TOKEN>
<TOKEN id="token-11-20" pos="word" morph="none" start_char="1285" end_char="1289">China</TOKEN>
<TOKEN id="token-11-21" pos="word" morph="none" start_char="1291" end_char="1292">is</TOKEN>
<TOKEN id="token-11-22" pos="word" morph="none" start_char="1294" end_char="1296">not</TOKEN>
<TOKEN id="token-11-23" pos="word" morph="none" start_char="1298" end_char="1302">ready</TOKEN>
<TOKEN id="token-11-24" pos="word" morph="none" start_char="1304" end_char="1305">to</TOKEN>
<TOKEN id="token-11-25" pos="word" morph="none" start_char="1307" end_char="1308">be</TOKEN>
<TOKEN id="token-11-26" pos="word" morph="none" start_char="1310" end_char="1319">recognized</TOKEN>
<TOKEN id="token-11-27" pos="word" morph="none" start_char="1321" end_char="1322">as</TOKEN>
<TOKEN id="token-11-28" pos="word" morph="none" start_char="1324" end_char="1325">an</TOKEN>
<TOKEN id="token-11-29" pos="word" morph="none" start_char="1327" end_char="1330">ally</TOKEN>
<TOKEN id="token-11-30" pos="word" morph="none" start_char="1332" end_char="1333">in</TOKEN>
<TOKEN id="token-11-31" pos="word" morph="none" start_char="1335" end_char="1337">any</TOKEN>
<TOKEN id="token-11-32" pos="word" morph="none" start_char="1339" end_char="1346">capacity</TOKEN>
<TOKEN id="token-11-33" pos="punct" morph="none" start_char="1347" end_char="1347">.</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1350" end_char="1377">
<ORIGINAL_TEXT>But there is some good news.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="word" morph="none" start_char="1350" end_char="1352">But</TOKEN>
<TOKEN id="token-12-1" pos="word" morph="none" start_char="1354" end_char="1358">there</TOKEN>
<TOKEN id="token-12-2" pos="word" morph="none" start_char="1360" end_char="1361">is</TOKEN>
<TOKEN id="token-12-3" pos="word" morph="none" start_char="1363" end_char="1366">some</TOKEN>
<TOKEN id="token-12-4" pos="word" morph="none" start_char="1368" end_char="1371">good</TOKEN>
<TOKEN id="token-12-5" pos="word" morph="none" start_char="1373" end_char="1376">news</TOKEN>
<TOKEN id="token-12-6" pos="punct" morph="none" start_char="1377" end_char="1377">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1379" end_char="1488">
<ORIGINAL_TEXT>Multiple companies have applied for emergency use of their vaccines which should be available early next year.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1379" end_char="1386">Multiple</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1388" end_char="1396">companies</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1398" end_char="1401">have</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1403" end_char="1409">applied</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1411" end_char="1413">for</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1415" end_char="1423">emergency</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1425" end_char="1427">use</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1429" end_char="1430">of</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1432" end_char="1436">their</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1438" end_char="1445">vaccines</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1447" end_char="1451">which</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1453" end_char="1458">should</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1460" end_char="1461">be</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="1463" end_char="1471">available</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="1473" end_char="1477">early</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="1479" end_char="1482">next</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="1484" end_char="1487">year</TOKEN>
<TOKEN id="token-13-17" pos="punct" morph="none" start_char="1488" end_char="1488">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1490" end_char="1559">
<ORIGINAL_TEXT>These vaccines are shown to be over 90% affective in preventing Covid.</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1490" end_char="1494">These</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1496" end_char="1503">vaccines</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1505" end_char="1507">are</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1509" end_char="1513">shown</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1515" end_char="1516">to</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1518" end_char="1519">be</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1521" end_char="1524">over</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1526" end_char="1527">90</TOKEN>
<TOKEN id="token-14-8" pos="punct" morph="none" start_char="1528" end_char="1528">%</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1530" end_char="1538">affective</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1540" end_char="1541">in</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1543" end_char="1552">preventing</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="1554" end_char="1558">Covid</TOKEN>
<TOKEN id="token-14-13" pos="punct" morph="none" start_char="1559" end_char="1559">.</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1561" end_char="1639">
<ORIGINAL_TEXT>After these vaccines get distributed, hopefully we can get back to normal life.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1561" end_char="1565">After</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1567" end_char="1571">these</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1573" end_char="1580">vaccines</TOKEN>
<TOKEN id="token-15-3" pos="word" morph="none" start_char="1582" end_char="1584">get</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1586" end_char="1596">distributed</TOKEN>
<TOKEN id="token-15-5" pos="punct" morph="none" start_char="1597" end_char="1597">,</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="1599" end_char="1607">hopefully</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1609" end_char="1610">we</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1612" end_char="1614">can</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="1616" end_char="1618">get</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1620" end_char="1623">back</TOKEN>
<TOKEN id="token-15-11" pos="word" morph="none" start_char="1625" end_char="1626">to</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="1628" end_char="1633">normal</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="1635" end_char="1638">life</TOKEN>
<TOKEN id="token-15-14" pos="punct" morph="none" start_char="1639" end_char="1639">.</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
