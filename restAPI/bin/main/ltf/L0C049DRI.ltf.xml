<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C049DRI" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="2518" raw_text_md5="c453a18ebdb81682193f1d33aef69704">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="73">
<ORIGINAL_TEXT>El bulo de que el coronavirus se creó en un laboratorio de Estados Unidos</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="2">El</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="4" end_char="7">bulo</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="9" end_char="10">de</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="12" end_char="14">que</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="16" end_char="17">el</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="19" end_char="29">coronavirus</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="31" end_char="32">se</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="34" end_char="37">creó</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="39" end_char="40">en</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="42" end_char="43">un</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="45" end_char="55">laboratorio</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="57" end_char="58">de</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="60" end_char="66">Estados</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="68" end_char="73">Unidos</TOKEN>
</SEG>
<SEG id="segment-1" start_char="77" end_char="135">
<ORIGINAL_TEXT>Esto de los bulos sobre el coronavirus parece no tener fin.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="77" end_char="80">Esto</TOKEN>
<TOKEN id="token-1-1" pos="word" morph="none" start_char="82" end_char="83">de</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="85" end_char="87">los</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="89" end_char="93">bulos</TOKEN>
<TOKEN id="token-1-4" pos="word" morph="none" start_char="95" end_char="99">sobre</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="101" end_char="102">el</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="104" end_char="114">coronavirus</TOKEN>
<TOKEN id="token-1-7" pos="word" morph="none" start_char="116" end_char="121">parece</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="123" end_char="124">no</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="126" end_char="130">tener</TOKEN>
<TOKEN id="token-1-10" pos="word" morph="none" start_char="132" end_char="134">fin</TOKEN>
<TOKEN id="token-1-11" pos="punct" morph="none" start_char="135" end_char="135">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="137" end_char="256">
<ORIGINAL_TEXT>Casi cada día aparece alguno que nos pone sobre aviso y al que tenemos que desenmascarar para que no engañe a más gente.</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="137" end_char="140">Casi</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="142" end_char="145">cada</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="147" end_char="149">día</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="151" end_char="157">aparece</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="159" end_char="164">alguno</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="166" end_char="168">que</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="170" end_char="172">nos</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="174" end_char="177">pone</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="179" end_char="183">sobre</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="185" end_char="189">aviso</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="191" end_char="191">y</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="193" end_char="194">al</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="196" end_char="198">que</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="200" end_char="206">tenemos</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="208" end_char="210">que</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="212" end_char="224">desenmascarar</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="226" end_char="229">para</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="231" end_char="233">que</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="235" end_char="236">no</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="238" end_char="243">engañe</TOKEN>
<TOKEN id="token-2-20" pos="word" morph="none" start_char="245" end_char="245">a</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="247" end_char="249">más</TOKEN>
<TOKEN id="token-2-22" pos="word" morph="none" start_char="251" end_char="255">gente</TOKEN>
<TOKEN id="token-2-23" pos="punct" morph="none" start_char="256" end_char="256">.</TOKEN>
</SEG>
<SEG id="segment-3" start_char="258" end_char="295">
<ORIGINAL_TEXT>A veces resulta complicado, porque las</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="258" end_char="258">A</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="260" end_char="264">veces</TOKEN>
<TOKEN id="token-3-2" pos="word" morph="none" start_char="266" end_char="272">resulta</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="274" end_char="283">complicado</TOKEN>
<TOKEN id="token-3-4" pos="punct" morph="none" start_char="284" end_char="284">,</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="286" end_char="291">porque</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="293" end_char="295">las</TOKEN>
</SEG>
<SEG id="segment-4" start_char="298" end_char="306">
<ORIGINAL_TEXT>fake news</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="298" end_char="301">fake</TOKEN>
<TOKEN id="token-4-1" pos="word" morph="none" start_char="303" end_char="306">news</TOKEN>
</SEG>
<SEG id="segment-5" start_char="309" end_char="386">
<ORIGINAL_TEXT>en cuestión se sirven de algunos datos verdaderos para despistar a más de uno.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="309" end_char="310">en</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="312" end_char="319">cuestión</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="321" end_char="322">se</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="324" end_char="329">sirven</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="331" end_char="332">de</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="334" end_char="340">algunos</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="342" end_char="346">datos</TOKEN>
<TOKEN id="token-5-7" pos="word" morph="none" start_char="348" end_char="357">verdaderos</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="359" end_char="362">para</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="364" end_char="372">despistar</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="374" end_char="374">a</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="376" end_char="378">más</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="380" end_char="381">de</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="383" end_char="385">uno</TOKEN>
<TOKEN id="token-5-14" pos="punct" morph="none" start_char="386" end_char="386">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="388" end_char="612">
<ORIGINAL_TEXT>Suerte que hay medios de comunicación e instituciones como la Guardia Civil y la Policía Nacional que nos alertan de estas mentiras que, día sí y día también, inundan nuestras redes sociales y de las que podemos ser víctimas.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="388" end_char="393">Suerte</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="395" end_char="397">que</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="399" end_char="401">hay</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="403" end_char="408">medios</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="410" end_char="411">de</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="413" end_char="424">comunicación</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="426" end_char="426">e</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="428" end_char="440">instituciones</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="442" end_char="445">como</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="447" end_char="448">la</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="450" end_char="456">Guardia</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="458" end_char="462">Civil</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="464" end_char="464">y</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="466" end_char="467">la</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="469" end_char="475">Policía</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="477" end_char="484">Nacional</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="486" end_char="488">que</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="490" end_char="492">nos</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="494" end_char="500">alertan</TOKEN>
<TOKEN id="token-6-19" pos="word" morph="none" start_char="502" end_char="503">de</TOKEN>
<TOKEN id="token-6-20" pos="word" morph="none" start_char="505" end_char="509">estas</TOKEN>
<TOKEN id="token-6-21" pos="word" morph="none" start_char="511" end_char="518">mentiras</TOKEN>
<TOKEN id="token-6-22" pos="word" morph="none" start_char="520" end_char="522">que</TOKEN>
<TOKEN id="token-6-23" pos="punct" morph="none" start_char="523" end_char="523">,</TOKEN>
<TOKEN id="token-6-24" pos="word" morph="none" start_char="525" end_char="527">día</TOKEN>
<TOKEN id="token-6-25" pos="word" morph="none" start_char="529" end_char="530">sí</TOKEN>
<TOKEN id="token-6-26" pos="word" morph="none" start_char="532" end_char="532">y</TOKEN>
<TOKEN id="token-6-27" pos="word" morph="none" start_char="534" end_char="536">día</TOKEN>
<TOKEN id="token-6-28" pos="word" morph="none" start_char="538" end_char="544">también</TOKEN>
<TOKEN id="token-6-29" pos="punct" morph="none" start_char="545" end_char="545">,</TOKEN>
<TOKEN id="token-6-30" pos="word" morph="none" start_char="547" end_char="553">inundan</TOKEN>
<TOKEN id="token-6-31" pos="word" morph="none" start_char="555" end_char="562">nuestras</TOKEN>
<TOKEN id="token-6-32" pos="word" morph="none" start_char="564" end_char="568">redes</TOKEN>
<TOKEN id="token-6-33" pos="word" morph="none" start_char="570" end_char="577">sociales</TOKEN>
<TOKEN id="token-6-34" pos="word" morph="none" start_char="579" end_char="579">y</TOKEN>
<TOKEN id="token-6-35" pos="word" morph="none" start_char="581" end_char="582">de</TOKEN>
<TOKEN id="token-6-36" pos="word" morph="none" start_char="584" end_char="586">las</TOKEN>
<TOKEN id="token-6-37" pos="word" morph="none" start_char="588" end_char="590">que</TOKEN>
<TOKEN id="token-6-38" pos="word" morph="none" start_char="592" end_char="598">podemos</TOKEN>
<TOKEN id="token-6-39" pos="word" morph="none" start_char="600" end_char="602">ser</TOKEN>
<TOKEN id="token-6-40" pos="word" morph="none" start_char="604" end_char="611">víctimas</TOKEN>
<TOKEN id="token-6-41" pos="punct" morph="none" start_char="612" end_char="612">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="615" end_char="848">
<ORIGINAL_TEXT>Corre desde hace unos días una noticia falsa que asegura que el Covid-19 fue creada en un laboratorio de Fort Detrick (Estados Unidos) con el objetivo de frenar el desarrollo de la tecnología 5G y para acabar con las personas mayores.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="615" end_char="619">Corre</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="621" end_char="625">desde</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="627" end_char="630">hace</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="632" end_char="635">unos</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="637" end_char="640">días</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="642" end_char="644">una</TOKEN>
<TOKEN id="token-7-6" pos="word" morph="none" start_char="646" end_char="652">noticia</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="654" end_char="658">falsa</TOKEN>
<TOKEN id="token-7-8" pos="word" morph="none" start_char="660" end_char="662">que</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="664" end_char="670">asegura</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="672" end_char="674">que</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="676" end_char="677">el</TOKEN>
<TOKEN id="token-7-12" pos="unknown" morph="none" start_char="679" end_char="686">Covid-19</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="688" end_char="690">fue</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="692" end_char="697">creada</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="699" end_char="700">en</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="702" end_char="703">un</TOKEN>
<TOKEN id="token-7-17" pos="word" morph="none" start_char="705" end_char="715">laboratorio</TOKEN>
<TOKEN id="token-7-18" pos="word" morph="none" start_char="717" end_char="718">de</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="720" end_char="723">Fort</TOKEN>
<TOKEN id="token-7-20" pos="word" morph="none" start_char="725" end_char="731">Detrick</TOKEN>
<TOKEN id="token-7-21" pos="punct" morph="none" start_char="733" end_char="733">(</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="734" end_char="740">Estados</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="742" end_char="747">Unidos</TOKEN>
<TOKEN id="token-7-24" pos="punct" morph="none" start_char="748" end_char="748">)</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="750" end_char="752">con</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="754" end_char="755">el</TOKEN>
<TOKEN id="token-7-27" pos="word" morph="none" start_char="757" end_char="764">objetivo</TOKEN>
<TOKEN id="token-7-28" pos="word" morph="none" start_char="766" end_char="767">de</TOKEN>
<TOKEN id="token-7-29" pos="word" morph="none" start_char="769" end_char="774">frenar</TOKEN>
<TOKEN id="token-7-30" pos="word" morph="none" start_char="776" end_char="777">el</TOKEN>
<TOKEN id="token-7-31" pos="word" morph="none" start_char="779" end_char="788">desarrollo</TOKEN>
<TOKEN id="token-7-32" pos="word" morph="none" start_char="790" end_char="791">de</TOKEN>
<TOKEN id="token-7-33" pos="word" morph="none" start_char="793" end_char="794">la</TOKEN>
<TOKEN id="token-7-34" pos="word" morph="none" start_char="796" end_char="805">tecnología</TOKEN>
<TOKEN id="token-7-35" pos="word" morph="none" start_char="807" end_char="808">5G</TOKEN>
<TOKEN id="token-7-36" pos="word" morph="none" start_char="810" end_char="810">y</TOKEN>
<TOKEN id="token-7-37" pos="word" morph="none" start_char="812" end_char="815">para</TOKEN>
<TOKEN id="token-7-38" pos="word" morph="none" start_char="817" end_char="822">acabar</TOKEN>
<TOKEN id="token-7-39" pos="word" morph="none" start_char="824" end_char="826">con</TOKEN>
<TOKEN id="token-7-40" pos="word" morph="none" start_char="828" end_char="830">las</TOKEN>
<TOKEN id="token-7-41" pos="word" morph="none" start_char="832" end_char="839">personas</TOKEN>
<TOKEN id="token-7-42" pos="word" morph="none" start_char="841" end_char="847">mayores</TOKEN>
<TOKEN id="token-7-43" pos="punct" morph="none" start_char="848" end_char="848">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="850" end_char="862">
<ORIGINAL_TEXT>¡Menudo bulo!</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="punct" morph="none" start_char="850" end_char="850">¡</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="851" end_char="856">Menudo</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="858" end_char="861">bulo</TOKEN>
<TOKEN id="token-8-3" pos="punct" morph="none" start_char="862" end_char="862">!</TOKEN>
</SEG>
<SEG id="segment-9" start_char="864" end_char="953">
<ORIGINAL_TEXT>No es más que otra teoría de la conspiración que no aporta ninguna prueba que la confirme.</ORIGINAL_TEXT>
<TOKEN id="token-9-0" pos="word" morph="none" start_char="864" end_char="865">No</TOKEN>
<TOKEN id="token-9-1" pos="word" morph="none" start_char="867" end_char="868">es</TOKEN>
<TOKEN id="token-9-2" pos="word" morph="none" start_char="870" end_char="872">más</TOKEN>
<TOKEN id="token-9-3" pos="word" morph="none" start_char="874" end_char="876">que</TOKEN>
<TOKEN id="token-9-4" pos="word" morph="none" start_char="878" end_char="881">otra</TOKEN>
<TOKEN id="token-9-5" pos="word" morph="none" start_char="883" end_char="888">teoría</TOKEN>
<TOKEN id="token-9-6" pos="word" morph="none" start_char="890" end_char="891">de</TOKEN>
<TOKEN id="token-9-7" pos="word" morph="none" start_char="893" end_char="894">la</TOKEN>
<TOKEN id="token-9-8" pos="word" morph="none" start_char="896" end_char="907">conspiración</TOKEN>
<TOKEN id="token-9-9" pos="word" morph="none" start_char="909" end_char="911">que</TOKEN>
<TOKEN id="token-9-10" pos="word" morph="none" start_char="913" end_char="914">no</TOKEN>
<TOKEN id="token-9-11" pos="word" morph="none" start_char="916" end_char="921">aporta</TOKEN>
<TOKEN id="token-9-12" pos="word" morph="none" start_char="923" end_char="929">ninguna</TOKEN>
<TOKEN id="token-9-13" pos="word" morph="none" start_char="931" end_char="936">prueba</TOKEN>
<TOKEN id="token-9-14" pos="word" morph="none" start_char="938" end_char="940">que</TOKEN>
<TOKEN id="token-9-15" pos="word" morph="none" start_char="942" end_char="943">la</TOKEN>
<TOKEN id="token-9-16" pos="word" morph="none" start_char="945" end_char="952">confirme</TOKEN>
<TOKEN id="token-9-17" pos="punct" morph="none" start_char="953" end_char="953">.</TOKEN>
</SEG>
<SEG id="segment-10" start_char="956" end_char="1009">
<ORIGINAL_TEXT>Esta en concreto ha salido publicada en el blog online</ORIGINAL_TEXT>
<TOKEN id="token-10-0" pos="word" morph="none" start_char="956" end_char="959">Esta</TOKEN>
<TOKEN id="token-10-1" pos="word" morph="none" start_char="961" end_char="962">en</TOKEN>
<TOKEN id="token-10-2" pos="word" morph="none" start_char="964" end_char="971">concreto</TOKEN>
<TOKEN id="token-10-3" pos="word" morph="none" start_char="973" end_char="974">ha</TOKEN>
<TOKEN id="token-10-4" pos="word" morph="none" start_char="976" end_char="981">salido</TOKEN>
<TOKEN id="token-10-5" pos="word" morph="none" start_char="983" end_char="991">publicada</TOKEN>
<TOKEN id="token-10-6" pos="word" morph="none" start_char="993" end_char="994">en</TOKEN>
<TOKEN id="token-10-7" pos="word" morph="none" start_char="996" end_char="997">el</TOKEN>
<TOKEN id="token-10-8" pos="word" morph="none" start_char="999" end_char="1002">blog</TOKEN>
<TOKEN id="token-10-9" pos="word" morph="none" start_char="1004" end_char="1009">online</TOKEN>
</SEG>
<SEG id="segment-11" start_char="1012" end_char="1034">
<ORIGINAL_TEXT>Kit Radio Internacional</ORIGINAL_TEXT>
<TOKEN id="token-11-0" pos="word" morph="none" start_char="1012" end_char="1014">Kit</TOKEN>
<TOKEN id="token-11-1" pos="word" morph="none" start_char="1016" end_char="1020">Radio</TOKEN>
<TOKEN id="token-11-2" pos="word" morph="none" start_char="1022" end_char="1034">Internacional</TOKEN>
</SEG>
<SEG id="segment-12" start_char="1037" end_char="1037">
<ORIGINAL_TEXT>.</ORIGINAL_TEXT>
<TOKEN id="token-12-0" pos="punct" morph="none" start_char="1037" end_char="1037">.</TOKEN>
</SEG>
<SEG id="segment-13" start_char="1039" end_char="1216">
<ORIGINAL_TEXT>Y lo sabemos porque científicos de varios países han analizado los genomas del agente que causa el virus y han llegado a la conclusión de que su origen está en la vida silvestre.</ORIGINAL_TEXT>
<TOKEN id="token-13-0" pos="word" morph="none" start_char="1039" end_char="1039">Y</TOKEN>
<TOKEN id="token-13-1" pos="word" morph="none" start_char="1041" end_char="1042">lo</TOKEN>
<TOKEN id="token-13-2" pos="word" morph="none" start_char="1044" end_char="1050">sabemos</TOKEN>
<TOKEN id="token-13-3" pos="word" morph="none" start_char="1052" end_char="1057">porque</TOKEN>
<TOKEN id="token-13-4" pos="word" morph="none" start_char="1059" end_char="1069">científicos</TOKEN>
<TOKEN id="token-13-5" pos="word" morph="none" start_char="1071" end_char="1072">de</TOKEN>
<TOKEN id="token-13-6" pos="word" morph="none" start_char="1074" end_char="1079">varios</TOKEN>
<TOKEN id="token-13-7" pos="word" morph="none" start_char="1081" end_char="1086">países</TOKEN>
<TOKEN id="token-13-8" pos="word" morph="none" start_char="1088" end_char="1090">han</TOKEN>
<TOKEN id="token-13-9" pos="word" morph="none" start_char="1092" end_char="1100">analizado</TOKEN>
<TOKEN id="token-13-10" pos="word" morph="none" start_char="1102" end_char="1104">los</TOKEN>
<TOKEN id="token-13-11" pos="word" morph="none" start_char="1106" end_char="1112">genomas</TOKEN>
<TOKEN id="token-13-12" pos="word" morph="none" start_char="1114" end_char="1116">del</TOKEN>
<TOKEN id="token-13-13" pos="word" morph="none" start_char="1118" end_char="1123">agente</TOKEN>
<TOKEN id="token-13-14" pos="word" morph="none" start_char="1125" end_char="1127">que</TOKEN>
<TOKEN id="token-13-15" pos="word" morph="none" start_char="1129" end_char="1133">causa</TOKEN>
<TOKEN id="token-13-16" pos="word" morph="none" start_char="1135" end_char="1136">el</TOKEN>
<TOKEN id="token-13-17" pos="word" morph="none" start_char="1138" end_char="1142">virus</TOKEN>
<TOKEN id="token-13-18" pos="word" morph="none" start_char="1144" end_char="1144">y</TOKEN>
<TOKEN id="token-13-19" pos="word" morph="none" start_char="1146" end_char="1148">han</TOKEN>
<TOKEN id="token-13-20" pos="word" morph="none" start_char="1150" end_char="1156">llegado</TOKEN>
<TOKEN id="token-13-21" pos="word" morph="none" start_char="1158" end_char="1158">a</TOKEN>
<TOKEN id="token-13-22" pos="word" morph="none" start_char="1160" end_char="1161">la</TOKEN>
<TOKEN id="token-13-23" pos="word" morph="none" start_char="1163" end_char="1172">conclusión</TOKEN>
<TOKEN id="token-13-24" pos="word" morph="none" start_char="1174" end_char="1175">de</TOKEN>
<TOKEN id="token-13-25" pos="word" morph="none" start_char="1177" end_char="1179">que</TOKEN>
<TOKEN id="token-13-26" pos="word" morph="none" start_char="1181" end_char="1182">su</TOKEN>
<TOKEN id="token-13-27" pos="word" morph="none" start_char="1184" end_char="1189">origen</TOKEN>
<TOKEN id="token-13-28" pos="word" morph="none" start_char="1191" end_char="1194">está</TOKEN>
<TOKEN id="token-13-29" pos="word" morph="none" start_char="1196" end_char="1197">en</TOKEN>
<TOKEN id="token-13-30" pos="word" morph="none" start_char="1199" end_char="1200">la</TOKEN>
<TOKEN id="token-13-31" pos="word" morph="none" start_char="1202" end_char="1205">vida</TOKEN>
<TOKEN id="token-13-32" pos="word" morph="none" start_char="1207" end_char="1215">silvestre</TOKEN>
<TOKEN id="token-13-33" pos="punct" morph="none" start_char="1216" end_char="1216">.</TOKEN>
</SEG>
<SEG id="segment-14" start_char="1219" end_char="1289">
<ORIGINAL_TEXT>El bulo de que el coronavirus salió de un laboratorio de Estados Unidos</ORIGINAL_TEXT>
<TOKEN id="token-14-0" pos="word" morph="none" start_char="1219" end_char="1220">El</TOKEN>
<TOKEN id="token-14-1" pos="word" morph="none" start_char="1222" end_char="1225">bulo</TOKEN>
<TOKEN id="token-14-2" pos="word" morph="none" start_char="1227" end_char="1228">de</TOKEN>
<TOKEN id="token-14-3" pos="word" morph="none" start_char="1230" end_char="1232">que</TOKEN>
<TOKEN id="token-14-4" pos="word" morph="none" start_char="1234" end_char="1235">el</TOKEN>
<TOKEN id="token-14-5" pos="word" morph="none" start_char="1237" end_char="1247">coronavirus</TOKEN>
<TOKEN id="token-14-6" pos="word" morph="none" start_char="1249" end_char="1253">salió</TOKEN>
<TOKEN id="token-14-7" pos="word" morph="none" start_char="1255" end_char="1256">de</TOKEN>
<TOKEN id="token-14-8" pos="word" morph="none" start_char="1258" end_char="1259">un</TOKEN>
<TOKEN id="token-14-9" pos="word" morph="none" start_char="1261" end_char="1271">laboratorio</TOKEN>
<TOKEN id="token-14-10" pos="word" morph="none" start_char="1273" end_char="1274">de</TOKEN>
<TOKEN id="token-14-11" pos="word" morph="none" start_char="1276" end_char="1282">Estados</TOKEN>
<TOKEN id="token-14-12" pos="word" morph="none" start_char="1284" end_char="1289">Unidos</TOKEN>
</SEG>
<SEG id="segment-15" start_char="1293" end_char="1487">
<ORIGINAL_TEXT>Albert Bosch Navarro, presidente de la Sociedad Española de Virología, se muestra tajante al respecto: "Siempre que aparece una nueva epidemia se oyen rumores sobre su creación en un laboratorio.</ORIGINAL_TEXT>
<TOKEN id="token-15-0" pos="word" morph="none" start_char="1293" end_char="1298">Albert</TOKEN>
<TOKEN id="token-15-1" pos="word" morph="none" start_char="1300" end_char="1304">Bosch</TOKEN>
<TOKEN id="token-15-2" pos="word" morph="none" start_char="1306" end_char="1312">Navarro</TOKEN>
<TOKEN id="token-15-3" pos="punct" morph="none" start_char="1313" end_char="1313">,</TOKEN>
<TOKEN id="token-15-4" pos="word" morph="none" start_char="1315" end_char="1324">presidente</TOKEN>
<TOKEN id="token-15-5" pos="word" morph="none" start_char="1326" end_char="1327">de</TOKEN>
<TOKEN id="token-15-6" pos="word" morph="none" start_char="1329" end_char="1330">la</TOKEN>
<TOKEN id="token-15-7" pos="word" morph="none" start_char="1332" end_char="1339">Sociedad</TOKEN>
<TOKEN id="token-15-8" pos="word" morph="none" start_char="1341" end_char="1348">Española</TOKEN>
<TOKEN id="token-15-9" pos="word" morph="none" start_char="1350" end_char="1351">de</TOKEN>
<TOKEN id="token-15-10" pos="word" morph="none" start_char="1353" end_char="1361">Virología</TOKEN>
<TOKEN id="token-15-11" pos="punct" morph="none" start_char="1362" end_char="1362">,</TOKEN>
<TOKEN id="token-15-12" pos="word" morph="none" start_char="1364" end_char="1365">se</TOKEN>
<TOKEN id="token-15-13" pos="word" morph="none" start_char="1367" end_char="1373">muestra</TOKEN>
<TOKEN id="token-15-14" pos="word" morph="none" start_char="1375" end_char="1381">tajante</TOKEN>
<TOKEN id="token-15-15" pos="word" morph="none" start_char="1383" end_char="1384">al</TOKEN>
<TOKEN id="token-15-16" pos="word" morph="none" start_char="1386" end_char="1393">respecto</TOKEN>
<TOKEN id="token-15-17" pos="punct" morph="none" start_char="1394" end_char="1394">:</TOKEN>
<TOKEN id="token-15-18" pos="punct" morph="none" start_char="1396" end_char="1396">"</TOKEN>
<TOKEN id="token-15-19" pos="word" morph="none" start_char="1397" end_char="1403">Siempre</TOKEN>
<TOKEN id="token-15-20" pos="word" morph="none" start_char="1405" end_char="1407">que</TOKEN>
<TOKEN id="token-15-21" pos="word" morph="none" start_char="1409" end_char="1415">aparece</TOKEN>
<TOKEN id="token-15-22" pos="word" morph="none" start_char="1417" end_char="1419">una</TOKEN>
<TOKEN id="token-15-23" pos="word" morph="none" start_char="1421" end_char="1425">nueva</TOKEN>
<TOKEN id="token-15-24" pos="word" morph="none" start_char="1427" end_char="1434">epidemia</TOKEN>
<TOKEN id="token-15-25" pos="word" morph="none" start_char="1436" end_char="1437">se</TOKEN>
<TOKEN id="token-15-26" pos="word" morph="none" start_char="1439" end_char="1442">oyen</TOKEN>
<TOKEN id="token-15-27" pos="word" morph="none" start_char="1444" end_char="1450">rumores</TOKEN>
<TOKEN id="token-15-28" pos="word" morph="none" start_char="1452" end_char="1456">sobre</TOKEN>
<TOKEN id="token-15-29" pos="word" morph="none" start_char="1458" end_char="1459">su</TOKEN>
<TOKEN id="token-15-30" pos="word" morph="none" start_char="1461" end_char="1468">creación</TOKEN>
<TOKEN id="token-15-31" pos="word" morph="none" start_char="1470" end_char="1471">en</TOKEN>
<TOKEN id="token-15-32" pos="word" morph="none" start_char="1473" end_char="1474">un</TOKEN>
<TOKEN id="token-15-33" pos="word" morph="none" start_char="1476" end_char="1486">laboratorio</TOKEN>
<TOKEN id="token-15-34" pos="punct" morph="none" start_char="1487" end_char="1487">.</TOKEN>
</SEG>
<SEG id="segment-16" start_char="1489" end_char="1568">
<ORIGINAL_TEXT>Sucedió en el 2002 con el SARS, en el 2009 con el H1N1 y ahora con la Covid-19".</ORIGINAL_TEXT>
<TOKEN id="token-16-0" pos="word" morph="none" start_char="1489" end_char="1495">Sucedió</TOKEN>
<TOKEN id="token-16-1" pos="word" morph="none" start_char="1497" end_char="1498">en</TOKEN>
<TOKEN id="token-16-2" pos="word" morph="none" start_char="1500" end_char="1501">el</TOKEN>
<TOKEN id="token-16-3" pos="word" morph="none" start_char="1503" end_char="1506">2002</TOKEN>
<TOKEN id="token-16-4" pos="word" morph="none" start_char="1508" end_char="1510">con</TOKEN>
<TOKEN id="token-16-5" pos="word" morph="none" start_char="1512" end_char="1513">el</TOKEN>
<TOKEN id="token-16-6" pos="word" morph="none" start_char="1515" end_char="1518">SARS</TOKEN>
<TOKEN id="token-16-7" pos="punct" morph="none" start_char="1519" end_char="1519">,</TOKEN>
<TOKEN id="token-16-8" pos="word" morph="none" start_char="1521" end_char="1522">en</TOKEN>
<TOKEN id="token-16-9" pos="word" morph="none" start_char="1524" end_char="1525">el</TOKEN>
<TOKEN id="token-16-10" pos="word" morph="none" start_char="1527" end_char="1530">2009</TOKEN>
<TOKEN id="token-16-11" pos="word" morph="none" start_char="1532" end_char="1534">con</TOKEN>
<TOKEN id="token-16-12" pos="word" morph="none" start_char="1536" end_char="1537">el</TOKEN>
<TOKEN id="token-16-13" pos="word" morph="none" start_char="1539" end_char="1542">H1N1</TOKEN>
<TOKEN id="token-16-14" pos="word" morph="none" start_char="1544" end_char="1544">y</TOKEN>
<TOKEN id="token-16-15" pos="word" morph="none" start_char="1546" end_char="1550">ahora</TOKEN>
<TOKEN id="token-16-16" pos="word" morph="none" start_char="1552" end_char="1554">con</TOKEN>
<TOKEN id="token-16-17" pos="word" morph="none" start_char="1556" end_char="1557">la</TOKEN>
<TOKEN id="token-16-18" pos="unknown" morph="none" start_char="1559" end_char="1566">Covid-19</TOKEN>
<TOKEN id="token-16-19" pos="punct" morph="none" start_char="1567" end_char="1568">".</TOKEN>
</SEG>
<SEG id="segment-17" start_char="1571" end_char="1632">
<ORIGINAL_TEXT>Artículos publicados en prestigiosas revistas científicas como</ORIGINAL_TEXT>
<TOKEN id="token-17-0" pos="word" morph="none" start_char="1571" end_char="1579">Artículos</TOKEN>
<TOKEN id="token-17-1" pos="word" morph="none" start_char="1581" end_char="1590">publicados</TOKEN>
<TOKEN id="token-17-2" pos="word" morph="none" start_char="1592" end_char="1593">en</TOKEN>
<TOKEN id="token-17-3" pos="word" morph="none" start_char="1595" end_char="1606">prestigiosas</TOKEN>
<TOKEN id="token-17-4" pos="word" morph="none" start_char="1608" end_char="1615">revistas</TOKEN>
<TOKEN id="token-17-5" pos="word" morph="none" start_char="1617" end_char="1627">científicas</TOKEN>
<TOKEN id="token-17-6" pos="word" morph="none" start_char="1629" end_char="1632">como</TOKEN>
</SEG>
<SEG id="segment-18" start_char="1635" end_char="1649">
<ORIGINAL_TEXT>Nature Medicine</ORIGINAL_TEXT>
<TOKEN id="token-18-0" pos="word" morph="none" start_char="1635" end_char="1640">Nature</TOKEN>
<TOKEN id="token-18-1" pos="word" morph="none" start_char="1642" end_char="1649">Medicine</TOKEN>
</SEG>
<SEG id="segment-19" start_char="1652" end_char="1652">
<ORIGINAL_TEXT>y</ORIGINAL_TEXT>
<TOKEN id="token-19-0" pos="word" morph="none" start_char="1652" end_char="1652">y</TOKEN>
</SEG>
<SEG id="segment-20" start_char="1655" end_char="1664">
<ORIGINAL_TEXT>The Lancet</ORIGINAL_TEXT>
<TOKEN id="token-20-0" pos="word" morph="none" start_char="1655" end_char="1657">The</TOKEN>
<TOKEN id="token-20-1" pos="word" morph="none" start_char="1659" end_char="1664">Lancet</TOKEN>
</SEG>
<SEG id="segment-21" start_char="1667" end_char="1804">
<ORIGINAL_TEXT>confirman las palabras de Bosch al sostener que ninguno de estos virus ha sido creado en un laboratorio ni ha sido manipulado a propósito.</ORIGINAL_TEXT>
<TOKEN id="token-21-0" pos="word" morph="none" start_char="1667" end_char="1675">confirman</TOKEN>
<TOKEN id="token-21-1" pos="word" morph="none" start_char="1677" end_char="1679">las</TOKEN>
<TOKEN id="token-21-2" pos="word" morph="none" start_char="1681" end_char="1688">palabras</TOKEN>
<TOKEN id="token-21-3" pos="word" morph="none" start_char="1690" end_char="1691">de</TOKEN>
<TOKEN id="token-21-4" pos="word" morph="none" start_char="1693" end_char="1697">Bosch</TOKEN>
<TOKEN id="token-21-5" pos="word" morph="none" start_char="1699" end_char="1700">al</TOKEN>
<TOKEN id="token-21-6" pos="word" morph="none" start_char="1702" end_char="1709">sostener</TOKEN>
<TOKEN id="token-21-7" pos="word" morph="none" start_char="1711" end_char="1713">que</TOKEN>
<TOKEN id="token-21-8" pos="word" morph="none" start_char="1715" end_char="1721">ninguno</TOKEN>
<TOKEN id="token-21-9" pos="word" morph="none" start_char="1723" end_char="1724">de</TOKEN>
<TOKEN id="token-21-10" pos="word" morph="none" start_char="1726" end_char="1730">estos</TOKEN>
<TOKEN id="token-21-11" pos="word" morph="none" start_char="1732" end_char="1736">virus</TOKEN>
<TOKEN id="token-21-12" pos="word" morph="none" start_char="1738" end_char="1739">ha</TOKEN>
<TOKEN id="token-21-13" pos="word" morph="none" start_char="1741" end_char="1744">sido</TOKEN>
<TOKEN id="token-21-14" pos="word" morph="none" start_char="1746" end_char="1751">creado</TOKEN>
<TOKEN id="token-21-15" pos="word" morph="none" start_char="1753" end_char="1754">en</TOKEN>
<TOKEN id="token-21-16" pos="word" morph="none" start_char="1756" end_char="1757">un</TOKEN>
<TOKEN id="token-21-17" pos="word" morph="none" start_char="1759" end_char="1769">laboratorio</TOKEN>
<TOKEN id="token-21-18" pos="word" morph="none" start_char="1771" end_char="1772">ni</TOKEN>
<TOKEN id="token-21-19" pos="word" morph="none" start_char="1774" end_char="1775">ha</TOKEN>
<TOKEN id="token-21-20" pos="word" morph="none" start_char="1777" end_char="1780">sido</TOKEN>
<TOKEN id="token-21-21" pos="word" morph="none" start_char="1782" end_char="1791">manipulado</TOKEN>
<TOKEN id="token-21-22" pos="word" morph="none" start_char="1793" end_char="1793">a</TOKEN>
<TOKEN id="token-21-23" pos="word" morph="none" start_char="1795" end_char="1803">propósito</TOKEN>
<TOKEN id="token-21-24" pos="punct" morph="none" start_char="1804" end_char="1804">.</TOKEN>
</SEG>
<SEG id="segment-22" start_char="1807" end_char="2115">
<ORIGINAL_TEXT>En lo que respecta al objetivo de acabar con las personas mayores, este bulo adquiere tintes dramáticos, ya que hace referencia a una supuesta cita de la expresidenta del Fondo Monetario Internacional, Christine Lagarde, según la cual "los ancianos viven demasiado y eso es un riesgo para la economía global".</ORIGINAL_TEXT>
<TOKEN id="token-22-0" pos="word" morph="none" start_char="1807" end_char="1808">En</TOKEN>
<TOKEN id="token-22-1" pos="word" morph="none" start_char="1810" end_char="1811">lo</TOKEN>
<TOKEN id="token-22-2" pos="word" morph="none" start_char="1813" end_char="1815">que</TOKEN>
<TOKEN id="token-22-3" pos="word" morph="none" start_char="1817" end_char="1824">respecta</TOKEN>
<TOKEN id="token-22-4" pos="word" morph="none" start_char="1826" end_char="1827">al</TOKEN>
<TOKEN id="token-22-5" pos="word" morph="none" start_char="1829" end_char="1836">objetivo</TOKEN>
<TOKEN id="token-22-6" pos="word" morph="none" start_char="1838" end_char="1839">de</TOKEN>
<TOKEN id="token-22-7" pos="word" morph="none" start_char="1841" end_char="1846">acabar</TOKEN>
<TOKEN id="token-22-8" pos="word" morph="none" start_char="1848" end_char="1850">con</TOKEN>
<TOKEN id="token-22-9" pos="word" morph="none" start_char="1852" end_char="1854">las</TOKEN>
<TOKEN id="token-22-10" pos="word" morph="none" start_char="1856" end_char="1863">personas</TOKEN>
<TOKEN id="token-22-11" pos="word" morph="none" start_char="1865" end_char="1871">mayores</TOKEN>
<TOKEN id="token-22-12" pos="punct" morph="none" start_char="1872" end_char="1872">,</TOKEN>
<TOKEN id="token-22-13" pos="word" morph="none" start_char="1874" end_char="1877">este</TOKEN>
<TOKEN id="token-22-14" pos="word" morph="none" start_char="1879" end_char="1882">bulo</TOKEN>
<TOKEN id="token-22-15" pos="word" morph="none" start_char="1884" end_char="1891">adquiere</TOKEN>
<TOKEN id="token-22-16" pos="word" morph="none" start_char="1893" end_char="1898">tintes</TOKEN>
<TOKEN id="token-22-17" pos="word" morph="none" start_char="1900" end_char="1909">dramáticos</TOKEN>
<TOKEN id="token-22-18" pos="punct" morph="none" start_char="1910" end_char="1910">,</TOKEN>
<TOKEN id="token-22-19" pos="word" morph="none" start_char="1912" end_char="1913">ya</TOKEN>
<TOKEN id="token-22-20" pos="word" morph="none" start_char="1915" end_char="1917">que</TOKEN>
<TOKEN id="token-22-21" pos="word" morph="none" start_char="1919" end_char="1922">hace</TOKEN>
<TOKEN id="token-22-22" pos="word" morph="none" start_char="1924" end_char="1933">referencia</TOKEN>
<TOKEN id="token-22-23" pos="word" morph="none" start_char="1935" end_char="1935">a</TOKEN>
<TOKEN id="token-22-24" pos="word" morph="none" start_char="1937" end_char="1939">una</TOKEN>
<TOKEN id="token-22-25" pos="word" morph="none" start_char="1941" end_char="1948">supuesta</TOKEN>
<TOKEN id="token-22-26" pos="word" morph="none" start_char="1950" end_char="1953">cita</TOKEN>
<TOKEN id="token-22-27" pos="word" morph="none" start_char="1955" end_char="1956">de</TOKEN>
<TOKEN id="token-22-28" pos="word" morph="none" start_char="1958" end_char="1959">la</TOKEN>
<TOKEN id="token-22-29" pos="word" morph="none" start_char="1961" end_char="1972">expresidenta</TOKEN>
<TOKEN id="token-22-30" pos="word" morph="none" start_char="1974" end_char="1976">del</TOKEN>
<TOKEN id="token-22-31" pos="word" morph="none" start_char="1978" end_char="1982">Fondo</TOKEN>
<TOKEN id="token-22-32" pos="word" morph="none" start_char="1984" end_char="1992">Monetario</TOKEN>
<TOKEN id="token-22-33" pos="word" morph="none" start_char="1994" end_char="2006">Internacional</TOKEN>
<TOKEN id="token-22-34" pos="punct" morph="none" start_char="2007" end_char="2007">,</TOKEN>
<TOKEN id="token-22-35" pos="word" morph="none" start_char="2009" end_char="2017">Christine</TOKEN>
<TOKEN id="token-22-36" pos="word" morph="none" start_char="2019" end_char="2025">Lagarde</TOKEN>
<TOKEN id="token-22-37" pos="punct" morph="none" start_char="2026" end_char="2026">,</TOKEN>
<TOKEN id="token-22-38" pos="word" morph="none" start_char="2028" end_char="2032">según</TOKEN>
<TOKEN id="token-22-39" pos="word" morph="none" start_char="2034" end_char="2035">la</TOKEN>
<TOKEN id="token-22-40" pos="word" morph="none" start_char="2037" end_char="2040">cual</TOKEN>
<TOKEN id="token-22-41" pos="punct" morph="none" start_char="2042" end_char="2042">"</TOKEN>
<TOKEN id="token-22-42" pos="word" morph="none" start_char="2043" end_char="2045">los</TOKEN>
<TOKEN id="token-22-43" pos="word" morph="none" start_char="2047" end_char="2054">ancianos</TOKEN>
<TOKEN id="token-22-44" pos="word" morph="none" start_char="2056" end_char="2060">viven</TOKEN>
<TOKEN id="token-22-45" pos="word" morph="none" start_char="2062" end_char="2070">demasiado</TOKEN>
<TOKEN id="token-22-46" pos="word" morph="none" start_char="2072" end_char="2072">y</TOKEN>
<TOKEN id="token-22-47" pos="word" morph="none" start_char="2074" end_char="2076">eso</TOKEN>
<TOKEN id="token-22-48" pos="word" morph="none" start_char="2078" end_char="2079">es</TOKEN>
<TOKEN id="token-22-49" pos="word" morph="none" start_char="2081" end_char="2082">un</TOKEN>
<TOKEN id="token-22-50" pos="word" morph="none" start_char="2084" end_char="2089">riesgo</TOKEN>
<TOKEN id="token-22-51" pos="word" morph="none" start_char="2091" end_char="2094">para</TOKEN>
<TOKEN id="token-22-52" pos="word" morph="none" start_char="2096" end_char="2097">la</TOKEN>
<TOKEN id="token-22-53" pos="word" morph="none" start_char="2099" end_char="2106">economía</TOKEN>
<TOKEN id="token-22-54" pos="word" morph="none" start_char="2108" end_char="2113">global</TOKEN>
<TOKEN id="token-22-55" pos="punct" morph="none" start_char="2114" end_char="2115">".</TOKEN>
</SEG>
<SEG id="segment-23" start_char="2118" end_char="2324">
<ORIGINAL_TEXT>Por supuesto, no hay pruebas de que Lagarde haya realizado nunca semejantes declaraciones; de hecho, desde el Banco Central Europeo, organismo que ahora preside Lagarde, ya han desmentido esas informaciones.</ORIGINAL_TEXT>
<TOKEN id="token-23-0" pos="word" morph="none" start_char="2118" end_char="2120">Por</TOKEN>
<TOKEN id="token-23-1" pos="word" morph="none" start_char="2122" end_char="2129">supuesto</TOKEN>
<TOKEN id="token-23-2" pos="punct" morph="none" start_char="2130" end_char="2130">,</TOKEN>
<TOKEN id="token-23-3" pos="word" morph="none" start_char="2132" end_char="2133">no</TOKEN>
<TOKEN id="token-23-4" pos="word" morph="none" start_char="2135" end_char="2137">hay</TOKEN>
<TOKEN id="token-23-5" pos="word" morph="none" start_char="2139" end_char="2145">pruebas</TOKEN>
<TOKEN id="token-23-6" pos="word" morph="none" start_char="2147" end_char="2148">de</TOKEN>
<TOKEN id="token-23-7" pos="word" morph="none" start_char="2150" end_char="2152">que</TOKEN>
<TOKEN id="token-23-8" pos="word" morph="none" start_char="2154" end_char="2160">Lagarde</TOKEN>
<TOKEN id="token-23-9" pos="word" morph="none" start_char="2162" end_char="2165">haya</TOKEN>
<TOKEN id="token-23-10" pos="word" morph="none" start_char="2167" end_char="2175">realizado</TOKEN>
<TOKEN id="token-23-11" pos="word" morph="none" start_char="2177" end_char="2181">nunca</TOKEN>
<TOKEN id="token-23-12" pos="word" morph="none" start_char="2183" end_char="2192">semejantes</TOKEN>
<TOKEN id="token-23-13" pos="word" morph="none" start_char="2194" end_char="2206">declaraciones</TOKEN>
<TOKEN id="token-23-14" pos="punct" morph="none" start_char="2207" end_char="2207">;</TOKEN>
<TOKEN id="token-23-15" pos="word" morph="none" start_char="2209" end_char="2210">de</TOKEN>
<TOKEN id="token-23-16" pos="word" morph="none" start_char="2212" end_char="2216">hecho</TOKEN>
<TOKEN id="token-23-17" pos="punct" morph="none" start_char="2217" end_char="2217">,</TOKEN>
<TOKEN id="token-23-18" pos="word" morph="none" start_char="2219" end_char="2223">desde</TOKEN>
<TOKEN id="token-23-19" pos="word" morph="none" start_char="2225" end_char="2226">el</TOKEN>
<TOKEN id="token-23-20" pos="word" morph="none" start_char="2228" end_char="2232">Banco</TOKEN>
<TOKEN id="token-23-21" pos="word" morph="none" start_char="2234" end_char="2240">Central</TOKEN>
<TOKEN id="token-23-22" pos="word" morph="none" start_char="2242" end_char="2248">Europeo</TOKEN>
<TOKEN id="token-23-23" pos="punct" morph="none" start_char="2249" end_char="2249">,</TOKEN>
<TOKEN id="token-23-24" pos="word" morph="none" start_char="2251" end_char="2259">organismo</TOKEN>
<TOKEN id="token-23-25" pos="word" morph="none" start_char="2261" end_char="2263">que</TOKEN>
<TOKEN id="token-23-26" pos="word" morph="none" start_char="2265" end_char="2269">ahora</TOKEN>
<TOKEN id="token-23-27" pos="word" morph="none" start_char="2271" end_char="2277">preside</TOKEN>
<TOKEN id="token-23-28" pos="word" morph="none" start_char="2279" end_char="2285">Lagarde</TOKEN>
<TOKEN id="token-23-29" pos="punct" morph="none" start_char="2286" end_char="2286">,</TOKEN>
<TOKEN id="token-23-30" pos="word" morph="none" start_char="2288" end_char="2289">ya</TOKEN>
<TOKEN id="token-23-31" pos="word" morph="none" start_char="2291" end_char="2293">han</TOKEN>
<TOKEN id="token-23-32" pos="word" morph="none" start_char="2295" end_char="2304">desmentido</TOKEN>
<TOKEN id="token-23-33" pos="word" morph="none" start_char="2306" end_char="2309">esas</TOKEN>
<TOKEN id="token-23-34" pos="word" morph="none" start_char="2311" end_char="2323">informaciones</TOKEN>
<TOKEN id="token-23-35" pos="punct" morph="none" start_char="2324" end_char="2324">.</TOKEN>
</SEG>
<SEG id="segment-24" start_char="2326" end_char="2347">
<ORIGINAL_TEXT>Pero así funcionan las</ORIGINAL_TEXT>
<TOKEN id="token-24-0" pos="word" morph="none" start_char="2326" end_char="2329">Pero</TOKEN>
<TOKEN id="token-24-1" pos="word" morph="none" start_char="2331" end_char="2333">así</TOKEN>
<TOKEN id="token-24-2" pos="word" morph="none" start_char="2335" end_char="2343">funcionan</TOKEN>
<TOKEN id="token-24-3" pos="word" morph="none" start_char="2345" end_char="2347">las</TOKEN>
</SEG>
<SEG id="segment-25" start_char="2350" end_char="2358">
<ORIGINAL_TEXT>fake news</ORIGINAL_TEXT>
<TOKEN id="token-25-0" pos="word" morph="none" start_char="2350" end_char="2353">fake</TOKEN>
<TOKEN id="token-25-1" pos="word" morph="none" start_char="2355" end_char="2358">news</TOKEN>
</SEG>
<SEG id="segment-26" start_char="2361" end_char="2487">
<ORIGINAL_TEXT>, dando apariencia de veracidad a expresiones que nunca se han pronunciado o que no las ha pronunciado esa persona en cuestión.</ORIGINAL_TEXT>
<TOKEN id="token-26-0" pos="punct" morph="none" start_char="2361" end_char="2361">,</TOKEN>
<TOKEN id="token-26-1" pos="word" morph="none" start_char="2363" end_char="2367">dando</TOKEN>
<TOKEN id="token-26-2" pos="word" morph="none" start_char="2369" end_char="2378">apariencia</TOKEN>
<TOKEN id="token-26-3" pos="word" morph="none" start_char="2380" end_char="2381">de</TOKEN>
<TOKEN id="token-26-4" pos="word" morph="none" start_char="2383" end_char="2391">veracidad</TOKEN>
<TOKEN id="token-26-5" pos="word" morph="none" start_char="2393" end_char="2393">a</TOKEN>
<TOKEN id="token-26-6" pos="word" morph="none" start_char="2395" end_char="2405">expresiones</TOKEN>
<TOKEN id="token-26-7" pos="word" morph="none" start_char="2407" end_char="2409">que</TOKEN>
<TOKEN id="token-26-8" pos="word" morph="none" start_char="2411" end_char="2415">nunca</TOKEN>
<TOKEN id="token-26-9" pos="word" morph="none" start_char="2417" end_char="2418">se</TOKEN>
<TOKEN id="token-26-10" pos="word" morph="none" start_char="2420" end_char="2422">han</TOKEN>
<TOKEN id="token-26-11" pos="word" morph="none" start_char="2424" end_char="2434">pronunciado</TOKEN>
<TOKEN id="token-26-12" pos="word" morph="none" start_char="2436" end_char="2436">o</TOKEN>
<TOKEN id="token-26-13" pos="word" morph="none" start_char="2438" end_char="2440">que</TOKEN>
<TOKEN id="token-26-14" pos="word" morph="none" start_char="2442" end_char="2443">no</TOKEN>
<TOKEN id="token-26-15" pos="word" morph="none" start_char="2445" end_char="2447">las</TOKEN>
<TOKEN id="token-26-16" pos="word" morph="none" start_char="2449" end_char="2450">ha</TOKEN>
<TOKEN id="token-26-17" pos="word" morph="none" start_char="2452" end_char="2462">pronunciado</TOKEN>
<TOKEN id="token-26-18" pos="word" morph="none" start_char="2464" end_char="2466">esa</TOKEN>
<TOKEN id="token-26-19" pos="word" morph="none" start_char="2468" end_char="2474">persona</TOKEN>
<TOKEN id="token-26-20" pos="word" morph="none" start_char="2476" end_char="2477">en</TOKEN>
<TOKEN id="token-26-21" pos="word" morph="none" start_char="2479" end_char="2486">cuestión</TOKEN>
<TOKEN id="token-26-22" pos="punct" morph="none" start_char="2487" end_char="2487">.</TOKEN>
</SEG>
<SEG id="segment-27" start_char="2489" end_char="2514">
<ORIGINAL_TEXT>¡Otro bulo desenmascarado!</ORIGINAL_TEXT>
<TOKEN id="token-27-0" pos="punct" morph="none" start_char="2489" end_char="2489">¡</TOKEN>
<TOKEN id="token-27-1" pos="word" morph="none" start_char="2490" end_char="2493">Otro</TOKEN>
<TOKEN id="token-27-2" pos="word" morph="none" start_char="2495" end_char="2498">bulo</TOKEN>
<TOKEN id="token-27-3" pos="word" morph="none" start_char="2500" end_char="2513">desenmascarado</TOKEN>
<TOKEN id="token-27-4" pos="punct" morph="none" start_char="2514" end_char="2514">!</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
