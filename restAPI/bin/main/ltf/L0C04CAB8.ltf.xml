<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE LCTL_TEXT SYSTEM "ltf.v1.5.dtd">
<LCTL_TEXT lang="spa">
<DOC id="L0C04CAB8" lang="spa" tokenization="tokenization_parameters.v5.0" grammar="none" raw_text_char_length="1306" raw_text_md5="6a0a153a27093bd3de6c460903331d04">
<TEXT>
<SEG id="segment-0" start_char="1" end_char="119">
<ORIGINAL_TEXT>Analysis of hospital traffic and search engine data in Wuhan China indicates early disease activity in the Fall of 2019</ORIGINAL_TEXT>
<TOKEN id="token-0-0" pos="word" morph="none" start_char="1" end_char="8">Analysis</TOKEN>
<TOKEN id="token-0-1" pos="word" morph="none" start_char="10" end_char="11">of</TOKEN>
<TOKEN id="token-0-2" pos="word" morph="none" start_char="13" end_char="20">hospital</TOKEN>
<TOKEN id="token-0-3" pos="word" morph="none" start_char="22" end_char="28">traffic</TOKEN>
<TOKEN id="token-0-4" pos="word" morph="none" start_char="30" end_char="32">and</TOKEN>
<TOKEN id="token-0-5" pos="word" morph="none" start_char="34" end_char="39">search</TOKEN>
<TOKEN id="token-0-6" pos="word" morph="none" start_char="41" end_char="46">engine</TOKEN>
<TOKEN id="token-0-7" pos="word" morph="none" start_char="48" end_char="51">data</TOKEN>
<TOKEN id="token-0-8" pos="word" morph="none" start_char="53" end_char="54">in</TOKEN>
<TOKEN id="token-0-9" pos="word" morph="none" start_char="56" end_char="60">Wuhan</TOKEN>
<TOKEN id="token-0-10" pos="word" morph="none" start_char="62" end_char="66">China</TOKEN>
<TOKEN id="token-0-11" pos="word" morph="none" start_char="68" end_char="76">indicates</TOKEN>
<TOKEN id="token-0-12" pos="word" morph="none" start_char="78" end_char="82">early</TOKEN>
<TOKEN id="token-0-13" pos="word" morph="none" start_char="84" end_char="90">disease</TOKEN>
<TOKEN id="token-0-14" pos="word" morph="none" start_char="92" end_char="99">activity</TOKEN>
<TOKEN id="token-0-15" pos="word" morph="none" start_char="101" end_char="102">in</TOKEN>
<TOKEN id="token-0-16" pos="word" morph="none" start_char="104" end_char="106">the</TOKEN>
<TOKEN id="token-0-17" pos="word" morph="none" start_char="108" end_char="111">Fall</TOKEN>
<TOKEN id="token-0-18" pos="word" morph="none" start_char="113" end_char="114">of</TOKEN>
<TOKEN id="token-0-19" pos="word" morph="none" start_char="116" end_char="119">2019</TOKEN>
</SEG>
<SEG id="segment-1" start_char="124" end_char="222">
<ORIGINAL_TEXT>Nsoesie, Elaine Okanyene, Benjamin Rader, Yiyao L. Barnoon, Lauren Goodwin, and John S. Brownstein.</ORIGINAL_TEXT>
<TOKEN id="token-1-0" pos="word" morph="none" start_char="124" end_char="130">Nsoesie</TOKEN>
<TOKEN id="token-1-1" pos="punct" morph="none" start_char="131" end_char="131">,</TOKEN>
<TOKEN id="token-1-2" pos="word" morph="none" start_char="133" end_char="138">Elaine</TOKEN>
<TOKEN id="token-1-3" pos="word" morph="none" start_char="140" end_char="147">Okanyene</TOKEN>
<TOKEN id="token-1-4" pos="punct" morph="none" start_char="148" end_char="148">,</TOKEN>
<TOKEN id="token-1-5" pos="word" morph="none" start_char="150" end_char="157">Benjamin</TOKEN>
<TOKEN id="token-1-6" pos="word" morph="none" start_char="159" end_char="163">Rader</TOKEN>
<TOKEN id="token-1-7" pos="punct" morph="none" start_char="164" end_char="164">,</TOKEN>
<TOKEN id="token-1-8" pos="word" morph="none" start_char="166" end_char="170">Yiyao</TOKEN>
<TOKEN id="token-1-9" pos="word" morph="none" start_char="172" end_char="172">L</TOKEN>
<TOKEN id="token-1-10" pos="punct" morph="none" start_char="173" end_char="173">.</TOKEN>
<TOKEN id="token-1-11" pos="word" morph="none" start_char="175" end_char="181">Barnoon</TOKEN>
<TOKEN id="token-1-12" pos="punct" morph="none" start_char="182" end_char="182">,</TOKEN>
<TOKEN id="token-1-13" pos="word" morph="none" start_char="184" end_char="189">Lauren</TOKEN>
<TOKEN id="token-1-14" pos="word" morph="none" start_char="191" end_char="197">Goodwin</TOKEN>
<TOKEN id="token-1-15" pos="punct" morph="none" start_char="198" end_char="198">,</TOKEN>
<TOKEN id="token-1-16" pos="word" morph="none" start_char="200" end_char="202">and</TOKEN>
<TOKEN id="token-1-17" pos="word" morph="none" start_char="204" end_char="207">John</TOKEN>
<TOKEN id="token-1-18" pos="word" morph="none" start_char="209" end_char="209">S</TOKEN>
<TOKEN id="token-1-19" pos="punct" morph="none" start_char="210" end_char="210">.</TOKEN>
<TOKEN id="token-1-20" pos="word" morph="none" start_char="212" end_char="221">Brownstein</TOKEN>
<TOKEN id="token-1-21" pos="punct" morph="none" start_char="222" end_char="222">.</TOKEN>
</SEG>
<SEG id="segment-2" start_char="224" end_char="350">
<ORIGINAL_TEXT>Analysis of hospital traffic and search engine data in Wuhan China indicates early disease activity in the Fall of 2019 (2020).</ORIGINAL_TEXT>
<TOKEN id="token-2-0" pos="word" morph="none" start_char="224" end_char="231">Analysis</TOKEN>
<TOKEN id="token-2-1" pos="word" morph="none" start_char="233" end_char="234">of</TOKEN>
<TOKEN id="token-2-2" pos="word" morph="none" start_char="236" end_char="243">hospital</TOKEN>
<TOKEN id="token-2-3" pos="word" morph="none" start_char="245" end_char="251">traffic</TOKEN>
<TOKEN id="token-2-4" pos="word" morph="none" start_char="253" end_char="255">and</TOKEN>
<TOKEN id="token-2-5" pos="word" morph="none" start_char="257" end_char="262">search</TOKEN>
<TOKEN id="token-2-6" pos="word" morph="none" start_char="264" end_char="269">engine</TOKEN>
<TOKEN id="token-2-7" pos="word" morph="none" start_char="271" end_char="274">data</TOKEN>
<TOKEN id="token-2-8" pos="word" morph="none" start_char="276" end_char="277">in</TOKEN>
<TOKEN id="token-2-9" pos="word" morph="none" start_char="279" end_char="283">Wuhan</TOKEN>
<TOKEN id="token-2-10" pos="word" morph="none" start_char="285" end_char="289">China</TOKEN>
<TOKEN id="token-2-11" pos="word" morph="none" start_char="291" end_char="299">indicates</TOKEN>
<TOKEN id="token-2-12" pos="word" morph="none" start_char="301" end_char="305">early</TOKEN>
<TOKEN id="token-2-13" pos="word" morph="none" start_char="307" end_char="313">disease</TOKEN>
<TOKEN id="token-2-14" pos="word" morph="none" start_char="315" end_char="322">activity</TOKEN>
<TOKEN id="token-2-15" pos="word" morph="none" start_char="324" end_char="325">in</TOKEN>
<TOKEN id="token-2-16" pos="word" morph="none" start_char="327" end_char="329">the</TOKEN>
<TOKEN id="token-2-17" pos="word" morph="none" start_char="331" end_char="334">Fall</TOKEN>
<TOKEN id="token-2-18" pos="word" morph="none" start_char="336" end_char="337">of</TOKEN>
<TOKEN id="token-2-19" pos="word" morph="none" start_char="339" end_char="342">2019</TOKEN>
<TOKEN id="token-2-20" pos="punct" morph="none" start_char="344" end_char="344">(</TOKEN>
<TOKEN id="token-2-21" pos="word" morph="none" start_char="345" end_char="348">2020</TOKEN>
<TOKEN id="token-2-22" pos="punct" morph="none" start_char="349" end_char="350">).</TOKEN>
</SEG>
<SEG id="segment-3" start_char="353" end_char="498">
<ORIGINAL_TEXT>The global COVID-19 pandemic was originally linked to a zoonotic spillover event in Wuhan’s Huanan Seafood Market in November or December of 2019.</ORIGINAL_TEXT>
<TOKEN id="token-3-0" pos="word" morph="none" start_char="353" end_char="355">The</TOKEN>
<TOKEN id="token-3-1" pos="word" morph="none" start_char="357" end_char="362">global</TOKEN>
<TOKEN id="token-3-2" pos="unknown" morph="none" start_char="364" end_char="371">COVID-19</TOKEN>
<TOKEN id="token-3-3" pos="word" morph="none" start_char="373" end_char="380">pandemic</TOKEN>
<TOKEN id="token-3-4" pos="word" morph="none" start_char="382" end_char="384">was</TOKEN>
<TOKEN id="token-3-5" pos="word" morph="none" start_char="386" end_char="395">originally</TOKEN>
<TOKEN id="token-3-6" pos="word" morph="none" start_char="397" end_char="402">linked</TOKEN>
<TOKEN id="token-3-7" pos="word" morph="none" start_char="404" end_char="405">to</TOKEN>
<TOKEN id="token-3-8" pos="word" morph="none" start_char="407" end_char="407">a</TOKEN>
<TOKEN id="token-3-9" pos="word" morph="none" start_char="409" end_char="416">zoonotic</TOKEN>
<TOKEN id="token-3-10" pos="word" morph="none" start_char="418" end_char="426">spillover</TOKEN>
<TOKEN id="token-3-11" pos="word" morph="none" start_char="428" end_char="432">event</TOKEN>
<TOKEN id="token-3-12" pos="word" morph="none" start_char="434" end_char="435">in</TOKEN>
<TOKEN id="token-3-13" pos="word" morph="none" start_char="437" end_char="443">Wuhan’s</TOKEN>
<TOKEN id="token-3-14" pos="word" morph="none" start_char="445" end_char="450">Huanan</TOKEN>
<TOKEN id="token-3-15" pos="word" morph="none" start_char="452" end_char="458">Seafood</TOKEN>
<TOKEN id="token-3-16" pos="word" morph="none" start_char="460" end_char="465">Market</TOKEN>
<TOKEN id="token-3-17" pos="word" morph="none" start_char="467" end_char="468">in</TOKEN>
<TOKEN id="token-3-18" pos="word" morph="none" start_char="470" end_char="477">November</TOKEN>
<TOKEN id="token-3-19" pos="word" morph="none" start_char="479" end_char="480">or</TOKEN>
<TOKEN id="token-3-20" pos="word" morph="none" start_char="482" end_char="489">December</TOKEN>
<TOKEN id="token-3-21" pos="word" morph="none" start_char="491" end_char="492">of</TOKEN>
<TOKEN id="token-3-22" pos="word" morph="none" start_char="494" end_char="497">2019</TOKEN>
<TOKEN id="token-3-23" pos="punct" morph="none" start_char="498" end_char="498">.</TOKEN>
</SEG>
<SEG id="segment-4" start_char="500" end_char="610">
<ORIGINAL_TEXT>However, recent evidence suggests that the virus may have already been circulating at the time of the outbreak.</ORIGINAL_TEXT>
<TOKEN id="token-4-0" pos="word" morph="none" start_char="500" end_char="506">However</TOKEN>
<TOKEN id="token-4-1" pos="punct" morph="none" start_char="507" end_char="507">,</TOKEN>
<TOKEN id="token-4-2" pos="word" morph="none" start_char="509" end_char="514">recent</TOKEN>
<TOKEN id="token-4-3" pos="word" morph="none" start_char="516" end_char="523">evidence</TOKEN>
<TOKEN id="token-4-4" pos="word" morph="none" start_char="525" end_char="532">suggests</TOKEN>
<TOKEN id="token-4-5" pos="word" morph="none" start_char="534" end_char="537">that</TOKEN>
<TOKEN id="token-4-6" pos="word" morph="none" start_char="539" end_char="541">the</TOKEN>
<TOKEN id="token-4-7" pos="word" morph="none" start_char="543" end_char="547">virus</TOKEN>
<TOKEN id="token-4-8" pos="word" morph="none" start_char="549" end_char="551">may</TOKEN>
<TOKEN id="token-4-9" pos="word" morph="none" start_char="553" end_char="556">have</TOKEN>
<TOKEN id="token-4-10" pos="word" morph="none" start_char="558" end_char="564">already</TOKEN>
<TOKEN id="token-4-11" pos="word" morph="none" start_char="566" end_char="569">been</TOKEN>
<TOKEN id="token-4-12" pos="word" morph="none" start_char="571" end_char="581">circulating</TOKEN>
<TOKEN id="token-4-13" pos="word" morph="none" start_char="583" end_char="584">at</TOKEN>
<TOKEN id="token-4-14" pos="word" morph="none" start_char="586" end_char="588">the</TOKEN>
<TOKEN id="token-4-15" pos="word" morph="none" start_char="590" end_char="593">time</TOKEN>
<TOKEN id="token-4-16" pos="word" morph="none" start_char="595" end_char="596">of</TOKEN>
<TOKEN id="token-4-17" pos="word" morph="none" start_char="598" end_char="600">the</TOKEN>
<TOKEN id="token-4-18" pos="word" morph="none" start_char="602" end_char="609">outbreak</TOKEN>
<TOKEN id="token-4-19" pos="punct" morph="none" start_char="610" end_char="610">.</TOKEN>
</SEG>
<SEG id="segment-5" start_char="612" end_char="786">
<ORIGINAL_TEXT>Here we use previously validated data streams - satellite imagery of hospital parking lots and Baidu search queries of disease related terms - to investigate this possibility.</ORIGINAL_TEXT>
<TOKEN id="token-5-0" pos="word" morph="none" start_char="612" end_char="615">Here</TOKEN>
<TOKEN id="token-5-1" pos="word" morph="none" start_char="617" end_char="618">we</TOKEN>
<TOKEN id="token-5-2" pos="word" morph="none" start_char="620" end_char="622">use</TOKEN>
<TOKEN id="token-5-3" pos="word" morph="none" start_char="624" end_char="633">previously</TOKEN>
<TOKEN id="token-5-4" pos="word" morph="none" start_char="635" end_char="643">validated</TOKEN>
<TOKEN id="token-5-5" pos="word" morph="none" start_char="645" end_char="648">data</TOKEN>
<TOKEN id="token-5-6" pos="word" morph="none" start_char="650" end_char="656">streams</TOKEN>
<TOKEN id="token-5-7" pos="punct" morph="none" start_char="658" end_char="658">-</TOKEN>
<TOKEN id="token-5-8" pos="word" morph="none" start_char="660" end_char="668">satellite</TOKEN>
<TOKEN id="token-5-9" pos="word" morph="none" start_char="670" end_char="676">imagery</TOKEN>
<TOKEN id="token-5-10" pos="word" morph="none" start_char="678" end_char="679">of</TOKEN>
<TOKEN id="token-5-11" pos="word" morph="none" start_char="681" end_char="688">hospital</TOKEN>
<TOKEN id="token-5-12" pos="word" morph="none" start_char="690" end_char="696">parking</TOKEN>
<TOKEN id="token-5-13" pos="word" morph="none" start_char="698" end_char="701">lots</TOKEN>
<TOKEN id="token-5-14" pos="word" morph="none" start_char="703" end_char="705">and</TOKEN>
<TOKEN id="token-5-15" pos="word" morph="none" start_char="707" end_char="711">Baidu</TOKEN>
<TOKEN id="token-5-16" pos="word" morph="none" start_char="713" end_char="718">search</TOKEN>
<TOKEN id="token-5-17" pos="word" morph="none" start_char="720" end_char="726">queries</TOKEN>
<TOKEN id="token-5-18" pos="word" morph="none" start_char="728" end_char="729">of</TOKEN>
<TOKEN id="token-5-19" pos="word" morph="none" start_char="731" end_char="737">disease</TOKEN>
<TOKEN id="token-5-20" pos="word" morph="none" start_char="739" end_char="745">related</TOKEN>
<TOKEN id="token-5-21" pos="word" morph="none" start_char="747" end_char="751">terms</TOKEN>
<TOKEN id="token-5-22" pos="punct" morph="none" start_char="753" end_char="753">-</TOKEN>
<TOKEN id="token-5-23" pos="word" morph="none" start_char="755" end_char="756">to</TOKEN>
<TOKEN id="token-5-24" pos="word" morph="none" start_char="758" end_char="768">investigate</TOKEN>
<TOKEN id="token-5-25" pos="word" morph="none" start_char="770" end_char="773">this</TOKEN>
<TOKEN id="token-5-26" pos="word" morph="none" start_char="775" end_char="785">possibility</TOKEN>
<TOKEN id="token-5-27" pos="punct" morph="none" start_char="786" end_char="786">.</TOKEN>
</SEG>
<SEG id="segment-6" start_char="788" end_char="897">
<ORIGINAL_TEXT>We observe an upward trend in hospital traffic and search volume beginning in late Summer and early Fall 2019.</ORIGINAL_TEXT>
<TOKEN id="token-6-0" pos="word" morph="none" start_char="788" end_char="789">We</TOKEN>
<TOKEN id="token-6-1" pos="word" morph="none" start_char="791" end_char="797">observe</TOKEN>
<TOKEN id="token-6-2" pos="word" morph="none" start_char="799" end_char="800">an</TOKEN>
<TOKEN id="token-6-3" pos="word" morph="none" start_char="802" end_char="807">upward</TOKEN>
<TOKEN id="token-6-4" pos="word" morph="none" start_char="809" end_char="813">trend</TOKEN>
<TOKEN id="token-6-5" pos="word" morph="none" start_char="815" end_char="816">in</TOKEN>
<TOKEN id="token-6-6" pos="word" morph="none" start_char="818" end_char="825">hospital</TOKEN>
<TOKEN id="token-6-7" pos="word" morph="none" start_char="827" end_char="833">traffic</TOKEN>
<TOKEN id="token-6-8" pos="word" morph="none" start_char="835" end_char="837">and</TOKEN>
<TOKEN id="token-6-9" pos="word" morph="none" start_char="839" end_char="844">search</TOKEN>
<TOKEN id="token-6-10" pos="word" morph="none" start_char="846" end_char="851">volume</TOKEN>
<TOKEN id="token-6-11" pos="word" morph="none" start_char="853" end_char="861">beginning</TOKEN>
<TOKEN id="token-6-12" pos="word" morph="none" start_char="863" end_char="864">in</TOKEN>
<TOKEN id="token-6-13" pos="word" morph="none" start_char="866" end_char="869">late</TOKEN>
<TOKEN id="token-6-14" pos="word" morph="none" start_char="871" end_char="876">Summer</TOKEN>
<TOKEN id="token-6-15" pos="word" morph="none" start_char="878" end_char="880">and</TOKEN>
<TOKEN id="token-6-16" pos="word" morph="none" start_char="882" end_char="886">early</TOKEN>
<TOKEN id="token-6-17" pos="word" morph="none" start_char="888" end_char="891">Fall</TOKEN>
<TOKEN id="token-6-18" pos="word" morph="none" start_char="893" end_char="896">2019</TOKEN>
<TOKEN id="token-6-19" pos="punct" morph="none" start_char="897" end_char="897">.</TOKEN>
</SEG>
<SEG id="segment-7" start_char="899" end_char="1119">
<ORIGINAL_TEXT>While queries of the respiratory symptom "cough" show seasonal fluctuations coinciding with yearly influenza seasons, "diarrhea" is a more COVID-19 specific symptom and only shows an association with the current epidemic.</ORIGINAL_TEXT>
<TOKEN id="token-7-0" pos="word" morph="none" start_char="899" end_char="903">While</TOKEN>
<TOKEN id="token-7-1" pos="word" morph="none" start_char="905" end_char="911">queries</TOKEN>
<TOKEN id="token-7-2" pos="word" morph="none" start_char="913" end_char="914">of</TOKEN>
<TOKEN id="token-7-3" pos="word" morph="none" start_char="916" end_char="918">the</TOKEN>
<TOKEN id="token-7-4" pos="word" morph="none" start_char="920" end_char="930">respiratory</TOKEN>
<TOKEN id="token-7-5" pos="word" morph="none" start_char="932" end_char="938">symptom</TOKEN>
<TOKEN id="token-7-6" pos="punct" morph="none" start_char="940" end_char="940">"</TOKEN>
<TOKEN id="token-7-7" pos="word" morph="none" start_char="941" end_char="945">cough</TOKEN>
<TOKEN id="token-7-8" pos="punct" morph="none" start_char="946" end_char="946">"</TOKEN>
<TOKEN id="token-7-9" pos="word" morph="none" start_char="948" end_char="951">show</TOKEN>
<TOKEN id="token-7-10" pos="word" morph="none" start_char="953" end_char="960">seasonal</TOKEN>
<TOKEN id="token-7-11" pos="word" morph="none" start_char="962" end_char="973">fluctuations</TOKEN>
<TOKEN id="token-7-12" pos="word" morph="none" start_char="975" end_char="984">coinciding</TOKEN>
<TOKEN id="token-7-13" pos="word" morph="none" start_char="986" end_char="989">with</TOKEN>
<TOKEN id="token-7-14" pos="word" morph="none" start_char="991" end_char="996">yearly</TOKEN>
<TOKEN id="token-7-15" pos="word" morph="none" start_char="998" end_char="1006">influenza</TOKEN>
<TOKEN id="token-7-16" pos="word" morph="none" start_char="1008" end_char="1014">seasons</TOKEN>
<TOKEN id="token-7-17" pos="punct" morph="none" start_char="1015" end_char="1015">,</TOKEN>
<TOKEN id="token-7-18" pos="punct" morph="none" start_char="1017" end_char="1017">"</TOKEN>
<TOKEN id="token-7-19" pos="word" morph="none" start_char="1018" end_char="1025">diarrhea</TOKEN>
<TOKEN id="token-7-20" pos="punct" morph="none" start_char="1026" end_char="1026">"</TOKEN>
<TOKEN id="token-7-21" pos="word" morph="none" start_char="1028" end_char="1029">is</TOKEN>
<TOKEN id="token-7-22" pos="word" morph="none" start_char="1031" end_char="1031">a</TOKEN>
<TOKEN id="token-7-23" pos="word" morph="none" start_char="1033" end_char="1036">more</TOKEN>
<TOKEN id="token-7-24" pos="unknown" morph="none" start_char="1038" end_char="1045">COVID-19</TOKEN>
<TOKEN id="token-7-25" pos="word" morph="none" start_char="1047" end_char="1054">specific</TOKEN>
<TOKEN id="token-7-26" pos="word" morph="none" start_char="1056" end_char="1062">symptom</TOKEN>
<TOKEN id="token-7-27" pos="word" morph="none" start_char="1064" end_char="1066">and</TOKEN>
<TOKEN id="token-7-28" pos="word" morph="none" start_char="1068" end_char="1071">only</TOKEN>
<TOKEN id="token-7-29" pos="word" morph="none" start_char="1073" end_char="1077">shows</TOKEN>
<TOKEN id="token-7-30" pos="word" morph="none" start_char="1079" end_char="1080">an</TOKEN>
<TOKEN id="token-7-31" pos="word" morph="none" start_char="1082" end_char="1092">association</TOKEN>
<TOKEN id="token-7-32" pos="word" morph="none" start_char="1094" end_char="1097">with</TOKEN>
<TOKEN id="token-7-33" pos="word" morph="none" start_char="1099" end_char="1101">the</TOKEN>
<TOKEN id="token-7-34" pos="word" morph="none" start_char="1103" end_char="1109">current</TOKEN>
<TOKEN id="token-7-35" pos="word" morph="none" start_char="1111" end_char="1118">epidemic</TOKEN>
<TOKEN id="token-7-36" pos="punct" morph="none" start_char="1119" end_char="1119">.</TOKEN>
</SEG>
<SEG id="segment-8" start_char="1121" end_char="1302">
<ORIGINAL_TEXT>The increase of both signals precede the documented start of the COVID-19 pandemic in December, highlighting the value of novel digital sources for surveillance of emerging pathogens</ORIGINAL_TEXT>
<TOKEN id="token-8-0" pos="word" morph="none" start_char="1121" end_char="1123">The</TOKEN>
<TOKEN id="token-8-1" pos="word" morph="none" start_char="1125" end_char="1132">increase</TOKEN>
<TOKEN id="token-8-2" pos="word" morph="none" start_char="1134" end_char="1135">of</TOKEN>
<TOKEN id="token-8-3" pos="word" morph="none" start_char="1137" end_char="1140">both</TOKEN>
<TOKEN id="token-8-4" pos="word" morph="none" start_char="1142" end_char="1148">signals</TOKEN>
<TOKEN id="token-8-5" pos="word" morph="none" start_char="1150" end_char="1156">precede</TOKEN>
<TOKEN id="token-8-6" pos="word" morph="none" start_char="1158" end_char="1160">the</TOKEN>
<TOKEN id="token-8-7" pos="word" morph="none" start_char="1162" end_char="1171">documented</TOKEN>
<TOKEN id="token-8-8" pos="word" morph="none" start_char="1173" end_char="1177">start</TOKEN>
<TOKEN id="token-8-9" pos="word" morph="none" start_char="1179" end_char="1180">of</TOKEN>
<TOKEN id="token-8-10" pos="word" morph="none" start_char="1182" end_char="1184">the</TOKEN>
<TOKEN id="token-8-11" pos="unknown" morph="none" start_char="1186" end_char="1193">COVID-19</TOKEN>
<TOKEN id="token-8-12" pos="word" morph="none" start_char="1195" end_char="1202">pandemic</TOKEN>
<TOKEN id="token-8-13" pos="word" morph="none" start_char="1204" end_char="1205">in</TOKEN>
<TOKEN id="token-8-14" pos="word" morph="none" start_char="1207" end_char="1214">December</TOKEN>
<TOKEN id="token-8-15" pos="punct" morph="none" start_char="1215" end_char="1215">,</TOKEN>
<TOKEN id="token-8-16" pos="word" morph="none" start_char="1217" end_char="1228">highlighting</TOKEN>
<TOKEN id="token-8-17" pos="word" morph="none" start_char="1230" end_char="1232">the</TOKEN>
<TOKEN id="token-8-18" pos="word" morph="none" start_char="1234" end_char="1238">value</TOKEN>
<TOKEN id="token-8-19" pos="word" morph="none" start_char="1240" end_char="1241">of</TOKEN>
<TOKEN id="token-8-20" pos="word" morph="none" start_char="1243" end_char="1247">novel</TOKEN>
<TOKEN id="token-8-21" pos="word" morph="none" start_char="1249" end_char="1255">digital</TOKEN>
<TOKEN id="token-8-22" pos="word" morph="none" start_char="1257" end_char="1263">sources</TOKEN>
<TOKEN id="token-8-23" pos="word" morph="none" start_char="1265" end_char="1267">for</TOKEN>
<TOKEN id="token-8-24" pos="word" morph="none" start_char="1269" end_char="1280">surveillance</TOKEN>
<TOKEN id="token-8-25" pos="word" morph="none" start_char="1282" end_char="1283">of</TOKEN>
<TOKEN id="token-8-26" pos="word" morph="none" start_char="1285" end_char="1292">emerging</TOKEN>
<TOKEN id="token-8-27" pos="word" morph="none" start_char="1294" end_char="1302">pathogens</TOKEN>
</SEG>
</TEXT>
</DOC>
</LCTL_TEXT>
